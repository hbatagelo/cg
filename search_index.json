[["index.html", "MCTA008-17 Computação Gráfica Apresentação", " MCTA008-17 Computação Gráfica Harlen Batagelo (harlen.batagelo@ufabc.edu.br) Bruno Marques (bruno.marques@ufabc.edu.br) Universidade Federal do ABC 3º quadrimestre de 2021 Apresentação Caro/a estudante, Este site contém as notas de aula da disciplina MCTA008-17 Computação Gráfica adaptada ao ensino remoto do 3º quadrimestre de 2021. O conteúdo é organizado no formato de um livro voltado ao estudo autodirigido. Os capítulos estão divididos em tópicos teóricos sobre o processo de geração de imagens no computador, e partes práticas de desenvolvimento de aplicações gráficas interativas. Cada capítulo corresponde aproximadamente ao conteúdo de uma semana de aula. Para informações sobre o cronograma das atividades e critérios de avaliação, consulte o plano de ensino disponível no Moodle. Bons estudos!  Harlen e Bruno "],["pré-requisitos.html", "Pré-requisitos", " Pré-requisitos Para acompanhar o curso de forma satisfatória é recomendável ter conhecimento prévio do conteúdo abordado nas disciplinas de Algoritmos e Estruturas de Dados I e Geometria Analítica. As atividades práticas avaliativas serão desenvolvidas na linguagem C++. Embora não seja necessário ter fluência em C++, é recomendável ter proficiência em programação em C e familiaridade com conceitos básicos de programação orientada a objetos. Também é recomendável ter familiaridade com o Git e ser capaz de gerenciar seus próprios repositórios. Atividades práticas Para realizar as atividades práticas é necessário ter um computador com sistema operacional 64 bits (Windows, Linux ou macOS) e placa de vídeo compatível com OpenGL 4.1 ou superior. O OpenGL 4.1 é suportado em placas gráficas da família Nvidia GeForce 400 (2010) ou mais recentes, AMD Radeon HD 5000 (2009) em diante e Intel HD Graphics a partir dos processadores Intel de 7ª geração (2012). Caso a sua placa de vídeo seja de uma geração a partir de 2012, provavelmente ela deve suportar OpenGL 4.1. Se não suportar, há a possibilidade de simular o processamento gráfico em software através do driver Gallium llvmpipe da biblioteca Mesa. Visualizando este site Parte do conteúdo deste site requer um navegador com suporte a WebGL 2.0. Para informações detalhadas sobre o suporte do seu navegador a WebGL 2.0, consulte o WebGL Report. Dica Para garantir a visualização correta do conteúdo WebGL 2.0, utilize a versão mais recente do Mozilla FireFox ou Google Chrome. Além disso, use o navegador em um computador desktop ou laptop. Embora o site funcione em tablets e smartphones, pode ser difícil interagir com o conteúdo WebGL nesses dispositivos. Dependendo das configurações de DPI utilizadas no sistema de janelas, podem ocorrer problemas de redimensionamento dos elementos de interface no Chrome e em navegadores baseados no Chromium, como o Microsoft Edge. Por exemplo, o cubo exibido acima pode ser redimensionado e as arestas podem apresentar distorções, parecendo mais serrilhadas que o normal: No Chrome, isso geralmente é resolvido iniciando o navegador com a opção /high-dpi-support=1 /force-device-scale-factor=1 na linha de comando (ou incluindo essas opções no atalho), ou ajustando o zoom . Importante No momento da escrita deste texto, o Apple Safari não possui suporte a WebGL 2.0. Entretanto, o Safari Technology Preview tem o WebGL 2.0 habilitado por padrão e está disponível a partir do macOS Catalina. Consulte em https://caniuse.com/webgl2 o suporte a WebGL 2.0 em diferentes navegadores. "],["config.html", "1 Configuração do ambiente", " 1 Configuração do ambiente Neste capítulo veremos como configurar o ambiente de desenvolvimento para realizar as atividades práticas no computador. Qualquer que seja a plataforma  Linux, macOS ou Windows  é necessário instalar as seguintes ferramentas e bibliotecas: CMake: para automatizar a geração de scripts de compilação e ligação de forma independente de plataforma; Emscripten: para compilar código C++ e gerar binário em WebAssembly de modo a executar nossas aplicações no navegador; Git: para clonar o repositório do SDK do Emscripten e da biblioteca de desenvolvimento que usaremos na disciplina, e para o controle de versão das atividades; GLEW: para carregamento das funções da API gráfica OpenGL; Simple DirectMedia Layer (SDL) 2.0: para gerenciamento de dispositivos de vídeo, dispositivos de entrada, áudio, entre outros componentes de hardware. SDL_image 2.0: para leitura de arquivos de imagem. Precisamos também usar um compilador recente com suporte a C++17 e suporte pelo menos parcial a C++20, como o GCC 10 ou Clang 11. Acompanhe nas seções a seguir o passo a passo da instalação desses recursos de acordo com o sistema operacional utilizado: Seção 1.1 para instalação no Linux; Seção 1.2 para instalação no macOS; Seção 1.3 para instalar no Windows. Não é necessário usar um IDE ou editor específico de código-fonte para o desenvolvimento das atividades. A compilação pode ser disparada através de scripts de linha de comando. Entretanto, como um exemplo, veremos na seção 1.4 como fazer a configuração básica do Visual Studio Code para o desenvolvimento de aplicações C++ com CMake. Na seção 1.5 veremos como instalar uma biblioteca auxiliar (a ABCg) criada especialmente para esta disciplina. Ela será utilizada em todas as atividades do curso para facilitar o desenvolvimento das aplicações gráficas. Dica Caso o seu computador tenha recursos de processamento e memória suficientes, é possível configurar todo o ambiente de desenvolvimento em um sistema operacional instalado em uma máquina virtual. O VMware Workstation Player (Windows e Linux) e VMWare Fusion Player (macOS) possuem suporte a aceleração gráfica 3D usando OpenGL 4.1 e são adequados para desenvolver as atividades da disciplina. Tanto o VMWare Workstation Player quanto o Fusion Player podem ser utilizados gratuitamente através de uma licença de uso pessoal. No Windows 10, o Windows Subsystem for Linux (WSL) também suporta aceleração gráfica 3D (disponível somente no WSL 2). Entretanto, a configuração é mais complexa e exige a instalação de um servidor do X Window System, como o VcXsrv. "],["linux.html", "1.1 Linux", " 1.1 Linux As ferramentas e bibliotecas necessárias estão disponíveis nos repositórios de pacotes das principais distribuições Linux. A seguir veremos como instalar os pacotes no Ubuntu. Entretanto, em outras distribuições há pacotes equivalentes e o procedimento é semelhante. Em um terminal, execute os passos a seguir. Atualize o sistema: sudo apt update &amp;&amp; sudo apt upgrade Instale o pacote build-essential (GCC, GDB, Make, etc): sudo apt install build-essential Instale o CMake e Git: sudo apt install cmake git Instale as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0: sudo apt install libglew-dev libsdl2-dev libsdl2-image-dev Opcionalmente, instale as ferramentas de linting do GLSL. Essas ferramentas poderão ser utilizadas no Visual Studio Code para fazer a análise estática do código da linguagem de shaders GLSL (OpenGL Shading Language) que será abordada na disciplina: sudo apt install glslang-tools Habilitando o OpenGL O suporte ao OpenGL já vem integrado no kernel do Linux através dos drivers de código aberto da biblioteca Mesa (drivers Intel/AMD/Nouveau). Para as placas da NVIDIA e AMD há a possibilidade de instalar os drivers proprietários do repositório nonfree (repositório restricted no Ubuntu), ou diretamente dos sites dos fabricantes: AMD ou NVIDIA. Os drivers proprietários, especialmente os da NVIDIA, geralmente têm desempenho superior aos de código aberto. Para verificar a versão do OpenGL suportada pelos drivers instalados, instale primeiro o pacote mesa-utils: sudo apt install mesa-utils Execute o comando: glxinfo | grep version O resultado deverá ser parecido com o seguinte: server glx version string: 1.4 client glx version string: 1.4 GLX version: 1.4 Max core profile version: 4.1 Max compat profile version: 4.1 Max GLES1 profile version: 1.1 Max GLES[23] profile version: 2.0 OpenGL core profile version string: 4.1 (Core Profile) Mesa 21.0.3 OpenGL core profile shading language version string: 4.10 OpenGL version string: 4.1 (Compatibility Profile) Mesa 21.0.3 OpenGL shading language version string: 4.10 OpenGL ES profile version string: OpenGL ES 2.0 Mesa 21.0.3 OpenGL ES profile shading language version string: OpenGL ES GLSL ES 1.0.16 Importante A versão em OpenGL version string ou OpenGL core profile version string deve ser 4.1 ou superior. Caso não seja, instale os drivers proprietários e certifique-se de que sua placa de vídeo suporta OpenGL 4.1. Atualizando o GCC As atividades farão uso de uma biblioteca de desenvolvimento que exige um compilador com suporte a C++17 e suporte parcial a C++20. Esse requisito é atendido se instalarmos uma versão recente do GCC, como o GCC 10. Nas últimas versões do Fedora e Manjaro, o GCC instalado por padrão já é a versão 10 ou superior. No Ubuntu, apenas o Ubuntu 20.10 (Groovy Gorilla) em diante vem com GCC 10 ou mais recente. Siga os passos a seguir caso sua distribuição seja de uma versão anterior, ou caso a saída de g++ --version mostre um número de versão menor que 10. Por exemplo, no Ubuntu 20.04 a saída de g++ --version é: g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. O padrão no Ubuntu 20.04 é o GCC 9.3. Logo, precisamos instalar uma versão mais recente. Em um terminal, adicione o PPA ubuntu-toolchain-r/test: sudo apt install software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test Instale o GCC 10: sudo apt install gcc-10 g++-10 A instalação do GCC 10 não substituirá a versão mais antiga já instalada. Entretanto, é necessário criar links simbólicos de gcc e g++ para a versão mais recente. Uma forma simples de fazer isso é através do update-alternatives. Primeiro, execute o comando a seguir para definir um valor de prioridade (neste caso, 100) para o GCC 10: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 100 --slave /usr/bin/g++ g++ /usr/bin/g++-10 --slave /usr/bin/gcov gcov /usr/bin/gcov-10 Use o comando a seguir para definir um valor de prioridade mais baixo (por exemplo, 90) para a versão anterior do GCC, que neste exemplo é a versão 9: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 --slave /usr/bin/gcov gcov /usr/bin/gcov-9 Agora execute o comando a seguir para escolher qual a versão do GCC instalada no sistema será utilizada: sudo update-alternatives --config gcc Na lista de versões instaladas, selecione o GCC 10 caso ainda não esteja selecionado. Isso criará os links simbólicos. Para testar se a versão correta do GCC está sendo utilizada, execute g++ --version. A saída deverá ser parecida com a seguinte: g++ (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o pacote ccache: sudo apt install ccache Atualize os links simbólicos dos compiladores instalados: sudo /usr/sbin/update-ccache-symlinks Insira a seguinte linha no final do arquivo ~/.bashrc de modo a prefixar o caminho do Ccache no PATH: export PATH=&quot;/usr/lib/ccache:$PATH&quot; Reabra o terminal ou execute source ~/.bashrc. Para testar se o Ccache está ativado, execute o comando which g++. A saída deverá incluir o caminho /usr/lib/ccache/, como a seguir: /usr/lib/ccache/g++`. Instalando o Emscripten Vá para o seu diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual. Um arquivo .emscripten será gerado: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha de ~/.bashrc. Isso fará com que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e erro padrão (stderr). "],["macos.html", "1.2 macOS", " 1.2 macOS Em um terminal, execute os passos a seguir: Execute o comando gcc. Se o GCC não estiver instalado, aparecerá uma caixa de diálogo solicitando a instalação das ferramentas de desenvolvimento de linha de comando. Clique em Install. Esse procedimento também instalará outras ferramentas, como o Make e Git. Para verificar se o GCC foi instalado, execute gcc --version. A saída deverá ser parecida com a seguinte (note que o GCC é apenas um atalho para o Apple Clang): Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1 Apple clang version 12.0.0 (clang-1200.0.32.28) Target: x86_64-apple-darwin19.6.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin Se o procedimento acima não funcionar (as instruções acima foram testadas no macOS Catalina), baixe o Command Line Tools for Xcode usando sua conta de desenvolvedor do Apple Developer, ou execute xcode-select --version no terminal. Em versões mais antigas do macOS pode ser necessário instalar o Xcode. Para instalar os demais pacotes de bibliotecas e ferramentas, instale o Homebrew com o seguinte comando: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Instale o CMake: brew install cmake Instale o GLEW, SDL 2.0 e SDL_image 2.0: brew install glew brew install sdl2 brew install sdl2_image Está pronto! Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o Ccache usando o Homebrew: brew install ccache Anote a saída de echo $(brew --prefix) (por exemplo, /usr/local). Abra o modo de edição do PATH: sudo nano /etc/paths Insira como primeira linha o caminho $(brew --prefix)/opt/ccache/libexec, onde $(brew --prefix) é a saída do passo 2. Por exemplo, /usr/local/opt/ccache/libexec. Salve (Ctrl+X e Y) e reinicie o terminal. Para testar, digite which gcc. A saída deverá ser um caminho que inclui o Ccache, como a seguir: /usr/local/opt/ccache/libexec/gc Instalando o Emscripten Vá para o seu diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual. Um arquivo .emscripten será gerado: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha do arquivo ~/.zshrc (shell Zsh no macOS Catalina ou posterior) ou ~/.bashrc (shell Bash em versões anteriores) para que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e erro padrão (stderr). "],["windows.html", "1.3 Windows", " 1.3 Windows Para a instalação das ferramentas e bibliotecas de desenvolvimento no Windows utilizaremos o MSYS2. MSYS2 é um ambiente de terminal tipo Unix com acesso a um repositório de ferramentas e bibliotecas de desenvolvimento de aplicações nativas em Windows através do gerenciador de pacotes pacman. Essas ferramentas e bibliotecas incluem o CMake, Git, GLEW, SDL 2.0 e SDL_image 2.0, entre outras que vamos utilizar na disciplina. O MSYS2 também permite instalar o MinGW-W64. Com isso podemos usar o GCC (compilador) e o GDB (depurador) no Windows e gerar binário nativo para 64-bit e 32-bit. Siga os passos a seguir para instalar o MSYS2 e as ferramentas/bibliotecas de desenvolvimento: Baixe o instalador de https://www.msys2.org e siga os passos descritos na página. Abra o shell do MSYS2 (aplicativo MSYS2 MSYS a partir no menu Iniciar) e execute o seguinte comando: pacman -S git mingw-w64-x86_64-ccache mingw-w64-x86_64-cmake mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb mingw-w64-x86_64-ninja mingw-w64-x86_64-glew mingw-w64-x86_64-SDL2 mingw-w64-x86_64-SDL2_image Isso instalará as ferramentas Git, Ccache, CMake, Ninja (o Ninja substitui o GNU Make no Windows), GCC e GDB (do MinGW-W64), e as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0. Opcionalmente, instale o pacote mingw-w64-x86_64-glslang com o comando a seguir. Isso instalará as ferramentas que poderão ser usadas para linting da linguagem GLSL (linguagem de shading que será abordada na disciplina): pacman -S mingw-w64-x86_64-glslang Ao terminar a instalação, feche o shell do MSYS2. Instale o Python 3.9+ para Windows. Durante a instalação, certifique-se de ativar a opção Add Python 3.9 to PATH. Após a instalação, abra o Prompt de Comando (cmd.exe) e execute o comando where python para mostrar os diferentes caminhos em que o python está sendo alcançado pela variável de ambiente Path: C:\\&gt;where python C:\\Users\\ufabc\\AppData\\Local\\Programs\\Python\\Python39\\python.exe C:\\Users\\ufabc\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe O primeiro caminho exibido deve ser o caminho do executável do Python que acabou de ser instalado. Neste caso está correto, pois o Python foi instalado no local padrão do instalador que é %LocalAppData%\\Programs\\Python\\Python39 (neste exemplo, o nome do usuário é ufabc). No Painel de Controle do Windows, abra a opção Editar as Variáveis de Ambiente do Sistema. Edite a variável Path do usuário atual1 e inclua os caminhos para mingw64\\bin e usr\\bin do MSYS2. Por exemplo, se o MSYS2 foi instalado em C:\\msys64, inclua os seguintes caminhos no Path: C:\\msys64\\usr\\bin C:\\msys64\\mingw64\\bin Edite a ordem dos caminhos de tal forma que o caminho do Python apareça no início da lista, seguido pelos caminhos do MSYS2, como mostra a figura a seguir: Para testar se o MSYS2 foi instalado corretamente, abra o Prompt de Comando e execute o comando g++ --version. A saída deverá ser parecida com a seguinte: g++ (Rev5, Built by MSYS2 project) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Habilitando o OpenGL O suporte ao OpenGL vem integrado no Windows. Apenas certifique-se de instalar os drivers mais recentes da placa de vídeo. Instalando o Emscripten Abra o Prompt de Comando em algum caminho onde queira instalar a pasta do SDK do Emscripten (por exemplo, C:\\). Note the o terminal deve ser o Prompt de Comando e não o PowerShell. Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre na pasta recém-criada: cd emsdk Baixe e instale o SDK atualizado (latest): emsdk install latest Ative o SDK latest para o usuário atual: emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: emsdk_env.bat Para testar se a instalação foi bem-sucedida, execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal ou execute o comando a seguir para registrar permanentemente as variáveis de ambiente no registro do sistema: emsdk_env.bat --permanent Caso não queira modificar o registro do sistema, uma opção é criar um atalho para cmd.exe e use a opção /k para executar o arquivo emsdk_env.bat sempre que o Prompt de Comando for aberto. Por exemplo: cmd.exe /k &quot;C:\\emsdk\\emsdk_env.bat&quot; Mude esse caminho de acordo com o local onde emsdk_env.bat foi instalado. A variável Path existe tanto nas variáveis do usuário quanto nas variáveis do sistema. A modificação do Path do usuário geralmente é suficiente para uma instalação correta. Se ocorrer algum erro ao seguir as instruções de construção do Hello, World! na seção 1.5 (erro de construção ou de DLL não encontrada), experimente alterar o Path do sistema ao invés do usuário. "],["vscode.html", "1.4 Visual Studio Code", " 1.4 Visual Studio Code Para desenvolver as atividades não é necessário usar um IDE ou editor em particular. Os códigos podem ser escritos em qualquer editor de texto não formatado e a compilação pode ser feita em linha de comando. Entretanto, é recomendável utilizar um editor/IDE como o CLion, Emacs, Sublime Text, Vim, Visual Studio Code, ou outro semelhante que seja capaz de oferecer funcionalidades de preenchimento automático de código, detecção de erros, ajuda sensível ao contexto e integração de construção com o CMake. A seguir veremos como configurar o Visual Studio Code (VS Code) para deixá-lo pronto para o desenvolvimento das atividades. O procedimento é simples e é o mesmo no Linux, macOS e Windows: Instale o VS Code através do instalador disponível em https://code.visualstudio.com/. No editor, instale as seguintes extensões: C/C++ for Visual Studio Code: para habilitar o suporte à edição e depuração de código C/C++ e as funcionalidades de preenchimento automático e referência cruzada do IntelliSense. CMake Tools: para integrar o CMake com o Visual Studio Code. Com essa extensão o editor consegue varrer o sistema em busca dos kits de compilação disponíveis e permite disparar o processo de configuração e compilação a partir do editor. Isso já é o suficiente para começarmos a trabalhar. Ainda precisaremos de algumas configurações extras para habilitar a depuração de código, mas veremos isso na seção a seguir (1.5). Caso você queira usar outro editor ou IDE, consulte a documentação específica do produto sobre como fazer a integração com o CMake e sobre como usar o GDB/LLDB para depurar código. Importante Qualquer que seja o IDE/editor utilizado, certifique-se de que o CMake e GCC estejam instalados e visíveis no PATH de acordo com as instruções mostradas nas seções anteriores. Dicas Opcionalmente, instale a extensão CMake For VisualStudio Code para ter suporte à edição de código dos scripts CMake. Em sistemas que possuem as ferramentas extras do Clang para linting e formatação de código tais como Clang-Tidy e ClangFormat, é possível instalar extensões para usá-las em tempo real e para formatar o código automaticamente sempre que o arquivo é salvo. Uma dessas extensões é o vscode-clangd, baseado no servidor clangd do LLVM. Infelizmente essa extensão não funciona corretamente no Windows com MSYS2, mas há diversas alternativas no Visual Studio Code Marketplace. Procure extensões com as palavras-chave clang-tidy e clang-format. Essas ferramentas ajudam a evitar os erros e bugs mais comuns e melhoram bastante a produtividade. No Ubuntu, o ClangFormat e Clang-Tidy podem ser instalados com: sudo apt install clang-tidy clang-format Isso instalará também o compilador Clang. No MSYS2: pacman -S mingw-w64-x86_64-clang-tools-extra Para ativar a formatação automática de código sempre que o arquivo é salvo, adicione a chave \"editor.formatOnSave\": true no arquivo de configuração do VS Code (settings.json). Para a análise estática em tempo real de código GLSL, instale a extensão GLSL Lint. Isso ajudará a evitar bugs e erros comuns na programação dos shaders em GLSL. "],["abcg.html", "1.5 ABCg", " 1.5 ABCg Para facilitar o desenvolvimento das atividades práticas utilizaremos a biblioteca ABCg desenvolvida especialmente para esta disciplina. A ABCg permite a prototipagem rápida de aplicações gráficas interativas 3D em C++ capazes de rodar tanto no desktop (binário nativo) quanto no navegador (binário WebAssembly). Internamente a ABCg utiliza a biblioteca SDL para gerenciar o acesso a dispositivos de entrada (mouse/teclado/gamepad) e saída (vídeo e áudio) de forma independente de plataforma, e a biblioteca GLEW para acesso às funções da API gráfica OpenGL. Além disso, a API do Emscripten é utilizada sempre que a aplicação é compilada para gerar binário WebAssembly. A ABCg é mais propriamente um framework do que uma biblioteca de funções, pois assume o controle da aplicação. Por outro lado, a camada de abstração para as APIs utilizadas é mínima e é possível acessar as funções da SDL e OpenGL diretamente (e faremos isso sempre que possível). Outras bibliotecas também utilizadas e que podem ser acessadas diretamente são: CPPIterTools: para o suporte a laços range-based em C++ usando funções do tipo range, enumerate e zip similares às do Python; Dear ImGui: para gerenciamento de widgets de interface gráfica do usuário, tais como janelas, botões e caixas de edição; {fmt}: como alternativa mais eficiente ao stdio da linguagem C (printf, scanf, etc) e iostreams do C++ (std::cout, std::cin, etc), e para formatação de strings com uma sintaxe similar ao str-format do Python; Guidelines Support Library (GSL): para uso de funções e tipos de dados recomendados pelo C++ Core Guidelines; OpenGL Mathematics (GLM): para suporte a operações de transformação geométrica com vetores e matrizes; tinyobjloader: para a leitura de modelos 3D no formato Wavefront OBJ. A seguir veremos como instalar e compilar a ABCg junto com um exemplo de uso. Instalação Em um terminal, clone o repositório do GitHub: git clone https://github.com/hbatagelo/abcg.git Observação A versão mais recente da ABCg (atualmente v2.0.0) também pode ser baixada como um arquivo compactado de https://github.com/hbatagelo/abcg/releases/latest. Atenção No Windows, certifique-se de clonar o repositório em um diretório cujo nome não contenha espaços ou caracteres especiais. Por exemplo, clone em C:\\cg em vez de C:\\computação gráfica. O repositório tem a estrutura mostrada a seguir. Para simplificar, os arquivos e subdiretórios .git* foram omitidos: abcg  .clang-format  .clang-tidy  build.bat  build.sh  build-wasm.bat  build-wasm.sh  CMakeLists.txt  LICENSE  README.md  runweb.bat  runweb.sh  VERSION.md  abcg   ...  cmake   ...  examples   ...  public  ... Os arquivos .clang-format e .clang-tidy são arquivos de configuração utilizados pelas ferramentas ClangFormat (formatação) e Clang-Tidy (linter) caso estejam instaladas. Os arquivos .sh são shell scripts de compilação e execução em linha de comando. Note que há scripts correspondentes com extensão .bat para usar no Prompt de Comando do Windows (o PowerShell não é suportado): build.sh: para compilar a biblioteca e os exemplos em binários nativos; build-wasm.sh: similar ao build.sh, mas para gerar binário em WebAssembly dentro do subdiretório public; runweb.sh: para rodar um servidor web local que serve o conteúdo de public. O arquivo CMakeLists.txt é o script de configuração utilizado internamente pelo CMake. Os subdiretórios são os seguintes: abcg contém o código-fonte da biblioteca e suas dependências; cmake contém scripts auxiliares de configuração do CMake; examples contém um exemplo de uso da ABCg: um Hello, World! que usa OpenGL e interface da ImGui; public contém páginas web para exibir o exemplo Hello, World! no navegador. Compilando em linha de comando Execute o script build.sh (Linux/macOS) ou build.bat (Windows) para iniciar o processo de configuração e construção. A saída será similar a esta (o exemplo a seguir é do Windows): -- The C compiler identification is GNU 10.3.0 -- The CXX compiler identification is GNU 10.3.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: C:/msys64/mingw64/bin/gcc.exe - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: C:/msys64/mingw64/bin/g++.exe - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done Using ccache -- Found OpenGL: opengl32 -- Found GLEW: C:/msys64/mingw64/lib/cmake/glew/glew-config.cmake -- Looking for pthread.h -- Looking for pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success -- Found Threads: TRUE -- Found SDL2: mingw32;-mwindows;C:/msys64/mingw64/lib/libSDL2main.a;C:/msys64/mingw64/lib/libSDL2.dll.a -- Found SDL2_image: C:/msys64/mingw64/lib/libSDL2_image.dll.a -- Configuring done -- Generating done -- Build files have been written to: C:/abcg/build ... [22/22] Linking CXX executable bin\\helloworld.exe Ao final, os binários estarão disponíveis no subdiretório build. A biblioteca estática estará em build/abcg/libabcg.a e o executável do exemplo Hello, World! estará em build/bin/helloworld. Para testar, execute o helloworld. No Linux/macOS: ./build/bin/helloworld/helloworld No Windows: .\\build\\bin\\helloworld\\helloworld.exe | cat Importante No Windows, a saída deve sempre ser redirecionada para cat ou tee. Se isso não for feito, nenhuma saída de texto será exibida no terminal. Isso se deve a um bug do MSYS2. Observação Observe o conteúdo de build.sh (build.bat contém instruções equivalentes): #!/bin/bash set -euo pipefail BUILD_TYPE=Debug # Reset build directory rm -rf build mkdir -p build &amp;&amp; cd build # Configure cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE .. # Build if [[ &quot;$OSTYPE&quot; == &quot;darwin&quot;* ]]; then # macOS NUM_PROCESSORS=$(sysctl -n hw.ncpu) else NUM_PROCESSORS=$(nproc) fi cmake --build . --config $BUILD_TYPE -- -j $NUM_PROCESSORS A variável BUILD_TYPE está como Debug, mas pode ser modificada para Release, MinSizeRel ou RelWithDebInfo. Use a opção Debug (padrão) ou RelWithDebInfo enquanto estiver depurando o código. Use Release para gerar um binário otimizado e sem arquivos de símbolos de depuração (otimiza para gerar código mais rápido) ou MinSizeRel (otimiza para gerar binário de menor tamanho). Observe que o script apaga o subdiretório build antes de criá-lo novamente. Portanto, não salve arquivos dentro de build pois eles serão apagados na próxima compilação! A geração dos binários usando o CMake é composta de duas etapas: configuração (cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE ..) e construção (cmake --build . --config $BUILD_TYPE). A configuração gera os scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja). A construção dispara a compilação e ligação usando tais scripts. Todos os arquivos gerados na configuração e construção ficam armazenados no subdiretório build. Compilando no Visual Studio Code Primeiramente, clone o repositório abcg do GitHub como mostrado na seção anterior. Apague o subdiretório build caso você já tenha compilado via linha de comando. No Visual Studio Code, selecione o menu File &gt; Open Folder e abra a pasta abcg. No canto inferior direito da janela aparecerá uma notificação solicitando se você quer configurar o projeto. Selecione Yes. Ao fazer isso, será feita uma varredura no sistema para identificar os compiladores e toolchains visíveis no PATH. Uma lista de kits encontrados aparecerá na janela Output do CMake/Build, como a seguir: [kit] Found Kit: Clang 11.0.0 [kit] Found Kit: GCC for x86_64-w64-mingw32 10.2.0 Ao final da varredura, selecione o kit GCC for x86_64-w64-mingw32 versão 10 ou posterior. Ao fazer isso, será iniciado o processo de configuração do CMake. Esse processo gera os arquivos que serão utilizados pelo sistema de construção nativo dentro de um subdiretório build do projeto. Se aparecer uma notificação pedindo para configurar o projeto sempre que ele for aberto, Responda Yes: Após o término da configuração do CMake, aparecerá uma outra notificação solicitando permissão para configurar o Intellisense. Responda Allow. Se, além disso, aparecer uma notificação sobre o arquivo compile_commands.json, como a seguir, responda Yes novamente: compile_commands.json é um arquivo gerado automaticamente pelo CMake e que contém os comandos de compilação e o caminho de cada unidade de tradução utilizada no projeto. O IntelliSense utiliza as informações desse arquivo para habilitar as referências cruzadas. Importante A construção dos projetos usando o CMake é feita em duas etapas: Configuração: consiste na geração dos scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja); Construção: consiste no disparo da compilação e ligação usando os scripts gerados na configuração, além da execução de etapas de pré e pós-construção definidas nos scripts dos arquivos CMakeList.txt. Tanto os arquivos da configuração quanto os da construção (binários) são gravados no subdiretório build. Geralmente a configuração só precisa ser feita uma vez e depois refeita caso o subdiretório build tenha sido apagado, ou após a alteração do kit de compilação, ou após a alteração do build type (por exemplo, de Debug para Release). Como indicado na figura abaixo, na barra de status há botões para selecionar o build type e configurar o CMake, selecionar o kit de compilação, e construir a aplicação. A opção de construir já se encarrega de configurar o CMake caso os arquivos de configuração ainda não tenham sido gerados. Essas opções também estão disponíveis na paleta de comandos do editor, acessada com Ctrl+Shift+P. Os comandos são: CMake: Select Variant: para selecionar um build type; CMake: Select a Kit: para selecionar um kit de compilação; CMake: Configure: para configurar o CMake usando o kit e o build type atual; CMake: Build: para construir o projeto. Observação Os build types permitidos no CMake são: Debug para gerar binários não otimizados e com arquivos de símbolos de depuração. Esse é o build type padrão; RelWithDebInfo para gerar arquivos de símbolos de depuração com binários otimizados; Release para gerar binários otimizados e favorecer código mais rápido. Essa opção não gera os arquivos de símbolos de depuração; MinSizeRel, semelhante ao Release, mas a otimização tenta gerar binário de menor tamanho. Para compilar e gerar os binários, tecle F7 ou clique em Build na barra de status. O progresso será exibido na janela Output do CMake/Build. Se a construção terminar com sucesso, a última linha de texto da janela Output será: [build] Build finished with exit code 0 Os arquivos gerados na construção ficam armazenados no subdiretório build, da mesma forma como ocorre na compilação via linha de comando. Para testar, abra um terminal e execute ./build/bin/helloworld/helloworld (Linux/macOS) ou .\\build\\bin\\helloworld\\helloworld.exe (Windows). Atenção A configuração do CMake gerada a partir do Visual Studio Code não é necessariamente a mesma gerada usando os scripts de linha de comando: o compilador pode ser diferente, ou o build type pode ser diferente. Se em algum momento você construir o projeto via linha de comando usando os scripts .sh ou .bat e depois quiser construir pelo editor, certifique-se de apagar o subdiretório build antes de entrar no VS Code. Isso forçará uma nova configuração do CMake e evitará erros de incompatibilidade entre as configurações. Depurando no Visual Studio Code Podemos depurar o código com GDB ou LLDB usando a interface do Visual Studio Code. Após construir o projeto com build type Debug ou RelWithDebInfo, selecione a opção Run (Ctrl+Shift+D ou botão na barra de atividades) e então a opção create a launch.json file para criar um arquivo launch.json no subdiretório .vscode da pasta do projeto: Em Select Environment, selecione C++ (GDB/LLDB). Isso criará uma configuração inicial para o arquivo json: No arquivo launch.json, modifique o valor da chave program para apontar para o executável que se deseja depurar. Por exemplo, ${workspaceFolder}/build/bin/helloworld/helloworld para apontar para o executável do Hello, World! Observação ${workspaceFolder} é uma variável pré-definida do Visual Studio Code que contém o caminho da pasta do projeto. Consulte a documentação para informações sobre outras variáveis disponíveis. Modifique o valor da chave miDebuggerPath para o caminho do executável do GDB ou LLDB, ou deixe vazio para usar o padrão do sistema. No Windows, a chave miDebuggerPath deve conter explicitamente o caminho completo para o GDB, que é C:\\msys64\\mingw64\\bin\\gdb.exe caso o MSYS2 tenha sido instalado em C:\\msys64. No Windows também é necessário configurar o terminal padrão do VS Code para Command Prompt no lugar de PowerShell. O exemplo abaixo mostra o conteúdo completo de launch.json para depurar o Hello, World! no Windows. { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld.exe&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: true, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;miDebuggerPath&quot;: &quot;C:\\\\msys64\\\\mingw64\\\\bin\\\\gdb.exe&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Observe que o valor da chave externalConsole foi modificado para true para que um terminal de saída seja aberto durante a depuração. Consulte a documentação sobre depuração para informações sobre outras opções e informações gerais sobre como depurar código no editor. Após modificar o arquivo launch.json, selecione novamente a opção Run na barra de atividades ou tecle F5 para iniciar o programa no modo de depuração. Reedite o arquivo launch.json sempre que mudar o nome do executável que se queira depurar. Compilando para WebAssembly Podemos compilar as aplicações ABCg para WebAssembly de modo a rodá-las diretamente no navegador. A construção é feita via linha de comando usando o toolchain Emscripten. Acompanhe a seguir como construir o exemplo Hello, World! para WebAssembly e abri-lo no navegador: Em um terminal (shell ou Prompt de Comando), ative as variáveis de ambiente do Emscripten (script emsdk_env.sh/emsdk_env.bat do SDK). Após isso, o compilador emcc deverá estar visível no PATH; No diretório abcg, execute build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Isso iniciará a configuração do CMake e a construção dos binários. Os arquivos resultantes serão gerados no subdiretório public: nesse caso, helloworld.data (arquivo de dados/assets), helloworld.js (arquivo JavaScript) e helloworld.wasm (binário WebAssembly); Execute o script runweb.sh (Linux/macOS) ou runweb.bat (Windows) para rodar um servidor web local. O conteúdo de public estará disponível em http://localhost:8080/; Abra a página http://localhost:8080/helloworld.html que chama o script helloworld.js recém-criado. A página HTML não faz parte do processo de construção e foi criada previamente. O resultado será semelhante ao exibido a seguir (uma aplicação mostrando um triângulo colorido e uma caixa de diálogo com alguns controles de interface). A pequena janela de texto abaixo da janela da aplicação mostra o conteúdo do terminal. Nesse caso, são exibidas algumas informações sobre o OpenGL (versão utilizada, fornecedor do driver, etc). Observação O subdiretório public contém, além do helloworld.html: full_window.html: para exibir o Hello, World! ocupando a janela inteira do navegador; full_window_console.html: idêntico ao anterior, mas com a sobreposição das mensagens do console na tela. Nos próximos capítulos veremos como construir novas aplicações usando a ABCg. Dica Aproveite o restante da primeira semana de aula para se familiarizar com os conceitos do chamado C++ moderno (C++11 em diante): ponteiros inteligentes (smart pointers), expressões lambda, variáveis auto e semântica de movimentação (move semantics). Isso facilitará o entendimento do código da ABCg nos próximos capítulos. Uma referência rápida (cheatsheet) ao C++ moderno está disponível em https://github.com/AnthonyCalandra/modern-cpp-features. Um excelente livro é o A Tour of C++, de Bjarne Stroustrup. Se não puder ter acesso ao livro, há recursos gratuitos como os sites learncpp.com e tutorialspoint.com. A documentação da Microsoft sobre C++ é uma opção em português. Há uma referência sobre a linguagem C++ e sobre a biblioteca C++ padrão. Consulte também o C++ Core Guidelines para ficar a par das boas práticas de programação. Para uma referência completa da linguagem, consulte cppreference.com. Algumas partes estão traduzidas para o português. "],["intro.html", "2 Introdução", " 2 Introdução Computação gráfica (CG) é o conjunto de métodos e técnicas para a construção, manipulação, armazenagem e exibição de imagens por meio de um computador (ISO 2015). A computação gráfica tem suas origens no desenvolvimento dos primeiros computadores eletrônicos equipados com dispositivos de exibição. A partir do final da década de 1940, o computador experimental Whirlwind I do MIT (Instituto de Tecnologia de Massachusetts) e o sistema SAGE (Semi-Automatic Ground Environment) da Força Aérea dos Estados Unidos, foram os primeiros a utilizar dispositivos de exibição do tipo CRT (cathod-ray tube, ou tubo de raios catódicos) para exibir gráficos vetoriais compostos de linhas e pontos. O uso do termo computação gráfica é um pouco mais recente. William Fetter, projetista gráfico da Boeing, utilizou o termo pela primeira vez em 1960 por sugestão de seu supervisor Verne L. Hudson, para descrever seu trabalho. Fetter utilizava gráficos tridimensionais no computador para criar um modelo estilizado de corpo humano que ficou conhecido como Boeing Man (figura 2.1). O modelo, composto de curvas e segmentos, era utilizado em simulações de ergonomia do piloto na cabine do avião. Figura 2.1: Boeing Man desenhado por William Fetter em um IBM 7094 (fonte). Além de CG, o acrônimo CGI (Computer-Generated Imagery) é frequentemente utilizado para se referir à geração de imagens e efeitos visuais em computador com aplicações em arte, entretenimento, simulação e visualização científica. Uma forma de CGI bastante conhecida e que chama a atenção por suas imagens bonitas é a síntese de imagens fotorrealistas. Um exemplo de imagem fotorrealista gerada em computador é mostrado na figura 2.2. A imagem é uma cena de arquitetura interior produzida com o Cycles Render Engine. Figura 2.2: Scandinavian Interior, por Arnaud Imobersteg (fonte). Atualmente, uma imagem como a da figura 2.2 pode ser gerada em qualquer computador pessoal que tenha sido fabricado nos últimos 10 anos. A imagem foi gerada com o software Blender, gratuito e de código aberto, e que tem o Cycles Render Engine como um de seus renderizadores. Entretanto, existe uma máxima em computação gráfica  a chamada 1ª lei de Peddie  que diz: Em computação gráfica, demais nunca é o suficiente.  John Peddie De fato, mesmo com toda a evolução do hardware gráfico e das técnicas de CG, ainda não é possível gerar em tempo real imagens com o nível de qualidade obtido por renderizadores como o Cycles. Eventualmente, esse dia chegará. Porém, quando isso acontecer, o nível de exigência de todos nós  usuários, pesquisadores e desenvolvedores  será maior do que é atualmente. Desejaremos imagens ainda mais detalhadas, com maior resolução, mais realistas, mais cinemáticas, mais interativas, etc. Demais nunca é o suficiente. Em computação gráfica, é necessário um cuidadoso compromisso entre a qualidade das imagens geradas e a eficiência com que essas imagens podem ser sintetizadas em um dado sistema computacional. Isso é particularmente importante quando as imagens precisam ser geradas em tempo real. Em jogos digitais, é comum que imagens de alta resolução tenham de ser geradas a uma taxa de, no mínimo, 30 quadros por segundo2. Mesmo os jogos que não visam o fotorrealismo exigem imagens em um nível de qualidade que só pode ser alcançado com o uso de técnicas avançadas de sombreamento e iluminação. Essas técnicas visam produzir resultados que, ainda que não sejam necessariamente acurados do ponto de vista físico, precisam ser suficientemente convincentes para um público cada vez mais exigente. No decorrer do quadrimestre veremos que a evolução das técnicas e ferramentas de computação gráfica em tempo real é impulsionada pela busca da melhor qualidade de imagem que pode ser obtida de forma eficiente no hardware gráfico disponível no momento. Como resultado, é comum que os métodos adotem simplificações inusitadas, mas ao mesmo tempo muito espertas, e explorem diferentes aspectos da percepção visual humana para criar uma ilusão de realismo que seja suficiente para chegar ao resultado desejado. Referências "],["áreas-correlatas.html", "2.1 Áreas correlatas", " 2.1 Áreas correlatas A computação gráfica se relaciona, e em certa medida se sobrepõe, a diferentes campos de atuação da ciência da computação. Uma breve introdução às principais áreas correlatas é dada a seguir: Síntese de imagem: é o que geralmente se entende por computação gráfica. Compreende o processo de rendering (imageamento ou renderização) que consiste em converter especificações de geometria, cor, textura, iluminação, entre outras especificações de características de uma cena, em uma imagem exibida em um display gráfico. A figura 2.3 mostra o resultado da síntese de imagem de uma cena fotorrealista usando técnicas combinadas de traçado de raios e radiosidade. Figura 2.3: Imagem gerada no renderizador POV-Ray, por Gilles Tran (fonte). Visão computacional: compreende o processo de adquirir, processar e interpretar dados visuais para gerar as especificações de uma cena. A partir de uma imagem digital, técnicas de visão computacional podem ser utilizadas para tarefas como a reconstrução dos modelos geométricos vistos na imagem, o particionamento dos pixels em segmentos correspondentes aos diferentes objetos da cena, reconhecimento de texturas, identificação dos atributos da câmera e da iluminação, e extração de outras informações semânticas. A visão computacional com frequência se relaciona com a visão de máquina, que compreende as técnicas e ferramentas voltadas a aplicações de visão em inspeção automática, controle de processos industriais e orientação de robôs. A figura 2.4 mostra um exemplo de aplicação de visão computacional: uma técnica de segmentação semântica utilizando aprendizagem profunda para identificar objetos em uma imagem. Figura 2.4: Segmentação semântica usando o sistema YOLO (fonte). Processamento de imagem: compreende o processo de aplicar filtros e operações sobre uma imagem digital que resultam em uma nova imagem digital. Técnicas de processamento digital de imagem podem ser utilizadas para enfatizar características de uma imagem (por exemplo, ajustar brilho, contraste, nitidez), restaurar imagens que sofreram algum tipo de degradação por ruído, mudar cores e tons, comprimir e quantizar, entre diversas outras operações. O escopo do processamento de imagens frequentemente se intersecta com aquele das técnicas de visão computacional. A figura 2.5 mostra um exemplo de processamento de imagem: a aplicação de filtros de remoção de ruído em uma imagem renderizada pelo método de traçado de raios estocástico. O ruído é inerente ao método de Monte Carlo utilizado nesse tipo de renderização. Figura 2.5: Uso dos filtros de processamento de imagem do Intel Open Image Denoise para remoção de ruído de uma imagem de traçado de raios (fonte). Modelagem geométrica: está relacionada com a criação e processamento de representações matemáticas de formas. Técnicas de modelagem geométrica podem ser utilizadas para criar modelos compostos de curvas e superfícies a partir de aquisição de dados (por exemplo, a partir de uma nuvem de pontos de uma aquisição por scanner 3D), construir e manipular modelos sintéticos através da combinação de primitivas geométricas, converter uma representação geométrica em outra, e realizar operações geométricas e topológicas diversas. A figura 2.6 mostra um exemplo de reconstrução de malha geométrica usando o software MeshLab. O modelo à esquerda é o modelo original. Na reconstrução (à direita), os buracos foram preenchidos e o resultado é uma única malha de triângulos. Figura 2.6: Reconstrução de malha geométrica usando o MeshLab (fonte). Neste curso teremos como foco a síntese de imagens. Em particular, a síntese de imagens em tempo real. Como parte disso, veremos como representar e processar cenários virtuais compostos de objetos tridimensionais animados. Veremos como implementar modelos de iluminação capazes de simular de forma eficiente a iluminação de superfícies, e como gerar imagens digitais do ponto de vista de uma câmera virtual. Faremos isso usando a API gráfica OpenGL de modo a explorar o pipeline de processamento gráfico programável das placas de vídeo atuais. Com isso conseguiremos obter o nível de eficiência necessário para produzir animações e permitir a sensação de interatividade. Na UFABC, os tópicos de visão computacional e processamento de imagens são abordados nas disciplinas ESZA019-17 Visão Computacional e MCZA018-17 Processamento Digital de Imagens. "],["linha-do-tempo.html", "2.2 Linha do tempo", " 2.2 Linha do tempo Nesta seção acompanharemos um resumo da evolução histórica da computação gráfica. Iniciaremos na década de 1950, com os primeiros computadores eletrônicos de uso geral e o surgimento das primeiras aplicações de computação gráfica, e seguiremos até a década atual com os desenvolvimentos mais recentes das atuais GPUs (Graphics Processing Units). Embora a computação gráfica seja recente, assim como a própria ciência da computação, o desenvolvimento de seus fundamentos é anterior ao século XX e só foi possível devido às contribuições artísticas e matemáticas de diversos pioneiros. Para citar apenas alguns: Euclides de Alexandria (300 a.C.), com sua contribuição para o desenvolvimento da geometria; Filippo Brunelleschi (13771446), com seus estudos sobre o uso da perspectiva; René Descartes (15961650), com o desenvolvimento da geometria analítica e a noção de sistema de coordenadas; Christiaan Huygens (16291695) e Isaac Newton (16431727) por suas investigações sobre os fenômenos da luz; Leonhard Euler (17071783), por sua contribuição na trigonometria e em topologia; James Joseph Sylvester (18141897), pela invenção da notação matricial. O uso de gráficos no computador também não teria sido possível sem os esforços que contribuíram para o surgimento dos primeiros dispositivos de exibição, como o tubo de raios catódicos no final do século XIX. 1950 Os primeiros computadores eletrônicos com dispositivos de exibição surgem neste período. O computador Whirlwind I, do MIT, originalmente projetado para ser parte de um simulador de vôo, foi um dos primeiros computadores digitais de uso geral com processamento em tempo real. O Whirlwind I era equipado com um CRT vetorial capaz de desenhar linhas e pontos. Charles W. Adams e John T. Gilmore, programadores da equipe de desenvolvimento do Whirlwind, implementaram um programa de avaliação de equações diferenciais para produzir a animação da trajetória de uma bola quicando. Essa simulação pode ser considerada a primeira aplicação de computação gráfica interativa e o primeiro jogo de computador, pois o operador podia controlar, através de um botão, a frequência do quicar na tentativa de fazer a bola acertar uma lacuna na tela que simulava um buraco no chão. O sistema de defesa aérea SAGE evoluiu a partir do Whirlwind ao longo da década de 1950. As estações do SAGE contavam com telas CRT que exibiam dados de diferentes radares combinados com informações de referência geográfica. Cada estação era também equipada com uma caneta óptica. Através da caneta óptica, o operador podia apontar e selecionar elementos gráficos diretamente na tela (figura 2.7). Figura 2.7: Operador do SAGE usando uma caneta óptica em um CRT vetorial (fonte). 1960 Nesse período a computação gráfica se desenvolve nos laboratórios de pesquisa de universidades e surgem as primeiras aplicações de CAD (Computer-Aided Design) nas indústrias automotiva e aeroespacial. Na década de 1960 ocorrem importantes desenvolvimentos na área de modelagem geométrica, como o uso de curvas de Bézier e NURBS (Non-Uniform Rational Basis Spline). Em 1960, a Digital Equipment Corporation (DEC) começa a produzir em escala comercial o computador PDP-1, equipado com CRT e caneta óptica. Em 1961, o cientista da computação Steve Russell (MIT) cria o Spacewar! (figura 2.8). O jogo ganha popularidade dentro e fora da universidade e vira referência no desenvolvimento de jogos digitais3. Figura 2.8: Steve Russell e seu jogo Spacewar! no DEC PDP-1 (fonte). Em 1963, Ivan Sutherland desenvolve o SketchPad, um sistema de projeto gráfico interativo que permite ao usuário manipular primitivas gráficas vetoriais através de uma caneta óptica e um CRT (Sutherland 1963). A figura 2.9 mostra Sutherland operando o SketchPad no computador TX-2 do MIT. O SketchPad é um marco no uso da interface gráfica do usuário (GUI, acrônimo de Graphical User Interface) e um precursor das aplicações de projeto assistido por computador (CAD). Figura 2.9: Ivan Sutherland operando o SketchPad em 1962 (fonte). Na década de 1960 surgem também os primeiros seminários e grupos de interesse em pesquisa sobre gráficos em computador. Na ACM (Association for Computing Machinery), tradicional sociedade científica e educacional dedicada à computação, é fundado o grupo SICGRAPH (Special Interest Committe on Computer Graphics) para promover seminários de computação gráfica. No final da década, o SICGRAPH muda de nome para SIGGRAPH (Special Interest Group on Computer Graphics and Interactive Techniques). A conferência SIGGRAPH é realizada anualmente e é hoje uma das principais conferências de computação gráfica no mundo. 1970 Durante a década de 1970 são desenvolvidas muitas das técnicas de síntese de imagens em tempo real utilizadas atualmente. Em 1971, o então aluno de doutorado Henri Gouraud, trabalhando com Dave Evans e Ivan Sutherland na Universidade de Utah, desenvolve uma técnica eficiente de melhoramento da percepção visual do sombreamento (shading) de superfícies suaves aproximadas por malhas poligonais (Gouraud 1971). Tal técnica, conhecida como Gouraud shading, consiste em interpolar linearmente os valores de intensidade de luz refletida dos vértices da malha poligonal. O resultado é a suavização da variação da reflexão de luz sem a necessidade de aumentar a resolução da malha geométrica (Figura 2.10). Figura 2.10: Visualização de uma esfera aproximada por polígonos, exibindo o aspecto facetado (esquerda) e suavizado com Gouraud shading (direita). Em 1973, Bui Phong, também na Universidade de Utah, desenvolve o Phong shading como um melhoramento de Gouraud shading para reproduzir com mais fidelidade as reflexões especulares em aproximações de superfícies curvas (Phong 1973). Na figura 2.11 é possível comparar Gouraud shading e Phong shading lado a lado. Phong shading reproduz de forma mais acurada o brilho especular da esfera sem precisar usar uma malha poligonal mais refinada. Figura 2.11: Visualização de uma esfera com Gouraud shading (esquerda) e Phong shading (direita). Phong também propôs um modelo empírico de iluminação local de pontos sobre superfícies conhecido como modelo de reflexão de Phong. Em 1977, Jim Blinn, aluno da mesma universidade, propôs uma alteração do modelo de reflexão de Phong  o modelo de BlinnPhong  mais acurado fisicamente e mais eficiente sob certas condições de visualização e iluminação (Blinn 1977). Nas décadas seguintes, o modelo de BlinnPhong tornaria-se o padrão de indústria para síntese de imagens em tempo real, e ainda é muito utilizado atualmente. Em 1974, Wolfgang Straßer, na Universidade Técnica de Berlim, e Ed Catmull, na Universidade de Utah, desenvolvem ao mesmo tempo, mas de forma independente, uma técnica que viria a ser conhecida como Z-buffering. Tal técnica permite identificar, de forma conceituamente simples e favorável à implementação em hardware, quais partes da geometria 3D estão visíveis de um determinado ponto de vista. Atualmente, essa técnica é largamente utilizada em síntese de imagens e é suportada em todo hardware gráfico. Além de ter contribuído com a técnica de Z-buffering, Catmull também trouxe diversos avanços na área de modelagem geométrica, especialmente em subdivisão de superfícies e representação paramétrica de superfícies bicúbicas (Catmull 1974). Outra importante contribuição de Catmull foi o desenvolvimento da técnica de mapeamento de textura, ubíqua nas aplicações gráficas atuais e que permite aumentar a percepção de detalhes de superfícies sem aumentar a complexidade da geometria (figura 2.12). Figura 2.12: Animação do mapeamento de uma textura 2D sobre um modelo poligonal 3D (fonte). Em 1975, o matemático Benoît Mandelbrot, na IBM, desenvolve o conceito de geometria de dimensão fracionária e cria o termo fractal (Albers and Alexanderson 2008). Desde então, fractais começam a ser explorados em síntese de imagens e modelagem geométrica para representar os mais diversos padrões e fenômenos naturais tais como contornos de mapas, relevo de terrenos, nuvens, texturas e plantas. Vol Libre Vol Libre, de Loren Carpenter, foi o primeiro filme criado com fractais. O vídeo, de apenas dois minutos, foi apresentado pela primeira vez na conferência SIGGRAPH 80 após uma palestra técnica de Carpenter sobre a renderização de curvas e superfícies fractais: De acordo com o livro Droidmaker: George Lucas And the Digital Revolution (Rubin 2005), ao final da exibição do vídeo, Ed Catmull e Alvy Smith, da Lucasfilm, abordaram Carpenter e ofereceram a ele um emprego na divisão de computação da empresa. Carpenter aceitou imediatamente. Após a carreira na Lucasfilm, Carpenter ainda seria co-fundador da Pixar (junto com Catmull, Smith e outros) e cientista-chefe do estúdio de animação. Em 1976, Steve Jobs, Steve Wozniak e Ronald Wayne fundam a Apple Computer (atualmente Apple Inc.). Em 1979, Steve Jobs entra em contato com as pesquisas de desenvolvimento de interface gráfica na Xerox PARC (atualmente PARC), divisão de pesquisa da Xerox em Palo Alto, Califórnia. Na PARC, Jobs conhece o Xerox Alto, o primeiro computador com uma interface gráfica baseada na metáfora do desktop e no uso do mouse (figura 2.13). Figura 2.13: Xerox Alto (fonte). O Xerox Alto foi o resultado de desenvolvimentos iniciados por Douglas Engelbart e Dustin Lindberg no Standard Research Institute, atual SRI International, por sua vez inspirados no SketchPad de Sutherland. Alguns anos depois, a Apple implementaria os conceitos do Xerox Alto nos computadores Apple Lisa e Macintosh, iniciando uma revolução no uso da interface gráfica nos computadores pessoais (PCs). Em 1977, surge a primeira tentativa de padronização de especificação de comandos em sistemas gráficos: o Core Graphics System (ou simplesmente Core), proposto pelo Graphic Standards Planning Committee (GSPC) da ACM SIGGRAPH (Chappell and Bono 1978). Em 1978, Jim Blinn desenvolve uma técnica de mapeamento de textura para simulação de vincos e rugosidades em superfícies: o bump mapping (Blinn 1978). Uma forma de bump mapping muito utilizada atualmente é o normal mapping. A técnica pode ser muito efetiva para manter a ilusão de uma superfície detalhada, mesmo quando a geometria utilizada é muito simples. A figura 2.14 mostra um exemplo dessa simplificação. Ao longo do quadrimestre implementaremos esta e outras técnicas de texturização. Figura 2.14: Uso de normal mapping para simular a renderização, com apenas dois triângulos, de um modelo de quatro milhões de triângulos (fonte). No final da década, J. Turner Whitted desenvolve a técnica de traçado de raios (Whitted 1979). O traçado de raios consegue simular com mais precisão, e de forma conceitualmente simples, efeitos ópticos de reflexão, refração, espalhamento e dispersão da luz. Como resultado, consegue gerar imagens mais fotorrealistas, ainda que sob um custo computacional muito elevado quando comparado com a renderização baseada na rasterização, que consiste na varredura e preenchimento de primitivas geométricas projetadas. Figura 2.15: Esferas e tabuleiro de xadrez: uma das primeiras imagens geradas com traçado de raios, por Turner Whitted. 1980 Essa é a década em que a computação gráfica marca sua presença definitiva na indústria de cinema. O uso de cenas de computação gráfica é popularizado a partir de filmes como Star Trek II: The Wrath of Khan (1982), Tron (1982) e \"Young Sherlock Holmes (1985), como resultado dos avanços das técnicas de síntese de imagem e modelagem geométrica da década anterior, combinado com o avanço da capacidade de processamento dos computadores. Durante essa década ocorrem também importantes avanços nas técnicas de síntese de imagens. Em 1984, Robert Cook, Thomas Porter e Loren Carpenter desenvolvem o traçado de raios distribuído (distributed ray tracing), o qual permite reproduzir efeitos de sombras suaves, entre outros efeitos não contemplados pelo método original de Whitted (Cook, Porter, and Carpenter 1984). A figura 2.16 mostra um exemplo de renderização da cena de teste Cornell box usando essa técnica. A imagem tende a ser granulada como resultado da natureza estocástica do algoritmo. Figura 2.16: Imagem gerada com traçado de raios distribuído/estocástico. Ainda em 1984, Donald Greenberg, Michael Cohen e Kenneth Torrance propõem a técnica de radiosidade (Greenberg, Cohen, and Torrance 1986) baseada no uso do método de elementos finitos para simular interreflexões de luz entre superfícies idealmente difusas. A solução da radiosidade de uma cena pode ser pré-processada e não depende da posição da câmera. Isso permite a visualização da cena em tempo real, desde que a posição dos objetos e fontes de luz mantenha-se estática. A figura 2.17 mostra um exemplo de cena renderizada com radiosidade usando o software RRV (Radiosity Renderer and Visualizer). O método de radiosidade pode ser combinado com traçado de raios para gerar imagens com melhor fidelidade de simulação de reflexão difusa e especular. Figura 2.17: Imagem gerada com radiosidade (fonte). Em 1985, o GKS (Graphical Kernel System), desenvolvido como um melhoramento da API Core, torna-se a API padrão ISO para gráficos independentes do dispositivo (ISO 1985). Através do GKS, o código de descrição de comandos para manipulação de gráficos 2D permite a portabilidade entre diferentes linguagens de programação, sistemas operacionais e hardware gráfico compatível. Entretanto, gráficos 3D ainda não são contemplados nesta API. Em 1986, Steve Jobs adquire a divisão de computação gráfica da Lucasfilm e funda a Pixar junto com Ed Catmull, Alvy Smith e outros. Nessa época, Catmull, Loren Carpenter e Robert Cook desenvolvem o sistema de renderização RenderMan, muito utilizado na produção de efeitos visuais em filmes e animações. Após 14 anos, Catmull, Carpenter e Cook receberiam da Academia de Artes e Ciências Cinematográficas a estatueta do Oscar na categoria Academy Scientific and Technical Awards pelas contribuições à indústria do cinema representadas pelo desenvolvimento do RenderMan. O sucesso do RenderMan deve-se em parte à sua elegante API  a RenderMan Interface (RISpec)  inspirada na linguagem PostScript. A API permite a descrição completa de cenas 3D com todos os componentes necessários à renderização. Isso garante resultados consistentes, independentes do software de modelagem utilizado. O conceito de shaders, amplamente utilizado em hardware gráfico atual, surge do RenderMan shading language, desenvolvido na década de 1990 e incorporado no RISpec em 2005 como uma linguagem  dessa vez inspirada na linguagem C  de especificação de propriedades de superfícies, fontes de luz e efeitos atmosféricos de cena. Em 1988 é organizado o 1º Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens (SIBGRAPI), em Petrópolis, RJ. O evento, organizado anualmente pela CEGRAPI/SBC, internacionalizou-se e atualmente é chamado de Conference on Graphics, Patterns and Images. Neste ano, o SIBGRAPI, que estava planejado para ser realizado em Gramado (RS), será inteiramente virtual por causa da pandemia de COVID-19. 1990 1990 é a década das APIs gráficas 3D e da popularização do hardware gráfico nos PCs. Empresas como a Sun Microsystems (adquirida pela Oracle em 2010), IBM, HP (Hewlett-Packard), e as agora extintas NeXT, SGI (Silicon Graphics, Inc.) e DEC, desenvolvem estações gráficas de alto desempenho equipadas com hardware capaz de acelerar operações de renderização baseadas em rasterização com suporte a Z-buffer, mapeamento de texturas, iluminação e sombreamento de superfícies (figura 2.18). Figura 2.18: Workstation SGI IRIS Indigo (fonte). Neste período surgem as primeiras APIs para gráficos 3D como tentativa de padronizar a interface de programação entre as diferentes arquiteturas de hardware. Uma dessas APIs, desenvolvida ao longo da década de 1980 e que se estabelece como padrão da indústria na década de 1990, é o PHIGS (Programmers Hierarchical Interactive Graphics System) (Shuey 1987). PHIGS utiliza o conceito de grafo de cena: uma estrutura de dados hierárquica que representa as relações entre os modelos geométricos e outras entidades de uma cena. A API trabalha com malhas poligonais e síntese de imagens baseada na rasterização (em oposição ao traçado de raios), prevê o suporte a Gouraud e Phong shading, mas não oferece suporte a mapeamento de texturas. Em oposição ao PHIGS, a SGI utiliza em suas estações gráficas IRIS a API proprietária IRIS GL (Integrated Raster Imaging System Graphics Library) com características semelhantes ao PHIGS, porém com suporte a mapeamento de texturas (McLendon 1992). Diferentemente do PHIGS, o IRIS GL não adota o conceito de grafo de cena. As primitivas gráficas são enviadas imediatamente ao hardware gráfico em um pipeline de transformação geométrica e visualização. Esse modo de enviar os dados, conhecido como immediate mode, acaba por revelar-se mais apropriado para implemetação em hardware do que o retained mode do PHIGS com seu grafo de cena. Em 1991, Mark Segal e Kurt Akeley, da SGI, iniciam o desenvolvimento de uma versão aberta do IRIS GL como tentativa de criar um novo padrão de indústria. Para isso, removem o código proprietário e modificam a API de modo a torná-la independente do sistema de janelas e de dispositivos de entrada. Deste desenvolvimento surge, em 1992, o OpenGL (Open Graphics Library) (Woo et al. 1999), que rapidamente ocupa o lugar do PHIGS como API padrão para gráficos 3D. Desde então, revisões periódicas do OpenGL são feitas de modo a suportar os aprimoramentos mais recentes do hardware gráfico. O aspecto minimalista e de facilidade de uso do IRIS GL continuam presentes no OpenGL. Essas características fizeram  e ainda fazem  do OpenGL uma das APIs gráficas 3D mais populares em aplicações multiplataforma. InfiniteReality No início da década de 1990, as estações gráficas de alto desempenho suportavam apenas um número reduzido de características do OpenGL, sendo o restante simulado em software. O sistema RealityEngine (Akeley 1993), lançado em 1992 pela SGI, foi o primeiro hardware gráfico capaz de oferecer suporte para todas as etapas de transformação e iluminação da versão 1.0 do OpenGL, incluindo o mapeamento de texturas 2D com mipmapping (uma técnica de pré-filtragem de texturas) e antialiasing (suavização de serrilhado). A arquitetura foi sucedida em 1996 pelo InfiniteReality (Montrym et al. 1997), desenvolvido especificamente para o OpenGL. Dependendo da configuração final, o custo de uma estação gráfica baseada no InfiniteReality poderia ser superior a 1 milhão de dólares. Uma demonstração da SGI sobre as capacidades de renderização em tempo real do InfiniteReality em 1996 pode ser vista no vídeo de YouTube Silicon Graphics - Onyx Infinite Reallity 50FPS. A partir de 1995, surgem nos PCs as primeiras placas de vídeo com aceleração de processamento gráfico 3D, também chamadas de aceleradoras gráficas 3D. As primeiras aceleradoras gráficas eram capazes de realizar apenas a varredura de linhas não texturizadas e, em alguns casos, tinham desempenho similar ao código de máquina otimizado na CPU. Por outro lado, logo essas limitações foram vencidas e surgiram placas eficientes e com suporte a mapeamento de textura, impulsionadas pelo emergente mercado de jogos de computador. Enquanto as primeiras estações gráficas da SGI implementavam um pipeline completo de transformação de vértices, ainda que sem suporte à texturização, as aceleradoras gráficas para PCs, produzidas por empresas como Diamond Multimedia, S3 Graphics (extinta em 2003), Trident Microsystems (extinta em 2012), Matrox Graphics e NVIDIA, ofereciam suporte ao mapeamento de texturas, porém sem transformação de geometria ou processamento de iluminação. A 3Dfx Interactive (adquirida em 2000 pela NVIDIA), com a sua série de aceleradoras Voodoo Graphics lançadas a partir de 1996, ampliou enormemente o uso do hardware gráfico em jogos de computador. As placas Voodoo eram capazes de exibir triângulos texturizados com mipmapping e filtragem bilinear (figura 2.19). Entretanto, o hardware ainda dependia da CPU para preparar os triângulos para a rasterização. Os triângulos só poderiam ser processados pelo hardware gráfico se fossem previamente convertidos em trapézios degenerados, alinhados em coordenadas da tela. Figura 2.19: Jogo Carmageddon II: Carpocalypse Now (Stainless Games) em uma placa gráfica 3Dfx Voodoo, de 1998 (fonte). Outra limitação das aceleradoras gráficas nesse período era a falta de suporte adequado a uma API padrão de indústria. A arquitetura de tais placas era incompatível com aquela especificada no OpenGL e fazia com que os desenvolvedores precisassem recorrer a APIs proprietárias, como a API Glide da 3Dfx (3Dfx 1997). As placas da 3Dfx foram populares até o final da década quando então o OpenGL e a API Direct3D, da Microsoft, começaram a ser suportados de maneira eficiente pelas placas de concorrentes como a ATI Technologies (adquirida em 2006 pela AMD), Matrox e NVIDIA. Na segunda metade da década, o desenvolvimento das placas gráficas para PCs acompanhou a evolução da API Direct3D. Em 1995, a Microsoft lança o Windows 95 Games SDK, um conjunto de APIs de baixo nível para o desenvolvimento de jogos e aplicações multimídia de alto desempenho no Windows. Em 1996, o Windows 95 Games SDK muda de nome para DirectX e sua segunda e terceira versões são disponibilizadas em junho e setembro desse mesmo ano. Entre as APIs contidas no DirectX, o Direct3D é concebido como uma API para hardware gráfico compatível com o pipeline de processamento do OpenGL. Embora no início o Direct3D fosse criticado por sua arquitetura demasiadamente confusa em comparação com o OpenGL (como relatado por John Carmack, da id Software, em sua carta sobre o OpenGL), eventualmente torna-se a API mais utilizada em jogos uma vez que novas versões começam a ser distribuídas em intervalos menores que aqueles do OpenGL. A revisão do OpenGL dependia do ARB (Architecture Review Board): um consórcio formado por representantes de diversas empresas de hardware e software que se reuniam periodicamente para propor e aprovar mudanças na API. O Direct3D, por ser proprietário, respondia melhor ao rápido desenvolvimento das placas gráficas naquele momento e passou a ditar a especificação das futuras aceleradoras gráficas voltadas ao mercado de jogos. Em 1997 é anunciado o DirectX 5 (o DirectX 4 nunca chegou a ser lançado), acompanhando as primeiras placas capazes de renderizar triângulos, tais como a ATI Rage Pro e NVIDIA Riva 128 (figura 2.20). A Riva 128 não alcançava a mesma qualidade de imagem produzida pelas placas da 3Dfx, mas ultrapassava as placas Voodoo em várias medições de desempenho. Ainda assim, a aceleração de processamento de geometria era inexistente e a CPU era responsável por calcular as transformações geométricas e interpolações de atributos de vértices ao longo das arestas para cada triângulo transformado. Figura 2.20: Placa gráfica Diamond com o chip NVIDIA Riva 128, de 1997 (fonte). Em 1998 é lançado o DirectX 6 e surgem as primeiras aceleradoras gráficas capazes de interpolar atributos ao longo de arestas. Nessa geração de hardware gráfico, a CPU ainda era responsável pela transformação e iluminação de cada vértice, mas agora bastava enviar à placa gráfica os atributos de cada vértice em vez de atributos interpolados para cada aresta de cada triângulo. Um ano depois, o DirectX 7 é lançado com suporte para aceleração em hardware de transformação e iluminação (figura 2.21). As primeiras placas compatíveis com DirectX 7 surgiriam no ano seguinte. Figura 2.21: Demonstração do benchmark 3DMark2000 (UL) usando DirectX 7 com transformação de geometria e cálculo de iluminação em hardware. 2000 A década de 2000 presencia o que pode ser considerado uma revolução no uso do hardware gráfico: surgem os primeiros processadores gráficos programáveis (programmable GPUs) capazes de alterar o comportamento do pipeline de renderização sem depender da CPU. Isso torna possível a implementação de diversos novos modelos de reflexão para além do tradicional modelo de BlinnPhong disponível no pipeline de função fixa (pipeline não programável). Além disso, a capacidade de programar processadores gráficos possibilita a implementação de um incontável número de novos efeitos visuais. As GPUs programáveis tornam-se muito populares em PCs, impulsionadas pelas exigentes demandas do mercado de jogos. Ao mesmo tempo, tornam-se muito flexíveis e poderosas não só para jogos, mas também para processamento de propósito geral. O hardware gráfico programável surge no início de 2001 com o lançamento da GPU NVIDIA GeForce 3 (figura 2.22), inicialmente para o computador Apple Macintosh (Lindholm, Kilgard, and Moreton 2001). Figura 2.22: GPU NVIDIA GeForce (fonte). No início de 2001, durante o evento MacWorld Expo Tokyo, é exibido o curta metragem Luxo Jr. produzido pela Pixar em 1986. Entretanto, desta vez o filme é renderizado em tempo real em um computador equipado com uma GeForce 3. Steve Jobs, então CEO da Apple, observou: Há 15 anos, o que levava 75 horas para produzir cada segundo de vídeo, está agora sendo renderizado em tempo real na GeForce 3.  Steve Jobs (Morris 2001) Mais tarde, as potencialidades de uma GPU similar seriam exibidas durante uma demonstração de tecnologia na conferência SIGGRAPH 2001: uma versão interativa do filme Final Fantasy: The Spirits Within, de Hironobu Sakaguchi, renderizada em tempo real em uma GPU NVIDIA Quadro DCC (Sakaguchi and Aida 2001). Neste evento, a NVIDIA destacou que o desempenho em operações em ponto flutuante utilizadas para desenhar apenas um quadro do filme era superior ao poder computacional total de um supercomputador Cray (tradicional fabricante de supercomputadores, adquirida em 2019 pela Hewlett Packard Enterprise) naquele momento. Ao longo da década, as GPUs de baixo custo (na faixa de 100 a 250 dólares), produzidas por empresas como NVIDIA e ATI, desbancam as estações gráficas de alto desempenho ainda baseadas em tecnologias da década anterior. As placas gráficas para computadores pessoais ultrapassam rapidamente as capacidades computacionais de sistemas como o RealityEngine da SGI, mas ao mesmo tempo com uma redução de custo superior a 90% em comparação com esses sistemas. De acordo com a Lei de Moore, e observando a diminuição do custo das CPUs nesse período, tais placas deveriam custar muito mais, em torno de 15 mil dólares. Esse avanço expressivo das GPUs é implacável com as fabricantes de estações gráficas. Em 2009, a SGI decreta falência. As APIs Direct3D (em 2006) e OpenGL (em 2009) anunciam a descontinuidade do suporte ao pipeline de função fixa. Com isso, as aplicações migram definitivamente ao uso dos shaders: programas que modificam o comportamento das etapas programáveis do pipeline, como o processamento de geometria e fragmentos (amostras de primitivas rasterizadas). Com o aumento do conjunto de instruções suportadas nas GPUs, percebe-se que é possível usar o hardware gráfico para processamento de propósito geral em tarefas como simulação de dinâmica de fluidos, operações em bancos de dados, modelagem de dinâmica molecular, criptoanálise, entre muitas outras tarefas capazes de se beneficiar de processamento paralelo. O termo GPGPU (General-Purpose Computation on GPUs) é utilizado para se referir a esse uso. A figura 2.23 mostra um exemplo atual de aplicação de GPGPU para a modelagem de DNA. Figura 2.23: Ligante de sulco menor do DNA, modelado através de GPGPU com o software Abalone. (fonte). Em 2007, a NVIDIA lança a plataforma CUDA (Compute Unified Device Architecture), composta por um conjunto de ferramentas/bibliotecas e API de GPGPU para placas da NVIDIA. A plataforma é muito popular atualmente, impulsionada pelo crescimento das aplicações em ciência de dados e aprendizado de máquina. Influenciada pelo CUDA, surgem em 2009 outras plataformas como o DirectCompute, da Microsoft (como parte do Direct3D 11), e a especificação aberta OpenCL do Khronos Group, mesmo consórcio de indústrias que mantém o OpenGL. As primeiras oficinas e conferências sobre GPGPU, como a ACM GPGPU e a GPU Technology Conference (GTC), da NVIDIA, surgem neste período. 2010 A partir da década de 2010, a aceleração de gráficos 3D se expande e se populariza nos dispositivos móveis. O uso de multitexturização (uso de vários estágios de texturização) e de técnicas como normal mapping, cube mapping (para simulação de superfícies reflexivas) e shadow mapping (para simulação de sombras) torna-se comum em aplicações gráficas interativas. Em 2011, o Khronos Group anuncia o padrão WebGL, ampliando a possibilidade de uso de aceleração de gráficos 3D nos navegadores. Na segunda metade da década, a renderização baseada em física, do inglês physically based rendering (PBR), começa a ser empregada em jogos de computador e em consoles. O jogo Alien: Isolation (Creative Assembly), de 2014, é um dos primeiros a explorar esta tecnologia. Renderização baseada em física A renderização baseada em física, ou PBR, procura simular de forma fisicamente correta a interação da luz com os diferentes materiais de uma cena (Pharr, Jakob, and Humphreys 2016). Em PBR, os materiais são descritos por informações de detalhes de microsuperfície, frequentemente obtidas por fotogrametria e armazenadas em mapas de textura. O vídeo a seguir (em inglês) apresenta uma breve introdução ao conceito de PBR e sua implementação usando o Unity Standard Shader do motor de jogo Unity: Em 2016 são lançadas novas gerações de headsets de realidade virtual como o Oculus Rift e HTC Vive, que elevam as exigências de hardware gráfico para jogos que utilizam essa tecnologia. Também em 2016, o Khronos Group lança a API Vulkan como uma API de baixo nível com maior capacidade de explorar os recursos gráficos e de computação das novas gerações de GPUs. Em 2018 é incorporado ao Direct3D 12 o DirectX Raytracing (DXR), que introduz um novo pipeline gráfico destinado ao traçado de raios em tempo real. Ainda em 2018, as GPUs NVIDIA RTX série 20 são as primeiras a suportar essa tecnologia. 2020 Em 2020, as GPUs NVIDIA RTX série 20 são sucedidas pela série 30, ampliando ainda mais a possibilidade de uso de traçado de raios em tempo real. A AMD lança as GPUs da série Radeon RX 6000, também com suporte a traçado de raios. Além disso, uma API de traçado de raios é incorporada ao Vulkan como o conjunto de extensões Vulkan Ray Tracing. Ainda estamos no início da década, mas o aumento da capacidade de processamento e largura de banda de memória do hardware gráfico deve continuar a empurrar os limites do que é possível renderizar em tempo real. Efeitos atmosféricos, texturas de altíssima resolução e simulações de iluminação global4 em tempo real devem se popularizar nos próximos anos. O vídeo de demonstração a seguir (em inglês) exibe algumas das capacidades de processamento de iluminação global em tempo real do motor de jogo Unreal Engine 5 no console PlayStation 5. O lançamento do Unreal Engine 5 está previsto para o final de 2021: Referências "],["firstapp.html", "2.3 Primeiro programa", " 2.3 Primeiro programa Nesta seção seguiremos um passo a passo de construção de um primeiro programa com a biblioteca ABCg. Será o nosso Hello, World! similar ao exemplo da ABCg mostrado na seção 1.5, mas sem o triângulo colorido renderizado com OpenGL. Configuração inicial Faça uma cópia (ou fork) de https://github.com/hbatagelo/abcg.git. Assim você poderá modificar livremente a biblioteca e armazená-la em seu repositório pessoal. Como a ABCg já tem um projeto de exemplo chamado helloworld, vamos chamar o nosso de firstapp. Em abcg/examples, crie o subdiretório abcg/examples/firstapp. A escolha de deixar o projeto como um subdiretório de abcg/examples é conveniente pois podemos replicar a configuração de abcg/examples/helloworld. Assim, bastará construir a ABCg e o nosso projeto será automaticamente construído como um exemplo adicional da biblioteca. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(firstapp). O conteúdo ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) Dessa forma o CMake incluirá o subdiretório firstapp na busca de um script CMakeLists.txt contendo a configuração do projeto. Crie o arquivo abcg/examples/firstapp/CMakeLists.txt. Edite-o com o seguinte conteúdo: project(firstapp) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) O comando project na primeira linha define o nome do projeto. Em seguida, add_executable define que o executável terá o mesmo nome definido em project e será gerado a partir dos fontes main.cpp e openglwindow.cpp (não é necessário colocar os arquivos .h ou .hpp). Por fim, a função enable_abcg() configura o projeto para usar a ABCg. Essa função é definida em abcg/cmake/ABCg.cmake, que é um script CMake chamado a partir do CMakeLists.txt do diretório raiz. Em abcg/examples/firstapp, crie os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp. Vamos editá-los a seguir. main.cpp Em main.cpp definiremos a função main: #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(std::move(window)); return 0; } Nas duas primeiras linhas são incluídos os arquivos de cabeçalho: abcg.hpp faz parte da ABCg e dá acesso às principais classes e funções da biblioteca; openglwindow.hpp é o arquivo que acabamos de criar e que terá a definição de uma classe OpenGLWindow responsável pelo comportamento da janela da aplicação; Na linha 6 é definido um objeto app da classe abcg::Application, responsável pelo gerenciamento da aplicação; Na linha 9 é criado um ponteiro inteligente (smart pointer) window para uma instância de OpenGLWindow; Na linha 10 é definido o título da janela. setWindowSettings é um método de abcg::OpenGLWindow (classe base de OpenGLWindow) e recebe uma estrutura abcg::WindowSettings contendo as configurações da janela; Na linha 13, o método abcg::Application::run é chamado para inicializar os subsistemas da SDL, inicializar a janela recém-criada e entrar no laço principal da aplicação. Observação Todas as classes e funções da ABCg fazem parte do namespace abcg. O código acima usa diferentes conceitos de C++ moderno: A palavra-chave auto para dedução automática do tipo de variável a partir de sua inicialização; A criação de um ponteiro inteligente com std::make_unique; O uso de inicialização uniforme com chaves; O uso de inicializadores designados para inicializar o membro title da estrutura abcg::WindowSettings diretamente através de seu nome; O uso de std::move para indicar que o ponteiro inteligente window está sendo transferido (e não copiado) para abcg::Application. Internamente a ABCg usa tratamento de exceções. As exceções são lançadas como objetos da classe abcg::Exception, derivada de std::exception. Vamos alterar um pouco o código anterior para capturar as exceções que possam ocorrer e imprimir no console a mensagem de erro correspondente. O código final de main.cpp ficará assim: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } O que mudou aqui é que o código anterior foi colocado dentro do escopo try de um bloco try...catch. No escopo catch, a função fmt::print imprime no erro padrão (stderr) a mensagem de erro associada com a exceção capturada. fmt::print faz parte da biblioteca {fmt}, incluída pelo cabeçalho fmt/core.h. Ela permite a formatação de strings usando uma sintaxe parecida com as f-strings da linguagem Python5. openglwindow.hpp No arquivo openglwindow.hpp vamos definir a classe OpenGLWindow que será responsável pelo gerenciamento do conteúdo da janela da aplicação: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow {}; #endif Observe que nossa classe OpenGLWindow é derivada de abcg::OpenGLWindow, que faz parte da ABCg. abcg::OpenGLWindow gerencia uma janela capaz de renderizar gráficos com a API OpenGL. A classe possui um conjunto de métodos virtuais que podem ser substituídos pela classe derivada de modo a alterar o comportamento da janela. O comportamento padrão é desenhar a janela com fundo preto, com um contador de FPS (frames per second, ou quadros por segundo) sobreposto no canto superior esquerdo da janela, e um botão no canto inferior esquerdo para alternar entre tela cheia e modo janela (com atalho pela tecla F11). O contador e o botão são gerenciados pela biblioteca Dear ImGui (no restante do texto vamos chamá-la apenas de ImGui). Por enquanto nossa classe não faz nada de especial. Ela só deriva de abcg::OpenGLWindow e não define nenhum método ou atributo. Mesmo assim, já podemos construir a aplicação. Experimente fazer isso. Na linha de comando, use o script build.sh (Linux/macOS) ou build.bat (Windows). Se você estiver no Visual Studio Code, abra a pasta abcg pelo editor, use a opção de configuração do CMake e então construa o projeto (F7). O executável será gerado em abcg/build/bin/firstapp. Da forma como está, a aplicação mostrará uma janela com fundo preto e os dois controles de GUI (widgets) mencionados anteriomente. Isso acontece porque OpenGLWindow não está substituindo nenhum dos métodos virtuais de abcg::OpenGLWindow. Todo o comportamento está sendo definido pela classe base: Vamos alterar o conteúdo e o comportamento dessa nossa janela OpenGLWindow. Imitaremos o comportamento do projeto helloworld que cria uma pequena janela da ImGui. Modifique openglwindow.hpp para o código a seguir: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; private: std::array&lt;float, 4&gt; m_clearColor{0.906f, 0.910f, 0.918f, 1.0f}; }; #endif initializeGL, paintGL e paintUI substituem métodos virtuais de abcg::OpenGLWindow. A palavra-chave override é opcional, mas deixa explícito que os métodos são substituições dos métodos da classe base: initializeGL é onde colocaremos os comandos de inicialização do estado da janela e do OpenGL. Internamente a ABCg chama essa função apenas uma vez no início do programa, após ter inicializado os subsistemas da SDL e o OpenGL. paintGL é onde colocaremos todas as funções de desenho do OpenGL. Internamente a ABCg chama essa função continuamente no laço principal da aplicação, uma vez para cada quadro (frame) de exibição. Por exemplo, na imagem acima, paintGL estava sendo chamada a uma média de 3988.7 vezes por segundo; paintUI é onde colocaremos todas as funções de desenho de widgets da ImGui (botões, menus, caixas de seleção, etc). Internamente, paintUI é chamado sempre que paintGL é chamado; m_clearColor é um arranjo de quatro valores float entre 0 e 1. Esses valores definem a cor de fundo da janela (neste caso, um cinza claro). Observação Poderíamos ter definido m_clearColor da seguinte forma, mais familiar aos programadores em C: float m_clearColor[4] = {0.906f, 0.910f, 0.918f, 1.0f}; Entretanto, em C++ o std::array é a forma recomendada e mais segura de trabalhar com arranjos. openglwindow.cpp Em openglwindow.cpp definiremos os métodos virtuais substituídos: #include &lt;fmt/core.h&gt; #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; void OpenGLWindow::initializeGL() { auto windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } void OpenGLWindow::paintGL() { // Set the clear color abcg::glClearColor(m_clearColor[0], m_clearColor[1], m_clearColor[2], m_clearColor[3]); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::paintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::paintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } No início do arquivo, observe que é incluído o cabeçalho imgui.h para o uso das funções da ImGui. Vejamos com mais atenção o trecho com a definição de OpenGLWindow::initializeGL: void OpenGLWindow::initializeGL() { auto windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } Na linha 8, windowSettings é uma estrutura abcg::WindowSettings retornada por abcg::OpenGLWindow::getWindowSettings() com as configurações da janela. Na linha 9, fmt::print imprime no console o tamanho da janela6. Observe agora o trecho com a definição de OpenGLWindow::paintGL: void OpenGLWindow::paintGL() { // Set the clear color abcg::glClearColor(m_clearColor[0], m_clearColor[1], m_clearColor[2], m_clearColor[3]); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } Aqui são chamadas duas funções do OpenGL: glClearColor e glClear. glClearColor é utilizada para determinar a cor que será usada para limpar a janela7. A função recebe quatro parâmetros do tipo float (red, green, blue, alpha), que correspondem a componentes de cor RGB e um valor adicional de opacidade (alpha). Esse formato de cor é chamado de RGBA. Os valores são fixados (clamped) no intervalo \\([0,1]\\) em ponto flutuante. glClear, usando como argumento a constante GL_COLOR_BUFFER_BIT, limpa a janela com a cor especificada na última chamada de glClearColor. Em resumo, nosso paintGL limpa a tela com a cor RGBA especificada em m_clearColor. Importante As funções do OpenGL são prefixadas com as letras gl; As constantes do OpenGL são prefixadas com GL_. A versão mais recente do OpenGL é a 4.6. A documentação de cada versão está disponível em https://www.khronos.org/registry/OpenGL/. Neste curso, usaremos as funções do OpenGL que são comuns ao OpenGL ES 3.0 de modo a manter compatibilidade com o WebGL 2.0. Assim conseguiremos fazer aplicações que rodam tanto no desktop quanto no navegador. Observação Na ABCg, podemos usar as funções gl dentro do namespace abcg de modo a rastrear erros do OpenGL com o sistema de tratamento de exceções da ABCg. Por exemplo, ao escrevermos abcg::glClear no lugar de glClear, estamos na verdade chamando uma função wrapper que verifica automaticamente se a chamada da função OpenGL é válida. Se algum erro ocorrer, uma exceção é lançada e capturada pelo catch que implementamos na função main. A mensagem de erro (a string em exception.what()) inclui a descrição do erro, o nome do arquivo, o nome da função e o número da linha do código onde o erro foi detectado. Isso pode ser muito útil para a depuração de erros do OpenGL. Por isso, sempre que possível, prefixe as funções do OpenGL com abcg::. A verificação automática de erros do OpenGL é habilitada somente quando a aplicação é compilada no modo Debug. Não há sobrecarga nas chamadas das funções do OpenGL com o namespace abcg quando a aplicação é compilada em modo Release. Agora vamos à definição de OpenGLWindow::paintUI, responsável pelo desenho da interface usando a ImGui: void OpenGLWindow::paintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::paintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Na linha 23 é chamado o método paintUI da classe base. O paintUI da classe base mostra o medidor de FPS e o botão para alternar a tela cheia. Na linha 28 é criada uma janela da ImGui com o título Hello, First App! A partir desta linha, até a linha 47, todas as chamadas a funções da ImGui criam widgets dentro dessa janela. Apenas para isso ficar mais explícito, todo o código está dentro do escopo delimitado pelas chaves nas linhas 26 e 48. Na linha 32 é criado um texto estático mostrando o tamanho atual da janela. Na linha 37 é criado um slider horizontal que pode variar de 0 a 1 em ponto flutuante. O valor do slider é armazenado em f. A variável f é declarada como static para que seu estado seja retido entre as chamadas de paintUI (outra opção é declarar a variável como membro da classe). Na linha 40 é criado um widget de edição de cor para alterar os valores de m_clearColor. Na linha 43 é criado mais um texto estático com informações de FPS extraídas de ImGui::GetIO().Framerate. Esse código é praticamente o mesmo do Hello, World! Construa a aplicação para ver o resultado: A seguir temos alguns exemplos de uso de outros widgets da ImGui. Experimente incluir esses trechos de código no paintUI: Botões: // 100x50 button if (ImGui::Button(&quot;Press me!&quot;, ImVec2(100, 50))) { fmt::print(&quot;Button pressed.\\n&quot;); } // Nx50 button, where N is the remaining width available ImGui::Button(&quot;Press me!&quot;, ImVec2(-1, 50)); // See also IsItemHovered, IsItemActive, etc if (ImGui::IsItemClicked()) { fmt::print(&quot;Button pressed.\\n&quot;); } Checkbox: static bool enabled{true}; ImGui::Checkbox(&quot;Some option&quot;, &amp;enabled); fmt::print(&quot;The checkbox is {}\\n&quot;, enabled ? &quot;enabled&quot; : &quot;disabled&quot;); Combo box: static std::size_t currentIndex{}; std::vector&lt;std::string&gt; comboItems{&quot;AAA&quot;, &quot;BBB&quot;, &quot;CCC&quot;}; if (ImGui::BeginCombo(&quot;Combo box&quot;, comboItems.at(currentIndex).c_str())) { for (auto index{0u}; index &lt; comboItems.size(); ++index) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; // Set the initial focus when opening the combo (scrolling + keyboard // navigation focus) if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } fmt::print(&quot;Selected combo box item: {}\\n&quot;, comboItems.at(currentIndex)); Menu (em uma janela de tamanho fixo e com o flag adicional ImGuiWindowFlags_MenuBar para permitir o uso da barra de menu): ImGui::SetNextWindowSize(ImVec2(300, 100)); auto flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize}; ImGui::Begin(&quot;Window with menu&quot;, nullptr, flags); { bool save{}; static bool showCompliment{}; // Hold state // Menu Bar if (ImGui::BeginMenuBar()) { // File menu if (ImGui::BeginMenu(&quot;File&quot;)) { ImGui::MenuItem(&quot;Save&quot;, nullptr, &amp;save); ImGui::EndMenu(); } // View menu if (ImGui::BeginMenu(&quot;View&quot;)) { ImGui::MenuItem(&quot;Show Compliment&quot;, nullptr, &amp;showCompliment); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (save) { // Save file... } if (showCompliment) { ImGui::Text(&quot;You&#39;re a beautiful person.&quot;); } } ImGui::End(); Mais sliders: static std::array pos2d{0.0f, 0.0f}; ImGui::SliderFloat2(&quot;2D position&quot;, pos2d.data(), 0.0, 50.0); static std::array pos3d{0.0f, 0.0f, 0.0f}; ImGui::SliderFloat3(&quot;3D position&quot;, pos3d.data(), -1.0, 1.0); Infelizmente, a ImGui não tem um manual com exemplos de uso de todos os widgets suportados. A melhor referência atualmente é o código da função ImGui::ShowDemoWindow em abcg/external/imgui/imgui_demo.cpp. Essa função cria uma janela de demonstração contendo uma grande variedade de exemplos de uso de widgets e recursos da ImGui. No exemplo Hello, World! tal janela é exibida quando o checkbox Show demo window está ativado. Observação A ImGui é uma biblioteca de GUI que trabalha em modo imediato, isto é, não retém o estado das janelas e widgets (o Im de ImGui vem de immediate mode). Isso significa que, sempre que paintUI é chamado, a GUI é redefinida por completo. O gerenciamento da persistência de estado deve ser feito pelo usuário, por exemplo, através de variáveis estáticas ou variáveis membros da classe. Exercício Usando o projeto firstapp como base, faça um Jogo da Velha com interface composta de widgets da ImGui: Simule o tabuleiro do jogo com um arranjo de 3x3 botões. Inicialmente deixe os botões sem texto. Cada vez que um botão for pressionado, substitua-o por um X ou O de acordo com o turno do jogador; Use um widget de texto estático para mostrar informações como o resultado do jogo e o turno atual; Inclua um menu com uma opção para reiniciar o jogo. Explore diferentes funções da biblioteca, tais como: ImGui::Columns para fazer arranjos de widgets; ImGui::Spacing para adicionar espaçamentos verticais; ImGui::SameLine para criar widgets lado-a-lado e ajustar o espaçamento horizontal; ImGui::Separator() para desenhar linhas de separação. Um exemplo é dado a seguir: Compilando para WebAssembly Para compilar para WebAssembly basta usar o script build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Apenas certifique-se de habilitar antes as variáveis de ambiente do SDK do Emscripten como fizemos na seção 1.5. Após a construção do projeto, os arquivos resultantes (firstapp.js e firstapp.wasm) estarão no subdiretório public. Para usá-los, precisamos de um arquivo HTML. Faça uma cópia de um dos arquivos HTML já existentes (helloworld.html, full_window.html ou full_window_console.html). No final do arquivo copiado, mude a string src=\"helloworld.js\" para src=\"firstapp.js\", assim: &lt;script async type=&quot;text/javascript&quot; src=&quot;firstapp.js&quot;&gt;&lt;/script&gt; Para testar, monte o servidor local com runweb.sh ou runweb.bat e abra o arquivo HTML em http://localhost:8080/. Dica Disponibilize o conteúdo web de seus projetos no GitHub Pages para formar um portfólio de atividades feitas no curso: Na sua conta do GitHub, crie um repositório com visibilidade pública. Pode ser seu próprio fork da ABCg. No diretório raiz, crie um subdiretório firstapp com os arquivos firstapp.*, mas renomeie firstapp.html para index.html; Nas configurações do repositório no GitHub, habilite o GitHub Pages informando o branch que será utilizado (por exemplo, main). O conteúdo estará disponível em https://username.github.io/reponame/firstapp/ onde username e reponame são respectivamente seu nome de usuário e o nome do repositório. Ao longo do curso, suba seus projetos nesse repositório. No diretório raiz, mantenha um index.html com a descrição do portfólio e o link para cada página de projeto. Um subconjunto da {fmt} foi incorporado à biblioteca de formatação de texto no C++20, mas ainda não há suporte equivalente ao fmt::print (impressão formatada com saída padrão). O tamanho padrão para uma aplicação desktop é 800x600. Na versão para web, a janela pode ser redimensionada de acordo com a regra CSS do elemento canvas do HTML5. Mais precisamente, glClearColor define a cor que será utilizada para limpar os buffers de cor do framebuffer. Veremos mais sobre o conceito de framebuffer nos próximos capítulos. "],["graphicssystem.html", "3 Sistemas gráficos", " 3 Sistemas gráficos Um sistema gráfico é um sistema computacional com capacidade de processar dados para gerar imagens em um dispositivo de exibição. Em sistemas interativos, a interação com os modelos de dados gráficos se dá através de um ou mais dispositivos de entrada. Assim, de maneira geral, um sistema gráfico é composto pelos seguintes componentes: Dispositivos de entrada: teclado e dispositivos apontadores como mouse, touch pad, touch screen, graphics tablet, trackball, joystick, gamepad, entre outros. Processadores: CPU (central processing unit), GPU (graphics processing unit) e seus subsistemas (controladores, memórias e barramentos) necessários ao processamento dos modelos de dados e conversão em representações visuais; Dispositivos de saída: monitores e telas de LCD (liquid-crystal display), OLED (organic light-emitting diodes), CRT (cathode-ray tube) ou plasma, entre outros dispositivos de exibição. A definição de um sistema gráfico com esses componentes é bastante geral e pode incluir tanto as estações gráficas de alto desempenho equipadas com várias GPUs, quanto os computadores pessoais sem GPU dedicada. Pode incluir também os consoles de videogames, smartphones, smartwatches, smart TVs, GPSs, entre outros dispositivos com poder computacional suficiente para produzir saída em um dispositivo de exibição (figura 3.1). Figura 3.1: Exemplos de sistemas gráficos. Em sistemas gráficos atuais, o papel principal do processador gráfico é realizar a conversão de primitivas geométricas em uma imagem digital. Por exemplo, para visualizar na tela um segmento de reta definido por dois vértices, o processador deve gerar um conjunto de pixels que aproxime o aspecto do segmento visto pelo observador. Esse processo de determinar a cor dos pixels da imagem pode ser tão simples quanto avaliar a equação de uma reta \\(y=mx+b\\) para um conjunto discreto de valores de \\(x\\) em coordenadas na tela, ou tão complexo quanto gerar uma imagem fotorrealista de uma cena virtual composta de milhares de objetos. Nos sistemas gráficos mais simples, sem aceleração de gráficos em hardware, a CPU é responsável por todo o processamento gráfico e não há distinção entre a memória da CPU e a memória de processamento gráfico. Em sistemas com GPU, a GPU pode estar localizada na placa mãe do computador, integrada com o chip da CPU (como o Intel HD Graphics comum nos computadores com processador Intel), ou situada em uma placa de vídeo com memória dedicada. Em sistemas de alto desempenho, várias GPUs dedicadas podem ser combinadas para dividir a carga de processamento usando tecnologias como a SLI da NVIDIA e CrossFire da AMD. Também é comum sistemas que combinam GPUs dedicadas e GPUs integradas, em diferentes configurações. Em todos os sistemas gráficos, a saída é uma imagem digital armazenada em uma área de memória chamada de framebuffer. Essa área de memória é utilizada pelo controlador gráfico para atualizar o dispositivo de exibição (figura 3.2). Figura 3.2: Sistema gráfico com framebuffer. O restante do capítulo está organizado como a seguir: A seção 3.1 descreve as duas principais formas de representação de gráficos utilizadas em CG: a representação vetorial e a representação matricial; A seção 3.2 apresenta conceitos e tecnologias relacionadas aos dispositivos de entrada e saída dos sistemas gráficos; A seção 3.3 descreve conceitos fundamentais do uso de framebuffers. A seção 3.4 apresenta uma atividade prática de um primeiro programa que renderiza primitivas com o OpenGL. "],["vectorxraster.html", "3.1 Vetorial x matricial", " 3.1 Vetorial x matricial Em computação gráfica, é comum trabalharmos com dois tipos de representações de gráficos: a representação vetorial, utilizada na descrição de formas 2D e 3D compostas por primitivas geométricas, e a representação matricial, utilizada em imagens digitais e definição de texturas. O processo de converter representações vetoriais em representações matriciais desempenha um papel central no pipeline de processamento gráfico, uma vez que a representação matricial é a representação final de uma imagem nos dispositivos de exibição. Essa conversão matricial, também chamada de rasterização (raster conversion ou scan conversion), é implementada em hardware nas GPUs atuais. A figura 3.3 ilustra o resultado da conversão de uma representação vetorial em representação matricial. As formas geométricas à esquerda estão representadas originalmente no formato SVG (Scalable Vector Graphics), que é o formato padrão de gráficos vetoriais nos navegadores Web. A imagem à direita é um arranjo bidimensional de valores de cor, resultado da renderização das formas SVG em uma imagem digital (neste caso, uma imagem de baixa resolução). Figura 3.3: Rasterização de um círculo e triângulo. Observação A figura 3.3 é apenas ilustrativa. Rigorosamente falando, a imagem da esquerda também está no formato matricial. O navegador converte automaticamente o código SVG em comandos da API gráfica que fazem com que a GPU renderize a imagem que vemos na tela. A rasterização ocorre durante este processamento. A imagem à direita não precisa passar pelo processo de renderização pois já é uma imagem digital em seu formato nativo. Representação vetorial Na representação vetorial, os gráficos são descritos em termos de primitivas geométricas. Por exemplo, o formato SVG é um formato de descrição de gráficos vetoriais 2D através de sequências de comandos de desenho. Uma forma 2D pode ser descrita através da definição de um caminho (path) composto por uma sequência de passos de movimentação de uma caneta virtual sobre um plano. Os principais passos utilizados são comandos do tipo MoveTo, LineTo e ClosePath: MoveTo (denotado por M ou m em SVG8) move a caneta virtual para uma nova posição na área de desenho, como se ela fosse levantada da superfície e posicionada em outro local; LineTo (L ou l) traça um segmento de reta da posição atual da caneta até uma nova posição, que passa a ser a nova posição da caneta; Em uma sequência de comandos LineTo, o comando ClosePath (Z ou z) traça um segmento de reta que fecha o caminho da posição atual da caneta ao ponto inicial. Observe o código SVG a seguir que resulta no desenho do triângulo visto mais abaixo: &lt;svg width=&quot;250&quot; height=&quot;210&quot;&gt; &lt;path d=&quot;M125 0 L0 200 L250 200 Z&quot; stroke=&quot;black&quot; fill=&quot;none&quot; /&gt; &lt;/svg&gt; No rótulo &lt;svg&gt;, os atributos width=\"250\" e height=\"210\" definem que a área de desenho tem largura 250 e altura 210. Por padrão, a origem fica no canto superior esquerdo. O eixo horizontal (\\(x\\)) é positivo para a direita, e o eixo vertical (\\(y\\)) é positivo para baixo. O atributo d do rótulo &lt;path&gt; contém os comandos de desenho do caminho. M125 0 move a caneta virtual para a posição (125,0). Em seguida, L0 200 traça um segmento da posição atual até a posição (0, 200), que passa a ser a nova posição da caneta. L250 200 traça um novo segmento até (250, 200). O comando Z fecha o caminho até a posição inicial em (125, 0), completando o triângulo. O atributo stroke=\"black\" define a cor do traço como preto, e fill=\"lightgray\" define a cor de preenchimento como cinza claro: O formato SVG também suporta a descrição de curvas, arcos, retângulos, círculos, elipses, entre outras primitivas geométricas. Comandos similares são suportados em outros formatos de gráficos vetoriais, como o EPS (Encapsulated PostScript), PDF (Portable Document Format), AI (Adobe Illustrator Artwork) e DXF (AutoCAD Drawing Exchange Format). Representação vetorial no OpenGL No OpenGL, a representação vetorial é utilizada para definir a geometria que será processada durante a renderização. Todas as primitivas geométricas são definidas a partir de vértices que representam posições no espaço, além de atributos definidos pelo programador (por exemplo, a cor do vértice). Esses vértices são armazenados em arranjos ordenados que são processados em um fluxo de vértices no pipeline de renderização especificado pelo OpenGL. Os vértices podem ser utilizados para formar diferentes primitivas. Por exemplo, o uso do identificador GL_TRIANGLES na função de renderização glDrawArrays faz com que seja formado um triângulo a cada grupo de três vértices do arranjo de vértices. Assim, se o arranjo tiver seis vértices (numa sequência de 0 a 5), serão formados dois triângulos: um triângulo com os vértices 0, 1, 2, e outro com os vértices 3, 4, 5. Para o mesmo arranjo de vértices, GL_POINTS faz com que o pipeline de renderização interprete cada vértice como um ponto separado, e GL_LINE_STRIP faz com que o pipeline de renderização forme uma sequência de segmentos (uma polilinha) conectando os vértices. A figura 3.4 ilustra a formação dessas primitivas para um arranjo de seis vértices no plano. A numeração indica a ordem dos vértices no arranjo. Figura 3.4: Formando diferentes primitivas do OpenGL com um mesmo arranjo de vértices. A figura 3.5 mostra como a geometria das primitivas pode mudar (com exceção de GL_POINTS) caso os vértices estejam em uma ordem diferente no arranjo. Figura 3.5: A ordem dos vértices no arranjo altera a geometria das primitivas. Veremos com mais detalhes o uso de primitivas no próximo capítulo quando abordaremos as diferentes etapas de processamento do pipeline de renderização do OpenGL. Observação Até a década de 2010, a maneira mais comum de renderizar primitivas no OpenGL era através de comandos do modo imediato de renderização, como a seguir (em C/C++): glColor3f(0.83f, 0.83f, 0.83f); // Light gray color glBegin(GL_TRIANGLES); glVertex2i(-1, -1); glVertex2i( 1, -1); glVertex2i( 0, 1); glEnd(); Nesse código, a função glColor3f informa que a cor dos vértices que estão prestes a ser definidos será um cinza claro, como no triângulo desenhado com SVG. O sufixo 3f de glColor3f indica que os argumentos são três valores do tipo float. Entre as funções glBegin e glEnd é definida a sequência de vértices. Cada chamada a glVertex2i define as coordenadas 2D de um vértice (o sufixo 2i indica que as coordenadas são compostas por dois números inteiros). Como há três vértices e a primitiva é identificada com GL_TRIANGLES, será desenhado um triângulo cinza similar ao triângulo desenhado com SVG, porém sem o contorno preto9. O sistema de coordenadas nativo do OpenGL não é o mesmo da área de desenho do formato SVG. No OpenGL, a origem é o centro da janela de visualização, sendo que o eixo \\(x\\) é positivo à direita e o eixo \\(y\\) é positivo para cima. Além disso, para que a primitiva possa ser vista, as coordenadas dos vértices precisam estar entre -1 e 1 (em ponto flutuante). Para desenhar o triângulo colorido do exemplo Hello, World! como visto na seção 1.5, poderíamos utilizar o seguinte código: glBegin(GL_TRIANGLES); glColor3f(1.0f, 0.0f, 0.0f); // Red glVertex2f(0.0f, 0.5f); glColor3f(1.0f, 0.0f, 1.0f); // Magenta glVertex2f(0.5f, -0.5f); glColor3f(0.0f, 0.0f, 1.0f); // Green glVertex2f(-0.5f, -0.5f); glEnd(); Observe que, antes da definição de cada vértice, é definida a sua cor. Quando o triângulo é processado na GPU, as cores em cada vértice são interpoladas bilinearmente (em \\(x\\) e em \\(y\\)) ao longo da superfície do triângulo, formando um gradiente de cores. Em nossos programas usando a ABCg, bastaria colocar esse código no método paintGL de nossa classe derivada de abcg::OpenGLWindow. Internamente o OpenGL utilizaria o pipeline de renderização de função fixa (pipeline não programável) para desenhar o triângulo. No entanto, se compararmos com o código atual do projeto no subdiretório abcg\\examples\\helloworld, perceberemos que não há nenhum comando glBegin, glVertex* ou glColor*. Isso acontece porque o código acima é obsoleto. As funções do modo imediato foram retiradas do OpenGL na versão 3.1 (de 2009). Ainda é possível habilitar um perfil de compatibilidade (compatibility profile) para usar funções obsoletas do OpenGL, mas esse perfil já não é suportado em vários drivers e plataformas. Por isso, não o utilizaremos neste curso. Atualmente, para desenhar primitivas com o OpenGL, o arranjo ordenado de vértices precisa ser enviada previamente à GPU juntamente com programas chamados shaders que definem como os vértices serão processados e como os pixels serão preenchidos após a rasterização. Desenhar um simples triângulo preenchido no OpenGL não é tão simples como antigamente, mas essa dificuldade é compensada pela maior eficiência e flexibilidade obtida com a possibilidade de programar o comportamento da GPU. Representação matricial Na representação matricial, também chamada de representação raster, as imagens são compostas por arranjos bidimensionais de elementos discretos e finitos chamados de pixels (picture elements). Um pixel contém uma informação de amostra de cor e corresponde ao menor elemento que compõe a imagem. A resolução da imagem é o número de linhas e colunas do arranjo bidimensional. Esse é o formato utilizado nos arquivos GIF (Graphics Interchange Format), TIFF (Tag Image File Format), PNG (Portable Graphics Format), JPEG e BMP. A figura 3.6 mostra uma imagem digital e um detalhe ampliado. Figura 3.6: Imagem digital de 300x394 pixels e detalhe ampliado de 38x38 pixels. Observação Embora os pixels ampliados da figura 3.6 sejam mostrados como pequenos quadrados coloridos, um pixel não tem necessariamente o formato de um quadrado. Um pixel é apenas uma amostra de cor e pode ser exibido em diferentes formatos de acordo com o dispositivo de exibição. Uma imagem digital pode ser armazenada como um mapa de bits (bitmap). A quantidade de cores que podem ser representadas em um pixel  a profundidade da cor (color depth)  depende do número de bits designados a cada pixel. Em uma imagem binária, cada pixel é representado por apenas 1 bit. Desse modo, a imagem só pode ter duas cores, como preto (para os bits com estado 0) e branco (para os bits com estado 1). A figura 3.7 mostra uma imagem binária em formato BMP, que é um formato simples e muito utilizado para armazenar mapas de bits. Figura 3.7: Imagem binária. A imagem da figura 3.7 foi gerada a partir de outra de maior profundidade de cor (figura 3.11) usando o algoritmo Floyd-Steinberg de dithering (Floyd and Steinberg 1976). Dithering é o processo de introduzir um ruído ou padrão de pontilhado que atenua a percepção de bandas de cor (color banding) resultantes da quantização da cor. A figura 3.8 mostra esse efeito. A imagem da esquerda é a imagem original, com 24 bits de profundidade de cor. A imagem do centro teve a profundidade de cor reduzida para 4 bits (16 cores). É possível perceber as bandas de cor no gradiente do céu. Na imagem da direita, a profundidade de cor também foi reduzida para 4 bits, mas o uso de dithering reduz a percepção das variações bruscas de tom. Figura 3.8: Redução de bandas de cor com dithering. Esquerda: imagem original de 24 bits/pixel. Centro: redução para 4 bits/pixel. Direita: redução para 4 bits/pixel usando dithering. Em imagens com profundidade de cor de 8 bits, cada pixel pode assumir um valor de 0 a 255. Esse valor pode ser interpretado como um nível de luminosidade para, por exemplo, descrever imagens monocromáticas de 256 tons de cinza (figura 3.9). Figura 3.9: Imagem monocromática de 8 bits por pixel. Uma outra possibilidade é fazer com que cada valor corresponda a um índice de uma paleta de cores que determina qual será a cor do pixel. Em imagens de 8 bits, a paleta de cores é uma tabela de 256 cores, sendo que cada cor é definida por 3 bytes, um para cada componente de cor RGB (vermelho, verde, azul). Esse formato de cor indexada foi o formato predominante em computadores pessoais na década de 1990, quando os controladores gráficos só conseguiam exibir um máximo de 256 cores simultâneas no modo VGA (Video Graphics Array). O formato GIF, criado em 1987, utiliza cores indexadas. A figura 3.10 exibe uma imagem GIF e sua paleta correspondente de 256 cores. Figura 3.10: Imagem de 8 bits com cores indexadas (esquerda) e paleta utilizada (direita). Atualmente, as imagens digitais coloridas usam o formato true color no qual cada pixel tem 24 bits (3 bytes, um para cada componente de cor RGB), sem o uso de paleta de cor (figura 3.11). Isso possibilita a exibição de \\(2^{24}\\) (16.777.216) cores simultâneas. Figura 3.11: Imagem de 24 bits por pixel. Em arquivos de imagens, também é comum o uso de 32 bits por pixel (4 bytes), sendo 3 bytes para as componentes de cor e 1 byte para definir o nível de opacidade do pixel. Geralmente, os valores de intensidade de cor de um pixel são representados por números inteiros. Entretanto, em sistemas gráficos que usam imagens HDR (high dynamic range), cada componente de cor pode ter até 32 bits em formato de ponto flutuante, permitindo alcançar uma faixa muito superior de intensidades. As GPUs atuais fornecem suporte a um variado conjunto de formatos de bits, incluindo suporte a mapas de bits compactados e tipos de dados em formato de ponto flutuante de 16 e 32 bits. Referências "],["es.html", "3.2 Dispositivos de E/S", " 3.2 Dispositivos de E/S A seguir apresentamos uma visão geral de conceitos e tecnologias relacionadas a dispositivos de entrada e saída utilizados em sistemas gráficos. Dispositivos de entrada Um sistema gráfico possui um ou mais dispositivos de entrada que permitem ao usuário interagir com os modelos de dados gráficos. O dispositivo de teclado inclui o tradicional teclado físico dos computadores de mesa e laptops (figura 3.12). O teclado produz um código (scancode) composto por um byte ou sequência de bytes que identifica cada tecla pressionada e liberada. Figura 3.12: Teclado de teclas mecânicas (fonte). O teclado virtual de um smartphone ou tablet é também um dispositivo de teclado. Embora não possua teclas físicas, o resultado dos toques na tela e o resultado da conversão de uma anotação manuscrita em texto é um conjunto de scancodes que corresponde aos mesmos caracteres de teclas de um teclado físico (figura 3.13). Figura 3.13: Teclado virtual de um smartphone (fonte). Os códigos de um dispositivo de teclado podem ser interpretados como direções de movimentação de um cursor de desenho para permitir a interação com dados gráficos. Uma forma de utilizar o teclado para a interação com cenas virtuais 3D é mapear as setas (esquerda, direita, para cima, para baixo) ou as teclas WASD a transformações de translação de uma câmera em primeira pessoa, e usar outro conjunto de teclas para alterar a orientação da câmera. Além do teclado, é comum que um sistema gráfico tenha também pelo menos um dispositivo apontador, como o mouse (figura 3.14), capaz de fornecer dados de movimentação ou posicionamento sobre uma superfície, geralmente mapeados para uma posição na tela. Figura 3.14: Mouse com dois botões e botão de rolagem. (fonte). O mouse produz dados de deslocamento em duas direções ortogonais que correspondem ao movimento horizontal (\\(x\\)) e vertical (\\(y\\)) do dispositivo10. Como os dados produzidos são apenas deslocamentos, e não posições, o mouse é considerado um dispositivo de posicionamento relativo. Entretanto, os deslocamentos em \\(x\\) e \\(y\\) podem ser interpretados como velocidades e acumulados ao longo do tempo para determinar a posição de um cursor na tela. Outros dispositivos populares de posicionamento relativo são os touch pads, trackballs, joysticks e gamepads (figura 3.15). Tais dispositivos também possuem botões que podem ser configurados da mesma forma que as teclas de um teclado. Figura 3.15: Dispositivos apontadores de posicionamento relativo. Da esquerda para a direita: trackball (fonte), joystick (fonte), gamepad (fonte). Dispositivos apontadores como a tela sensível ao toque (touch screen) e a mesa digitalizadora (graphics tablet) são capazes de fornecer dados de posicionamento absoluto (figura 3.16). Os toques produzidos com o dedo ou com uma caneta de toque (stylus pen) produzem dados que correspondem a um par de coordenadas sobre a superfície de desenho, além de um valor que corresponde à pressão aplicada. Esses dispositivos também podem ser configurados para gerar dados de posicionamento relativo e detecção de gestos de arrasto (swipe e drag and drop) através do rastreamento dos pontos de pressão. Telas sensíveis ao toque frequentemente também são capazes de detectar múltiplos toques simultâneos, permitindo a detecção de gestos mais complexos como pinça (pinch) e rotação (rotate). Figura 3.16: Mesa digitalizadora com caneta (fonte). Dispositivos de saída Um sistema gráfico possui pelo menos um dispositivo de saída para exibição de gráficos. Esses dispositivos podem ser do tipo vetorial ou matricial. Dispositivos vetoriais O primeiro dispositivo de exibição utilizado em computador foi o CRT vetorial, que é o mesmo tipo de tecnologia utilizada nos osciloscópios analógicos (figura 3.17). Figura 3.17: Osciloscópio analógico com CRT vetorial (fonte). No CRT vetorial, um canhão de elétrons emite um feixe de elétrons que incide sobre uma tela revestida por um material fotoluminescente (fósforo). Um conjunto de placas defletoras permite alterar a posição horizontal (\\(x\\)) e vertical (\\(y\\)) de incidência do feixe, de modo que gráficos de linhas e curvas podem ser traçados na tela. Em um sistema gráfico, a posição de incidência do feixe pode ser descrita por comandos do tipo MoveTo e LineTo (figuras 3.18 e 3.19). Como o brilho do fósforo tem persistência baixa, na ordem de milissegundos, é preciso redesenhar o traço continuamente. Figura 3.18: Desenhando um triângulo em um CRT vetorial. A sequência de passos de 1 a 4 precisa ser repetida continuamente para manter a imagem na tela. Figura 3.19: Jogo estilo Asteroides (Space Rocks) sendo exibido em um CRT vetorial de um antigo osciloscópio (fonte). Dispositivos de exibição vetorial não conseguem desenhar de forma adequada áreas preenchidas. Além disso, a velocidade de geração do desenho é proporcional à quantidade de primitivas e ao comprimento dos caminhos, impondo um limite à complexidade do desenho. Por essas desvantagens, CRTs vetoriais tornaram-se obsoletos e foram substituídos inteiramente pelos dispositivos matriciais. Dispositivos matriciais O primeiro dispositivo de exibição matricial utilizado em computadores também foi o CRT (Noll 1971). No CRT matricial, o feixe de elétrons é direcionado por deflexão eletromagnética e varre continuamente a tela de cima para baixo, da esquerda para direita. A cada linha percorrida, o canhão de elétrons é desligado momentaneamente e religado no início da próxima linha (retraço horizontal). Ao completar a varredura no canto inferior direito, o canhão de elétrons é desligado e direcionado para o ponto inicial, no canto superior esquerdo (retraço vertical). O feixe de elétrons é então religado e uma nova varredura é feita, iniciando uma nova imagem ou quadro de exibição. Esse processo é feito continuamente, a uma taxa que, nos televisores antigos, era sincronizada com a frequência da rede elétrica: 50 Hz ou 60 Hz11. Durante a varredura, a intensidade do feixe é controlada por um sinal analógico de vídeo. Esse sinal pode ser produzido por um conversor digital-analógico a partir de uma imagem digital, reproduzindo na tela os pontos que formam a imagem (figura 3.20). Figura 3.20: Varredura de um quadro em um CRT matricial. CRTs coloridos utilizam três canhões de elétrons, um para cada componente de cor RGB. A tela é coberta por um padrão de fósforos nessas cores, em grupos de três. Uma máscara ou grelha metálica próxima da tela (shadow mask, slot mask ou apperture grille, dependendo da tecnologia utilizada) assegura que cada tipo de fósforo recebe elétrons apenas do canhão correspondente. A figura 3.21 mostra o detalhe ampliado da tela de um CRT de TV e um CRT de computador, mostrando o padrão das tríades RGB formadas pelo slot mask (no CRT de TV) e shadow mask (no CRT de computador). Uma vez que os padrões são muito pequenos e cobrem a tela por completo, o usuário percebe a combinação das cores primárias que resultam na cor da imagem. A figura 3.22 mostra o detalhe ampliado de uma letra e exibida em um CRT de TV que usa a tecnologia de apperture grille (tecnologia Trinitron, da Sony), e o detalhe ampliado de um cursor em um CRT de computador. Figura 3.21: Padrões de fósforos RGB em CRTs. Esquerda: slot mask em um CRT de TV. Direita: shadow mask em um CRT de PC. (fonte) Figura 3.22: Detalhes ampliados de telas de CRT. Esquerda: letra e em um CRT de TV Sony Trinitron (fonte). Direita: cursor na tela de um CRT de computador (fonte). Os CRTs não são mais utilizados desde meados de 2000 e foram substituídos pelos monitores LCD (liquid-crystal display). Até a metade de 2010 eram também comuns os monitores de tela de plasma. Nessa tecnologia, tensões aplicadas em eletrodos de endereçamento de linhas e colunas energizam um gás (geralmente néon e xenônio) contido em minúsculas células envoltas em painéis de vidro. O fundo das células é coberto por fósforo nas cores RGB, de modo que cada grupo de 3 cores forma um pixel. Como em uma lâmpada fluorescente, o gás ionizado se torna um plasma emissor de luz ultravioleta que faz com que os fósforos emitam a luz visível que forma as cores da imagem (figura 3.23). Figura 3.23: Estrutura de uma tela de plasma (fonte). A tecnologia LCD é a mais utilizada nos dispositivos de exibição atuais. Uma tela de LCD é composta por um sanduíche de vários painéis (figura 3.24). Na parte de trás dos painéis, lâmpadas fluorescentes ou LEDs emitem uma luz branca que é espalhada uniformemente por um painel difusor. Essa luz incide sobre um filtro que só permite passar luz polarizada em uma direção. Na frente dos painéis há uma outra camada que só permite passar a luz polarizada na direção ortogonal ao primeiro filtro, de modo que o resultado é o bloqueio total da luz. Para controlar eletronicamente a passagem da luz, entre os dois filtros é colocado um substrato de vidro contendo uma camada de cristais líquidos e eletrodos e/ou transistores que alteram a orientação dos cristais  e com isso a polarização da luz  através de campos elétricos. Uma camada de filtros de cor divide a tela em pixels compostos de três subpixels, um para cada componente RGB, coincidentes com a camada de eletrodos. Desse modo, a passagem de luz em cada subpixel é controlada individualmente para formar a imagem final. O conteúdo da tela LCD é atualizado continuamente, geralmente a uma taxa de 60 Hz, mas em monitores mais recentes essa taxa pode chegar a 240 Hz. Figura 3.24: Estrutura de uma tela de LCD (fonte). A tecnologia OLED (organic light-emitting diodes) tem se popularizado em telas de smart TVs e smartphones e tem a promessa de substituir a tecnologia de LCD. Telas OLED não utilizam luz de fundo, pois cada subpixel emite sua própria luz: cada subpixel é um LED no qual a camada eletroluminescente é um filme de compostos orgânicos. A figura 3.25 mostra o detalhe ampliado de uma tela com tecnologia AMOLED (active-matrix organic light-emitting diode) que utiliza transistores de filme fino para manter o fluxo de corrente em cada subpixel. Figura 3.25: Detalhe da tela AMOLED de um smartphone Google Nexus One. Foto por Matthew Rollings (fonte). Telas OLED conseguem obter níveis mais profundos de preto e melhor contraste em ambientes escuros quando comparadas com as telas de LCD. A tecnologia tem ainda outras vantagens, como a possibilidade de ser utilizada em telas mais finas, flexíveis e transparentes (figura 3.26). Figura 3.26: Demonstração de uma tela OLED flexível (fonte). Referências "],["framebuffer.html", "3.3 Framebuffer", " 3.3 Framebuffer O framebuffer é uma área contígua da memória utilizada para armazenar a imagem que será mostrada no dispositivo de exibição. O controlador gráfico lê continuamente o conteúdo do framebuffer e atualiza o dispositivo de exibição, tipicamente a uma taxa de 60 Hz nos monitores de LCD. Nos primeiros PCs e em sistemas gráficos mais antigos, o framebuffer fazia parte da memória padrão que poderia ser acessada diretamente pela CPU. Nos PCs do início da década de 1990, o framebuffer podia ser acessado com um simples ponteiro para o endereço 0xA000 no chamado modo 13h do controlador VGA (um modo gráfico de cores indexadas de 8 bits com resolução de 320x200). Atualmente, o framebuffer é acessado através da GPU e está localizado na mesma placa de circuito da GPU (figura 3.27). Figura 3.27: Configuração do framebuffer em sistema gráfico com GPU dedicada. Observação Em hardware compatível com OpenGL (o que inclui todas as GPUs atuais), o framebuffer pode ser composto por vários buffers. Pelo menos um deles é um color buffer (buffer de cor) no qual cada pixel contém uma informação de cor, geralmente no formato RGB (24 bits) ou RGBA (32 bits). Um framebuffer pode ter vários buffers de cor associados. Por exemplo, em implementações que suportam visão estereoscópica, dois buffers de cor podem ser utilizados: um para a tela da visão esquerda e outro para a tela da visão direita. Na técnica de backbuffering (descrita no fim da seção), também dois buffers de cor são utilizados: o backbuffer, que é um buffer off-screen (invisível) onde a imagem é renderizada antes de ser exibida na tela, e o frontbuffer, que recebe o conteúdo do backbuffer ao fim da renderização, para exibição na tela. Em renderização estéreo, cada lado esquerdo e direito tem o seu backbuffer e frontbuffer. Além dos buffers de cor, o framebuffer também pode incluir: Um depth buffer (buffer de profundidade), no qual cada pixel contém uma informação de profundidade utilizada no teste de profundidade. O teste de profundidade faz parte da implementação da técnica de Z-buffering de determinação de superfícies visíveis. A informação de profundidade pode ser um inteiro ou ponto flutuante de 16, 24 ou 32 bits (geralmente 24 bits). Um stencil buffer (buffer de estêncil), utilizado no teste de estêncil para operações de mascaramento e composição de imagens. No buffer estêncil, cada pixel contém um inteiro sem sinal, de 1, 4, 8 ou 16 bits (geralmente 8 bits). Screen tearing A taxa de atualização do dispositivo de exibição (chamada de vertical refresh rate) é controlada pelo controlador gráfico. Entretanto, a taxa em que o framebuffer é atualizado pode ser bem maior. Essa taxa é o número de quadros por segundo (FPS) que o processador gráfico consegue renderizar. Se o framebuffer for atualizado muito rapidamente, o controlador pode começar a atualizar o dispositivo de exibição com o conteúdo de um quadro e terminar com o conteúdo de outro, mais recente. Essa quebra entre os quadros de exibição gera um defeito na imagem conhecido como screen tearing ou simplesmente tearing (figura 3.28). Figura 3.28: Screen tearing. (fonte). Vsync Para contornar o problema de screen tearing, a GPU pode sincronizar a atualização do framebuffer com a atualização do controlador gráfico, efetivamente limitando o número de FPS à frequência do monitor em Hz. Esse processo de sincronização é chamado de vertical synchronization, ou vsync. Em monitores mais recentes, compatíveis com as tecnologias G-SYNC da NVIDIA, e FreeSync da AMD, é possível fazer a sincronização na direção contrária: a frequência do monitor é ajustada pela GPU de acordo com a taxa de FPS. Backbuffering Defeitos de screen tearing também podem ocorrer quando a taxa de renderização é menor que a taxa de atualização do dispositivo de exibição. Nesse caso, o dispositivo de exibição pode mostrar o quadro de exibição antes da renderização ter sido finalizada. O resultado pode ser uma mistura de elementos do quadro atual com elementos do quadro anterior, ou a percepção de que o quadro está sendo desenhado. Uma forma de reduzir o efeito de screen tearing é o uso de dois buffers de cor: o backbuffer e o frontbuffer. O processador renderiza os gráficos apenas no backbuffer. Após o fim da renderização, o conteúdo é transferido para o frontbuffer, que é então utilizado pelo controlador gráfico para atualizar o dispositivo de exibição. Dessa forma, o controlador sempre utiliza um buffer que já contém um quadro completo. Atualmente, essa técnica de backbuffering é implementada em hardware. O backbuffer e frontbuffer podem ser páginas de memória do framebuffer que são trocadas continuamente em um processo de page flipping, sem precisar realizar transferência de dados (figura 3.29). Figura 3.29: Configuração do framebuffer com suporte a backbuffering. "],["sierpinski.html", "3.4 Triângulo de Sierpinski", " 3.4 Triângulo de Sierpinski O triângulo de Sierpinski é um fractal que pode ser gerado por um tipo de sistema dinâmico chamado de sistema de função iterativa (iterated function system, ou IFS). Esse processo pode ser implementado através de um algoritmo chamado jogo do caos. Para jogar o jogo do caos, vamos começar primeiro definindo três pontos \\(A\\), \\(B\\) e \\(C\\) não colineares. Esses pontos formarão os vértices de um triângulo fractal. Por exemplo, \\(A=(0, 1)\\), \\(B=(-1, -1)\\) e \\(C=(1, -1)\\): Além dos pontos \\(A\\), \\(B\\) e \\(C\\), definiremos mais um ponto \\(P\\) em uma posição aleatória do plano. Com \\(A\\), \\(B\\), \\(C\\) e \\(P\\) definidos, o jogo do caos consiste nos seguintes passos: Mova \\(P\\) para o ponto médio entre \\(P\\) e um dos pontos \\(A\\), \\(B\\), \\(C\\) escolhido de forma aleatória; Volte ao passo 1. Para gerar o triângulo de Sierpinski, basta desenharmos \\(P\\) a cada iteração. O jogo não tem fim, mas quanto maior o número de iterações, mais pontos serão desenhados e mais detalhes terá o fractal (figura 3.30). Figura 3.30: Triângulo de Sierpinski desenhado com 1.000, 10.000 e 100.000 iterações em uma área de 210x210 pixels. Vamos implementar o jogo do caos com a ABCg, usando a estrutura da aplicação que fizemos no projeto firstapp (seção 2.3). O procedimento será simples: para cada chamada de paintGL, faremos uma iteração do jogo e desenharemos um ponto na posição \\(P\\) usando um comando de renderização do OpenGL. Os pontos desenhados serão acumulados no framebuffer e visualizaremos o fractal. Configuração inicial Repita a configuração inicial do projeto firstapp, mas mudando o nome do projeto para sierpinski. O arquivo abcg/examples/CMakeLists.txt ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) add_subdirectory(sierpinski) Para a construção não ficar muito lenta, podemos comentar as linhas de add_subdirectory dos projetos anteriores para que eles não sejam compilados. Por exemplo: #add_subdirectory(helloworld) #add_subdirectory(firstapp) add_subdirectory(sierpinski) O arquivo abcg/examples/sierpinski/CMakeLists.txt ficará assim: project(sierpinski) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Crie também os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp em abcg/examples/sierpinski. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp ficará como a seguir: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings( {.samples = 2, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); // Run application app.run(std::move(window); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Esse código é bem parecido com o main.cpp do projeto firstapp. As únicas diferenças estão nas linhas 13 a 18: window-&gt;setOpenGLSettings( {.samples = 2, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); setOpenGLSettings é um método de abcg::OpenGLWindow que recebe uma estrutura abcg::OpenGLSettings com as configurações de inicialização do OpenGL. Essas configurações são usadas pela SDL no momento da criação de um contexto do OpenGL que representa o framebuffer vinculado à janela: O atributo samples = 2 faz com que o framebuffer suporte suavização de serrilhado (antialiasing) das primitivas do OpenGL; O atributo preserveWebGLDrawingBuffer = true é utilizado apenas no binário em WebAssembly. No WebGL, preserveDrawingBuffer é uma configuração de criação do contexto do OpenGL que faz com que o framebuffer vinculado ao canvas da página Web não seja apagado entre os quadros de exibição. Em setWindowSettings, utilizamos alguns atributos novos de definição de propriedades da janela. Definimos a largura (width) e altura (height) inicial da janela, e desligamos a exibição do botão de tela cheia (showFullscreenButton = false) para que o botão não obstrua o desenho do triângulo. Mesmo sem o botão, o modo janela pode ser alternado com o modo de tela cheia pela tecla F11. openglwindow.hpp Na definição da classe OpenGLWindow, vamos substituir novos métodos virtuais de abcg::OpenGLWindow e vamos definir variáveis que serão utilizados para atualizar o jogo do caos e para desenhar o ponto na tela: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_vao{}; GLuint m_vboVertices{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; const std::array&lt;glm::vec2, 3&gt; m_points{glm::vec2( 0, 1), glm::vec2(-1, -1), glm::vec2( 1, -1)}; glm::vec2 m_P{}; void setupModel(); }; #endif Observe que, além de usarmos os métodos initializeGL, paintGL e paintUI, estamos agora substituindo mais dois métodos virtuais de abcg::OpenGLWindow: resizeGL é chamado pela ABCg sempre que o tamanho da janela é alterado. O novo tamanho é recebido pelos parâmetros width e height. Na nossa aplicação, vamos armazenar esses valores nas variáveis m_viewportWidth (linha 23) e m_viewportHeight (linha 24). Precisamos disso para fazer com que a janela de exibição (viewport) do OpenGL tenha o mesmo tamanho da janela da aplicação. O conceito de viewport será detalhado mais adiante. terminateGL é chamado pela ABCg quando a janela é destruída, no fim da aplicação. Essa é a função complementar de initializeGL, usada para liberar os recursos do OpenGL que foram alocados no initializeGL ou durante a aplicação. Da linha 19 a 31 temos a definição das variáveis da classe: GLuint m_vao{}; GLuint m_vboVertices{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; const std::array&lt;glm::vec2, 3&gt; m_points{glm::vec2( 0, 1), glm::vec2(-1, -1), glm::vec2( 1, -1)}; glm::vec2 m_P{}; m_vao, m_vboVertices e m_program são identificadores de recursos alocados pelo OpenGL (recursos geralmente armazenados na memória da GPU). Esses recursos correspondem ao arranjo ordenado de vértices utilizado para montar as primitivas geométricas no pipeline de renderização12 e os shaders que definem o comportamento da renderização. m_viewportWidth e m_viewportHeight servem para armazenar o tamanho da janela da aplicação informado pelo resizeGL. m_randomEngine é um objeto do gerador de números pseudoaleatórios do C++ (observe o uso do #include &lt;random&gt; na linha 6). Esse objeto é utilizado para sortear a posição inicial de \\(P\\) e para sortear qual ponto (\\(A\\), \\(B\\) ou \\(C\\)) será utilizado em cada iteração do jogo do caos. m_points é um arranjo que contém a posição dos pontos \\(A\\), \\(B\\) e \\(C\\). As coordenadas dos pontos são descritas por uma estrutura glm::vec2. O namespace glm contém definições da biblioteca OpenGL Mathematics (GLM) que fornece estruturas e funções de operações matemáticas compatíveis com a especificação da linguagem de shaders do OpenGL. Observe que, para usar glm::vec2, incluímos o arquivo de cabeçalho glm/vec2.hpp. m_P é a posição do ponto \\(P\\). Além da definição das variáveis, na linha 33 é definido o método setupModel que cria os recursos identificados por m_vao e m_vboVertices. O método é chamado sempre que um novo ponto \\(P\\) precisa ser desenhado. openglwindow.cpp Vamos implementar primeiro a lógica do jogo do caos, sem desenhar nada na tela. Em seguida incluiremos o código que usa o OpenGL para desenhar os pontos. Vamos começar incluindo os seguintes arquivos de cabeçalho: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;chrono&gt; Em OpenGLWindow::initializeGL, iniciaremos o gerador de números pseudoaleatórios e sortearemos as coordenadas iniciais de \\(P\\) (que no código é m_P): void OpenGLWindow::initializeGL() { // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } O gerador m_randomEngine é iniciado usando como semente o tempo do sistema (para isso é preciso incluir o cabeçalho &lt;chrono&gt;). As coordenadas de m_P são iniciadas como valores sorteados de um intervalo de -1 a 1. O intervalo poderia ser qualquer outro, mas fazendo assim garantimos que o ponto inicial será visto na tela. Na configuração padrão do OpenGL, só conseguimos visualizar as primitivas gráficas que estão situadas entre as coordenadas \\((-1, -1)\\) e \\((1, 1)\\). A coordenada \\((-1, -1)\\) geralmente é mapeada ao canto inferior esquerdo da janela, e a coordenada \\((1, 1)\\) é mapeada ao canto superior direito (esse mapeamento será configurado posteriormente com a função glViewport). Vamos agora implementar o passo iterativo do jogo. Faremos isso no paintGL, de modo que cada quadro de exibição corresponderá a uma iteração13: void OpenGLWindow::paintGL() { // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Neste trecho de código, index é um índice do arranjo m_points. Assim, m_points.at(index) é um dos pontos \\(A\\), \\(B\\) ou \\(C\\) que definem os vértices do triângulo. Observe que utilizamos uma distribuição uniforme para sortear o índice. Isso é importante para que o fractal seja desenhado como esperado14. A nova posição de m_P é calculada como o ponto médio entre m_P e o ponto de m_points. O código comentado pode ser utilizado para imprimir no terminal as novas coordenadas de m_P. Basicamente isso conclui a lógica do jogo do caos. Todo o resto do código será para desenhar m_P como um ponto na tela. No OpenGL anterior à versão 3.1, isso seria tão simples quanto acrescentar o seguinte código em paintGL: glBegin(GL_POINTS); glVertex2f(m_P.x, m_P.y); glEnd(); Entretanto, como vimos na seção 3.1, esse código é obsoleto e não é mais suportado em muitos drivers e plataformas. Para desenhar um simples ponto na tela, precisaremos seguir os seguintes passos: Criar um buffer de vértices como recurso do OpenGL. Esse recurso é chamado VBO (Vertex Buffer Object) e corresponde ao arranjo ordenado de vértices utilizado pelo pipeline de renderização para montar as primitivas que serão renderizadas. No nosso caso, o buffer de vértices só precisa ter um vértice, que é a coordenada do ponto que queremos desenhar. A variável m_vboVertices é um inteiro que identifica esse recurso. Programar o comportamento do pipeline de renderização. Isso é feito compilando e ligando um par de shaders que fica armazenado na GPU como um único programa de shader, identificado pela variável m_program. No OpenGL, os shaders são escritos na linguagem GLSL (OpenGL Shading Language), que é parecida com a linguagem C, mas possui novos tipos de dados e operações. Especificar como o buffer de vértices será lido pelo programa de shader. No nosso código, o estado dessa configuração é armazenado como um objeto do OpenGL chamado VAO (Vertex Array Object), identificado pela variável m_vao. Somente após alocar e ativar esses recursos é que podemos iniciar o pipeline de renderização, chamando uma função de desenho no paintGL. Não se preocupe se tudo isso está parecendo muito complexo nesse momento. Nos próximos capítulos revisitaremos cada etapa várias vezes até nos familiarizarmos com todo o processo. Por enquanto, utilizaremos o código já pronto. Primeiro, defina o método setupModel como a seguir: void OpenGLWindow::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_vboVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Esse código cria o VBO (m_vboVertices) e VAO (m_VAO) usando a posição atual de m_P. Agora, modifique initializeGL para o seguinte código final: void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); #endif std::array&lt;GLfloat, 2&gt; sizes{}; abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } Neste método, vertexShader e fragmentShader são strings que contêm o código-fonte dos shaders. vertexShader é o código do chamado vertex shader, que programa o processamento de vértices na GPU. fragmentShader é o código do fragment shader que programa o processamento de pixels na GPU (ou, mais precisamente, o processamento de fragmentos, que são conjuntos de atributos que representam uma amostra de geometria rasterizada). A compilação e ligação dos shaders é feita pelo método createProgramFromString que faz parte da abcg::OpenGLWindow. Se acontecer algum erro de compilação, a mensagem de erro será exibida no console e uma exceção será lançada. Note que limpamos o buffer de cor com a cor preta, usando glClearColor e glClear (linhas 28 e 29). Observe o trecho de código entre as diretivas de pré-processamento: #if !defined(__EMSCRIPTEN__) glEnable(GL_PROGRAM_POINT_SIZE); #endif Esse código só é compilado quando não estamos usando o Emscripten, isto é, quando estamos compilando para o desktop. No OpenGL para desktop, o código é necessário para que o tamanho do ponto que será desenhado possa ser definido no vertex shader. Quando o código é compilado com o Emscripten, a definição do tamanho do ponto no vertex shader já é suportada por padrão, pois o OpenGL utilizado é o OpenGL ES (o WebGL usa um subconjunto de funções do OpenGL ES). Observe, no código do vertex shader, que o tamanho do ponto é definido com gl_PointSize = 2.0 (tamanho em pixels). Definimos o tamanho como dois, mas poderíamos ter escolhido outro valor. A faixa de tamanhos depende do que é suportado pelo hardware. Para imprimir no console o tamanho mínimo e máximo, usamos esse trecho de código: std::array&lt;GLfloat, 2&gt; sizes{}; abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); A função glFloatv com o identificador GL_ALIASED_POINT_SIZE_RANGE preenche o arranjo sizes com os tamanhos mínimo e máximo suportados. Em seguida, fmt::print mostra os valores no console. O código final de paintGL fica assim: void OpenGLWindow::paintGL() { // Create OpenGL buffers for the single point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Na linha 50, setupModel cria os recursos do OpenGL necessários para desenhar um ponto na posição atual de m_P. Na linha 53, glViewport configura o mapeamento entre o sistema de coordenadas no qual nossos pontos foram definidos (coordenadas normalizadas do dispositivo, ou NDC, de normalized device coordinates), e o sistema de coordenadas da janela (window coordinates), em pixels, com origem no canto inferior esquerdo da janela da aplicação. A figura 3.31 ilustra como fica configurado o mapeamento entre coordenadas em NDC para coordenadas da janela, supondo uma chamada a glViewport(x, y, w, h), onde x, y, w e h são inteiros dados em pixels da tela. Na figura, o chamado viewport do OpenGL é a janela formada pelo retângulo entre os pontos \\((x,y)\\) e \\((x+w,y+h)\\). No nosso código com glViewport(0, 0, m_viewportWidth, m_viewportHeight), o ponto \\((-1,-1)\\) em NDC é mapeado para o pixel \\((0, 0)\\) da janela (canto inferior esquerdo), e o ponto \\((1,1)\\) em NDC é mapeado para o pixel \\((0,0)\\) + (m_viewportWidth, m_viewportHeight). Isso faz com que o viewport ocupe toda a janela da aplicação. Figura 3.31: Mapeamento das coordenadas normalizadas no dispositivo (NDC) para coordenadas da janela usando glViewport(x, y, w, h). Com o viewport devidamente configurado, iniciamos o pipeline de renderização neste trecho: // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); Na linha 56, glUseProgram ativa os shaders compilados no programa m_program. Na linha 58, glBindVertexArray ativa o VAO (m_VAO), que contém as especificações de como o arranjo de vértices (VBO) será lido no vertex shader atualmente ativo. Ao ativar o VAO, também é ativado automaticamente o VBO identificado por m_VBO. Finalmente, na linha 61, glDrawArrays inicia o pipeline de renderização usando os shaders e o VBO ativo. O primeiro argumento (GL_POINTS) indica que os vértices do arranjo de vértices devem ser tratados como pontos. O segundo argumento (0) é o índice inicial dos vértices no VBO, e o terceiro argumento (1) informa quantos vértices devem ser processados. O processamento no pipeline de renderização é realizado de forma paralela (assíncrona) com a CPU. Isto é, glDrawArrays retorna imediatamente, enquanto a GPU trabalha em paralelo renderizando a geometria no framebuffer15. Após o comando de renderização, as linhas 64 e 66 desativam o VAO e os shaders. Essa desativação é opcional pois, de qualquer forma, o mesmo VAO e os mesmos shaders serão utilizados na próxima chamada de paintGL. Ainda assim, é uma boa prática de programação desativá-los logo após o uso. Vamos agora definir o método resizeGL, assim: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } Como vimos, resizeGL é chamado sempre que a janela da aplicação muda de tamanho. Observe que simplesmente armazenamos o tamanho da janela em m_viewportWidth e m_viewportHeight. Como essas variáveis são usadas em glViewport, garantimos que o viewport sempre ocupará toda a janela da aplicação. Observe que também chamamos glClear para apagar o buffer de cor. Dessa forma, o triângulo de Sierpinski no novo tamanho não é desenhado sobre o triângulo do tamanho anterior, o que estragaria o fractal. O método terminateGL é definido da seguinte maneira: void OpenGLWindow::terminateGL() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Os comandos glDelete* liberam os recursos alocado em setupModel. Para finalizar, vamos definir paintUI usando o seguinte código: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 81)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } Na linha 81 chamamos o paintUI da classe base, que é responsável por mostrar o contador de FPS (lembre-se que desabilitamos a exibição do botão de tela cheia). O código nas linhas 83 a 92 cria um botão Clear window que chama glClear sempre que pressionado. Isso é tudo! O código completo de openglwindow.cpp é mostrado a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;chrono&gt; void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); #endif std::array&lt;GLfloat, 2&gt; sizes{}; abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } void OpenGLWindow::paintGL() { // Create OpenGL buffers for the single point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 5 + 50 + 16 + 5)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); } void OpenGLWindow::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_vboVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Construa a aplicação para ver o resultado: No nosso caso o arranjo de vértices contém apenas um vértice e equivale ao ponto \\(P\\) que queremos desenhar. A numeração das linhas é a mesma do código completo de openglwindow.cpp mostrado no final do capítulo. Experimente outras distribuições e observe a mudança no comportamento do fractal. A ABCg habilita a técnica de backbuffering vista na seção 3.3. Desse modo, a GPU renderiza primeiro a geometria no backbuffer. Quando a renderização é concluída, o conteúdo é enviado automaticamente para o frontbuffer, que atualiza o dispositivo de exibição. "],["pipeline.html", "4 Pipeline gráfico", " 4 Pipeline gráfico O pipeline gráfico ou pipeline de renderização é um modelo conceitual de descrição da sequência de passos que um sistema gráfico utiliza para transformar um modelo matemático de dados gráficos em uma imagem digital. O termo pipeline é utilizado porque o processamento é realizado em uma sequência linear de etapas alimentadas por um fluxo de dados, de modo que cada etapa pode processar novos dados tão logo tenha enviado a saída à etapa seguinte. O pipeline é iniciado pela aplicação na CPU. A aplicação é responsável por alimentar o pipeline com os dados gráficos que serão renderizados. Em geral, esses dados descrevem objetos de uma cena 3D. A geometria de cada objeto é formada por malhas de triângulos, e uma câmera virtual define o ponto de vista da renderização. O pipeline típico implementado nas atuais GPUs envolve etapas que compreendem o processamento geométrico, a rasterização e o processamento de fragmentos (figura 4.1): Figura 4.1: Etapas de um pipeline gráfico. Processamento geométrico: envolve operações realizadas sobre vértices, como transformações afins e transformações projetivas que serão abordadas em capítulos futuros. O processamento geométrico pode envolver também a criação de geometria e o refinamento de malhas. Ao final desse processamento é feito o recorte ou descarte das primitivas geométricas que estão fora da janela de visualização. Rasterização: compreende a conversão matricial das primitivas. O resultado é um conjunto de amostras de primitivas. Durante o processamento no pipeline, o termo fragmento é frequentemente utilizado para designar essas amostras no lugar de pixel. Cada fragmento é uma coleção de valores que inclui atributos interpolados a partir dos vértices e a posição \\((x,y,z)\\) da amostra em coordenadas da janela (o valor \\(z\\) é considerado a profundidade do fragmento). O pixel é o valor final da cor no buffer de cor, que pode ser uma combinação da cor de vários fragmentos. Processamento de fragmentos: envolve operações realizadas sobre cada fragmento para determinar sua cor e outros atributos. A cor pode ser determinada através da avaliação de modelos de iluminação que levam em conta os atributos de iluminação fornecidos pela aplicação, tais como fontes de luz, texturas e descrição de materiais de superfícies. Após essas operações são realizados testes de descarte de fragmentos e combinação de cores entre os fragmentos processados e os pixels já existentes no framebuffer. O resultado é armazenado em diferentes buffers do framebuffer: buffers de cor, buffer de profundidade e buffer de estêncil. "],["dados-gráficos.html", "4.1 Dados gráficos", " 4.1 Dados gráficos O processamento de um pipeline gráfico começa com a definição dos dados gráficos pela aplicação. Esses dados são frequentemente representações de objetos  abstrações de objetos do mundo real  dispostos em uma cena virtual tridimensional. Uma cena é tipicamente composta por: Objetos com geometria descrita através de modelos geométricos, geralmente no formato de malhas de triângulos. Em renderizadores offline, também é comum representar a geometria de objetos através de equações paramétricas ou implícitas. Propriedades dos materiais dos objetos, tais como cor, textura, transparência e refletividade. Fontes de luz descritas por informações como intensidade, direção de propagação da luz e fatores de atenuação. Uma câmera virtual descrita por informações que permitem definir um ponto de vista na cena, tais como posição da câmera, orientação e campo de visão. A câmera virtual é uma abstração de uma câmera ou observador do mundo real. A síntese de imagens pode ser considerada como o processo de gerar a fotografia digital tirada pela câmera virtual. Em uma câmera de verdade, a imagem é formada a partir da energia luminosa que atravessa as lentes e é captada pelo sensor RGB durante um certo tempo de exposição. Poderíamos tentar simular de forma precisa o comportamento de uma câmera de verdade, mas seria muito custoso simular o comportamento dos fótons passando pelo sistema de lentes e então integrá-los sobre o hemisfério de todas as possíveis direções de luz que incidem em cada subpixel RGB do sensor da câmera. Felizmente, podemos simplificar de forma significativa este processo. Para aplicações de síntese de imagens em tempo real, a cor de cada pixel pode ser uma aproximação simples da energia luminosa refletida na direção do pixel. Essa aproximação pode ser fisicamente incorreta, desde que suficiente para dar a percepção de sombreamento (shading) dos objetos. É comum considerar que a câmera virtual é uma câmera pinhole ideal (figura 4.2). A câmera pinhole é uma câmera que não possui lentes. A luz passa por um pequeno furo (chamado de centro de projeção) e incide sobre um filme ou sensor localizado no fundo da câmera (o plano de imagem). A abertura do campo de visão pode ser ajustada mudando a distância focal, que é a distância entre o centro de projeção e o plano de imagem. Na câmera pinhole ideal, a abertura do furo é infinitamente pequena e as imagens formadas são perfeitamente nítidas (isto é, em foco). Efeitos de difração são ignorados nesse modelo. Figura 4.2: Camera pinhole ideal. Para evitar ter de lidar com a imagem invertida formada no plano de imagem da câmera pinhole, podemos considerar que o plano de imagem está localizado na frente do centro de projeção, o que seria impossível de fazer numa câmera real. Podemos também ajustar arbitrariamente a distância focal sem preocupação com limitações físicas. A distância focal pode até mesmo ser infinita, se desejarmos uma projeção paralela. Por fim, podemos considerar que o plano de imagem é o framebuffer. Nessa configuração, é comum considerar que o centro de projeção corresponde ao olho do observador, como mostra a figura (figura 4.3). Figura 4.3: Câmera virtual com plano de imagem na frente do centro de projeção. Para determinar a cor de cada pixel do framebuffer para um determinado ponto de vista da câmera virtual, podemos considerar duas abordagens de renderização: ray casting e rasterização. Essas abordagens são apresentadas na seção a seguir. "],["ray-casting-x-rasterização.html", "4.2 Ray casting x rasterização", " 4.2 Ray casting x rasterização Ray casting e rasterização são duas abordagens distintas de se renderizar uma cena, e resultam em pipelines também distintos. Ray casting consiste em lançar raios que saem do centro de projeção, atravessam os pixels da tela e intersectam os objetos da cena. A rasterização faz o caminho inverso: os objetos da cena são projetados na tela na direção do centro de projeção, e são então convertidos em pixels. Neste curso usaremos apenas a rasterização, que é a forma de renderização utilizada na maioria das aplicações gráficas interativas. É também a única abordagem de renderização suportada atualmente no pipeline gráfico do OpenGL. Entretanto, é importante observar que novos pipelines baseados em traçado de raios (uma forma de ray casting) têm sido incorporados às APIs gráficas e tendem a conquistar cada vez mais espaço em síntese de imagens em tempo real. Ray casting Na sua forma mais simples, o algoritmo de ray casting (Roth 1982) consiste nos seguintes passos (figura 4.4): Figura 4.4: Ray casting. Para cada pixel do framebuffer: Calcule o raio \\(R\\) que sai do centro de projeção e passa pelo pixel. Seja \\(P\\) a interseção mais próxima (se houver) de \\(R\\) com um objeto da cena. Faça com que a cor do pixel seja a cor calculada em \\(P\\). Em ray casting, cada pixel é visitado apenas uma vez. Entretanto, para cada pixel visitado, potencialmente todos os objetos da cena podem ser consultados para calcular a interseção mais próxima. Assim, o principal custo da geração de imagem usando ray casting está relacionado ao cálculo das interseções. Estruturas de dados de subdivisão espacial como k-d tree (Bentley 1975) e octree (Meagher 1980) podem ser utilizadas para que seja possível descartar rapidamente a geometria não intersectada pelo raio e com isso diminuir o número de testes de interseção. Embora o ray casting seja conceitualmente simples, é pouco adequado para implementação em hardware, pois cada iteração do laço principal exige a manutenção de toda a cena na memória do renderizador, além da estrutura de subdivisão espacial. Essa limitação tem sido cada vez menos significativa nas GPUs mais recentes, mas ray casting ainda é pouco utilizado em síntese de imagens em tempo real. Observação Para produzir imagens fotorrealistas, a cor em \\(P\\) deve ser calculada através da integração da energia luminosa que incide sobre o ponto vindo de todas as direções da cena, e da determinação da quantidade dessa energia que é refletida na direção do pixel na tela. Isso pode ser feito de diferentes formas e em diferentes níveis de aproximação. Uma aproximação pouco acurada, mas muito eficiente, é avaliar a equação de um modelo de iluminação local como o modelo de reflexão de Phong (Phong 1973) ou Blinn-Phong (Blinn 1977) que considera que a cor em uma superfície é determinada unicamente pela luz que incide diretamente sobre a superfície, e não pela luz indireta refletida por outros objetos. Outra aproximação, mais acurada porém bem menos eficiente, é a técnica de traçado de raios recursivo (Whitted 1979) que consiste em lançar novos raios a partir de \\(P\\) (figura 4.5). Esses raios são: Um raio de sombra (shadow ray) em direção a cada fonte de luz, para saber se \\(P\\) encontra-se na sombra em relação à fonte de luz correspondente; Um raio de reflexão (reflection ray) na direção espelhada em relação ao vetor normal à superfície em \\(P\\); Um raio de refração (refraction ray) que atravessa a superfície do objeto, caso o objeto seja transparente. Figura 4.5: Traçado de raios recursivo. Os raios de reflexão e refração podem intersectar outros objetos, e novos raios podem ser gerados a partir desses pontos de interseção, recursivamente, de tal modo que a cor final refletida em \\(P\\) é formada por uma combinação da energia luminosa representada por todos os raios. Rasterização Em oposição ao ray casting, a rasterização é centrada no processamento de primitivas em vez de pixels. Cada primitiva é projetada no plano de imagem e rasterizada em seguida (figura 4.6): Figura 4.6: Rasterização. Para cada primitiva da cena: Projete a primitiva no plano de imagem. Rasterize a primitiva projetada. Modifique o framebuffer com a cor calculada em cada pixel da primitiva, exceto se o pixel do framebuffer já tiver sido preenchido anteriormente com uma primitiva mais próxima do plano de imagem. Na etapa 3, a cor do pixel é geralmente avaliada através de um modelo de iluminação local como o modelo de Blinn-Phong. Outras técnicas podem ser utilizadas para melhorar a aproximação da luz refletida no ponto amostrado, mas não há lançamento de raios ou testes de interseção como no ray casting. A rasterização é mais adequada para implementação em hardware, pois cada iteração do laço principal só precisa armazenar a primitiva que está sendo processada, juntamente com o conteúdo do framebuffer. Como resultado, o processamento de transformação geométrica de vértices e a conversão matricial podem ser paralelizados de forma massiva, como de fato ocorre nas GPUs. Referências "],["glpipeline.html", "4.3 Pipeline do OpenGL", " 4.3 Pipeline do OpenGL A figura 4.7 mostra um diagrama dos estágios de processamento do pipeline gráfico do OpenGL (fundo amarelo) e de como os dados gráficos (fundo cinza) interagem com cada estágio. As etapas programáveis são mostradas com fundo preto. No lado esquerdo há uma ilustração do resultado de cada etapa para a renderização de um triângulo colorido. Figura 4.7: Pipeline gráfico do OpenGL. Observação Para simplificar, algumas etapas do pipeline foram omitidas, tais como: O geometry shader, utilizado para o processamento de geometria após a montagem de primitivas; Os shaders de tesselação (tessellation control shader e tessellation evaluation shader), utilizados para subdivisão de primitivas; O compute shader, utilizado para processamento de propósito geral (GPGPU). Essas etapas não serão utilizadas nas atividades da disciplina pois, no momento, não fazem parte do subconjunto do OpenGL ES utilizado pelo WebGL. Entretanto, são etapas frequentemente utilizadas em aplicações para OpenGL desktop. Consulte a especificação do OpenGL 4.6 para ter acesso ao pipeline completo. Aplicação Antes de iniciar o processamento, a aplicação deve especificar o formato dos dados gráficos e enviar esses dados à memória que será acessada durante a renderização. A aplicação também deve configurar as etapas programáveis do pipeline, compostas pelo vertex shader e fragment shader. Os shaders devem ser compilados, ligados e ativados previamente. A geometria a ser processada é especificada através de um arranjo ordenado de vértices. O tipo de primitiva que será formada a partir desses vértices é determinado no comando de renderização. As primitivas suportadas pelo OpenGL são descritas a seguir e mostradas na figura 4.8: GL_POINTS: cada vértice forma um ponto que será desenhado na tela como um pixel ou como um quadrilátero centralizado no vértice. O tamanho do ponto/quadrilátero pode ser definido pelo usuário16; GL_LINES: cada grupo de dois vértices forma um segmento de reta; GL_LINE_STRIP: os vértices são conectados em ordem para formar uma polilinha; GL_LINE_LOOP: os vértices são conectados em ordem para formar uma polilinha, e o último vértice forma um segmento com o primeiro vértice, formando um laço; GL_TRIANGLES: cada grupo de três vértices forma um triângulo; GL_TRIANGLE_STRIP: os vértices formam uma faixa de triângulos com arestas compartilhadas; GL_TRIANGLE_FAN: os vértices formam um leque de triângulos de modo que todos os triângulos compartilham o primeiro vértice. Figura 4.8: Primitivas do OpenGL. Cada vértice do arranjo de vértices de entrada é composto por um conjunto de atributos definidos pela aplicação. Cada atributo pode ser um único valor ou um conjunto de valores. A forma como esses valores são interpretados depende exclusivamente do que é definido no vertex shader. Geralmente, considera-se que cada vértice tem pelo menos uma posição 2D \\((x,y)\\) ou 3D \\((x,y,z)\\). Outros atributos comuns para cada vértice são o vetor normal, cor e coordenadas de textura. Para ser utilizado pelo pipeline, o arranjo de vértices deve ser armazenado na memória como um recurso chamado Vertex Buffer Object (VBO). Cada atributo de vértice pode ser armazenado como um VBO separado, mas também é possível deixar todos os atributos em um único VBO (interleaved data). Cabe à aplicação especificar o formato dos dados de cada VBO e como eles serão lidos pelo vertex shader. Isso deve ser feito sempre antes da chamada do comando de renderização, para todos os VBOs. Alternativamente, essa configuração pode ser feita apenas uma vez e armazenada em um Vertex Array Object (VAO), bastando então ativar o VAO antes de cada renderização. Além da criação dos VBOs, a aplicação pode criar variáveis globais, chamadas de variáveis uniformes (uniform variables), que podem ser lidas pelo vertex shader e fragment shader. Essa é uma outra forma de enviar dados ao pipeline. As variáveis uniformes contêm dados apenas de leitura e que não variam de vértice para vértice, por isso o nome uniforme. Por exemplo, uma matriz de transformação geométrica pode ser armazenada como uma variável uniforme pois todos os vértices serão transformados por essa matriz durante o processamento no vertex shader (isto é, a matriz de transformação é a mesma para todos os vértices). Também é possível criar buffers de dados uniformes (Uniform Buffer Objects, ou UBOs) para enviar arranjos de dados. A especificação do OpenGL garante ser possível enviar pelo menos 16KB de dados no formato de UBOs, mas é comum os drivers oferecerem suporte a até 64KB. A aplicação também pode enviar dados ao pipeline usando buffers de texturas (buffer textures). Os valores dos texels dessas texturas podem ser lidos no vertex shader e no fragment shader como se fossem valores de arranjos unidimensionais. Esses valores podem ser interpretados como cores RGBA normalizadas entre 0 e 1 ou como valores arbitrários em ponto flutuante de até 32 bits. Uma forma mais recente e flexível de enviar dados uniformes é através dos Shader Storage Buffer Objects (SSBOs). O tamanho de um SSBO pode ser de até 128MB segundo a especificação, mas na maioria das implementações pode ser tão grande quanto a memória de vídeo disponível. Além disso, esse recurso pode ser utilizado tanto para leitura quanto escrita. Há muitas formas de enviar e receber dados da GPU. Entretanto, para deixarmos as coisas mais simples, usaremos neste curso apenas os recursos mais básicos, como VBOs, VAOs e variáveis uniformes. Vertex shader Os shaders do OpenGL são programas escritos na linguagem OpenGL Shading Language (GLSL). GLSL é similar à linguagem C, mas utiliza novas palavras-chave, novos tipos de dados, qualificadores e operações. O vertex shader processa cada vértice individualmente. Entretanto, esse processamento é paralelizado de forma massiva na GPU. Cada execução de um vertex shader acessa apenas os atributos do vértice que está sendo processado. Não há como compartilhar o estado do processamento de um vértice com os demais vértices. A entrada do vertex shader é um conjunto de atributos de vértice definidos pelo usuário. Esses atributos são alimentados pelo pipeline de acordo com os VBOs atualmente ativos. A saída do vertex shader é também um conjunto de atributos de vértice definidos pelo usuário. Esses atributos podem ser diferentes dos atributos de entrada. Além de escrever o resultado nos atributos de saída, é esperado (mas não obrigatório) que o vertex shader preencha uma variável embutida gl_Position com a posição final do vértice em um sistema de coordenadas homogêneas 4D \\((x, y, z, w)\\) chamado de espaço de recorte (clip space). Nos próximos estágios, a geometria das primitivas será determinada com bases nessas coordenadas. Veremos mais detalhes sobre os diferentes sistemas de coordenadas do OpenGL em capítulos futuros. Exemplo A seguir é exibido o código-fonte de um vertex shader: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition.x, inPosition.y * 1.5, 0, 1); fragColor = inColor / 2; } A primeira linha do vertex shader é a diretiva de pré-processamento #version que identifica a versão da especificação GLSL utilizada. Neste exemplo, 410 corresponde à especificação GLSL 4.10. Podemos substituir por #version 300 es para restringir os comandos do GLSL à especificação GLSL 3.0 ES compatível com o WebGL 2.0. Na verdade isso é necessário se quisermos rodar a aplicação em WebAssembly. Felizmente não precisamos nos preocupar com isso pois a ABCg faz esse ajuste automaticamente para nós. Em todos os exemplos que veremos nesta disciplina, usaremos funções do OpenGL/GLSL 4.1 que também são compatíveis com OpenGL ES 3.0. Nas linhas 3 e 4 são definidas as variáveis que receberão os atributos de entrada. Essas variáveis são identificadas com o qualificador in17: inPosition é uma tupla de dois elementos (vec2) que recebe uma posição 2D (a posição do vértice). inColor é uma tupla de quatro elementos (vec4) que recebe componentes de cor RGBA (a cor do vértice). O vertex shader tem apenas um atributo de saída, definido na linha 6 através da variável fragColor com o qualificador out. A função main é chamada para cada vértice processado. Para cada chamada, inPosition e inColor recebem os atributos do vértice. Na linha 9, a variável embutida gl_Position é preenchida com \\((x, \\frac{3}{2}y,0,1)\\), onde \\(x\\) e \\(y\\) são as coordenadas da posição 2D de entrada. Isso significa que a geometria sofrerá uma escala não uniforme: será esticada verticalmente. Na linha 10, a variável de saída recebe a cor de entrada com a intensidade de cada componente RGBA dividida por dois. Isso significa que a cor de saída terá a metade da intensidade da cor de entrada. Montagem de primitivas A montagem de primitivas recebe os atributos de vértices processados pelo vertex shader e monta as primitivas de acordo com o que é informado na chamada do comando de renderização. As primitivas geradas são formadas por pontos, segmentos ou triângulos. As primitivas da figura 4.8 são sempre decompostas em uma dessas três primitivas básicas. Por exemplo, se a primitiva informada pela aplicação é GL_LINE_STRIP, a polilinha será desmembrada em uma sequência de segmentos individuais. Recorte Na etapa de recorte, as primitivas que estão fora do volume de visão (fora do viewport) são descartadas ou recortadas. Por exemplo, se a ponta de um triângulo estiver fora do volume de visão, o triângulo será recortado e formará um quadrilátero, que é então decomposto em dois triângulos. Os atributos dos vértices a mais gerados no recorte são obtidos através da interpolação linear dos atributos dos vértices originais. O recorte também pode operar sobre planos de recorte definidos pelo usuário no vertex shader. Após o recorte, ocorre a divisão perspectiva, que consiste na conversão das coordenadas homogêneas 4D \\((x, y, z, w)\\) em coordenadas cartesianas 3D \\((x, y, z)\\). Isso é feito dividindo \\(x\\), \\(y\\) e \\(z\\) por \\(w\\). O sistema de coordenadas resultante é chamado de coordenadas normalizadas do dispositivo (normalized device coordinates, ou NDC). Em NDC, todas as primitivas após o recorte estão situadas dentro de um volume de visão canônico: um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Ainda nesta etapa, as componentes \\(x\\) e \\(y\\) das coordenadas em NDC são mapeadas para o sistema de coordenadas da janela (chamado de espaço da janela, ou window space), em pixels. Esse mapeamento é configurado pelo comando glViewport. O valor \\(z\\) é mapeado de \\([-1, 1]\\) para \\([0, 1]\\) por padrão, mas isso pode ser configurado com glDepthRange. Rasterização Todas as primitivas contidas no volume de visão canônico passam por uma conversão matricial, na ordem em que foram processadas nas etapas anteriores. O resultado da rasterização de cada primitiva é um conjunto de fragmentos que representam amostras da primitiva no espaço da tela. Um fragmento pode ser interpretado como um pixel em potencial. A cor final de cada pixel no framebuffer poderá ser determinada por um fragmento ou pela combinação de vários fragmentos. Cada fragmento é descrito por dados como: Posição \\((x, y, z)\\) em coordenadas da janela18, sendo que \\(z\\) é a profundidade do fragmento (por padrão, um valor no intervalo \\([0, 1]\\)). Como cada fragmento tem uma profundidade, é possível determinar qual fragmento está mais na frente quando vários fragmentos são mapeados para a mesma posição \\((x, y)\\) da janela. Assim, a cor do pixel pode ser determinada apenas pelo fragmento mais próximo. Os demais podem ser descartados pois estão sendo escondidos pelo fragmento mais próximo. Atributos interpolados a partir dos vértices da primitiva. Isso inclui todos os atributos definidos na saída do vertex shader. Por exemplo, se a saída do vertex shader devolve um atributo de cor RGB para cada vértice (uma tupla de três valores), então cada fragmento terá também uma cor RGB, com valores obtidos através da interpolação (geralmente linear) dos atributos definidos nos vértices. Fragment shader O fragment shader é um programa que processa cada fragmento individualmente após a rasterização. A entrada do fragment shader é o mesmo conjunto de atributos definidos pelo usuário na saída do vertex shader. É possível acessar também outros atributos pré-definidos que compõem o conjunto de dados de cada fragmento. Por exemplo, a posição do fragmento pode ser acessada através de uma variável embutida chamada gl_FragCoord. A saída do fragment shader geralmente é uma cor em formato RGBA (uma tupla de quatro valores), mas é possível produzir também mais de uma cor caso o pipeline tenha sido configurado para renderizar simultaneamente em vários buffers de cor. O fragment shader também pode alterar as propriedades do fragmento através de variáveis embutidas. Por exemplo, a profundidade pode ser modificada através de gl_FragDepth. Exemplo A seguir é exibido o código-fonte do fragment shader que acompanha o vertex shader mostrado no exemplo anterior: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = vec4(fragColor.r, fragColor.r, fragColor.r, 1); } Esse fragment shader só tem um atributo de entrada, definido na linha 3 pela variável fragColor. O atributo de entrada é a cor RGBA correspondente ao atributo de saída do vertex shader. A saída do fragment shader também é uma cor RGBA, definida pela variável outColor. A função main é chamada para cada fragmento processado. Para cada chamada, fragColor recebe o atributo do fragmento, que é o atributo de saída do vertex shader, mas interpolado entre os vértices da primitiva. Por exemplo, se a primitiva é um segmento formado por um vértice de cor RGB branca \\((1,1,1)\\) e outro vértice de cor preta \\((0,0,0)\\), o fragmento produzido no ponto médio do segmento terá a cor cinza \\((0.5, 0.5, 0.5)\\). Na linha 8, outColor recebe uma cor RGBA na qual as componentes RGB são uma replicação da componente R da cor de entrada. Isso significa que a cor resultante é um tom de cinza que corresponde à intensidade de vermelho da cor original. Se esse fragment shader e o vertex shader do exemplo anterior fossem utilizados no projeto Hello, World! da ABCg (seção 1.5), o triângulo resultante seria igual ao mostrado à direita na figura 4.9. Observe o efeito da mudança de escala da geometria (feita no vertex shader) e modificação das cores (intensidade reduzida pela metade no vertex shader, e conversão para tons de cinza no fragment shader). Figura 4.9: Renderização do triângulo do projeto Hello, World! com os shaders originais (esquerda) e shaders utilizados nos exemplos (direita). Operações de fragmentos Após o processamento no fragment shader, cada fragmento passa por uma sequência de testes que podem resultar em seu descarte. Se o fragmento falhar em algum desses testes, ele será ignorado e não contribuirá para a cor do pixel final. O teste de propriedade de pixel (pixel ownership test) verifica se o fragmento corresponde a um pixel do framebuffer que está de fato visível no sistema de janelas. Por exemplo, se uma outra janela estiver sobrepondo a janela do OpenGL, os fragmentos mapeados para a área sobreposta serão descartados. O teste de tesoura (scissor test), quando ativado com glEnable, descarta fragmentos que estão fora de um retângulo definido no espaço da janela pela função glScissor. Por exemplo, usando o código a seguir, o teste de tesoura será ativado e serão descartados todos os fragmentos que estiverem fora do retângulo definido pelas coordenadas \\((50,30)\\) a \\((250,130)\\) pixels no espaço da janela (o pixel de coordenada \\((0,0)\\) corresponde ao canto inferior esquerdo da janela): glEnable(GL_SCISSOR_TEST); glScissor(50, 30, 200, 100); O teste de estêncil (stencil test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação entre um valor de estêncil do fragmento (um número inteiro, geralmente de 8 bits) e o valor de estêncil do buffer de estêncil (stencil buffer), que é um dos buffers do framebuffer. Por exemplo, no código a seguir, glStencilFunc estabelece que o teste deve comparar se o valor de estêncil do fragmento é maior que 5. Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_STENCIL_TEST); glStencilFunc(GL_GREATER, 5, 0xFF) O teste de profundidade (depth test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação do valor de profundidade do fragmento (valor \\(z\\) no espaço da janela) com o valor de profundidade armazenado atualmente no buffer de profundidade (depth buffer). Com o teste de profundidade é possível fazer com que só os fragmentos mais próximos sejam exibidos. Por exemplo, no código a seguir, glDepthFunc faz com que o teste de profundidade compare se o valor de profundidade do fragmento é menor que o valor do buffer de profundidade (GL_LESS é a comparação padrão). Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LESS); Muitos desses testes podem ser realizados antes do fragment shader, em uma otimização chamada de early per-fragment test, suportada pela maioria das GPUs atuais. Por exemplo, se o fragment shader não modificar gl_FragDepth, é possível fazer o teste de profundidade logo após a rasterização, evitando o processamento de um fragmento que já se sabe que não contribuirá para a formação da imagem. Se o fragmento passou por todos os testes e não foi descartado, sua cor será utilizada para modificar o pixel correspondente no(s) buffer(s) de cor. Mesmo que o fragmento não tenha passado por todos os testes, é possível que o buffer de estêncil e buffer de profundidade sejam modificados. Esse comportamento pode ser determinado pela aplicação. Também é possível usar operações de mascaramento para permitir, por exemplo, que somente as componentes RG da cor RGB sejam escritas no buffer de cor. Antes do buffer de cor ser modificado, também é possível fazer com que a cor do fragmento seja misturada com a cor atual do buffer de cor, em uma operação de mistura de cor (color blending). Por exemplo, considere o código a seguir: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND) habilita o modo de mistura de cor. As funções glBlendEquation e glBlendFunc configuram a mistura de cor para que cada componente de cor RGBA do buffer de cor seja calculada como \\(C=C_sF_s + C_dF_d\\), onde: \\(C_s\\) é a componente de cor do fragmento (origem); \\(C_d\\) é a componente de cor do buffer de cor (destino); \\(F_s\\) é um fator de mistura de cor, que nesse caso é a componente A da cor do fragmento (GL_SRC_ALPHA); \\(F_d\\) é um fator de mistura de cor, que nesse caso é 1 menos a componente A da cor do fragmento (GL_ONE_MINUS_SRC_ALPHA). O resultado é a combinação da cor do fragmento com a cor atual do buffer de cor, usando a componente A do fragmento como fator de opacidade (1=totalmente opaco, 0=totalmente transparente). O tamanho do ponto pode ser definido através da função glPointSize ou pela variável embutida (built-in) gl_PointSize no vertex shader. Neste exemplo, o nome das variáveis de entrada também começa com o prefixo in, mas isso é só uma convenção. A posição de cada fragmento também inclui o valor recíproco da coordenada \\(w\\) no espaço de recorte. "],["coloredtriangles.html", "4.4 Triângulos coloridos", " 4.4 Triângulos coloridos Na seção 3.4, renderizamos pontos (GL_POINTS) para gerar o Triângulo de Sierpinski. Neste projeto, desenharemos triângulos (GL_TRIANGLES). Para cada quadro de exibição, renderizaremos um triângulo colorido com coordenadas 2D aleatórias dentro da janela de exibição. O resultado ficará como a seguir: Ao longo da atividade veremos com mais detalhes os comandos do OpenGL utilizados para especificar os dados gráficos e configurar o pipeline. Configuração inicial Repita a configuração inicial dos projetos anteriores e mude o nome do projeto para coloredtriangles. O arquivo abcg/examples/CMakeLists.txt ficará assim (com a compilação desabilitada para os projetos anteriores): #add_subdirectory(helloworld) #add_subdirectory(firstapp) #add_subdirectory(sierpinski) add_subdirectory(coloredtriangles) O arquivo abcg/examples/coloredtriangles/CMakeLists.txt ficará assim: project(coloredtriangles) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Como nos projetos anteriores, crie os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp em abcg/examples/coloredtriangles. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é bem similar ao utilizado no projeto sierpinski e nos projetos anteriores: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings( {.samples = 2, .vsync = true, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Colored Triangles&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Em setOpenGLSettings, .vsync = true habilita a sincronização vertical (vsync), que está desabilitada por padrão. Assim, OpenGLWindow::paintGL será chamado na mesma taxa de atualização do monitor (geralmente 60 Hz). openglwindow.hpp A definição da classe OpenGLWindow também é parecida com aquela do projeto sierpinski: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &lt;glm/vec4.hpp&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_vao{}; GLuint m_vboPositions{}; GLuint m_vboColors{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec4, 3&gt; m_vertexColors{glm::vec4{0.36f, 0.83f, 1.00f, 1.0f}, glm::vec4{0.63f, 0.00f, 0.61f, 1.0f}, glm::vec4{1.00f, 0.69f, 0.30f, 1.0f}}; void setupModel(); }; #endif No projeto anterior utilizamos apenas um VBO (m_vboVertices). Agora há dois VBOs: um para a posição dos vértices (m_vboPositions) e outro para as cores (m_vboColors). O arranjo m_vertexColors contém as cores RGBA que serão copiadas para m_vboColors. São três cores, uma para cada vértice do triângulo. openglwindow.cpp Primeiramente, vamos incluir os arquivos de cabeçalho: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;glm/vec3.hpp&gt; #include &quot;abcg.hpp&quot; Agora vamos à definição do método OpenGLWindow::initializeGL: void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window glClearColor(0, 0, 0, 1); glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } O código das linhas 35 a 44 é praticamente idêntico ao do projeto anterior. Vamos nos concentrar nos códigos dos shaders. Observe o conteúdo da string em vertexShader: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Este vertex shader define dois atributos de entrada: inPosition, que recebe a posição 2D do vértice, e inColor que recebe a cor RGBA. A saída, fragColor, é também uma cor RGBA. Na função main, a posição \\((x,y)\\) do vértice é repassada sem modificações para gl_Position. A conversão de \\((x,y)\\) em coordenadas cartesianas para \\((x,y,0,1)\\) em coordenadas homogêneas preserva a geometria do triângulo19. A cor do atributo de entrada também é repassada sem modificações para o atributo de saída. Vejamos agora o fragment shader: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } O fragment shader é ainda mais simples. O atributo de entrada (fragColor) é copiado sem modificações para o atributo de saída (outColor). A compilação e ligação dos shaders é feita pela chamada a abcg::OpenGLWindow::createProgramFromString na linha 36. Consulte a definição dessa função em abcg/abcg/abcg_openglwindow.cpp para ver quais funções do OpenGL são utilizadas. O resultado de createProgramFromString é m_program, um número inteiro que identifica o programa de shader composto pelo par de vertex/fragment shader. Para ativar o programa no pipeline, devemos chamar glUseProgram(m_program). Para desativá-lo, podemos ativar outro programa (se existir) ou chamar glUseProgram(0). O método OpenGLWindow::paintGL() é definido assim: void OpenGLWindow::paintGL() { setupModel(); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Novamente, o código é similar ao utilizado no projeto sierpinski. A função de renderização, glDrawArrays, dessa vez usa GL_TRIANGLES e 3 vértices, sendo que o índice inicial dos vértices no arranjo é 0. Isso significa que o pipeline desenhará apenas um triângulo. Em OpenGLWindow::paintUI(), usaremos controles de interface da ImGui para criar uma pequena janela de edição das três cores dos vértices: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { auto widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_vertexColors[0].x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_vertexColors[1].x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_vertexColors[2].x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } As funções ImGui::SetNextWindowPos e ImGui::SetNextWindowSize definem a posição e tamanho da janela da ImGui que está prestes a ser criada na linha 70. A janela é inicializada com alguns flags para que ela não possa ser redimensionada (ImGuiWindowFlags_NoResize) e não tenha a barra de título (ImGuiWindowFlags_NoTitleBar). Os controles ImGui::ColorEdit3 também são criados com flags para desabilitar o color picker (ImGuiColorEditFlags_NoPicker) e os tooltips (ImGuiColorEditFlags_NoTooltip), pois eles podem atrapalhar o desenho dos triângulos. A definição de OpenGLWindow::resizeGL é idêntica à do projeto sierpinski. A definição de OpenGLWindow::terminateGL também é bem semelhante e libera os recursos do pipeline: void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Vamos agora definir o método OpenGLWindow::setupModel e detalhar as funções do OpenGL que são utilizadas. A definição completa do método é dada abaixo, mas em seguida faremos uma análise mais detalhada de cada trecho: void OpenGLWindow::setupModel() { abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } As linhas 100 a 102 liberam os VBOs e o VAO, caso tenham sido criados anteriormente: abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); É importante fazer isso, pois setupModel é chamado continuamente em paintGL. Se não liberarmos os recursos, em algum momento eles consumirão toda a memória da GPU e CPU20. As linhas 104 a 114 criam arranjos com os dados que serão copiados para os VBOs: // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); Observe que as coordenadas das posições dos vértices são números pseudoaleatórios do intervalo \\([-1.5, 1.5]\\). Vimos no projeto anterior que, para uma primitiva ser vista no viewport, ela precisa ser especificada entre \\([-1, -1]\\) e \\([1, 1]\\). Logo, nossos triângulos terão partes que ficarão para fora da janela. O pipeline se encarregará de recortar os triângulos e mostrar apenas os fragmentos que estão dentro do viewport. Nas linhas 116 a 128 são criados os VBOs (um para as posições 2D, outro para as cores RGBA): // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); glGenBuffers cria o identificador de um objeto de buffer (buffer object). Um objeto de buffer é um arranjo de dados alocado pelo OpenGL, geralmente na memória da GPU. glBindBuffer com o argumento GL_ARRAY_BUFFER vincula o objeto de buffer a um buffer de atributos de vértices. Isso define o objeto de buffer como um objeto de buffer de vértice (VBO). O objeto de buffer pode ser desvinculado com glBindBuffer(0), ou vinculando outro objeto de buffer. glBufferData aloca a memória e inicializa o buffer com o conteúdo copiado de um ponteiro alocado na CPU. O primeiro parâmetro indica o tipo de objeto de buffer utilizado. O segundo parâmetro é o tamanho do buffer em bytes. O terceiro parâmetro é um ponteiro para os dados que serão copiados, na quantidade de bytes correspondente ao tamanho do buffer. O quarto parâmetro é uma dica ao driver de vídeo de como o buffer será usado. GL_STATIC_DRAW significa que o buffer será modificado apenas uma vez, potencialmente será utilizado muitas vezes, e que os dados serão usados para renderizar algo no framebuffer. Após a cópia dos dados com o glBufferData, o arranjo de origem não é mais necessário e pode ser destruído. No nosso código, positions e colors estão alocados na pilha e são liberados no fim do escopo. As linhas 130 a 132 usam glGetAttribLocation para pegar a localização de cada atributo de entrada do vertex shader de m_program: // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; O resultado de positionAttribute será 0, pois o vertex shader define inPosition com layout(location = 0). Da mesma forma, colorAttribute será 1, pois o vertex shader define inColor com layout(location = 1). Poderíamos omitir esse código e usar os valores diretamente no trecho a seguir, mas é sempre preferível fazer a consulta da localização com glGetAttribLocation do que usar valores hard-coded. Agora que sabemos a localização dos atributos inPosition e inColor no vertex shader, podemos especificar ao OpenGL como os dados de cada VBO serão mapeados para esses atributos. Isso é feito nas linhas 134 a 153 a seguir: // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); Na linha 135, glGenVertexArray cria um VAO que, como vimos no projeto sierpinski, armazena o estado da especificação de vinculação dos VBOs com o vertex shader. Neste projeto, essa especificação é feita nas linhas 140 a 150. Em paintGL, antes de chamar glDrawArrays, quando vinculamos o VAO com glBindVertexArray, o estado da configuração dos VBOs com o programa de shader é recuperado automaticamente (isto é, é como se as linhas 140 a 150 fossem executadas). Na nossa aplicação isso não parece tão útil. As linhas 140 a 150 já são executadas para todo quadro de exibição, pois chamamos setupModel logo antes de glDrawArrays. Mas, em aplicações futuras, chamaremos setupModel apenas uma vez (por exemplo, em initializeGL). Geralmente, o modelo geométrico é definido apenas uma vez e não é mais alterado (ou é raramente alterado). Nesse caso, o VAO é útil para que não tenhamos de configurar manualmente a ligação dos VBOs com os atributos do vertex shader para todo quadro de exibição. O código completo de openglwindow.cpp é mostrado a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;glm/vec3.hpp&gt; #include &quot;abcg.hpp&quot; void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window glClearColor(0, 0, 0, 1); glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } void OpenGLWindow::paintGL() { setupModel(); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { auto widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_vertexColors[0].x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_vertexColors[1].x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_vertexColors[2].x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); } void OpenGLWindow::setupModel() { abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Dica Experimente habilitar o modo de mistura de cor usando o código mostrado na seção 4.3. Inclua o código a seguir em OpenGLWindow::initializeGL: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); Além disso, mude a componente A das cores RGBA de m_vertexColors. Por exemplo, com a definição a seguir, os triângulos ficarão 50% transparentes: std::array&lt;glm::vec4, 3&gt; m_vertexColors{glm::vec4{0.36f, 0.83f, 1.00f, 0.5f}, glm::vec4{0.63f, 0.00f, 0.61f, 0.5f}, glm::vec4{1.00f, 0.69f, 0.30f, 0.5f}}; Exercício Modifique o projeto coloredtriangles para suportar novas funcionalidades: Geração de cores aleatórias nos vértices; Possibilidade de desenhar cada triângulo com uma cor sólida; Ajuste do intervalo de tempo entre a renderização de cada triângulo. Um exemplo é dado a seguir: O conceito de coordenadas homogêneas será abordado futuramente, quando trabalharmos com transformações geométricas 3D. Em geral, destruir e criar os VBOs a cada quadro de exibição não é muito eficiente. É preferível criar o VBO apenas uma vez e, se necessário, modificá-lo com glBufferData a cada quadro. Em nossa aplicação, optamos por chamar setupModel a cada paintGL apenas para manter o código mais simples. "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
