[["texturing.html", "11 Texturização", " 11 Texturização Texturização, ou mapeamento de textura, é uma técnica de síntese de imagens que consiste em mapear coordenadas de pontos de uma superfície 3D para pontos de um mapa de textura, também chamado simplesmente de textura. Em geral, uma textura é uma imagem digital armazenada como um mapa de bits. Entretanto, texturas também podem ser geradas de forma procedural através da avaliação de funções ou algoritmos. Um exemplo de textura procedural é uma textura obtida através da avaliação de um fractal ou função de geração de ruído. Os valores de uma textura podem ser utilizados para modificar as propriedades locais do material em cada ponto da superfície. Com isso, pode-se aumentar o nível de detalhes do objeto renderizado sem precisar aumentar o número de vértices da malha geométrica. A figura 11.1 mostra um exemplo de mapeamento de uma textura retangular (uma imagem bitmap) sobre uma esfera unitária que representa a Terra. A esfera é renderizada com o modelo de reflexão de Blinn-Phong e sombreamento de Phong. As cores RGB da textura são usadas neste caso para definir os valores de reflexão difusa do material (\\(\\kappa_d\\)). Uma textura que, como esta, modifica os valores de reflexão difusa, é chamada de textura difusa. Figura 11.1: Mapeamento de textura sobre uma esfera representando a Terra. No mapeamento da figura 11.1, cada ponto \\((x,y,z) \\in \\mathbb{R}^3\\) da esfera centralizada na origem e com polos alinhados com o eixo \\(y\\), corresponde a um ponto \\((u,v) \\in \\mathbb{R}^2\\) do espaço de textura. O mapeamento realizado é um mapeamento esférico, definido pelas equações \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(x, z\\right)}{2\\pi}+0.5,\\\\ v&amp;=\\frac{\\arcsin(y)}{\\pi}+0.5, \\end{align} \\] onde \\(\\textrm{arctan2}(x, z)\\) é a função atan2 que corresponde ao arco tangente de \\(x/z\\), mas que retorna um ângulo no intervalo \\([-\\pi, \\pi]\\) e usa os sinais dos argumentos para determinar o quadrante correto no plano \\(zx\\). O mapeamento esférico é detalhado na seção 11.1 junto com outras formas comuns de mapeamento. A operação de determinar o valor da textura em um ponto \\((u,v)\\) é chamada de amostragem de textura. No OpenGL é possível amostrar texturas nos shaders e configurar o comportamento do amostrador para produzir diferentes efeitos. A menor unidade de uma textura é comumente chamada de texel (texture element). Um texel é simplesmente um pixel do mapa da textura. Entretanto, o termo texel é usado no lugar de pixel neste contexto porque um pixel da textura (isto é, um texel) nem sempre corresponde exatamente a um pixel da tela. Durante a amostragem de textura, é comum que um texel seja mapeado para vários pixels do framebuffer, ou que muitos texels sejam mapeados para apenas um pixel. Podemos definir critérios de filtragem de textura para definir o que deve ser feito em cada caso. Os modos de filtragem de textura utilizados no OpenGL são descritos na seção 11.3. As coordenadas \\((u,v)\\) do espaço de textura são chamadas de coordenadas de textura. No OpenGL, a origem das coordenadas do espaço de textura é o canto inferior esquerdo da textura. O canto superior direito tem coordenadas \\((1,1)\\). É possível amostrar uma textura em coordenadas fora do intervalo \\([0,1]\\), mas nesse caso o comportamento será definido de acordo com o modo de “empacotamento” de textura da API. Os diferentes comportamentos possíveis no OpenGL são descritos na seção 11.2. Nosso foco será no uso de texturas 2D (GL_TEXTURE_2D). Entretanto, o OpenGL também fornece suporte a texturas 1D (GL_TEXTURE_1D) e 3D (GL_TEXTURE_3D). Uma textura 1D só possui uma coordenada \\(u\\) e é composta por uma sequência linear de texels. De forma semelhante, uma textura 3D possui coordenadas \\((u,v,w)\\) e é composta por um volume de texels. "],["texmapping.html", "11.1 Mapeamento", " 11.1 Mapeamento Nesta seção, veremos algumas das funções de mapeamento mais utilizadas para mapear coordenadas \\((x,y,z)\\) do espaço 3D para coordenadas \\((u,v)\\) do espaço 2D de um mapa de textura: o mapeamento planar, o mapeamento cilíndrico e o mapeamento esférico. Além desses, abordaremos também o mapeamento UV unwrap, também chamado de desdobramento UV, muito utilizado em programas de modelagem 3D e jogos. Para simplificar, vamos considerar nesta seção que as coordenadas de textura estão restritas ao intervalo \\([0,1]\\). Entretanto, isso não é uma limitação rígida. Coordenadas fora desse intervalo podem ser tratadas de acordo com as abordagens descritas na seção 11.2. Mapeamento planar O mapeamento planar consiste em uma projeção linear e paralela dos pontos do espaço 3D para o plano do espaço de textura 2D. Geralmente a projeção é feita na direção de algum eixo principal do espaço 3D. Por exemplo, um mapeamento planar na direção do eixo \\(x\\) pode ser definido como: \\[ \\begin{align} u&amp;=1-z,\\\\ v&amp;=y. \\end{align} \\] Considere o mapa de textura difusa mostrado na figura 11.2. Usando mapeamento planar na direção de \\(x\\), um cubo unitário de \\((0,0,0)\\) a \\((1,1,1)\\) mapeado com essa textura terá a aparência mostrada na figura 11.3. Figura 11.2: Textura difusa com padrão de teste. Figura 11.3: Mapeamento planar na direção x sobre um cubo unitário. Observe como o lado de cima e lado esquerdo do cubo repetem, respectivamente, a cor dos texels com \\(v=1\\) e \\(u=0\\) (a mudança de tom é resultado da iluminação). Isso acontece porque todos os pontos ao longo de uma reta na direção \\(x\\) são mapeados para um mesmo texel. Um mapeamento planar na direção do eixo \\(y\\) pode ser definido como: \\[ \\begin{align} u&amp;=x,\\\\ v&amp;=1-z. \\end{align} \\] A figura 11.4 mostra o resultado desse mapeamento sobre o cubo unitário. Figura 11.4: Mapeamento planar na direção y sobre um cubo unitário. Um mapeamento planar na direção do eixo \\(z\\) pode ser definido como: \\[ \\begin{align} u&amp;=x,\\\\ v&amp;=y. \\end{align} \\] A figura 11.5 mostra o resultado desse mapeamento sobre o cubo unitário. Figura 11.5: Mapeamento planar na direção z sobre um cubo unitário. Os mapeamentos planares nas direções \\(x\\), \\(y\\) e \\(z\\) podem ser combinados para formar um mapeamento triplanar. A ideia consiste em calcular três pares de coordenadas \\((u,v)\\), um para cada mapeamento planar, \\[ (u_x,v_x),\\qquad(u_y,v_y),\\qquad(u_z,v_z), \\] e então combinar o valor dos texels amostrados com essas coordenadas de acordo com algum critério. Um critério simples é calcular uma média ponderada dos texels. Os pesos da média podem ser as coordenadas \\((n_x, n_y, n_z)\\) do vetor normal normalizado, em valor absoluto. Por exemplo, se \\(T(u,v)\\) é o texel amostrado na posição \\((u,v)\\) do espaço de textura, a combinação do mapeamento triplanar pode ser calculada como \\[ T(u_x, v_x)|n_x| + T(u_y, v_y)|n_y| + T(u_z, v_z)|n_z|. \\] Na renderização do cubo unitário, o resultado será semelhante ao exibido na figura 11.6. Figura 11.6: Mapeamento triplanar sobre um cubo unitário. Mapeamento cilíndrico No mapeamento cilíndrico, a textura é mapeada de tal forma que, se o objeto renderizado é um cilindro, o resultado será equivalente a envolver a área lateral do cilindro com a textura. A figura 11.7 ilustra um exemplo de mapeamento cilíndrico mostrando a visão de frente e de trás do cilindro. Neste caso, consideramos que o cilindro é unitário, alinhado e centralizado com o eixo \\(y\\), com base em \\(y=0\\). Observe como os lados esquerdo (\\(u=0\\)) e direito (\\(u=1\\)) da textura se unem na parte de trás do cilindro. Figura 11.7: Mapeamento cilíndrico em um cilindro unitário. Seja \\(\\mathbf{p}=\\begin{bmatrix}p_x &amp; p_y &amp; p_z\\end{bmatrix}^T\\) um ponto do espaço euclidiano. O mapeamento cilíndrico para o cilindro da figura 11.7 é definido a partir do ângulo \\(\\theta\\) que \\(\\mathbf{p}\\) forma em torno do eixo \\(y\\), e a elevação \\(p_y\\) (figura 11.8): Figura 11.8: Geometria do mapeamento cilíndrico. O ângulo \\(\\theta \\in [-\\pi, \\pi]\\) é mapeado para \\(u \\in [0,1]\\). A altura \\(y\\) é mapeada diretamente para \\(v\\), isto é, \\(v=p_y\\). Observe que \\[ \\tan{\\theta}=\\frac{p_x}{p_z}. \\] Logo, \\[ \\theta=\\arctan\\left({\\frac{p_x}{p_z}}\\right). \\] O ângulo é calculado corretamente para \\(p_z&gt;0\\). Entretanto, a imagem da função arco tangente está restrita ao intervalo \\(\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)\\). Para que \\(\\theta\\) seja um ângulo em um intervalo de 360 graus, precisamos ajustar o intervalo da função arco tangente de acordo com o sinal de \\(p_x\\) e \\(p_z\\). Como já vimos, isso pode ser obtido através da função atan2 Essa função está presente nas bibliotecas matemáticas das principais linguagens de programação. Por exemplo, na biblioteca padrão do C++, a função é implementada por std::atan2. Em GLSL, o nome da função é atan e recebe dois parâmetros (a função com um parâmetro é a função arco tangente convencional). A conversão de \\(\\theta \\in [-\\pi, \\pi]\\) para \\(u \\in [0,1]\\) é obtida com o simples mapeamento linear: \\[ u = \\frac{\\theta}{2\\pi}+0.5. \\] Logo, o mapeamento cilíndrico é definido como \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5,\\\\ v&amp;=p_y, \\end{align} \\] Mapeamento esférico No mapeamento esférico, a textura é mapeada de tal forma que, se o objeto renderizado é uma esfera centralizada na origem, o resultado será equivalente a envolver a esfera fazendo com que \\(u\\) e \\(v\\) sejam, respectivamente, a longitude e a latitude, como em uma projeção cilíndrica equidistante usada em cartografia. Vamos considerar que os polos estão alinhados ao eixo \\(y\\). Desse modo: Os texels da linha \\(v=0\\) e \\(v=1\\) serão mapeados, respectivamente, para o polo sul e polo norte; Os texels com \\(v=0.5\\) serão mapeados para o equador da esfera, que neste caso é o plano \\(y=0\\); Os texels com \\(u=0\\) e \\(u=1\\) serão mapeados para o meridiano central no plano \\(yz\\) com \\(z&lt;0\\). A figura 11.9 mostra um exemplo de esfera texturizada com mapeamento esférico, visto de frente e de cima (polo norte). Figura 11.9: Mapeamento esférico em uma esfera unitária. Seja \\(\\mathbf{p}=\\begin{bmatrix}p_x &amp; p_y &amp; p_z\\end{bmatrix}^T\\) um ponto do espaço euclidiano. O mapeamento cilíndrico é definido a partir do ângulo \\(\\theta\\) (longitude) e \\(\\phi\\) (latitude) da esfera que contém \\(\\mathbf{p}\\), como mostra a geometria da figura 11.10. Figura 11.10: Geometria do mapeamento esférico. A coordenada \\(u\\) é calculada como no mapeamento cilíndrico: \\[ u=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5. \\] Para determinar \\(v\\), observe, na figura 11.10, que \\[ \\begin{align} \\sin \\phi = \\frac{p_y}{|\\mathbf{p}|}.\\\\ \\end{align} \\] Logo, \\[ \\phi = \\arcsin \\left( \\frac{p_y}{|\\mathbf{p}|} \\right), \\] onde \\[ |\\mathbf{p}|=\\sqrt{x^2+y^2+z^2}. \\] A conversão de \\(\\phi \\in \\left[-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right]\\) para \\(v \\in [0,1]\\) é obtida com o mapeamento linear: \\[ v = \\frac{\\phi}{\\pi}+0.5. \\] Logo, o mapeamento esférico é definido como \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5,\\\\ v&amp;=\\frac{\\arcsin\\left(\\dfrac{p_y}{|\\mathbf{p}|}\\right)}{\\pi}+0.5, \\end{align} \\] Mapeamento UV unwrap Em vez de usar as funções anteriores de mapeamento de textura, podemos definir diretamente quais são as coordenadas \\((u,v)\\) de cada vértice da malha geométrica. Isso pode ser feito através da inclusão de um atributo adicional de vértice, de forma semelhante como fizemos para a definição de cores nos vértices ou normais de vértices. Durante a rasterização, as coordenadas de textura definidas nos vértices são interpoladas para cada fragmento da primitiva. Assim, cada fragmento terá coordenadas de textura interpoladas, e a textura pode então ser amostrada no fragment shader. Para determinar as coordenadas \\((u,v)\\) de cada vértice de uma malha poligonal, técnicas de modelagem geométrica podem ser utilizadas para “desdobrar” a malha sobre o plano da textura, em um processo chamado de UV unwrap ou desdobramento UV. A figura 11.11 mostra um exemplo de objeto 3D renderizado com mapeamento UV unwrap. O objeto é uma lâmpada à óleo romana do acervo do Museu Britânico. O modelo original tem 500 mil triângulos e foi obtido através de varredura por um scanner 3D. Aqui, o modelo exibido tem apenas 10 mil triângulos para facilitar a visualização do mapeamento no sistema de coordenadas de textura. Observe, no mapa UV, como a malha é recortada em diferentes pedaços para ser desdobrada sem sobreposições sobre o plano. O mapeamento procura manter a proporção entre a área dos polígonos originais de modo que a densidade dos texels amostrados na superfície seja a mais uniforme possível. Figura 11.11: Mapeamento UV. Como referência, a figura 11.12 mostra o mapa de textura utilizado e o objeto sem texturização. Figura 11.12: Mapa de textura da lâmpada romana, e objeto sem textura. O desdobramento UV é a técnica mais utilizada de mapeamento de texturas em programas de modelagem e renderização, como o Blender. O formato OBJ suporta modelos com coordenadas de textura por vértice definidas através de desdobramento UV. "],["texwrapping.html", "11.2 Empacotamento", " 11.2 Empacotamento No OpenGL, o modo de empacotamento da textura (em inglês, texture wrapping) define o comportamento do amostrador de textura quando as coordenadas \\((u,v)\\) estão fora do intervalo \\([0, 1]\\). Há três comportamentos principais: GL_REPEAT: repete a textura fora do intervalo. Esse é o modo padrão de empacotamento. GL_MIRRORED_REPEAT: igual ao anterior, mas a textura é espelhada em \\(u\\) e/ou \\(v\\) quando a parte inteira da coordenada é um número ímpar. GL_CLAMP_TO_EDGE: Fixa as coordenadas no intervalo \\([0, 1]\\). O resultado é a repetição dos valores das primeiras e últimas linhas/colunas da textura. A figura 11.13 mostra o resultado dos diferentes modos de empacotamento no intervalo de \\((-1,-1)\\) a \\((2,2)\\) no espaço de textura. Figura 11.13: Modos de empacotamento do OpenGL. O modo de repetição (GL_REPEAT) é frequentemente utilizado para produzir padrões formados por texturas ininterruptas (seamless textures). Essas texturas, quando dispostas lado a lado ou como um ladrilho, formam um padrão contínuo. Um exemplo é mostrado na figura 11.14. Figura 11.14: Textura ininterrupta com empacotamento GL_REPEAT (adaptado do original). O modo de empacotamento é configurado com a função glTexParameteri. É possível configurar um comportamento diferente para a direção \\(u\\) (com GL_TEXTURE_WRAP_S) e direção \\(v\\) (com GL_TEXTURE_WRAP_T). Por exemplo, o código a seguir habilita o modo de repetição em \\(u\\) e o modo de repetição espelhada em \\(v\\): glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_MIRRORED_REPEAT); Em texturas 3D, é possível usar ainda a direção \\(w\\) (com GL_TEXTURE_WRAP_R). Outros modos de empacotamento estão disponíveis além dos modos GL_REPEAT, GL_MIRRORED_REPEAT e GL_CLAMP_TO_EDGE, mas não são suportados em todas as especificações do OpenGL. Por exemplo, o modo GL_CLAMP_TO_BORDER, que usa uma cor sólida para texels fora do intervalo \\([0,1]\\), não é suportado no OpenGL ES utilizado no WebGL. "],["texfiltering.html", "11.3 Filtragem", " 11.3 Filtragem Na amostragem de texturas, raramente temos um mapeamento \\(1:1\\) entre texels e pixels. Geralmente, cada pixel corresponde a vários texels, ou um mesmo texel pode ser mapeado a vários pixels. Tanto o problema de minificação (mais de um texel mapeado para um único pixel) quanto de magnificação (mais de um pixel mapeado para um mesmo texel) exigem o uso de filtros de interpolação para calcular a cor correspondente a uma posição \\((u,v)\\) no espaço da textura. No OpenGL, o funcionamento desses filtros pode ser configurado de forma independente usando os identificadores GL_TEXTURE_MAG_FILTER e GL_TEXTURE_MIN_FILTER com a função glTexParameteri. Magnificação Os filtros de magnificação (GL_TEXTURE_MAG_FILTER) são dois: Interpolação por vizinho mais próximo (GL_NEAREST): consiste em usar o valor do texel que está mais próximo da posição \\((u,v)\\) de amostragem, considerando a menor distância Manhattan entre os quatro texels mais próximos. Assim, os texels da textura aumentada têm um aspecto pixelado. Esse modo pode ser habilitado com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); Interpolação bilinear (GL_LINEAR): consiste em realizar uma média entre os quatro texels mais próximos da posição \\((u, v)\\) de amostragem. Isso é feito calculando a interpolação linear entre dois pares de texels em uma direção (por exemplo, direção \\(u\\)), e calculando em seguida a interpolação linear dos dois valores resultantes na outra direção (por exemplo, direção \\(v\\)). Essa é a filtragem padrão do OpenGL. A interpolação bilinear é ativada com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); A figura 11.15 mostra a comparação entre os dois filtros em um mesmo detalhe ampliado de textura. Figura 11.15: Exemplo de uso de filtros de magnificação do OpenGL. Minificação Os filtros de minificação do OpenGL (GL_TEXTURE_MIN_FILTER) também incluem os filtros de interpolação por vizinho mais próximo (GL_NEAREST) e interpolação bilinear (GL_LINEAR), e podem ser ativados respectivamente com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); e glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); Entretanto, na minificação, é possível que mais do que quatro texels sejam mapeados para um mesmo pixel. Em tais casos, a filtragem bilinear usando somente os quatro vizinhos mais próximos não é suficiente, e erros de aliasing podem ser introduzidos. Uma forma eficiente de reduzir esse problema é através da técnica de mipmapping. Mipmapping Um mipmap é uma sequência ordenada de texturas repetidas, cada uma com metade da resolução da textura anterior. O nível base ou nível 0 do mipmap é a textura em sua resolução original. A textura do nível seguinte contém metade da resolução da textura do nível anterior. No último nível do mipmap, a textura tem tamanho \\(1 \\times 1\\). A figura 11.16 mostra todos os níveis de um mipmap para uma textura de resolução \\(256 \\times 256\\). Figura 11.16: Níveis de mipmap. Se a textura é quadrada e a resolução é uma potência de dois, um texel de um nível do mipmap é definido de tal forma que seu valor é exatamente a média entre quatro texels do nível anterior. Isso pode ser visto na figura 11.17, que mostra o detalhe ampliado dos três últimos níveis do mipmap da figura 11.16. Observe, no nível 8, que a cor do único texel resultante é cinza. Essa cor cinza é a média de todos os texels do nível anterior (nível 7), que é o mesmo que a média de todos os texels da textura. De forma semelhante, no nível 7, o texel mais escuro (canto inferior esquerdo) é a média dos texels de \\((0,0)\\) até \\((0.5, 0.5)\\) dos níveis anteriores. Figura 11.17: Detalhe ampliado dos três últimos níveis de um mipmap. A figura 11.18 mostra uma comparação da qualidade de renderização de um padrão de xadrez sobre um plano, sem e com mipmapping. A renderização sem mapping introduz artefatos de aliasing espacial resultantes da subamostragem. Figura 11.18: Comparação de renderização sem mipmapping (esquerda) e com mipmapping (direita) (fonte). No OpenGL, um mipmap pode ser gerado facilmente com a função glGenerateMipmap. Quando a função é chamada, o mipmap é gerado para a textura atualmente ligada com glBindTexture. Por exemplo, glBindTexture(GL_TEXTURE_2D, textureID); glGenerateMipmap(GL_TEXTURE_2D); gera o mipmap para a textura com identificador textureID. Mipmapping é a técnica de filtrar texels usando mipmaps. Quando uma textura com mipmap é amostrada, o amostrador primeiro determina qual nível do mipmap será utilizado. Isso é calculado com base na estimativa do número de texels mapeados para o pixel. Por exemplo, se a área do texel projetado no espaço da janela é tal que todos os texels da textura cabem em um único pixel, então a textura amostrada é a textura do último nível (textura \\(1 \\times 1\\)), pois o texel dessa textura contém a média de todos os texels. No OpenGL, há diferentes modos de filtragem usando mipmapping (válido apenas para GL_TEXTURE_MIN_FILTER): GL_NEAREST_MIPMAP_NEAREST: escolhe o nível de mipmap que mais se aproxima do tamanho do pixel que está sendo texturizado, e então amostra tal nível usando a interpolação por vizinho mais próximo. GL_LINEAR_MIPMAP_NEAREST: escolhe o nível de mipmap que mais se aproxima do tamanho do pixel que está sendo texturizado, e então amostra tal nível usando a interpolação bilinear (média dos quatro texels mais próximos). GL_NEAREST_MIPMAP_LINEAR: escolhe os dois níveis de mipmap que mais se aproximam do tamanho do pixel que está sendo texturizado, e então amostra os dois níveis usando a interpolação por vizinho mais próximo. O valor final é então calculado como a média entre os dois valores amostrados, ponderada de acordo com a proximidade dos níveis de mipmap ao tamanho do pixel. Esse é o modo padrão de filtro de minificação. GL_LINEAR_MIPMAP_LINEAR: escolhe os dois níveis de mipmap que mais se aproximam do tamanho do pixel que está sendo texturizado, e então amostra os dois níveis usando interpolação bilinear. O valor final é então calculado como a média entre os dois valores amostrados, ponderada de acordo com a proximidade dos níveis de mipmap ao tamanho do pixel. O modo GL_LINEAR_MIPMAP_LINEAR também é chamado de interpolação trilinear, pois faz uma interpolação linear sobre dois valores obtidos com interpolação bilinear. Esse é o modo de filtragem que produz resultados com menos artefatos de aliasing, mas também é o mais custoso. "],["viewer4.html", "11.4 Texturização na prática", " 11.4 Texturização na prática Nesta seção, daremos continuidade ao projeto do visualizador de modelos 3D apresentado na seção 10.6. Esta será a versão 4 do visualizador (viewer4) e terá um shader que usa uma textura para modificar as propriedades de reflexão difusa (\\(\\kappa_d\\)) e ambiente (\\(\\kappa_a\\)) do material utilizado no modelo de reflexão de Blinn–Phong. Se o objeto lido do arquivo OBJ já vier com coordenadas de textura definidas em seus vértices (mapeamento UV unwrap), o visualizador usará essas coordenadas para amostrar a textura. Entretanto, também poderemos selecionar, através da interface da ImGui, um mapeamento pré-definido: triplanar, cilíndrico ou esférico. Nesta nova versão, o botão “Load 3D Model” será transformado em um item do menu “File”. Há também uma opção de menu para carregar uma textura como um arquivo de imagem no formato PNG ou JPEG. O resultado ficará como a seguir: Como o código dessa versão contém apenas mudanças incrementais em relação ao anterior, nosso foco será apenas nessas mudanças. Baixe o código completo deste link. Carregando texturas Para carregar uma textura no OpenGL, primeiro devemos chamar a função glGenTextures que cria um ou mais recursos de textura. Por exemplo, para criar apenas uma textura, podemos fazer glGenTextures(1, &amp;textureID); onde textureID é uma variável do tipo GLuint que será preenchida com o identificador do recurso de textura criado pelo OpenGL. Em seguida, ligamos o recurso de textura a um “alvo de textura”, que é GL_TEXTURE_2D para texturas 2D: glBindTexture(GL_TEXTURE_2D, textureID); Neste momento, a textura ainda está vazia. Para definir seu conteúdo, devemos ter um mapa de bits. Podemos carregar o conteúdo do mapa de bits através da função glTexImage2D. Por exemplo, considere o código a seguir: glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, 800, 600, 0, GL_RGBA, GL_UNSIGNED_BYTE, pixels); glTexImage2D copia o mapa de bits contido no ponteiro pixels, supondo que o mapa é um arranjo de \\(800 \\times 600\\) pixels. A função considera que cada pixel é uma tupla de valores RGBA (GL_RGBA), e que cada componente de cor é um byte sem sinal (GL_UNSIGNED_BYTE). Na ABCg podemos usar a função auxiliar abcg::loadOpenGLTexture. Essa função recebe uma estrutura abcg::OpenGLTextureCreateInfo com informações sobre a criação da textura, tal como o nome de um arquivo de imagem. O valor de retorno é o identificar da textura criada. Internamente, a função usa funções da SDL para carregar o mapa de bits, e em seguida chama as funções do OpenGL para criar o recurso de textura. Assim, para criar uma textura a partir de um arquivo imagem.png, podemos fazer simplesmente: textureID = abcg::loadOpenGLTexture({.path = &quot;imagem.png&quot;}); Por padrão, abcg::loadOpenGLTexture cria também o mipmap da textura e usa o filtro de minificação GL_LINEAR_MIPMAP_LINEAR. Se quisermos que o mipmap não seja gerado, basta mudar o valor de OpenGLTextureCreateInfo::generateMipmaps para false: textureID = abcg::opengl::loadTexture({.path = &quot;imagem.png&quot;, .generateMipmaps = false}); Para destruir a textura e liberar seus recursos, devemos chamar manualmente glDeleteTextures. Por exemplo, o seguinte código libera a textura textureID criada com glGenTextures ou abcg::loadOpenGLTexture. glDeleteTextures(1, &amp;textureID); Nessa nova versão do visualizador, a classe Model implementa a função membro Model::loadDiffuseTexture, que carrega um arquivo de imagem e gera um identificador de recurso de textura difusa na variável m_diffuseTexture. A função é definida como a seguir (em model.cpp): void Model::loadDiffuseTexture(std::string_view path) { if (!std::filesystem::exists(path)) return; abcg::glDeleteTextures(1, &amp;m_diffuseTexture); m_diffuseTexture = abcg::loadOpenGLTexture({.path = path}); } A função recebe em path o caminho contendo o nome do arquivo de imagem PNG ou JPEG. Se o arquivo não existir, a função simplesmente retorna (linha 72). Caso contrário, o recurso de textura anterior é liberado, e abcg::loadOpenGLTexture é chamada para criar a nova textura. Carregando modelos com textura Um arquivo OBJ pode vir acompanhado de um arquivo .mtl opcional que contém a descrição das propriedades dos materiais de cada objeto1. Por exemplo, o arquivo roman_lamp.obj (lâmpada romana da figura 11.11) vem acompanhado do arquivo roman_lamp.mtl que tem o seguinte conteúdo: newmtl roman_lamp Ns 25.0000 Ni 1.5000 Tr 0.0000 Tf 1.0000 1.0000 1.0000 illum 2 Ka 0.2000 0.2000 0.2000 Kd 1.0000 1.0000 1.0000 Ks 0.6000 0.6000 0.6000 Ke 0.0000 0.0000 0.0000 map_Ka maps/roman_lamp_diffuse.jpg map_Kd maps/roman_lamp_diffuse.jpg map_bump maps/roman_lamp_normal.jpg bump maps/roman_lamp_normal.jpg Entre outras coisas, o arquivo contém o valor do expoente de brilho especular (Ns) e as propriedades de reflexão ambiente (Ka), difusa (Kd) e especular (Ks). Além disso, o arquivo contém o nome do mapa de textura que deve ser utilizado para modificar a reflexão ambiente (map_Ka) e reflexão difusa (map_Kd). Há outras propriedades, mas elas não serão utilizadas no momento. Nossa implementação de Model::loadObj, que utiliza funções da TinyObjLoader, carrega automaticamente a textura difusa, se ela existir. A definição completa dessa versão atualizada de Model::loadObj é mostrada seguir (arquivo model.cpp). void Model::loadObj(std::string_view path, bool standardize) { auto const basePath{std::filesystem::path{path}.parent_path().string() + &quot;/&quot;}; tinyobj::ObjReaderConfig readerConfig; readerConfig.mtl_search_path = basePath; // Path to material files tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data(), readerConfig)) { if (!reader.Error().empty()) { throw abcg::RuntimeError( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error())); } throw abcg::RuntimeError(fmt::format(&quot;Failed to load model {}&quot;, path)); } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } auto const &amp;attrib{reader.GetAttrib()}; auto const &amp;shapes{reader.GetShapes()}; auto const &amp;materials{reader.GetMaterials()}; m_vertices.clear(); m_indices.clear(); m_hasNormals = false; m_hasTexCoords = false; // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (auto const &amp;shape : shapes) { // Loop over indices for (auto const offset : iter::range(shape.mesh.indices.size())) { // Access to vertex auto const index{shape.mesh.indices.at(offset)}; // Position auto const startIndex{3 * index.vertex_index}; glm::vec3 position{attrib.vertices.at(startIndex + 0), attrib.vertices.at(startIndex + 1), attrib.vertices.at(startIndex + 2)}; // Normal glm::vec3 normal{}; if (index.normal_index &gt;= 0) { m_hasNormals = true; auto const normalStartIndex{3 * index.normal_index}; normal = {attrib.normals.at(normalStartIndex + 0), attrib.normals.at(normalStartIndex + 1), attrib.normals.at(normalStartIndex + 2)}; } // Texture coordinates glm::vec2 texCoord{}; if (index.texcoord_index &gt;= 0) { m_hasTexCoords = true; auto const texCoordsStartIndex{2 * index.texcoord_index}; texCoord = {attrib.texcoords.at(texCoordsStartIndex + 0), attrib.texcoords.at(texCoordsStartIndex + 1)}; } Vertex const vertex{ .position = position, .normal = normal, .texCoord = texCoord}; // If hash doesn&#39;t contain this vertex if (!hash.contains(vertex)) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } // Use properties of first material, if available if (!materials.empty()) { auto const &amp;mat{materials.at(0)}; // First material m_Ka = {mat.ambient[0], mat.ambient[1], mat.ambient[2], 1}; m_Kd = {mat.diffuse[0], mat.diffuse[1], mat.diffuse[2], 1}; m_Ks = {mat.specular[0], mat.specular[1], mat.specular[2], 1}; m_shininess = mat.shininess; if (!mat.diffuse_texname.empty()) loadDiffuseTexture(basePath + mat.diffuse_texname); } else { // Default values m_Ka = {0.1f, 0.1f, 0.1f, 1.0f}; m_Kd = {0.7f, 0.7f, 0.7f, 1.0f}; m_Ks = {1.0f, 1.0f, 1.0f, 1.0f}; m_shininess = 25.0f; } if (standardize) { Model::standardize(); } if (!m_hasNormals) { computeNormals(); } createBuffers(); } Note que, logo no início da função, definimos um objeto readerConfig (linha 81) que é utilizado como argumento de ObjReader::ParseFromFile da TinyObjLoader (linha 86) para informar o diretório onde estão os arquivos de materiais. Por padrão, esse caminho é o mesmo diretório onde está o arquivo OBJ: void Model::loadObj(std::string_view path, bool standardize) { auto const basePath{std::filesystem::path{path}.parent_path().string() + &quot;/&quot;}; tinyobj::ObjReaderConfig readerConfig; readerConfig.mtl_search_path = basePath; // Path to material files tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data(), readerConfig)) { if (!reader.Error().empty()) { throw abcg::RuntimeError( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error())); } throw abcg::RuntimeError(fmt::format(&quot;Failed to load model {}&quot;, path)); } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } Nas linhas 158 a 174, as propriedades do primeiro material são utilizadas. Se não houver nenhum material, valores padrão são utilizados: // Use properties of first material, if available if (!materials.empty()) { auto const &amp;mat{materials.at(0)}; // First material m_Ka = {mat.ambient[0], mat.ambient[1], mat.ambient[2], 1}; m_Kd = {mat.diffuse[0], mat.diffuse[1], mat.diffuse[2], 1}; m_Ks = {mat.specular[0], mat.specular[1], mat.specular[2], 1}; m_shininess = mat.shininess; if (!mat.diffuse_texname.empty()) loadDiffuseTexture(basePath + mat.diffuse_texname); } else { // Default values m_Ka = {0.1f, 0.1f, 0.1f, 1.0f}; m_Kd = {0.7f, 0.7f, 0.7f, 1.0f}; m_Ks = {1.0f, 1.0f, 1.0f, 1.0f}; m_shininess = 25.0f; } Durante a leitura dos atributos dos vértices, Model::loadObj também verifica se a malha contém coordenadas de textura. Isso é feito nas linhas 134 a 141: // Texture coordinates glm::vec2 texCoord{}; if (index.texcoord_index &gt;= 0) { m_hasTexCoords = true; auto const texCoordsStartIndex{2 * index.texcoord_index}; texCoord = {attrib.texcoords.at(texCoordsStartIndex + 0), attrib.texcoords.at(texCoordsStartIndex + 1)}; } Se o vértice contém coordenadas de textura (linha 136), o flag m_hasTexCoords é definido como true e as coordenadas são carregadas em texCoord. Se o arquivo OBJ não tiver coordenadas de textura, texCoord será (0, 0) para todos os vértices. A estrutura Vertex é criada com o novo atributo de coordenadas de textura: Vertex const vertex{ .position = position, .normal = normal, .texCoord = texCoord}; texCoord é um novo atributo de Vertex (um glm::vec2), criado de forma semelhante ao modo como criamos o atributo normal no projeto viewer2 (seção 10.5), isto é, usamos o atributo como chave de hash e carregamos seus dados no formato de um VBO de dados intercalados. Renderizando Em Model::render (chamado em Window::onPaint), incluímos a ativação da textura no pipeline. A definição completa ficará como a seguir: void Model::render(int numTriangles) const { abcg::glBindVertexArray(m_VAO); abcg::glActiveTexture(GL_TEXTURE0); abcg::glBindTexture(GL_TEXTURE_2D, m_diffuseTexture); // Set minification and magnification parameters abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); // Set texture wrapping parameters abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); auto const numIndices{(numTriangles &lt; 0) ? m_indices.size() : numTriangles * 3}; abcg::glDrawElements(GL_TRIANGLES, numIndices, GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); } Na linha 190, a função glActiveTexture é chamada para ativar a primeira “unidade de textura” (GL_TEXTURE0). O número de unidades de textura ativas corresponde ao número de texturas diferentes que podem ser utilizadas ao mesmo tempo em um programa de shader. Para cada estágio de shader (por exemplo, para o vertex shader, ou para o fragment shader) é possível acessar pelo menos 16 unidades de textura ao mesmo tempo. Esse número pode ser maior dependendo da implementação do driver. Se considerarmos todas os estágios de shaders disponíveis, então podemos ativar pelo menos 32 texturas ao mesmo tempo no OpenGL ES (16 para o vertex shader, mais 16 para o fragment shader), e 80 no OpenGL para desktop (16 para cada um dos 5 estágios de shaders). Como queremos manter as coisas simples, nosso visualizador por enquanto só utilizará uma unidade de textura. Nas próximas versões veremos como usar mais unidades. Após a ativação da unidade de textura, a função glBindTexture é chamada para ligar o identificar de textura à unidade recém ativada. Nas linha 193 a 199, a função glTexParameteri é chamada para configurar os filtros de textura e modos de empacotamento. Os valores aqui utilizados são os valores padrão do OpenGL. Isso é tudo para configurar a texturização. O restante agora é feito nos shaders. O projeto viewer4 define dois novos shaders com suporte a texturas: texture.vert e texture.frag. Esses shaders são bem similares aos shaders do modelo de Blinn–Phong. Observação Para o conteúdo de assets ficar mais organizado, a partir desta versão do visualizador, as texturas ficarão armazenadas em assets/maps, e os shaders ficarão armazenados em assets/shaders. Os arquivos .obj continuam na pasta assets, agora junto também com os arquivos .mtl. texture.vert O código completo do shader é mostrado a seguir: #version 300 es layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; layout(location = 2) in vec2 inTexCoord; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; uniform vec4 lightDirWorldSpace; out vec3 fragV; out vec3 fragL; out vec3 fragN; out vec2 fragTexCoord; out vec3 fragPObj; out vec3 fragNObj; void main() { vec3 P = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 N = normalMatrix * inNormal; vec3 L = -(viewMatrix * lightDirWorldSpace).xyz; fragL = L; fragV = -P; fragN = N; fragTexCoord = inTexCoord; fragPObj = inPosition; fragNObj = inNormal; gl_Position = projMatrix * vec4(P, 1.0); } Esse código contém algumas poucas modificações em relação ao conteúdo de blinnphong.vert. Observe como agora temos o atributo de entrada inTexCoord (linha 5) contendo as coordenadas de textura lidas do arquivo OBJ. O vertex shader não faz nenhum processamento com as coordenadas de textura e simplesmente passa-as adiante para o fragment shader através do atributo de saída fragTexCoord (linha 17). O vertex shader também possui dois outros atributos adicionais de saída: fragPObj (linha 18) é a posição do vértice no espaço do objeto (isto é, uma cópia de inPosition); fragNObj (linha 19) é o vetor normal no espaço do objeto (isto é, uma cópia de inNormal). Esses atributos são utilizados para calcular, no fragment shader, as coordenadas de textura do mapeamento triplanar, cilíndrico ou esférico (o vetor normal só é utilizado no mapeamento triplanar). Se o mapeamento utilizado é aquele fornecido pelo arquivo OBJ, isto é, o mapeamento determinado pelas coordenada de textura de inTexCoord, então os atributos fragPObj e fragNObj não são utilizados. texture.frag O código completo do shader é mostrado a seguir: #version 300 es precision mediump float; in vec3 fragN; in vec3 fragL; in vec3 fragV; in vec2 fragTexCoord; in vec3 fragPObj; in vec3 fragNObj; // Light properties uniform vec4 Ia, Id, Is; // Material properties uniform vec4 Ka, Kd, Ks; uniform float shininess; // Diffuse texture sampler uniform sampler2D diffuseTex; // Mapping mode // 0: triplanar; 1: cylindrical; 2: spherical; 3: from mesh uniform int mappingMode; out vec4 outColor; // Blinn-Phong reflection model vec4 BlinnPhong(vec3 N, vec3 L, vec3 V, vec2 texCoord) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { V = normalize(V); vec3 H = normalize(L + V); float angle = max(dot(H, N), 0.0); specular = pow(angle, shininess); } vec4 map_Kd = texture(diffuseTex, texCoord); vec4 map_Ka = map_Kd; vec4 diffuseColor = map_Kd * Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = map_Ka * Ka * Ia; return ambientColor + diffuseColor + specularColor; } // Planar mapping vec2 PlanarMappingX(vec3 P) { return vec2(1.0 - P.z, P.y); } vec2 PlanarMappingY(vec3 P) { return vec2(P.x, 1.0 - P.z); } vec2 PlanarMappingZ(vec3 P) { return P.xy; } #define PI 3.14159265358979323846 // Cylindrical mapping vec2 CylindricalMapping(vec3 P) { float longitude = atan(P.x, P.z); float height = P.y; float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = height - 0.5; // Base at y = -0.5 return vec2(u, v); } // Spherical mapping vec2 SphericalMapping(vec3 P) { float longitude = atan(P.x, P.z); float latitude = asin(P.y / length(P)); float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = latitude / PI + 0.5; // From [-pi/2, pi/2] to [0, 1] return vec2(u, v); } void main() { vec4 color; if (mappingMode == 0) { // Triplanar mapping // Sample with x planar mapping vec2 texCoord1 = PlanarMappingX(fragPObj); vec4 color1 = BlinnPhong(fragN, fragL, fragV, texCoord1); // Sample with y planar mapping vec2 texCoord2 = PlanarMappingY(fragPObj); vec4 color2 = BlinnPhong(fragN, fragL, fragV, texCoord2); // Sample with z planar mapping vec2 texCoord3 = PlanarMappingZ(fragPObj); vec4 color3 = BlinnPhong(fragN, fragL, fragV, texCoord3); // Compute average based on normal vec3 weight = abs(normalize(fragNObj)); color = color1 * weight.x + color2 * weight.y + color3 * weight.z; } else { vec2 texCoord; if (mappingMode == 1) { // Cylindrical mapping texCoord = CylindricalMapping(fragPObj); } else if (mappingMode == 2) { // Spherical mapping texCoord = SphericalMapping(fragPObj); } else if (mappingMode == 3) { // From mesh texCoord = fragTexCoord; } color = BlinnPhong(fragN, fragL, fragV, texCoord); } if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } A primeira mudança em relação ao shader blinnphong.frag é o número de atributos de entrada, que agora inclui os atributos fragTexCoord, fragPObj e fragNObj da saída do vertex shader: in vec3 fragN; in vec3 fragL; in vec3 fragV; in vec2 fragTexCoord; in vec3 fragPObj; in vec3 fragNObj; Há também duas novas variáveis uniformes: // Diffuse texture sampler uniform sampler2D diffuseTex; // Mapping mode // 0: triplanar; 1: cylindrical; 2: spherical; 3: from mesh uniform int mappingMode; diffuseTex é um amostrador de textura 2D (sampler2D) que acessa a primeira unidade de textura (GL_TEXTURE0) ativada com glActiveTexture em Model::render. É através desse amostrador que conseguiremos acessar os texels da textura difusa. mappingMode é um inteiro que identifica o modo de mapeamento escolhido pelo usuário. Esse valor é determinado pelo índice da caixa de combinação “UV mapping” da ImGui. O padrão é 3 para arquivos OBJ que possuem coordenadas de textura, e 0 quando as coordenadas de textura não foram encontradas. Importante O pipeline associa diffuseTex à unidade de textura GL_TEXTURE0 pois o valor dessa variável uniforme é definido como 0 no código em C++. Isso é feito na linha 106 de Window::onPaint junto com a definição das outras variáveis uniformes: auto const diffuseTexLoc{abcg::glGetUniformLocation(program, &quot;diffuseTex&quot;)}; auto const mappingModeLoc{abcg::glGetUniformLocation(program, &quot;mappingMode&quot;)}; // Set uniform variables that have the same value for every model abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); abcg::glUniform1i(diffuseTexLoc, 0); abcg::glUniform1i(mappingModeLoc, m_mappingMode); A definição da função BlinnPhong é ligeiramente diferente daquela do shader blinnphong.frag: // Blinn-Phong reflection model vec4 BlinnPhong(vec3 N, vec3 L, vec3 V, vec2 texCoord) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { V = normalize(V); vec3 H = normalize(L + V); float angle = max(dot(H, N), 0.0); specular = pow(angle, shininess); } vec4 map_Kd = texture(diffuseTex, texCoord); vec4 map_Ka = map_Kd; vec4 diffuseColor = map_Kd * Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = map_Ka * Ka * Ia; return ambientColor + diffuseColor + specularColor; } Agora, a função tem o parâmetro adicional de coordenadas de textura texCoord (linha 29). Até a linha 43, o código é igual ao anterior. A linha 45 contém o código utilizado para amostrar a textura difusa. A função texture recebe como argumentos o amostrador de textura (diffuseTex) e as coordenadas de textura (texCoord). O resultado é a cor RGBA amostrada na posição dada, usando o modo de filtragem e modo de empacotamento definidos pelo código C++ antes da renderização. Como estamos amostrando uma textura difusa, a cor da textura (map_Kd) é multiplicada por Kd * Id * lambertian para compor a componente difusa final (linha 48). Também criamos uma cor ambiente map_Ka (linha 46) que multiplica as componentes de reflexão ambiente (linha 50). Nesse caso, consideramos que map_Ka é igual a map_Kd, pois geralmente é esse o caso (a textura difusa é também a textura ambiente). Entretanto, é possível que um material defina uma textura diferente para a componente ambiente. Nesse caso teríamos de mudar o shader para incluir um outro amostrador específico para map_Ka. Além da função BlinnPhong, o shader também define as funções de geração de coordenadas de textura usando mapeamento planar (PlanarMappingX, PlanarMappingY, PlanarMappingZ), cilíndrico (CylindricalMapping) e esférico (SphericalMapping): // Planar mapping vec2 PlanarMappingX(vec3 P) { return vec2(1.0 - P.z, P.y); } vec2 PlanarMappingY(vec3 P) { return vec2(P.x, 1.0 - P.z); } vec2 PlanarMappingZ(vec3 P) { return P.xy; } #define PI 3.14159265358979323846 // Cylindrical mapping vec2 CylindricalMapping(vec3 P) { float longitude = atan(P.x, P.z); float height = P.y; float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = height - 0.5; // Base at y = -0.5 return vec2(u, v); } // Spherical mapping vec2 SphericalMapping(vec3 P) { float longitude = atan(P.x, P.z); float latitude = asin(P.y / length(P)); float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = latitude / PI + 0.5; // From [-pi/2, pi/2] to [0, 1] return vec2(u, v); } Todas as funções recebem como parâmetro a posição do ponto (P) e retornam as coordenadas de textura correspondentes ao mapeamento. O código reproduz as equações descritas na seção 11.1. A única exceção é o cálculo da componente \\(v\\) do mapeamento cilíndrico (linha 68), que aqui é deslocada para fazer com que o cilindro tenha base em \\(-0.5\\) em vez de \\(0\\). Assim, a textura fica centralizada verticalmente em objetos de raio unitário centralizados na origem, que é o nosso caso pois usamos a função Model::standardize após a leitura do arquivo OBJ. As funções de geração de coordenadas de textura são chamadas em main de acordo com o valor de mappingMode: void main() { vec4 color; if (mappingMode == 0) { // Triplanar mapping // Sample with x planar mapping vec2 texCoord1 = PlanarMappingX(fragPObj); vec4 color1 = BlinnPhong(fragN, fragL, fragV, texCoord1); // Sample with y planar mapping vec2 texCoord2 = PlanarMappingY(fragPObj); vec4 color2 = BlinnPhong(fragN, fragL, fragV, texCoord2); // Sample with z planar mapping vec2 texCoord3 = PlanarMappingZ(fragPObj); vec4 color3 = BlinnPhong(fragN, fragL, fragV, texCoord3); // Compute average based on normal vec3 weight = abs(normalize(fragNObj)); color = color1 * weight.x + color2 * weight.y + color3 * weight.z; } else { vec2 texCoord; if (mappingMode == 1) { // Cylindrical mapping texCoord = CylindricalMapping(fragPObj); } else if (mappingMode == 2) { // Spherical mapping texCoord = SphericalMapping(fragPObj); } else if (mappingMode == 3) { // From mesh texCoord = fragTexCoord; } color = BlinnPhong(fragN, fragL, fragV, texCoord); } if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } Observe, no mapeamento triplanar (linhas 88–104), como a textura é amostrada três vezes (linhas 90–100) e as cores amostrada são combinadas em uma média ponderada pelo valor absoluto das componentes do vetor normal (linhas 103–104). Observação Observe que, no mapeamento triplanar, não seria necessário chamar BlinnPhong três vezes. Afinal, a iluminação sem a textura é a mesma nas três chamadas. O código ficaria mais eficiente se BlinnPhong retornasse uma estrutura contendo as componentes ambiente, difusa e especular separadas. Poderíamos então criar uma outra função só para fazer a amostragem da textura difusa e compor a cor final. Se mappingMode é 3, então nenhuma função de mapeamento é chamada e as coordenadas de textura utilizadas em BlinnPhong são aquelas contidas em fragTexCoord, pois essas são as coordenadas de textura interpoladas a partir das coordenadas definidas nos vértices. Isso resume as modificações necessárias para habilitar a texturização. O restante do código contém modificações complementares relacionadas a conceitos que já foram abordados em projetos anteriores, como a mudança da interface da ImGui e a determinação de um ângulo e eixo de rotação inicial para o trackball virtual. Nosso visualizador suporta apenas um objeto por arquivo OBJ e, portanto, suporta apenas um material. Por isso, todos os arquivos OBJ que utilizaremos devem ter apenas um objeto e um material.↩︎ "],["normalmapping.html", "11.5 Mapeamento de normais", " 11.5 Mapeamento de normais Mapeamento de normais (normal mapping) é uma técnica de melhoramento da percepção da iluminação de detalhes de uma superfície. Assim como a textura difusa é utilizada para modificar o atributo de reflexão difusa (componente \\(\\kappa_d\\) do modelo de reflexão) de cada ponto de uma superfície, o mapeamento de normais usa uma textura de normais (ou mapa de normais, do inglês normal map) para determinar o valor do vetor normal (vetor \\(\\hat{\\mathbf{n}}\\)) utilizado na avaliação da equação do modelo de reflexão. Observe, na figura 11.19, como o uso de mapeamento de normais melhora a percepção de detalhes da lâmpada romana apresentada originalmente na figura 11.11. Figura 11.19: Melhoramento da percepção de detalhes usando mapeamento de normais. O exemplo interativo a seguir permite habilitar e desabilitar o mapeamento de normais sobre um cubo texturizado com um padrão de tijolos: Os mapas de textura utilizados na renderização do cubo são mostrados na figura 11.20. Figura 11.20: Mapa de textura difusa e mapa de normais. Na textura de normais, cada texel corresponde às coordenadas de um vetor normal convertidas em uma cor RGB. O critério de conversão de uma tupla \\((x,y,z)\\) para \\((r, g, b)\\) é o mesmo que utilizamos no projeto viewer2 (seção 10.5) para exibir as normais como cores: \\[ r = \\frac{x+1}{2}, \\qquad g = \\frac{y+1}{2}, \\qquad b = \\frac{z+1}{2}. \\] Assim, quando a textura de normais é amostrada, as coordenadas do vetor normal podem ser obtidas pelo mapeamento inverso: \\[ x = 2r-1, \\qquad y = 2g-1, \\qquad z = 2b-1. \\] Espaço tangente Os vetores normais do mapa de normais estão representados em um espaço tangente ao plano sobre o qual a textura é aplicada. A figura 11.21 mostra uma ilustração dos vetores que formam a base de um espaço tangente em um triângulo. O eixo \\(z\\) do espaço tangente aponta na direção do vetor normal \\(\\hat{\\mathbf{n}}\\). Isso significa que, no espaço tangente, o vetor normal do triângulo é o vetor \\(\\hat{\\mathbf{n}}=(0,0,1)\\). Os eixos \\(x\\) e \\(y\\) apontam na direção de dois vetores tangentes ao plano, chamados respectivamente de vetor tangente (\\(\\hat{\\mathbf{t}}\\)) e vetor bitangente (\\(\\hat{\\mathbf{b}}\\)). Os vetores tangente e bitangente são calculados de forma a ficarem alinhados, respectivamente, às direções das coordenadas \\(u\\) e \\(v\\) da textura de normais. Figura 11.21: Espaço tangente de um triângulo. Se a textura de normais for mapeada para um quadrilátero como mostra a figura 11.22, o vetor tangente apontará na direção do eixo \\(u\\), e o vetor bitangente apontará na direção do eixo \\(v\\). Figura 11.22: Espaço tangente em relação ao espaço da textura. Os texels da textura de normais representam direções do vetor normal no espaço tangente. Assim, a textura pode ser usada para alterar localmente o valor do vetor normal após a rasterização. Essas alterações locais do vetor normal são mais pronunciadas quanto mais rugosa for a textura que queremos aplicar. No caso da textura de muro de tijolinhos, a cor é azulada na superfície de cada tijolo pois o vetor normal nesses pontos é próximo de \\((0,0,1)\\) em relação ao espaço tangente, que é o vetor normal do triângulo em relação ao espaço do mundo. A cor é ligeiramente avermelhada no lado direito dos tijolos, pois nesses pontos o vetor normal desvia para um valor mais próximo de \\((1,0,0)\\) no espaço tangente. De forma semelhante, a cor é esverdeada no lado de cima dos tijolos, pois nesses pontos o vetor normal está apontando em uma direção mais próxima de \\((0,1,0)\\) no espaço tangente. Se o vetor normal da textura de normais for utilizado na equação do modelo de reflexão, a iluminação será calculada como se a superfície tivesse sido deformada localmente, criando a ilusão de uma superfície com mais detalhes. Observação Uma forma alternativa e mais antiga de aumentar o detalhe de superfícies é o bump mapping (Blinn 1978). A técnica de bump mapping utiliza um mapa de deslocamento (displacement map) como o mostrado na figura 11.23. Figura 11.23: Mapa de deslocamento. A intensidade de cada texel do mapa de deslocamento determina a altura da superfície texturizada em relação à altura da superfície sem textura. Entretanto, assim como no mapeamento de normais, bump mapping não modifica a geometria do objeto 3D, mas apenas os vetores normais utilizados no modelo de reflexão. Mapeamento de normais é simplemente uma variação do bump mapping na qual os vetores normais já estão pré-calculados e representados em um espaço tangente à superfície. No bump mapping original, os vetores normais são calculados no espaço do objeto a partir da variação da intensidade dos texels do mapa de deslocamento (método de diferenças finitas). O mapa de deslocamento também pode ser empregado na técnica de displacement mapping. Nessa técnica, a malha de triângulos é refinada de tal modo que cada texel do mapa de deslocamento corresponda a um vértice da malha. Cada vértice é então deslocado ao longo de seu vetor normal usando o valor do texel como magnitude do deslocamento. Ao contrário do mapeamento de normais e do bump mapping, displacement mapping altera a geometria do objeto. Nas GPUs atuais, o refinamento da malha pode ser feito nos estágios de tesselação, a partir do VBO original (não refinado). Ainda assim, displacement mapping é um processo computacionalmente intensivo pois aumenta o número de primitivas que devem ser rasterizadas. Transformação para o espaço tangente Na avaliação da equação do modelo de reflexão, é importante que todos os vetores envolvidos estejam representados em um mesmo espaço. Em nossos projetos até agora, avaliamos o modelo de Phong e Blinn–Phong usando os vetores \\(\\hat{\\mathbf{n}}\\) (vetor normal), \\(\\hat{\\mathbf{l}}\\) (vetor de direção à fonte de luz) e \\(\\hat{\\mathbf{v}}\\) (vetor de direção à câmera) no espaço da câmera. No mapeamento de normais, o vetor normal \\(\\hat{\\mathbf{n}}\\) amostrado da textura de normais está no espaço tangente. Para que todos os vetores estejam em um mesmo espaço, temos duas opções: ou convertemos esse vetor normal para o espaço da câmera, ou primeiro convertemos \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\) para o espaço tangente, e então calculamos a iluminação nesse novo espaço tangente. Esta última opção é a mais utilizada, pois a conversão de \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\) pode ser feita no vertex shader. Para converter \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\) do espaço da câmera para o espaço tangente, precisamos criar uma nova matriz de mudança de base. Isso pode feito desde que tenhamos os vetores \\(\\hat{\\mathbf{t}}=(t_x, t_y, t_z)\\), \\(\\hat{\\mathbf{b}}=(b_x, b_y, b_z)\\) e \\(\\hat{\\mathbf{n}}=(n_x, n_y, n_z)\\) que formam uma base do espaço tangente. Suponha que temos tais vetores de base, e que esses vetores estão representados em relação ao espaço do objeto (mais adiante veremos como calcular \\(\\hat{\\mathbf{t}}\\) e \\(\\hat{\\mathbf{b}}\\)). Então, a matriz \\[ \\mathbf{M}_{\\mathrm{tan}\\rightarrow\\mathrm{obj}}= \\begin{bmatrix} t_x &amp; b_x &amp; n_x \\\\ t_y &amp; b_y &amp; n_y \\\\ t_z &amp; b_z &amp; n_z \\end{bmatrix} \\] transforma vetores do espaço tangente para vetores do espaço do objeto. Para a transformação no sentido oposto, devemos calcular a inversa da matriz. Como a matriz é ortogonal, a inversa é a própria transposta. Assim, \\[ \\mathbf{M}_{\\mathrm{obj}\\rightarrow\\mathrm{tan}}= \\begin{bmatrix} t_x &amp; t_y &amp; t_z \\\\ b_x &amp; b_y &amp; b_z \\\\ n_x &amp; n_y &amp; n_z \\end{bmatrix} \\] é a matriz que transforma vetores do espaço do objeto para vetores do espaço tangente. Na verdade, o que queremos é uma matriz que seja capaz de transformar do espaço da câmera para o espaço tangente. Para isso precisamos transformar primeiro os vetores de base (\\(\\hat{\\mathbf{t}}\\), \\(\\hat{\\mathbf{b}}\\) e \\(\\hat{\\mathbf{n}}\\)) do espaço do objeto para o espaço da câmera. Já sabemos como transformar um vetor normal do espaço do objeto para o espaço da câmera: basta multiplicarmos a matriz \\(M_{\\mathrm{normal}}=(\\mathbf{M_{\\textrm{modelview}}}^{-1})^T\\) pela normal de vértice \\(\\hat{\\mathbf{n}}\\) (atributo de entrada do vertex shader) como vimos na seção 10.5. Os vetores tangente e bitangente podem ser transformados da mesma forma. Logo, se tivermos os vetores no espaço da câmera \\[ \\hat{\\mathbf{n}}&#39;=M_{\\mathrm{normal}}.\\hat{\\mathbf{n}},\\\\ \\hat{\\mathbf{t}}&#39;=M_{\\mathrm{normal}}.\\hat{\\mathbf{t}},\\\\ \\hat{\\mathbf{b}}&#39;=M_{\\mathrm{normal}}.\\hat{\\mathbf{b}}, \\] então \\[ \\mathbf{M}_{\\mathrm{eye}\\rightarrow\\mathrm{tan}}= \\begin{bmatrix} t&#39;_x &amp; t&#39;_y &amp; t&#39;_z \\\\ b&#39;_x &amp; b&#39;_y &amp; b&#39;_z \\\\ n&#39;_x &amp; n&#39;_y &amp; n&#39;_z \\end{bmatrix} \\] é a matriz que transforma vetores do espaço da câmera para o espaço tangente. Uma vez calculada a matriz, podemos transformar \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\). Em seguida, podemos avaliar a equação do modelo de reflexão no espaço tangente usando \\(\\hat{\\mathbf{n}}\\) amostrado da textura de normais. Vetor tangente e bitangente A figura 11.24 ilustra um triângulo \\(\\triangle ABC\\) no espaço do objeto (esquerda) e mapeado no espaço tangente (direita). Figura 11.24: Mapeamento de um triângulo no espaço tangente, e geometria do cálculo do vetor tangente e bitangente. Nessa figura, \\(A\\), \\(B\\) e \\(C\\) são pontos no espaço do objeto, e \\(\\mathbf{e}_1\\) e \\(\\mathbf{e}_2\\) são vetores formados por dois lados do triângulo: \\[ \\mathbf{e}_{1}=(e_{1x}, e_{1y}, e_{1z})=B-A,\\\\ \\mathbf{e}_{2}=(e_{2x}, e_{2y}, e_{2z})=C-A. \\] Primeiro calculamos as diferenças entre as coordenadas de textura dos lados \\(\\mathbf{e}_{1}\\) e \\(\\mathbf{e}_{2}\\): \\[ \\Delta u_1=B_u-A_u,\\qquad\\Delta v_1=B_v-A_v,\\\\ \\Delta u_2=C_u-A_u,\\qquad\\Delta v_2=C_v-A_v. \\] Observe que, no espaço tangente, os vetores tangente e bitangente são os vetores \\[ \\hat{\\mathbf{t}}_{\\mathrm{uv}}=(1,0,0),\\\\ \\hat{\\mathbf{b}}_{\\mathrm{uv}}=(0,1,0). \\] Logo, os vetores \\(\\mathbf{e}_{1}\\) e \\(\\mathbf{e}_{2}\\) podem ser representados no espaço tangente como uma combinação linear: \\[ \\mathbf{e}_{1uv}=\\Delta u_1 \\hat{\\mathbf{t}}_{\\mathrm{uv}} + \\Delta v_1 \\hat{\\mathbf{b}}_{\\mathrm{uv}},\\\\ \\mathbf{e}_{2uv}=\\Delta u_2 \\hat{\\mathbf{t}}_{\\mathrm{uv}} + \\Delta v_2 \\hat{\\mathbf{b}}_{\\mathrm{uv}}. \\] Entretanto, precisamos calcular \\(\\hat{\\mathbf{t}}=(t_x, t_y, t_z)\\) e \\(\\hat{\\mathbf{b}}=(b_x, b_y, b_z)\\) no espaço do objeto. No espaço do objeto, os vetores \\(\\mathbf{e}_{1}\\) e \\(\\mathbf{e}_{2}\\) podem ser escritos como \\[ \\mathbf{e}_1=\\Delta u_1 \\hat{\\mathbf{t}} + \\Delta v_1 \\hat{\\mathbf{b}},\\\\ \\mathbf{e}_2=\\Delta u_2 \\hat{\\mathbf{t}} + \\Delta v_2 \\hat{\\mathbf{b}}, \\] que é o mesmo que \\[ (e_{1x}, e_{1y}, e_{1z})=\\Delta u_1 (t_x, t_y, t_z) + \\Delta v_1 (b_x, b_y, b_z),\\\\ (e_{2x}, e_{2y}, e_{2z})=\\Delta u_2 (t_x, t_y, t_z) + \\Delta v_2 (b_x, b_y, b_z). \\] Em notação matricial, \\[ \\begin{bmatrix} e_{1x} &amp; e_{1y} &amp; e_{1z}\\\\ e_{2x} &amp; e_{2y} &amp; e_{2z} \\end{bmatrix} = \\begin{bmatrix} \\Delta u_1 &amp; \\Delta v_1\\\\ \\Delta u_2 &amp; \\Delta v_2 \\end{bmatrix} \\begin{bmatrix} t_x &amp; t_y &amp; t_z\\\\ b_x &amp; b_y &amp; b_z \\end{bmatrix}. \\] Para determinarmos os valores de \\((t_x, t_y, t_z)\\) e \\((b_x, b_y, b_z)\\), multiplicamos à esquerda os dois lados da equação pela inversa da matriz dos deltas: \\[ \\begin{bmatrix} \\Delta u_1 &amp; \\Delta v_1\\\\ \\Delta u_2 &amp; \\Delta v_2 \\end{bmatrix}^{-1} \\begin{bmatrix} e_{1x} &amp; e_{1y} &amp; e_{1z}\\\\ e_{2x} &amp; e_{2y} &amp; e_{2z} \\end{bmatrix} = \\begin{bmatrix} t_x &amp; t_y &amp; t_z\\\\ b_x &amp; b_y &amp; b_z \\end{bmatrix}. \\] Como a matriz dos deltas é uma matriz de ordem 2, podemos calcular explicitamente a inversa através do valor recíproco do determinante multiplicado pela matriz adjunta: \\[ \\begin{bmatrix} t_x &amp; t_y &amp; t_z\\\\ b_x &amp; b_y &amp; b_z \\end{bmatrix}= \\frac{1}{\\Delta u_1\\Delta v_2 - \\Delta u_2\\Delta v_1} \\begin{bmatrix} \\phantom{-}\\Delta v_2 &amp; -\\Delta v_1\\\\ -\\Delta u_2 &amp; \\phantom{-}\\Delta u_1 \\end{bmatrix} \\begin{bmatrix} e_{1x} &amp; e_{1y} &amp; e_{1z}\\\\ e_{2x} &amp; e_{2y} &amp; e_{2z} \\end{bmatrix} . \\] Com isso obtemos \\(\\hat{\\mathbf{t}}\\) e \\(\\hat{\\mathbf{b}}\\) para cada triângulo. Entretanto, se a malha aproximar uma superfície suave, precisamos de um passo a mais para calcular vetores tangente e bitangente para cada vértice. Para calcular vetores tangente e bitangente para cada vértice, podemos seguir a mesma estratégia que utilizamos para calcular os vetores normais em uma malha indexada. Primeiro calculamos os vetores tangente e bitangente de cada triângulo. Em seguida, acumulamos esses vetores nos vértices da malha indexada. Por fim, normalizamos os vetores acumulados. O resultado será uma média dos vetores dos triângulos adjacentes. Assim, junto com as normais de vértices, teremos também tangentes de vértices e bitangentes de vértices que podem ser armazenados como atributos dos vértices. Observação Ao calcular a média dos vetores tangente e bitangente, é possível que os vetores resultantes não sejam mais ortogonais entre si. Para corrigir isso podemos usar o processo de ortogonalização de Gram-Schmidt descrito a seguir e ilustrado na figura 11.25. Para fazer com que \\(\\hat{\\mathbf{t}}\\) volte a ser ortogonal a \\(\\hat{\\mathbf{n}}\\), primeiro projetamos \\(\\hat{\\mathbf{t}}\\) sobre \\(\\hat{\\mathbf{n}}\\): \\[ a\\hat{\\mathbf{n}}=(\\hat{\\mathbf{t}} \\cdot \\hat{\\mathbf{n}})\\hat{\\mathbf{n}}. \\] Em seguida, calculamos \\(\\hat{\\mathbf{t}}&#39;\\) como \\[ \\mathbf{t}&#39;=\\hat{\\mathbf{t}}-a\\hat{\\mathbf{n}}. \\] \\(\\mathbf{t}&#39;\\) é ortogonal a \\(\\hat{\\mathbf{n}}\\), mas não tem tamanho unitário. Então, como passo final, normalizamos \\(\\mathbf{t}&#39;\\): \\[ \\hat{\\mathbf{t}}&#39;=\\frac{\\mathbf{t}&#39;}{|\\mathbf{t}&#39;|}. \\] Logo, \\(\\hat{\\mathbf{t}}&#39;\\) é o vetor \\(\\hat{\\mathbf{t}}\\) ortogonalizado. Figura 11.25: Ortogonalização do vetor tangente. Para fazer com que \\(\\hat{\\mathbf{b}}\\) volte a ser ortogonal a \\(\\hat{\\mathbf{t}}\\) e \\(\\hat{\\mathbf{n}}\\), basta calcular o produto vetorial \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}&#39;\\) ou \\(\\hat{\\mathbf{t}}&#39; \\times \\hat{\\mathbf{n}}\\). A ordem dependerá de qual vetor tem o menor ângulo quando comparado com o vetor \\(\\hat{\\mathbf{b}}\\) original: \\[ \\hat{\\mathbf{b}}&#39; = \\begin{cases} \\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}&#39; &amp;\\text{se } (\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}&#39;) \\cdot \\hat{\\mathbf{b}} \\geq 0, \\\\ \\hat{\\mathbf{t}}&#39; \\times \\hat{\\mathbf{n}} &amp;\\text{caso contrário}. \\end{cases} \\] Mapeamento de normais na prática Vamos agora implementar o mapeamento de normais no visualizador de modelos 3D apresentado na seção 11.4. Esta será a versão 5 do visualizador (viewer5) e terá os shaders normalmapping.vert e normalmapping.frag modificados a partir dos shaders texture.vert e texture.frag do projeto anterior. Nesta nova versão, o menu “File” terá a opção de carregar uma textura difusa (“Load Diffuse Map”) e carregar uma textura de normais (“Load Normal Map”). Se o arquivo .mtl tiver a descrição de uma textura de normais (como no arquivo roman_lamp.mtl), a textura será carregada automaticamente. O resultado ficará como a seguir: Baixe o código completo deste link. Experimente usar as texturas brick_base.jpg (textura difusa) e brick_normal.jpg (textura de normais) com diferentes mapeamentos (triplanar, cilíndrico e esférico) nos modelos chamferbox.obj e teapot.obj. Carregando a textura de normais No projeto anterior, utilizamos a função Model::loadDiffuseTexture para carregar um arquivo de imagem e criar um identificador de textura difusa em uma variável m_diffuseTexture. Agora, incluiremos a função Model::loadNormalTexture para carregar a textura de normais em uma variável m_normalTexture. O código é praticamente o mesmo de Model::loadDiffuseTexture. A definição ficará como a seguir: void Model::loadNormalTexture(std::string_view path) { if (!std::filesystem::exists(path)) return; abcg::glDeleteTextures(1, &amp;m_normalTexture); m_normalTexture = abcg::loadOpenGLTexture({.path = path}); } Essa função é chamada em Model::loadObj junto com o trecho de código que chama Model::loadDiffuseTexture. A textura de normais é carregada apenas se o arquivo .mtl fornecer um nome de textura de normais (em mat.normal_texname ou mat.bump_texname): if (!mat.diffuse_texname.empty()) loadDiffuseTexture(basePath + mat.diffuse_texname); if (!mat.normal_texname.empty()) { loadNormalTexture(basePath + mat.normal_texname); } else if (!mat.bump_texname.empty()) { loadNormalTexture(basePath + mat.bump_texname); } Em Model::render, precisamos habilitar a unidade de textura que utilizará a textura de normais. No projeto anterior (viewer4), ativamos apenas a primeira unidade de textura (GL_TEXTURE0) com a textura difusa: abcg::glActiveTexture(GL_TEXTURE0); abcg::glBindTexture(GL_TEXTURE_2D, m_diffuseTexture); Agora, ativaremos também a segunda unidade de textura (GL_TEXTURE1), usando m_normalTexture: abcg::glActiveTexture(GL_TEXTURE0); abcg::glBindTexture(GL_TEXTURE_2D, m_diffuseTexture); abcg::glActiveTexture(GL_TEXTURE1); abcg::glBindTexture(GL_TEXTURE_2D, m_normalTexture); Desse modo, no fragment shader poderemos usar dois amostradores de textura definidos por variáveis uniformes. O nome dessas variáveis uniformes será diffuseTex (como no projeto anterior) e normalTex. Há ainda mais um passo necessário para habilitar o uso da textura no shader. Em Window::onPaint, precisamos definir o valor da variável uniforme normalTex. Esse valor deve ser 1 pois queremos que essa variável use a unidade de textura GL_TEXTURE1. Assim, em Window::onPaint teremos o seguinte trecho de código atualizado: auto const diffuseTexLoc{abcg::glGetUniformLocation(program, &quot;diffuseTex&quot;)}; auto const normalTexLoc{abcg::glGetUniformLocation(program, &quot;normalTex&quot;)}; auto const mappingModeLoc{abcg::glGetUniformLocation(program, &quot;mappingMode&quot;)}; // Set uniform variables that have the same value for every model abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); abcg::glUniform1i(diffuseTexLoc, 0); abcg::glUniform1i(normalTexLoc, 1); abcg::glUniform1i(mappingModeLoc, m_mappingMode); Calculando os vetores tangente e bitangente Na classe Model é definida uma função Model::computeTangents para calcular, para cada vértice, os vetores tangente e bitangente a partir das coordenadas de textura definidas no arquivo OBJ. Se o arquivo não fornecer coordenadas de textura, Model::computeTangents não será chamada e precisaremos calcular os vetores tangente e bitangente diretamente no shader, da mesma forma como geramos as coordenadas de textura usando o mapeamento planar, cilíndrico ou esférico. Por enquanto, vamos considerar que o objeto tem coordenadas de textura. No final de Model::loadObj, temos o seguinte código atualizado: if (standardize) { Model::standardize(); } if (!m_hasNormals) { computeNormals(); } if (m_hasTexCoords) { computeTangents(); } createBuffers(); Note que Model::computeTangents é chamada depois de Model::computeNormals, pois os vetores normais também são necessários para o cálculo dos vetores tangente e bitangente. Em model.hpp, a estrutura Vertex é atualizada para armazenar o vetor tangente que será calculado: struct Vertex { glm::vec3 position{}; glm::vec3 normal{}; glm::vec2 texCoord{}; glm::vec4 tangent{}; friend bool operator==(Vertex const &amp;, Vertex const &amp;) = default; }; Observe que agora temos o atributo de vetor tangente (tangent), mas não temos o atributo de vetor bitangente. O vetor bitangente não precisa ser armazenado como um atributo de vértice, pois pode ser calculado diretamente no shader como \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}\\) ou \\(\\hat{\\mathbf{t}} \\times \\hat{\\mathbf{n}}\\) (sendo que \\(\\hat{\\mathbf{n}}\\) é normal, e \\(\\hat{\\mathbf{t}}\\) é tangent). Fazendo isso economizamos memória no VBO. Só precisamos saber a ordem dos operandos do produto vetorial. Note que tangent é um glm::vec4 em vez de glm::vec3. A coordenada \\(w\\) é utilizada para armazenar um escalar que multiplica o resultado do produto vetorial de \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}\\). Se \\(w=1\\), então o vetor bitangente será \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}\\). Se \\(w=-1\\), o vetor bitangente será \\(-(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}})\\), que é o mesmo que \\(\\hat{\\mathbf{t}} \\times \\hat{\\mathbf{n}}\\). Vamos à definição de Model::computeTangents: void Model::computeTangents() { // Reserve space for bitangents std::vector bitangents(m_vertices.size(), glm::vec3(0)); // Compute face tangents and bitangents for (auto const offset : iter::range(0UL, m_indices.size(), 3UL)) { // Get face indices auto const i1{m_indices.at(offset + 0)}; auto const i2{m_indices.at(offset + 1)}; auto const i3{m_indices.at(offset + 2)}; // Get face vertices auto &amp;v1{m_vertices.at(i1)}; auto &amp;v2{m_vertices.at(i2)}; auto &amp;v3{m_vertices.at(i3)}; auto const e1{v2.position - v1.position}; auto const e2{v3.position - v1.position}; auto const delta1{v2.texCoord - v1.texCoord}; auto const delta2{v3.texCoord - v1.texCoord}; glm::mat2 M; M[0][0] = delta2.t; M[0][1] = -delta1.t; M[1][0] = -delta2.s; M[1][1] = delta1.s; M *= (1.0f / (delta1.s * delta2.t - delta2.s * delta1.t)); auto const tangent{glm::vec4(M[0][0] * e1.x + M[0][1] * e2.x, M[0][0] * e1.y + M[0][1] * e2.y, M[0][0] * e1.z + M[0][1] * e2.z, 0.0f)}; auto const bitangent{glm::vec3(M[1][0] * e1.x + M[1][1] * e2.x, M[1][0] * e1.y + M[1][1] * e2.y, M[1][0] * e1.z + M[1][1] * e2.z)}; // Accumulate on vertices v1.tangent += tangent; v2.tangent += tangent; v3.tangent += tangent; bitangents.at(i1) += bitangent; bitangents.at(i2) += bitangent; bitangents.at(i3) += bitangent; } for (auto &amp;&amp;[i, vertex] : iter::enumerate(m_vertices)) { auto const &amp;n{vertex.normal}; auto const &amp;t{glm::vec3(vertex.tangent)}; // Orthogonalize t with respect to n auto const tangent{t - n * glm::dot(n, t)}; vertex.tangent = glm::vec4(glm::normalize(tangent), 0); // Compute handedness of re-orthogonalized basis auto const b{glm::cross(n, t)}; auto const handedness{glm::dot(b, bitangents.at(i))}; vertex.tangent.w = (handedness &lt; 0.0f) ? -1.0f : 1.0f; } } Esta função adota uma estratégia parecida com aquela que utilizamos no código de Model::computeNormals. Primeiramente, os vetores tangente e bitangente são calculados para cada triângulo (laço da linha 53). O resultado é acumulado nos vértices da malha indexada: o vetor tangente é acumulado no atributo tangent (linhas 85 a 87), e o vetor bitangente é acumulado em um arranjo bitangents temporário (linhas 89 a 91), uma vez que os vértices não têm um atributo bitangent. Após a acumulação dos vetores tangente e bitangente nos vértices, o laço da linha 94 itera sobre os vértices e usa o método de Gram-Schmidt para ortogonalizar os vetores tangente em relação às normais de vértice (linhas 99 a 100). Em seguida (linhas 103 a 105), o valor \\(w\\) de tangent é calculado comparando o resultado do produto vetorial \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{t}}\\) com bitangents (vetor bitangente acumulado). Shaders Os shaders de mapeamento de normais são adaptados de texture.vert e texture.frag do projeto anterior. normalmapping.vert O código completo do vertex shader é mostrado a seguir: #version 300 es layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; layout(location = 2) in vec2 inTexCoord; layout(location = 3) in vec4 inTangent; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform vec4 lightDirWorldSpace; out vec2 fragTexCoord; out vec3 fragPObj; out vec3 fragTObj; out vec3 fragBObj; out vec3 fragNObj; out vec3 fragLEye; out vec3 fragVEye; void main() { vec3 PEye = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 LEye = -(viewMatrix * lightDirWorldSpace).xyz; fragTexCoord = inTexCoord; fragPObj = inPosition; fragTObj = inTangent.xyz; fragBObj = inTangent.w * cross(inNormal, inTangent.xyz); fragNObj = inNormal; fragLEye = LEye; fragVEye = -PEye; gl_Position = projMatrix * vec4(PEye, 1.0); } Em comparação com o código de texture.vert, temos agora o atributo de entrada inTangent (linha 6) e alguns novos atributos de saída: fragTObj é o vetor tangente no espaço do objeto. fragBObj é o vetor bitangente no espaço do objeto. fragNObj é o vetor normal no espaço do objeto. Com esses atributos podemos criar a matriz de mudança de base para transformar vetores do espaço do objeto para o espaço tangente. As variáveis fragV e fragL de texture.vert foram renomeadas para fragVEye e fragLEye para deixar explícito que são vetores no espaço da câmera (eye space). A variável fragN foi removida pois o vetor normal utilizado na equação do modelo de Blinn–Phong é lido diretamente da textura de normais. Observe que não temos mais a variável uniforme normalMatrix. Ela foi movida para o fragment shader. No fragment shader, fragTObj, fragBObj e fragNObj são transformados por normalMatrix para obter vetores no espaço da câmera. Com isso é possível construir a matriz que transforma os vetores fragLEye e fragVEye do espaço da câmera para o espaço tangente. Observação A matriz que transforma vetores do espaço da câmera para vetores do espaço tangente pode ser criada no vertex shader. Assim, podemos enviar ao fragment shader os vetores \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\) já no espaço tangente (o vetor \\(\\hat{\\mathbf{n}}\\) é obtido da textura de normais). Isso deixa o código mais eficiente, pois é mais custoso transformar os vetores para cada fragmento do que para cada vértice. Entretanto, nesta versão do visualizador optamos por calcular a matriz no fragment shader para manter a compatibilidade com objetos que usam o mapeamento planar, cilíndrico e esférico no fragment shader. Quando usamos esses mapeamentos, precisamos calcular manualmente os vetores tangente e bitangente. Nesse caso, a matriz só pode ser construída no fragment shader. normalmapping.frag A maior parte do processamento do mapeamento de normais é feita no fragment shader. Primeiramente, definimos uma função ComputeTBN que retorna a matriz que será utilizada para transformar vetores no espaço da câmera para vetores no espaço tangente: // Compute matrix to transform from camera space to tangent space mat3 ComputeTBN(vec3 TObj, vec3 BObj, vec3 NObj) { vec3 TEye = normalMatrix * normalize(TObj); vec3 BEye = normalMatrix * normalize(BObj); vec3 NEye = normalMatrix * normalize(NObj); return mat3(TEye.x, BEye.x, NEye.x, TEye.y, BEye.y, NEye.y, TEye.z, BEye.z, NEye.z); } A matriz recebe vetores no espaço do objeto e transforma-os para o espaço da câmera usando normalMatrix. O resultado é utilizado para criar a matriz \\(\\mathbf{M}_{\\mathrm{eye}\\rightarrow\\mathrm{tan}}\\). Importante Em GLSL, as matrizes são armazenadas na ordem “column-major”. Isso significa que, na matriz construída com o código a seguir, os três primeiros argumentos (TEye.x, BEye.x, NEye.x) definem os elementos da primeira coluna da matriz, e não os elementos da primeira linha! mat3(TEye.x, BEye.x, NEye.x, TEye.y, BEye.y, NEye.y, TEye.z, BEye.z, NEye.z); Logo, a matriz resultante é a matriz \\[ \\mathbf{M}_{\\mathrm{eye}\\rightarrow\\mathrm{tan}}= \\begin{bmatrix} t&#39;_x &amp; t&#39;_y &amp; t&#39;_z \\\\ b&#39;_x &amp; b&#39;_y &amp; b&#39;_z \\\\ n&#39;_x &amp; n&#39;_y &amp; n&#39;_z \\end{bmatrix}. \\] onde \\(\\hat{\\mathbf{t}}&#39;\\), \\(\\hat{\\mathbf{b}}&#39;\\), \\(\\hat{\\mathbf{n}}&#39;\\) são, respectivamente, os vetores tangente, bitangente e normal no espaço da câmera. Com a função ComputeTBN definida, podemos criar a matriz TBN a partir dos vetores fragTObj, fragBObj e fragNObj recebidos do vertex shader: mat3 TBN = ComputeTBN(fragTObj, fragBObj, fragNObj); Em seguida, usamos TBN para transformar fragLEye e fragVEye para o espaço tangente: vec3 LTan = TBN * normalize(fragLEye); vec3 VTan = TBN * normalize(fragVEye); O vetor normal no espaço tangente é lido da textura de normais usando o amostrador normalTex: vec3 NTan = texture(normalTex, fragTexCoord).xyz; NTan = normalize(NTan * 2.0 - 1.0); // From [0, 1] to [-1, 1] Agora, basta chamarmos BlinnPhong com os vetores calculados: vec4 color = BlinnPhong(NTan, LTan, VTan, fragTexCoord); Observação Se o objeto renderizado não tiver coordenadas de textura fornecidas pelo arquivo OBJ, também não terá vetores tangentes e bitangentes. Então, a estratégia anterior não poderá ser utilizada. No projeto viewer4, deixamos a possibilidade do usuário escolher entre o mapeamento triplanar, cilíndrico ou esférico para esses objetos. Para esses casos, as coordenadas de textura foram calculadas no fragment shader pelas funções: PlanarMappingX, PlanarMappingY e PlanarMappingZ para o mapeamento triplanar; CylindricalMapping para o mapeamento cilíndrico; SphericalMapping para o mapeamento esférico. Para usar mapeamento de normais, precisamos criar igualmente funções que calculem o vetor tangente e bitangente. Em normalmapping.frag, definiremos as seguintes funções adicionais que retornam a matriz TBN correspondente a cada mapeamento: mat3 PlanarMappingXTBN(vec3 P) { vec3 T = vec3(0, 0, -1); vec3 N = fragNObj; vec3 B = cross(N, T); return ComputeTBN(T, B, N); } mat3 PlanarMappingYTBN(vec3 P) { vec3 T = vec3(1, 0, 0); vec3 N = fragNObj; vec3 B = cross(N, T); return ComputeTBN(T, B, N); } mat3 PlanarMappingZTBN(vec3 P) { vec3 T = vec3(1, 0, 0); vec3 N = fragNObj; vec3 B = cross(N, T); return ComputeTBN(T, B, N); } mat3 CylindricalTBN(vec3 P) { vec3 T = vec3(P.z, 0, -P.x); vec3 N = fragNObj; vec3 B = cross(N, T); return ComputeTBN(T, B, N); } mat3 SphericalTBN(vec3 P) { vec3 T = vec3(P.z, 0, -P.x); vec3 N = fragNObj; vec3 B = cross(N, T); return ComputeTBN(T, B, N); } Observe como o vetor T é construído explicitamente em cada mapeamento. Por exemplo, no mapeamento planar na direção \\(y\\), o vetor tangente é sempre o vetor \\((1,0,0)\\) (direção \\(x\\)). No mapeamento cilíndrico ou esférico, o vetor tangente é o vetor que tangencia o círculo no plano \\(y=0\\). Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
