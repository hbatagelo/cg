[["index.html", "MCTA008-17 Computação Gráfica Apresentação", " MCTA008-17 Computação Gráfica Harlen Batagelo (harlen.batagelo@ufabc.edu.br) Bruno Marques (bruno.marques@ufabc.edu.br) Universidade Federal do ABC 3º quadrimestre de 2021 Apresentação Caro/a estudante, Este site contém as notas de aula da disciplina MCTA008-17 Computação Gráfica adaptada ao ensino remoto do 3º quadrimestre de 2021. O conteúdo é organizado no formato de um livro voltado ao estudo autodirigido. Os capítulos estão divididos em tópicos teóricos sobre o processo de geração de imagens no computador, e partes práticas de desenvolvimento de aplicações gráficas interativas. Cada capítulo corresponde aproximadamente ao conteúdo de uma semana de aula. Para informações sobre o cronograma das atividades e critérios de avaliação, consulte o plano de ensino disponível no Moodle. Bons estudos!  Harlen e Bruno "],["pré-requisitos.html", "Pré-requisitos", " Pré-requisitos Para acompanhar o curso de forma satisfatória é recomendável ter conhecimento prévio do conteúdo abordado nas disciplinas de Algoritmos e Estruturas de Dados I e Geometria Analítica. As atividades práticas avaliativas serão desenvolvidas na linguagem C++. Embora não seja necessário ter fluência em C++, é recomendável ter proficiência em programação em C e familiaridade com conceitos básicos de programação orientada a objetos. Também é recomendável ter familiaridade com o Git e ser capaz de gerenciar seus próprios repositórios. Atividades práticas Para realizar as atividades práticas é necessário ter um computador com sistema operacional 64 bits (Windows, Linux ou macOS) e placa de vídeo compatível com OpenGL 4.1 ou superior. O OpenGL 4.1 é suportado em placas gráficas da família Nvidia GeForce 400 (2010) ou mais recentes, AMD Radeon HD 5000 (2009) em diante e Intel HD Graphics a partir dos processadores Intel de 7ª geração (2012). Caso a sua placa de vídeo seja de uma geração a partir de 2012, provavelmente ela deve suportar OpenGL 4.1. Se não suportar, há a possibilidade de simular o processamento gráfico em software através do driver Gallium llvmpipe da biblioteca Mesa. Visualizando este site Parte do conteúdo deste site requer um navegador com suporte a WebGL 2.0. Para informações detalhadas sobre o suporte do seu navegador a WebGL 2.0, consulte o WebGL Report. Dica Para garantir a visualização correta do conteúdo WebGL 2.0, utilize a versão mais recente do Mozilla FireFox ou Google Chrome. Além disso, use o navegador em um computador desktop ou laptop. Embora o site funcione em tablets e smartphones, pode ser difícil interagir com o conteúdo WebGL nesses dispositivos. Dependendo das configurações de DPI utilizadas no sistema de janelas, podem ocorrer problemas de redimensionamento dos elementos de interface no Chrome e em navegadores baseados no Chromium, como o Microsoft Edge. Por exemplo, o cubo exibido acima pode ser redimensionado e as arestas podem apresentar distorções, parecendo mais serrilhadas que o normal: No Chrome, isso geralmente é resolvido iniciando o navegador com a opção /high-dpi-support=1 /force-device-scale-factor=1 na linha de comando (ou incluindo essas opções no atalho), ou ajustando o zoom . Importante No momento da escrita deste texto, o Apple Safari não possui suporte a WebGL 2.0. Entretanto, o Safari Technology Preview tem o WebGL 2.0 habilitado por padrão e está disponível a partir do macOS Catalina. Consulte em https://caniuse.com/webgl2 o suporte a WebGL 2.0 em diferentes navegadores. "],["config.html", "1 Configuração do ambiente", " 1 Configuração do ambiente Neste capítulo veremos como configurar o ambiente de desenvolvimento para realizar as atividades práticas no computador. Qualquer que seja a plataforma  Linux, macOS ou Windows  é necessário instalar as seguintes ferramentas e bibliotecas: CMake: para automatizar a geração de scripts de compilação e ligação de forma independente de plataforma; Emscripten: para compilar código C++ e gerar binário em WebAssembly de modo a executar nossas aplicações no navegador; Git: para clonar o repositório do SDK do Emscripten e da biblioteca de desenvolvimento que usaremos na disciplina, e para o controle de versão das atividades; GLEW: para carregamento das funções da API gráfica OpenGL; Simple DirectMedia Layer (SDL) 2.0: para gerenciamento de dispositivos de vídeo, dispositivos de entrada, áudio, entre outros componentes de hardware. SDL_image 2.0: para leitura de arquivos de imagem. Precisamos também usar um compilador recente com suporte a C++17 e suporte pelo menos parcial a C++20, como o GCC 10 ou Clang 11. Acompanhe nas seções a seguir o passo a passo da instalação desses recursos de acordo com o sistema operacional utilizado: Seção 1.1 para instalação no Linux; Seção 1.2 para instalação no macOS; Seção 1.3 para instalar no Windows. Não é necessário usar um IDE ou editor específico de código-fonte para o desenvolvimento das atividades. A compilação pode ser disparada através de scripts de linha de comando. Entretanto, como um exemplo, veremos na seção 1.4 como fazer a configuração básica do Visual Studio Code para o desenvolvimento de aplicações C++ com CMake. Na seção 1.5 veremos como instalar uma biblioteca auxiliar (a ABCg) criada especialmente para esta disciplina. Ela será utilizada em todas as atividades do curso para facilitar o desenvolvimento das aplicações gráficas. Dica Caso o seu computador tenha recursos de processamento e memória suficientes, é possível configurar todo o ambiente de desenvolvimento em um sistema operacional instalado em uma máquina virtual. O VMware Workstation Player (Windows e Linux) e VMWare Fusion Player (macOS) possuem suporte a aceleração gráfica 3D usando OpenGL 4.1 e são adequados para desenvolver as atividades da disciplina. Tanto o VMWare Workstation Player quanto o Fusion Player podem ser utilizados gratuitamente através de uma licença de uso pessoal. No Windows 10, o Windows Subsystem for Linux (WSL) também suporta aceleração gráfica 3D (disponível somente no WSL 2). Entretanto, a configuração é mais complexa e exige a instalação de um servidor do X Window System, como o VcXsrv. "],["linux.html", "1.1 Linux", " 1.1 Linux As ferramentas e bibliotecas necessárias estão disponíveis nos repositórios de pacotes das principais distribuições Linux. A seguir veremos como instalar os pacotes no Ubuntu. Entretanto, em outras distribuições há pacotes equivalentes e o procedimento é semelhante. Em um terminal, execute os passos a seguir. Atualize o sistema: sudo apt update &amp;&amp; sudo apt upgrade Instale o pacote build-essential (GCC, GDB, Make, etc): sudo apt install build-essential Instale o CMake e Git: sudo apt install cmake git Instale as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0: sudo apt install libglew-dev libsdl2-dev libsdl2-image-dev Opcionalmente, instale as ferramentas de linting do GLSL. Essas ferramentas poderão ser utilizadas no Visual Studio Code para fazer a análise estática do código da linguagem de shaders GLSL (OpenGL Shading Language) que será abordada na disciplina: sudo apt install glslang-tools Habilitando o OpenGL O suporte ao OpenGL já vem integrado no kernel do Linux através dos drivers de código aberto da biblioteca Mesa (drivers Intel/AMD/Nouveau). Para as placas da NVIDIA e AMD há a possibilidade de instalar os drivers proprietários do repositório nonfree (repositório restricted no Ubuntu), ou diretamente dos sites dos fabricantes: AMD ou NVIDIA. Os drivers proprietários, especialmente os da NVIDIA, geralmente têm desempenho superior aos de código aberto. Para verificar a versão do OpenGL suportada pelos drivers instalados, instale primeiro o pacote mesa-utils: sudo apt install mesa-utils Execute o comando: glxinfo | grep version O resultado deverá ser parecido com o seguinte: server glx version string: 1.4 client glx version string: 1.4 GLX version: 1.4 Max core profile version: 4.1 Max compat profile version: 4.1 Max GLES1 profile version: 1.1 Max GLES[23] profile version: 2.0 OpenGL core profile version string: 4.1 (Core Profile) Mesa 21.0.3 OpenGL core profile shading language version string: 4.10 OpenGL version string: 4.1 (Compatibility Profile) Mesa 21.0.3 OpenGL shading language version string: 4.10 OpenGL ES profile version string: OpenGL ES 2.0 Mesa 21.0.3 OpenGL ES profile shading language version string: OpenGL ES GLSL ES 1.0.16 Importante A versão em OpenGL version string ou OpenGL core profile version string deve ser 4.1 ou superior. Caso não seja, instale os drivers proprietários e certifique-se de que sua placa de vídeo suporta OpenGL 4.1. Atualizando o GCC As atividades farão uso de uma biblioteca de desenvolvimento que exige um compilador com suporte a C++17 e suporte parcial a C++20. Esse requisito é atendido se instalarmos uma versão recente do GCC, como o GCC 10. Nas últimas versões do Fedora e Manjaro, o GCC instalado por padrão já é a versão 10 ou superior. No Ubuntu, apenas o Ubuntu 20.10 (Groovy Gorilla) em diante vem com GCC 10 ou mais recente. Siga os passos a seguir caso sua distribuição seja de uma versão anterior, ou caso a saída de g++ --version mostre um número de versão menor que 10. Por exemplo, no Ubuntu 20.04 a saída de g++ --version é: g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. O padrão no Ubuntu 20.04 é o GCC 9.3. Logo, precisamos instalar uma versão mais recente. Em um terminal, adicione o PPA ubuntu-toolchain-r/test: sudo apt install software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test Instale o GCC 10: sudo apt install gcc-10 g++-10 A instalação do GCC 10 não substituirá a versão mais antiga já instalada. Entretanto, é necessário criar links simbólicos de gcc e g++ para a versão mais recente. Uma forma simples de fazer isso é através do update-alternatives. Primeiro, execute o comando a seguir para definir um valor de prioridade (neste caso, 100) para o GCC 10: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 100 --slave /usr/bin/g++ g++ /usr/bin/g++-10 --slave /usr/bin/gcov gcov /usr/bin/gcov-10 Use o comando a seguir para definir um valor de prioridade mais baixo (por exemplo, 90) para a versão anterior do GCC, que neste exemplo é a versão 9: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 --slave /usr/bin/gcov gcov /usr/bin/gcov-9 Agora execute o comando a seguir para escolher qual a versão do GCC instalada no sistema será utilizada: sudo update-alternatives --config gcc Na lista de versões instaladas, selecione o GCC 10 caso ainda não esteja selecionado. Isso criará os links simbólicos. Para testar se a versão correta do GCC está sendo utilizada, execute g++ --version. A saída deverá ser parecida com a seguinte: g++ (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o pacote ccache: sudo apt install ccache Atualize os links simbólicos dos compiladores instalados: sudo /usr/sbin/update-ccache-symlinks Insira a seguinte linha no final do arquivo ~/.bashrc de modo a prefixar o caminho do Ccache no PATH: export PATH=&quot;/usr/lib/ccache:$PATH&quot; Reabra o terminal ou execute source ~/.bashrc. Para testar se o Ccache está ativado, execute o comando which g++. A saída deverá incluir o caminho /usr/lib/ccache/, como a seguir: /usr/lib/ccache/g++`. Instalando o Emscripten Vá para o seu diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual. Um arquivo .emscripten será gerado: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha de ~/.bashrc. Isso fará com que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e erro padrão (stderr). "],["macos.html", "1.2 macOS", " 1.2 macOS Em um terminal, execute os passos a seguir: Execute o comando gcc. Se o GCC não estiver instalado, aparecerá uma caixa de diálogo solicitando a instalação das ferramentas de desenvolvimento de linha de comando. Clique em Install. Esse procedimento também instalará outras ferramentas, como o Make e Git. Para verificar se o GCC foi instalado, execute gcc --version. A saída deverá ser parecida com a seguinte (note que o GCC é apenas um atalho para o Apple Clang): Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1 Apple clang version 12.0.0 (clang-1200.0.32.28) Target: x86_64-apple-darwin19.6.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin Se o procedimento acima não funcionar (as instruções acima foram testadas no macOS Catalina), baixe o Command Line Tools for Xcode usando sua conta de desenvolvedor do Apple Developer, ou execute xcode-select --version no terminal. Em versões mais antigas do macOS pode ser necessário instalar o Xcode. Para instalar os demais pacotes de bibliotecas e ferramentas, instale o Homebrew com o seguinte comando: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Instale o CMake: brew install cmake Instale o GLEW, SDL 2.0 e SDL_image 2.0: brew install glew brew install sdl2 brew install sdl2_image Está pronto! Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o Ccache usando o Homebrew: brew install ccache Anote a saída de echo $(brew --prefix) (por exemplo, /usr/local). Abra o modo de edição do PATH: sudo nano /etc/paths Insira como primeira linha o caminho $(brew --prefix)/opt/ccache/libexec, onde $(brew --prefix) é a saída do passo 2. Por exemplo, /usr/local/opt/ccache/libexec. Salve (Ctrl+X e Y) e reinicie o terminal. Para testar, digite which gcc. A saída deverá ser um caminho que inclui o Ccache, como a seguir: /usr/local/opt/ccache/libexec/gc Instalando o Emscripten Vá para o seu diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual. Um arquivo .emscripten será gerado: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha do arquivo ~/.zshrc (shell Zsh no macOS Catalina ou posterior) ou ~/.bashrc (shell Bash em versões anteriores) para que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e erro padrão (stderr). "],["windows.html", "1.3 Windows", " 1.3 Windows Para a instalação das ferramentas e bibliotecas de desenvolvimento no Windows utilizaremos o MSYS2. MSYS2 é um ambiente de terminal tipo Unix com acesso a um repositório de ferramentas e bibliotecas de desenvolvimento de aplicações nativas em Windows através do gerenciador de pacotes pacman. Essas ferramentas e bibliotecas incluem o CMake, Git, GLEW, SDL 2.0 e SDL_image 2.0, entre outras que vamos utilizar na disciplina. O MSYS2 também permite instalar o MinGW-W64. Com isso podemos usar o GCC (compilador) e o GDB (depurador) no Windows e gerar binário nativo para 64-bit e 32-bit. Siga os passos a seguir para instalar o MSYS2 e as ferramentas/bibliotecas de desenvolvimento: Baixe o instalador de https://www.msys2.org e siga os passos descritos na página. Abra o shell do MSYS2 (aplicativo MSYS2 MSYS a partir no menu Iniciar) e execute o seguinte comando: pacman -S git mingw-w64-x86_64-ccache mingw-w64-x86_64-cmake mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb mingw-w64-x86_64-ninja mingw-w64-x86_64-glew mingw-w64-x86_64-SDL2 mingw-w64-x86_64-SDL2_image Isso instalará as ferramentas Git, Ccache, CMake, Ninja (o Ninja substitui o GNU Make no Windows), GCC e GDB (do MinGW-W64), e as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0. Opcionalmente, instale o pacote mingw-w64-x86_64-glslang com o comando a seguir. Isso instalará as ferramentas que poderão ser usadas para linting da linguagem GLSL (linguagem de shading que será abordada na disciplina): pacman -S mingw-w64-x86_64-glslang Ao terminar a instalação, feche o shell do MSYS2. Instale o Python 3.9+ para Windows. Durante a instalação, certifique-se de ativar a opção Add Python 3.9 to PATH. Após a instalação, abra o Prompt de Comando (cmd.exe) e execute o comando where python para mostrar os diferentes caminhos em que o python está sendo alcançado pela variável de ambiente Path: C:\\&gt;where python C:\\Users\\ufabc\\AppData\\Local\\Programs\\Python\\Python39\\python.exe C:\\Users\\ufabc\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe O primeiro caminho exibido deve ser o caminho do executável do Python que acabou de ser instalado. Neste caso está correto, pois o Python foi instalado no local padrão do instalador que é %LocalAppData%\\Programs\\Python\\Python39 (neste exemplo, o nome do usuário é ufabc). No Painel de Controle do Windows, abra a opção Editar as Variáveis de Ambiente do Sistema. Edite a variável Path do usuário atual1 e inclua os caminhos para mingw64\\bin e usr\\bin do MSYS2. Por exemplo, se o MSYS2 foi instalado em C:\\msys64, inclua os seguintes caminhos no Path: C:\\msys64\\usr\\bin C:\\msys64\\mingw64\\bin Edite a ordem dos caminhos de tal forma que o caminho do Python apareça no início da lista, seguido pelos caminhos do MSYS2, como mostra a figura a seguir: Para testar se o MSYS2 foi instalado corretamente, abra o Prompt de Comando e execute o comando g++ --version. A saída deverá ser parecida com a seguinte: g++ (Rev5, Built by MSYS2 project) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Habilitando o OpenGL O suporte ao OpenGL vem integrado no Windows. Apenas certifique-se de instalar os drivers mais recentes da placa de vídeo. Instalando o Emscripten Abra o Prompt de Comando em algum caminho onde queira instalar a pasta do SDK do Emscripten (por exemplo, C:\\). Note the o terminal deve ser o Prompt de Comando e não o PowerShell. Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre na pasta recém-criada: cd emsdk Baixe e instale o SDK atualizado (latest): emsdk install latest Ative o SDK latest para o usuário atual: emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: emsdk_env.bat Para testar se a instalação foi bem-sucedida, execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 2.0.29 (28ca7fb7ce895b21013212e4644a5794a15a76f9) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal ou execute o comando a seguir para registrar permanentemente as variáveis de ambiente no registro do sistema: emsdk_env.bat --permanent Caso não queira modificar o registro do sistema, uma opção é criar um atalho para cmd.exe e use a opção /k para executar o arquivo emsdk_env.bat sempre que o Prompt de Comando for aberto. Por exemplo: cmd.exe /k &quot;C:\\emsdk\\emsdk_env.bat&quot; Mude esse caminho de acordo com o local onde emsdk_env.bat foi instalado. A variável Path existe tanto nas variáveis do usuário quanto nas variáveis do sistema. A modificação do Path do usuário geralmente é suficiente para uma instalação correta. Se ocorrer algum erro ao seguir as instruções de construção do Hello, World! na seção 1.5 (erro de construção ou de DLL não encontrada), experimente alterar o Path do sistema ao invés do usuário. "],["vscode.html", "1.4 Visual Studio Code", " 1.4 Visual Studio Code Para desenvolver as atividades não é necessário usar um IDE ou editor em particular. Os códigos podem ser escritos em qualquer editor de texto não formatado e a compilação pode ser feita em linha de comando. Entretanto, é recomendável utilizar um editor/IDE como o CLion, Emacs, Sublime Text, Vim, Visual Studio Code, ou outro semelhante que seja capaz de oferecer funcionalidades de preenchimento automático de código, detecção de erros, ajuda sensível ao contexto e integração de construção com o CMake. A seguir veremos como configurar o Visual Studio Code (VS Code) para deixá-lo pronto para o desenvolvimento das atividades. O procedimento é simples e é o mesmo no Linux, macOS e Windows: Instale o VS Code através do instalador disponível em https://code.visualstudio.com/. No editor, instale as seguintes extensões: C/C++ for Visual Studio Code: para habilitar o suporte à edição e depuração de código C/C++ e as funcionalidades de preenchimento automático e referência cruzada do IntelliSense. CMake Tools: para integrar o CMake com o Visual Studio Code. Com essa extensão o editor consegue varrer o sistema em busca dos kits de compilação disponíveis e permite disparar o processo de configuração e compilação a partir do editor. Isso já é o suficiente para começarmos a trabalhar. Ainda precisaremos de algumas configurações extras para habilitar a depuração de código, mas veremos isso na seção a seguir (1.5). Caso você queira usar outro editor ou IDE, consulte a documentação específica do produto sobre como fazer a integração com o CMake e sobre como usar o GDB/LLDB para depurar código. Importante Qualquer que seja o IDE/editor utilizado, certifique-se de que o CMake e GCC estejam instalados e visíveis no PATH de acordo com as instruções mostradas nas seções anteriores. Dicas Opcionalmente, instale a extensão CMake For VisualStudio Code para ter suporte à edição de código dos scripts CMake. Em sistemas que possuem as ferramentas extras do Clang para linting e formatação de código tais como Clang-Tidy e ClangFormat, é possível instalar extensões para usá-las em tempo real e para formatar o código automaticamente sempre que o arquivo é salvo. Uma dessas extensões é o vscode-clangd, baseado no servidor clangd do LLVM. Infelizmente essa extensão não funciona corretamente no Windows com MSYS2, mas há diversas alternativas no Visual Studio Code Marketplace. Procure extensões com as palavras-chave clang-tidy e clang-format. Essas ferramentas ajudam a evitar os erros e bugs mais comuns e melhoram bastante a produtividade. No Ubuntu, o ClangFormat e Clang-Tidy podem ser instalados com: sudo apt install clang-tidy clang-format Isso instalará também o compilador Clang. No MSYS2: pacman -S mingw-w64-x86_64-clang-tools-extra Para ativar a formatação automática de código sempre que o arquivo é salvo, adicione a chave \"editor.formatOnSave\": true no arquivo de configuração do VS Code (settings.json). Para a análise estática em tempo real de código GLSL, instale a extensão GLSL Lint. Isso ajudará a evitar bugs e erros comuns na programação dos shaders em GLSL. "],["abcg.html", "1.5 ABCg", " 1.5 ABCg Para facilitar o desenvolvimento das atividades práticas utilizaremos a biblioteca ABCg desenvolvida especialmente para esta disciplina. A ABCg permite a prototipagem rápida de aplicações gráficas interativas 3D em C++ capazes de rodar tanto no desktop (binário nativo) quanto no navegador (binário WebAssembly). Internamente a ABCg utiliza a biblioteca SDL para gerenciar o acesso a dispositivos de entrada (mouse/teclado/gamepad) e saída (vídeo e áudio) de forma independente de plataforma, e a biblioteca GLEW para acesso às funções da API gráfica OpenGL. Além disso, a API do Emscripten é utilizada sempre que a aplicação é compilada para gerar binário WebAssembly. A ABCg é mais propriamente um framework do que uma biblioteca de funções, pois assume o controle da aplicação. Por outro lado, a camada de abstração para as APIs utilizadas é mínima e é possível acessar as funções da SDL e OpenGL diretamente (e faremos isso sempre que possível). Outras bibliotecas também utilizadas e que podem ser acessadas diretamente são: CPPIterTools: para o suporte a laços range-based em C++ usando funções do tipo range, enumerate e zip similares às do Python; Dear ImGui: para gerenciamento de widgets de interface gráfica do usuário, tais como janelas, botões e caixas de edição; {fmt}: como alternativa mais eficiente ao stdio da linguagem C (printf, scanf, etc) e iostreams do C++ (std::cout, std::cin, etc), e para formatação de strings com uma sintaxe similar ao str-format do Python; Guidelines Support Library (GSL): para uso de funções e tipos de dados recomendados pelo C++ Core Guidelines; OpenGL Mathematics (GLM): para suporte a operações de transformação geométrica com vetores e matrizes; tinyobjloader: para a leitura de modelos 3D no formato Wavefront OBJ. A seguir veremos como instalar e compilar a ABCg junto com um exemplo de uso. Instalação Em um terminal, clone o repositório do GitHub: git clone https://github.com/hbatagelo/abcg.git Observação A versão mais recente da ABCg (atualmente v2.0.0) também pode ser baixada como um arquivo compactado de https://github.com/hbatagelo/abcg/releases/latest. Atenção No Windows, certifique-se de clonar o repositório em um diretório cujo nome não contenha espaços ou caracteres especiais. Por exemplo, clone em C:\\cg em vez de C:\\computação gráfica. O repositório tem a estrutura mostrada a seguir. Para simplificar, os arquivos e subdiretórios .git* foram omitidos: abcg  .clang-format  .clang-tidy  build.bat  build.sh  build-wasm.bat  build-wasm.sh  CMakeLists.txt  LICENSE  README.md  runweb.bat  runweb.sh  VERSION.md  abcg   ...  cmake   ...  examples   ...  public  ... Os arquivos .clang-format e .clang-tidy são arquivos de configuração utilizados pelas ferramentas ClangFormat (formatação) e Clang-Tidy (linter) caso estejam instaladas. Os arquivos .sh são shell scripts de compilação e execução em linha de comando. Note que há scripts correspondentes com extensão .bat para usar no Prompt de Comando do Windows (o PowerShell não é suportado): build.sh: para compilar a biblioteca e os exemplos em binários nativos; build-wasm.sh: similar ao build.sh, mas para gerar binário em WebAssembly dentro do subdiretório public; runweb.sh: para rodar um servidor web local que serve o conteúdo de public. O arquivo CMakeLists.txt é o script de configuração utilizado internamente pelo CMake. Os subdiretórios são os seguintes: abcg contém o código-fonte da biblioteca e suas dependências; cmake contém scripts auxiliares de configuração do CMake; examples contém um exemplo de uso da ABCg: um Hello, World! que usa OpenGL e interface da ImGui; public contém páginas web para exibir o exemplo Hello, World! no navegador. Compilando em linha de comando Execute o script build.sh (Linux/macOS) ou build.bat (Windows) para iniciar o processo de configuração e construção. A saída será similar a esta (o exemplo a seguir é do Windows): -- The C compiler identification is GNU 10.3.0 -- The CXX compiler identification is GNU 10.3.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: C:/msys64/mingw64/bin/gcc.exe - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: C:/msys64/mingw64/bin/g++.exe - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done Using ccache -- Found OpenGL: opengl32 -- Found GLEW: C:/msys64/mingw64/lib/cmake/glew/glew-config.cmake -- Looking for pthread.h -- Looking for pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success -- Found Threads: TRUE -- Found SDL2: mingw32;-mwindows;C:/msys64/mingw64/lib/libSDL2main.a;C:/msys64/mingw64/lib/libSDL2.dll.a -- Found SDL2_image: C:/msys64/mingw64/lib/libSDL2_image.dll.a -- Configuring done -- Generating done -- Build files have been written to: C:/abcg/build ... [22/22] Linking CXX executable bin\\helloworld.exe Ao final, os binários estarão disponíveis no subdiretório build. A biblioteca estática estará em build/abcg/libabcg.a e o executável do exemplo Hello, World! estará em build/bin/helloworld. Para testar, execute o helloworld. No Linux/macOS: ./build/bin/helloworld/helloworld No Windows: .\\build\\bin\\helloworld\\helloworld.exe | cat Importante No Windows, a saída deve sempre ser redirecionada para cat ou tee. Se isso não for feito, nenhuma saída de texto será exibida no terminal. Isso se deve a um bug do MSYS2. Observação Observe o conteúdo de build.sh (build.bat contém instruções equivalentes): #!/bin/bash set -euo pipefail BUILD_TYPE=Debug # Reset build directory rm -rf build mkdir -p build &amp;&amp; cd build # Configure cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE .. # Build if [[ &quot;$OSTYPE&quot; == &quot;darwin&quot;* ]]; then # macOS NUM_PROCESSORS=$(sysctl -n hw.ncpu) else NUM_PROCESSORS=$(nproc) fi cmake --build . --config $BUILD_TYPE -- -j $NUM_PROCESSORS A variável BUILD_TYPE está como Debug, mas pode ser modificada para Release, MinSizeRel ou RelWithDebInfo. Use a opção Debug (padrão) ou RelWithDebInfo enquanto estiver depurando o código. Use Release para gerar um binário otimizado e sem arquivos de símbolos de depuração (otimiza para gerar código mais rápido) ou MinSizeRel (otimiza para gerar binário de menor tamanho). Observe que o script apaga o subdiretório build antes de criá-lo novamente. Portanto, não salve arquivos dentro de build pois eles serão apagados na próxima compilação! A geração dos binários usando o CMake é composta de duas etapas: configuração (cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE ..) e construção (cmake --build . --config $BUILD_TYPE). A configuração gera os scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja). A construção dispara a compilação e ligação usando tais scripts. Todos os arquivos gerados na configuração e construção ficam armazenados no subdiretório build. Compilando no Visual Studio Code Primeiramente, clone o repositório abcg do GitHub como mostrado na seção anterior. Apague o subdiretório build caso você já tenha compilado via linha de comando. No Visual Studio Code, selecione o menu File &gt; Open Folder e abra a pasta abcg. No canto inferior direito da janela aparecerá uma notificação solicitando se você quer configurar o projeto. Selecione Yes. Ao fazer isso, será feita uma varredura no sistema para identificar os compiladores e toolchains visíveis no PATH. Uma lista de kits encontrados aparecerá na janela Output do CMake/Build, como a seguir: [kit] Found Kit: Clang 11.0.0 [kit] Found Kit: GCC for x86_64-w64-mingw32 10.2.0 Ao final da varredura, selecione o kit GCC for x86_64-w64-mingw32 versão 10 ou posterior. Ao fazer isso, será iniciado o processo de configuração do CMake. Esse processo gera os arquivos que serão utilizados pelo sistema de construção nativo dentro de um subdiretório build do projeto. Se aparecer uma notificação pedindo para configurar o projeto sempre que ele for aberto, Responda Yes: Após o término da configuração do CMake, aparecerá uma outra notificação solicitando permissão para configurar o Intellisense. Responda Allow. Se, além disso, aparecer uma notificação sobre o arquivo compile_commands.json, como a seguir, responda Yes novamente: compile_commands.json é um arquivo gerado automaticamente pelo CMake e que contém os comandos de compilação e o caminho de cada unidade de tradução utilizada no projeto. O IntelliSense utiliza as informações desse arquivo para habilitar as referências cruzadas. Importante A construção dos projetos usando o CMake é feita em duas etapas: Configuração: consiste na geração dos scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja); Construção: consiste no disparo da compilação e ligação usando os scripts gerados na configuração, além da execução de etapas de pré e pós-construção definidas nos scripts dos arquivos CMakeList.txt. Tanto os arquivos da configuração quanto os da construção (binários) são gravados no subdiretório build. Geralmente a configuração só precisa ser feita uma vez e depois refeita caso o subdiretório build tenha sido apagado, ou após a alteração do kit de compilação, ou após a alteração do build type (por exemplo, de Debug para Release). Como indicado na figura abaixo, na barra de status há botões para selecionar o build type e configurar o CMake, selecionar o kit de compilação, e construir a aplicação. A opção de construir já se encarrega de configurar o CMake caso os arquivos de configuração ainda não tenham sido gerados. Essas opções também estão disponíveis na paleta de comandos do editor, acessada com Ctrl+Shift+P. Os comandos são: CMake: Select Variant: para selecionar um build type; CMake: Select a Kit: para selecionar um kit de compilação; CMake: Configure: para configurar o CMake usando o kit e o build type atual; CMake: Build: para construir o projeto. Observação Os build types permitidos no CMake são: Debug para gerar binários não otimizados e com arquivos de símbolos de depuração. Esse é o build type padrão; RelWithDebInfo para gerar arquivos de símbolos de depuração com binários otimizados; Release para gerar binários otimizados e favorecer código mais rápido. Essa opção não gera os arquivos de símbolos de depuração; MinSizeRel, semelhante ao Release, mas a otimização tenta gerar binário de menor tamanho. Para compilar e gerar os binários, tecle F7 ou clique em Build na barra de status. O progresso será exibido na janela Output do CMake/Build. Se a construção terminar com sucesso, a última linha de texto da janela Output será: [build] Build finished with exit code 0 Os arquivos gerados na construção ficam armazenados no subdiretório build, da mesma forma como ocorre na compilação via linha de comando. Para testar, abra um terminal e execute ./build/bin/helloworld/helloworld (Linux/macOS) ou .\\build\\bin\\helloworld\\helloworld.exe (Windows). Atenção A configuração do CMake gerada a partir do Visual Studio Code não é necessariamente a mesma gerada usando os scripts de linha de comando: o compilador pode ser diferente, ou o build type pode ser diferente. Se em algum momento você construir o projeto via linha de comando usando os scripts .sh ou .bat e depois quiser construir pelo editor, certifique-se de apagar o subdiretório build antes de entrar no VS Code. Isso forçará uma nova configuração do CMake e evitará erros de incompatibilidade entre as configurações. Depurando no Visual Studio Code Podemos depurar o código com GDB ou LLDB usando a interface do Visual Studio Code. Após construir o projeto com build type Debug ou RelWithDebInfo, selecione a opção Run (Ctrl+Shift+D ou botão na barra de atividades) e então a opção create a launch.json file para criar um arquivo launch.json no subdiretório .vscode da pasta do projeto: Em Select Environment, selecione C++ (GDB/LLDB). Isso criará uma configuração inicial para o arquivo json: No arquivo launch.json, modifique o valor da chave program para apontar para o executável que se deseja depurar. Por exemplo, ${workspaceFolder}/build/bin/helloworld/helloworld para apontar para o executável do Hello, World! Observação ${workspaceFolder} é uma variável pré-definida do Visual Studio Code que contém o caminho da pasta do projeto. Consulte a documentação para informações sobre outras variáveis disponíveis. Modifique o valor da chave miDebuggerPath para o caminho do executável do GDB ou LLDB, ou deixe vazio para usar o padrão do sistema. No Windows, a chave miDebuggerPath deve conter explicitamente o caminho completo para o GDB, que é C:\\msys64\\mingw64\\bin\\gdb.exe caso o MSYS2 tenha sido instalado em C:\\msys64. No Windows também é necessário configurar o terminal padrão do VS Code para Command Prompt no lugar de PowerShell. O exemplo abaixo mostra o conteúdo completo de launch.json para depurar o Hello, World! no Windows. { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld.exe&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: true, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;miDebuggerPath&quot;: &quot;C:\\\\msys64\\\\mingw64\\\\bin\\\\gdb.exe&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Observe que o valor da chave externalConsole foi modificado para true para que um terminal de saída seja aberto durante a depuração. Consulte a documentação sobre depuração para informações sobre outras opções e informações gerais sobre como depurar código no editor. Após modificar o arquivo launch.json, selecione novamente a opção Run na barra de atividades ou tecle F5 para iniciar o programa no modo de depuração. Reedite o arquivo launch.json sempre que mudar o nome do executável que se queira depurar. Compilando para WebAssembly Podemos compilar as aplicações ABCg para WebAssembly de modo a rodá-las diretamente no navegador. A construção é feita via linha de comando usando o toolchain Emscripten. Acompanhe a seguir como construir o exemplo Hello, World! para WebAssembly e abri-lo no navegador: Em um terminal (shell ou Prompt de Comando), ative as variáveis de ambiente do Emscripten (script emsdk_env.sh/emsdk_env.bat do SDK). Após isso, o compilador emcc deverá estar visível no PATH; No diretório abcg, execute build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Isso iniciará a configuração do CMake e a construção dos binários. Os arquivos resultantes serão gerados no subdiretório public: nesse caso, helloworld.data (arquivo de dados/assets), helloworld.js (arquivo JavaScript) e helloworld.wasm (binário WebAssembly); Execute o script runweb.sh (Linux/macOS) ou runweb.bat (Windows) para rodar um servidor web local. O conteúdo de public estará disponível em http://localhost:8080/; Abra a página http://localhost:8080/helloworld.html que chama o script helloworld.js recém-criado. A página HTML não faz parte do processo de construção e foi criada previamente. O resultado será semelhante ao exibido a seguir (uma aplicação mostrando um triângulo colorido e uma caixa de diálogo com alguns controles de interface). A pequena janela de texto abaixo da janela da aplicação mostra o conteúdo do terminal. Nesse caso, são exibidas algumas informações sobre o OpenGL (versão utilizada, fornecedor do driver, etc). Observação O subdiretório public contém, além do helloworld.html: full_window.html: para exibir o Hello, World! ocupando a janela inteira do navegador; full_window_console.html: idêntico ao anterior, mas com a sobreposição das mensagens do console na tela. Nos próximos capítulos veremos como construir novas aplicações usando a ABCg. Dica Aproveite o restante da primeira semana de aula para se familiarizar com os conceitos do chamado C++ moderno (C++11 em diante): ponteiros inteligentes (smart pointers), expressões lambda, variáveis auto e semântica de movimentação (move semantics). Isso facilitará o entendimento do código da ABCg nos próximos capítulos. Uma referência rápida (cheatsheet) ao C++ moderno está disponível em https://github.com/AnthonyCalandra/modern-cpp-features. Um excelente livro é o A Tour of C++, de Bjarne Stroustrup. Se não puder ter acesso ao livro, há recursos gratuitos como os sites learncpp.com e tutorialspoint.com. A documentação da Microsoft sobre C++ é uma opção em português. Há uma referência sobre a linguagem C++ e sobre a biblioteca C++ padrão. Consulte também o C++ Core Guidelines para ficar a par das boas práticas de programação. Para uma referência completa da linguagem, consulte cppreference.com. Algumas partes estão traduzidas para o português. "],["intro.html", "2 Introdução", " 2 Introdução Computação gráfica (CG) é o conjunto de métodos e técnicas para a construção, manipulação, armazenagem e exibição de imagens por meio de um computador (ISO 2015). A computação gráfica tem suas origens no desenvolvimento dos primeiros computadores eletrônicos equipados com dispositivos de exibição. A partir do final da década de 1940, o computador experimental Whirlwind I do MIT (Instituto de Tecnologia de Massachusetts) e o sistema SAGE (Semi-Automatic Ground Environment) da Força Aérea dos Estados Unidos, foram os primeiros a utilizar dispositivos de exibição do tipo CRT (cathod-ray tube, ou tubo de raios catódicos) para exibir gráficos vetoriais compostos de linhas e pontos. O uso do termo computação gráfica é um pouco mais recente. William Fetter, projetista gráfico da Boeing, utilizou o termo pela primeira vez em 1960 por sugestão de seu supervisor Verne L. Hudson, para descrever seu trabalho. Fetter utilizava gráficos tridimensionais no computador para criar um modelo estilizado de corpo humano que ficou conhecido como Boeing Man (figura 2.1). O modelo, composto de curvas e segmentos, era utilizado em simulações de ergonomia do piloto na cabine do avião. Figura 2.1: Boeing Man desenhado por William Fetter em um IBM 7094 (fonte). Além de CG, o acrônimo CGI (Computer-Generated Imagery) é frequentemente utilizado para se referir à geração de imagens e efeitos visuais em computador com aplicações em arte, entretenimento, simulação e visualização científica. Uma forma de CGI bastante conhecida e que chama a atenção por suas imagens bonitas é a síntese de imagens fotorrealistas. Um exemplo de imagem fotorrealista gerada em computador é mostrado na figura 2.2. A imagem é uma cena de arquitetura interior produzida com o Cycles Render Engine. Figura 2.2: Scandinavian Interior, por Arnaud Imobersteg (fonte). Atualmente, uma imagem como a da figura 2.2 pode ser gerada em qualquer computador pessoal que tenha sido fabricado nos últimos 10 anos. A imagem foi gerada com o software Blender, gratuito e de código aberto, e que tem o Cycles Render Engine como um de seus renderizadores. Entretanto, existe uma máxima em computação gráfica  a chamada 1ª lei de Peddie  que diz: Em computação gráfica, demais nunca é o suficiente.  John Peddie De fato, mesmo com toda a evolução do hardware gráfico e das técnicas de CG, ainda não é possível gerar em tempo real imagens com o nível de qualidade obtido por renderizadores como o Cycles. Eventualmente, esse dia chegará. Porém, quando isso acontecer, o nível de exigência de todos nós  usuários, pesquisadores e desenvolvedores  será maior do que é atualmente. Desejaremos imagens ainda mais detalhadas, com maior resolução, mais realistas, mais cinemáticas, mais interativas, etc. Demais nunca é o suficiente. Em computação gráfica, é necessário um cuidadoso compromisso entre a qualidade das imagens geradas e a eficiência com que essas imagens podem ser sintetizadas em um dado sistema computacional. Isso é particularmente importante quando as imagens precisam ser geradas em tempo real. Em jogos digitais, é comum que imagens de alta resolução tenham de ser geradas a uma taxa de, no mínimo, 30 quadros por segundo2. Mesmo os jogos que não visam o fotorrealismo exigem imagens em um nível de qualidade que só pode ser alcançado com o uso de técnicas avançadas de sombreamento e iluminação. Essas técnicas visam produzir resultados que, ainda que não sejam necessariamente acurados do ponto de vista físico, precisam ser suficientemente convincentes para um público cada vez mais exigente. No decorrer do quadrimestre veremos que a evolução das técnicas e ferramentas de computação gráfica em tempo real é impulsionada pela busca da melhor qualidade de imagem que pode ser obtida de forma eficiente no hardware gráfico disponível no momento. Como resultado, é comum que os métodos adotem simplificações inusitadas, mas ao mesmo tempo muito espertas, e explorem diferentes aspectos da percepção visual humana para criar uma ilusão de realismo que seja suficiente para chegar ao resultado desejado. Referências "],["áreas-correlatas.html", "2.1 Áreas correlatas", " 2.1 Áreas correlatas A computação gráfica se relaciona, e em certa medida se sobrepõe, a diferentes campos de atuação da ciência da computação. Uma breve introdução às principais áreas correlatas é dada a seguir: Síntese de imagem: é o que geralmente se entende por computação gráfica. Compreende o processo de rendering (imageamento ou renderização) que consiste em converter especificações de geometria, cor, textura, iluminação, entre outras especificações de características de uma cena, em uma imagem exibida em um display gráfico. A figura 2.3 mostra o resultado da síntese de imagem de uma cena fotorrealista usando técnicas combinadas de traçado de raios e radiosidade. Figura 2.3: Imagem gerada no renderizador POV-Ray, por Gilles Tran (fonte). Visão computacional: compreende o processo de adquirir, processar e interpretar dados visuais para gerar as especificações de uma cena. A partir de uma imagem digital, técnicas de visão computacional podem ser utilizadas para tarefas como a reconstrução dos modelos geométricos vistos na imagem, o particionamento dos pixels em segmentos correspondentes aos diferentes objetos da cena, reconhecimento de texturas, identificação dos atributos da câmera e da iluminação, e extração de outras informações semânticas. A visão computacional com frequência se relaciona com a visão de máquina, que compreende as técnicas e ferramentas voltadas a aplicações de visão em inspeção automática, controle de processos industriais e orientação de robôs. A figura 2.4 mostra um exemplo de aplicação de visão computacional: uma técnica de segmentação semântica utilizando aprendizagem profunda para identificar objetos em uma imagem. Figura 2.4: Segmentação semântica usando o sistema YOLO (fonte). Processamento de imagem: compreende o processo de aplicar filtros e operações sobre uma imagem digital que resultam em uma nova imagem digital. Técnicas de processamento digital de imagem podem ser utilizadas para enfatizar características de uma imagem (por exemplo, ajustar brilho, contraste, nitidez), restaurar imagens que sofreram algum tipo de degradação por ruído, mudar cores e tons, comprimir e quantizar, entre diversas outras operações. O escopo do processamento de imagens frequentemente se intersecta com aquele das técnicas de visão computacional. A figura 2.5 mostra um exemplo de processamento de imagem: a aplicação de filtros de remoção de ruído em uma imagem renderizada pelo método de traçado de raios estocástico. O ruído é inerente ao método de Monte Carlo utilizado nesse tipo de renderização. Figura 2.5: Uso dos filtros de processamento de imagem do Intel Open Image Denoise para remoção de ruído de uma imagem de traçado de raios (fonte). Modelagem geométrica: está relacionada com a criação e processamento de representações matemáticas de formas. Técnicas de modelagem geométrica podem ser utilizadas para criar modelos compostos de curvas e superfícies a partir de aquisição de dados (por exemplo, a partir de uma nuvem de pontos de uma aquisição por scanner 3D), construir e manipular modelos sintéticos através da combinação de primitivas geométricas, converter uma representação geométrica em outra, e realizar operações geométricas e topológicas diversas. A figura 2.6 mostra um exemplo de reconstrução de malha geométrica usando o software MeshLab. O modelo à esquerda é o modelo original. Na reconstrução (à direita), os buracos foram preenchidos e o resultado é uma única malha de triângulos. Figura 2.6: Reconstrução de malha geométrica usando o MeshLab (fonte). Neste curso teremos como foco a síntese de imagens. Em particular, a síntese de imagens em tempo real. Como parte disso, veremos como representar e processar cenários virtuais compostos de objetos tridimensionais animados. Veremos como implementar modelos de iluminação capazes de simular de forma eficiente a iluminação de superfícies, e como gerar imagens digitais do ponto de vista de uma câmera virtual. Faremos isso usando a API gráfica OpenGL de modo a explorar o pipeline de processamento gráfico programável das placas de vídeo atuais. Com isso conseguiremos obter o nível de eficiência necessário para produzir animações e permitir a sensação de interatividade. Na UFABC, os tópicos de visão computacional e processamento de imagens são abordados nas disciplinas ESZA019-17 Visão Computacional e MCZA018-17 Processamento Digital de Imagens. "],["linha-do-tempo.html", "2.2 Linha do tempo", " 2.2 Linha do tempo Nesta seção acompanharemos um resumo da evolução histórica da computação gráfica. Iniciaremos na década de 1950, com os primeiros computadores eletrônicos de uso geral e o surgimento das primeiras aplicações de computação gráfica, e seguiremos até a década atual com os desenvolvimentos mais recentes das atuais GPUs (Graphics Processing Units). Embora a computação gráfica seja recente, assim como a própria ciência da computação, o desenvolvimento de seus fundamentos é anterior ao século XX e só foi possível devido às contribuições artísticas e matemáticas de diversos pioneiros. Para citar apenas alguns: Euclides de Alexandria (300 a.C.), com sua contribuição para o desenvolvimento da geometria; Filippo Brunelleschi (13771446), com seus estudos sobre o uso da perspectiva; René Descartes (15961650), com o desenvolvimento da geometria analítica e a noção de sistema de coordenadas; Christiaan Huygens (16291695) e Isaac Newton (16431727) por suas investigações sobre os fenômenos da luz; Leonhard Euler (17071783), por sua contribuição na trigonometria e em topologia; James Joseph Sylvester (18141897), pela invenção da notação matricial. O uso de gráficos no computador também não teria sido possível sem os esforços que contribuíram para o surgimento dos primeiros dispositivos de exibição, como o tubo de raios catódicos no final do século XIX. 1950 Os primeiros computadores eletrônicos com dispositivos de exibição surgem neste período. O computador Whirlwind I, do MIT, originalmente projetado para ser parte de um simulador de vôo, foi um dos primeiros computadores digitais de uso geral com processamento em tempo real. O Whirlwind I era equipado com um CRT vetorial capaz de desenhar linhas e pontos. Charles W. Adams e John T. Gilmore, programadores da equipe de desenvolvimento do Whirlwind, implementaram um programa de avaliação de equações diferenciais para produzir a animação da trajetória de uma bola quicando. Essa simulação pode ser considerada a primeira aplicação de computação gráfica interativa e o primeiro jogo de computador, pois o operador podia controlar, através de um botão, a frequência do quicar na tentativa de fazer a bola acertar uma lacuna na tela que simulava um buraco no chão. O sistema de defesa aérea SAGE evoluiu a partir do Whirlwind ao longo da década de 1950. As estações do SAGE contavam com telas CRT que exibiam dados de diferentes radares combinados com informações de referência geográfica. Cada estação era também equipada com uma caneta óptica. Através da caneta óptica, o operador podia apontar e selecionar elementos gráficos diretamente na tela (figura 2.7). Figura 2.7: Operador do SAGE usando uma caneta óptica em um CRT vetorial (fonte). 1960 Nesse período a computação gráfica se desenvolve nos laboratórios de pesquisa de universidades e surgem as primeiras aplicações de CAD (Computer-Aided Design) nas indústrias automotiva e aeroespacial. Na década de 1960 ocorrem importantes desenvolvimentos na área de modelagem geométrica, como o uso de curvas de Bézier e NURBS (Non-Uniform Rational Basis Spline). Em 1960, a Digital Equipment Corporation (DEC) começa a produzir em escala comercial o computador PDP-1, equipado com CRT e caneta óptica. Em 1961, o cientista da computação Steve Russell (MIT) cria o Spacewar! (figura 2.8). O jogo ganha popularidade dentro e fora da universidade e vira referência no desenvolvimento de jogos digitais3. Figura 2.8: Steve Russell e seu jogo Spacewar! no DEC PDP-1 (fonte). Em 1963, Ivan Sutherland desenvolve o SketchPad, um sistema de projeto gráfico interativo que permite ao usuário manipular primitivas gráficas vetoriais através de uma caneta óptica e um CRT (Sutherland 1963). A figura 2.9 mostra Sutherland operando o SketchPad no computador TX-2 do MIT. O SketchPad é um marco no uso da interface gráfica do usuário (GUI, acrônimo de Graphical User Interface) e um precursor das aplicações de projeto assistido por computador (CAD). Figura 2.9: Ivan Sutherland operando o SketchPad em 1962 (fonte). Na década de 1960 surgem também os primeiros seminários e grupos de interesse em pesquisa sobre gráficos em computador. Na ACM (Association for Computing Machinery), tradicional sociedade científica e educacional dedicada à computação, é fundado o grupo SICGRAPH (Special Interest Committe on Computer Graphics) para promover seminários de computação gráfica. No final da década, o SICGRAPH muda de nome para SIGGRAPH (Special Interest Group on Computer Graphics and Interactive Techniques). A conferência SIGGRAPH é realizada anualmente e é hoje uma das principais conferências de computação gráfica no mundo. 1970 Durante a década de 1970 são desenvolvidas muitas das técnicas de síntese de imagens em tempo real utilizadas atualmente. Em 1971, o então aluno de doutorado Henri Gouraud, trabalhando com Dave Evans e Ivan Sutherland na Universidade de Utah, desenvolve uma técnica eficiente de melhoramento da percepção visual do sombreamento (shading) de superfícies suaves aproximadas por malhas poligonais (Gouraud 1971). Tal técnica, conhecida como Gouraud shading, consiste em interpolar linearmente os valores de intensidade de luz refletida dos vértices da malha poligonal. O resultado é a suavização da variação da reflexão de luz sem a necessidade de aumentar a resolução da malha geométrica (Figura 2.10). Figura 2.10: Visualização de uma esfera aproximada por polígonos, exibindo o aspecto facetado (esquerda) e suavizado com Gouraud shading (direita). Em 1973, Bui Phong, também na Universidade de Utah, desenvolve o Phong shading como um melhoramento de Gouraud shading para reproduzir com mais fidelidade as reflexões especulares em aproximações de superfícies curvas (Phong 1973). Na figura 2.11 é possível comparar Gouraud shading e Phong shading lado a lado. Phong shading reproduz de forma mais acurada o brilho especular da esfera sem precisar usar uma malha poligonal mais refinada. Figura 2.11: Visualização de uma esfera com Gouraud shading (esquerda) e Phong shading (direita). Phong também propôs um modelo empírico de iluminação local de pontos sobre superfícies conhecido como modelo de reflexão de Phong. Em 1977, Jim Blinn, aluno da mesma universidade, propôs uma alteração do modelo de reflexão de Phong  o modelo de BlinnPhong  mais acurado fisicamente e mais eficiente sob certas condições de visualização e iluminação (Blinn 1977). Nas décadas seguintes, o modelo de BlinnPhong tornaria-se o padrão de indústria para síntese de imagens em tempo real, e ainda é muito utilizado atualmente. Em 1974, Wolfgang Straßer, na Universidade Técnica de Berlim, e Ed Catmull, na Universidade de Utah, desenvolvem ao mesmo tempo, mas de forma independente, uma técnica que viria a ser conhecida como Z-buffering. Tal técnica permite identificar, de forma conceituamente simples e favorável à implementação em hardware, quais partes da geometria 3D estão visíveis de um determinado ponto de vista. Atualmente, essa técnica é largamente utilizada em síntese de imagens e é suportada em todo hardware gráfico. Além de ter contribuído com a técnica de Z-buffering, Catmull também trouxe diversos avanços na área de modelagem geométrica, especialmente em subdivisão de superfícies e representação paramétrica de superfícies bicúbicas (Catmull 1974). Outra importante contribuição de Catmull foi o desenvolvimento da técnica de mapeamento de textura, ubíqua nas aplicações gráficas atuais e que permite aumentar a percepção de detalhes de superfícies sem aumentar a complexidade da geometria (figura 2.12). Figura 2.12: Animação do mapeamento de uma textura 2D sobre um modelo poligonal 3D (fonte). Em 1975, o matemático Benoît Mandelbrot, na IBM, desenvolve o conceito de geometria de dimensão fracionária e cria o termo fractal (Albers and Alexanderson 2008). Desde então, fractais começam a ser explorados em síntese de imagens e modelagem geométrica para representar os mais diversos padrões e fenômenos naturais tais como contornos de mapas, relevo de terrenos, nuvens, texturas e plantas. Vol Libre Vol Libre, de Loren Carpenter, foi o primeiro filme criado com fractais. O vídeo, de apenas dois minutos, foi apresentado pela primeira vez na conferência SIGGRAPH 80 após uma palestra técnica de Carpenter sobre a renderização de curvas e superfícies fractais: De acordo com o livro Droidmaker: George Lucas And the Digital Revolution (Rubin 2005), ao final da exibição do vídeo, Ed Catmull e Alvy Smith, da Lucasfilm, abordaram Carpenter e ofereceram a ele um emprego na divisão de computação da empresa. Carpenter aceitou imediatamente. Após a carreira na Lucasfilm, Carpenter ainda seria co-fundador da Pixar (junto com Catmull, Smith e outros) e cientista-chefe do estúdio de animação. Em 1976, Steve Jobs, Steve Wozniak e Ronald Wayne fundam a Apple Computer (atualmente Apple Inc.). Em 1979, Steve Jobs entra em contato com as pesquisas de desenvolvimento de interface gráfica na Xerox PARC (atualmente PARC), divisão de pesquisa da Xerox em Palo Alto, Califórnia. Na PARC, Jobs conhece o Xerox Alto, o primeiro computador com uma interface gráfica baseada na metáfora do desktop e no uso do mouse (figura 2.13). Figura 2.13: Xerox Alto (fonte). O Xerox Alto foi o resultado de desenvolvimentos iniciados por Douglas Engelbart e Dustin Lindberg no Standard Research Institute, atual SRI International, por sua vez inspirados no SketchPad de Sutherland. Alguns anos depois, a Apple implementaria os conceitos do Xerox Alto nos computadores Apple Lisa e Macintosh, iniciando uma revolução no uso da interface gráfica nos computadores pessoais (PCs). Em 1977, surge a primeira tentativa de padronização de especificação de comandos em sistemas gráficos: o Core Graphics System (ou simplesmente Core), proposto pelo Graphic Standards Planning Committee (GSPC) da ACM SIGGRAPH (Chappell and Bono 1978). Em 1978, Jim Blinn desenvolve uma técnica de mapeamento de textura para simulação de vincos e rugosidades em superfícies: o bump mapping (Blinn 1978). Uma forma de bump mapping muito utilizada atualmente é o normal mapping. A técnica pode ser muito efetiva para manter a ilusão de uma superfície detalhada, mesmo quando a geometria utilizada é muito simples. A figura 2.14 mostra um exemplo dessa simplificação. Ao longo do quadrimestre implementaremos esta e outras técnicas de texturização. Figura 2.14: Uso de normal mapping para simular a renderização, com apenas dois triângulos, de um modelo de quatro milhões de triângulos (fonte). No final da década, J. Turner Whitted desenvolve a técnica de traçado de raios (Whitted 1979). O traçado de raios consegue simular com mais precisão, e de forma conceitualmente simples, efeitos ópticos de reflexão, refração, espalhamento e dispersão da luz. Como resultado, consegue gerar imagens mais fotorrealistas, ainda que sob um custo computacional muito elevado quando comparado com a renderização baseada na rasterização, que consiste na varredura e preenchimento de primitivas geométricas projetadas. Figura 2.15: Esferas e tabuleiro de xadrez: uma das primeiras imagens geradas com traçado de raios, por Turner Whitted. 1980 Essa é a década em que a computação gráfica marca sua presença definitiva na indústria de cinema. O uso de cenas de computação gráfica é popularizado a partir de filmes como Star Trek II: The Wrath of Khan (1982), Tron (1982) e \"Young Sherlock Holmes (1985), como resultado dos avanços das técnicas de síntese de imagem e modelagem geométrica da década anterior, combinado com o avanço da capacidade de processamento dos computadores. Durante essa década ocorrem também importantes avanços nas técnicas de síntese de imagens. Em 1984, Robert Cook, Thomas Porter e Loren Carpenter desenvolvem o traçado de raios distribuído (distributed ray tracing), o qual permite reproduzir efeitos de sombras suaves, entre outros efeitos não contemplados pelo método original de Whitted (Cook, Porter, and Carpenter 1984). A figura 2.16 mostra um exemplo de renderização da cena de teste Cornell box usando essa técnica. A imagem tende a ser granulada como resultado da natureza estocástica do algoritmo. Figura 2.16: Imagem gerada com traçado de raios distribuído/estocástico. Ainda em 1984, Donald Greenberg, Michael Cohen e Kenneth Torrance propõem a técnica de radiosidade (Greenberg, Cohen, and Torrance 1986) baseada no uso do método de elementos finitos para simular interreflexões de luz entre superfícies idealmente difusas. A solução da radiosidade de uma cena pode ser pré-processada e não depende da posição da câmera. Isso permite a visualização da cena em tempo real, desde que a posição dos objetos e fontes de luz mantenha-se estática. A figura 2.17 mostra um exemplo de cena renderizada com radiosidade usando o software RRV (Radiosity Renderer and Visualizer). O método de radiosidade pode ser combinado com traçado de raios para gerar imagens com melhor fidelidade de simulação de reflexão difusa e especular. Figura 2.17: Imagem gerada com radiosidade (fonte). Em 1985, o GKS (Graphical Kernel System), desenvolvido como um melhoramento da API Core, torna-se a API padrão ISO para gráficos independentes do dispositivo (ISO 1985). Através do GKS, o código de descrição de comandos para manipulação de gráficos 2D permite a portabilidade entre diferentes linguagens de programação, sistemas operacionais e hardware gráfico compatível. Entretanto, gráficos 3D ainda não são contemplados nesta API. Em 1986, Steve Jobs adquire a divisão de computação gráfica da Lucasfilm e funda a Pixar junto com Ed Catmull, Alvy Smith e outros. Nessa época, Catmull, Loren Carpenter e Robert Cook desenvolvem o sistema de renderização RenderMan, muito utilizado na produção de efeitos visuais em filmes e animações. Após 14 anos, Catmull, Carpenter e Cook receberiam da Academia de Artes e Ciências Cinematográficas a estatueta do Oscar na categoria Academy Scientific and Technical Awards pelas contribuições à indústria do cinema representadas pelo desenvolvimento do RenderMan. O sucesso do RenderMan deve-se em parte à sua elegante API  a RenderMan Interface (RISpec)  inspirada na linguagem PostScript. A API permite a descrição completa de cenas 3D com todos os componentes necessários à renderização. Isso garante resultados consistentes, independentes do software de modelagem utilizado. O conceito de shaders, amplamente utilizado em hardware gráfico atual, surge do RenderMan shading language, desenvolvido na década de 1990 e incorporado no RISpec em 2005 como uma linguagem  dessa vez inspirada na linguagem C  de especificação de propriedades de superfícies, fontes de luz e efeitos atmosféricos de cena. Em 1988 é organizado o 1º Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens (SIBGRAPI), em Petrópolis, RJ. O evento, organizado anualmente pela CEGRAPI/SBC, internacionalizou-se e atualmente é chamado de Conference on Graphics, Patterns and Images. Neste ano, o SIBGRAPI, que estava planejado para ser realizado em Gramado (RS), será inteiramente virtual por causa da pandemia de COVID-19. 1990 1990 é a década das APIs gráficas 3D e da popularização do hardware gráfico nos PCs. Empresas como a Sun Microsystems (adquirida pela Oracle em 2010), IBM, HP (Hewlett-Packard), e as agora extintas NeXT, SGI (Silicon Graphics, Inc.) e DEC, desenvolvem estações gráficas de alto desempenho equipadas com hardware capaz de acelerar operações de renderização baseadas em rasterização com suporte a Z-buffer, mapeamento de texturas, iluminação e sombreamento de superfícies (figura 2.18). Figura 2.18: Workstation SGI IRIS Indigo (fonte). Neste período surgem as primeiras APIs para gráficos 3D como tentativa de padronizar a interface de programação entre as diferentes arquiteturas de hardware. Uma dessas APIs, desenvolvida ao longo da década de 1980 e que se estabelece como padrão da indústria na década de 1990, é o PHIGS (Programmers Hierarchical Interactive Graphics System) (Shuey 1987). PHIGS utiliza o conceito de grafo de cena: uma estrutura de dados hierárquica que representa as relações entre os modelos geométricos e outras entidades de uma cena. A API trabalha com malhas poligonais e síntese de imagens baseada na rasterização (em oposição ao traçado de raios), prevê o suporte a Gouraud e Phong shading, mas não oferece suporte a mapeamento de texturas. Em oposição ao PHIGS, a SGI utiliza em suas estações gráficas IRIS a API proprietária IRIS GL (Integrated Raster Imaging System Graphics Library) com características semelhantes ao PHIGS, porém com suporte a mapeamento de texturas (McLendon 1992). Diferentemente do PHIGS, o IRIS GL não adota o conceito de grafo de cena. As primitivas gráficas são enviadas imediatamente ao hardware gráfico em um pipeline de transformação geométrica e visualização. Esse modo de enviar os dados, conhecido como immediate mode, acaba por revelar-se mais apropriado para implemetação em hardware do que o retained mode do PHIGS com seu grafo de cena. Em 1991, Mark Segal e Kurt Akeley, da SGI, iniciam o desenvolvimento de uma versão aberta do IRIS GL como tentativa de criar um novo padrão de indústria. Para isso, removem o código proprietário e modificam a API de modo a torná-la independente do sistema de janelas e de dispositivos de entrada. Deste desenvolvimento surge, em 1992, o OpenGL (Open Graphics Library) (Woo et al. 1999), que rapidamente ocupa o lugar do PHIGS como API padrão para gráficos 3D. Desde então, revisões periódicas do OpenGL são feitas de modo a suportar os aprimoramentos mais recentes do hardware gráfico. O aspecto minimalista e de facilidade de uso do IRIS GL continuam presentes no OpenGL. Essas características fizeram  e ainda fazem  do OpenGL uma das APIs gráficas 3D mais populares em aplicações multiplataforma. InfiniteReality No início da década de 1990, as estações gráficas de alto desempenho suportavam apenas um número reduzido de características do OpenGL, sendo o restante simulado em software. O sistema RealityEngine (Akeley 1993), lançado em 1992 pela SGI, foi o primeiro hardware gráfico capaz de oferecer suporte para todas as etapas de transformação e iluminação da versão 1.0 do OpenGL, incluindo o mapeamento de texturas 2D com mipmapping (uma técnica de pré-filtragem de texturas) e antialiasing (suavização de serrilhado). A arquitetura foi sucedida em 1996 pelo InfiniteReality (Montrym et al. 1997), desenvolvido especificamente para o OpenGL. Dependendo da configuração final, o custo de uma estação gráfica baseada no InfiniteReality poderia ser superior a 1 milhão de dólares. Uma demonstração da SGI sobre as capacidades de renderização em tempo real do InfiniteReality em 1996 pode ser vista no vídeo de YouTube Silicon Graphics - Onyx Infinite Reallity 50FPS. A partir de 1995, surgem nos PCs as primeiras placas de vídeo com aceleração de processamento gráfico 3D, também chamadas de aceleradoras gráficas 3D. As primeiras aceleradoras gráficas eram capazes de realizar apenas a varredura de linhas não texturizadas e, em alguns casos, tinham desempenho similar ao código de máquina otimizado na CPU. Por outro lado, logo essas limitações foram vencidas e surgiram placas eficientes e com suporte a mapeamento de textura, impulsionadas pelo emergente mercado de jogos de computador. Enquanto as primeiras estações gráficas da SGI implementavam um pipeline completo de transformação de vértices, ainda que sem suporte à texturização, as aceleradoras gráficas para PCs, produzidas por empresas como Diamond Multimedia, S3 Graphics (extinta em 2003), Trident Microsystems (extinta em 2012), Matrox Graphics e NVIDIA, ofereciam suporte ao mapeamento de texturas, porém sem transformação de geometria ou processamento de iluminação. A 3Dfx Interactive (adquirida em 2000 pela NVIDIA), com a sua série de aceleradoras Voodoo Graphics lançadas a partir de 1996, ampliou enormemente o uso do hardware gráfico em jogos de computador. As placas Voodoo eram capazes de exibir triângulos texturizados com mipmapping e filtragem bilinear (figura 2.19). Entretanto, o hardware ainda dependia da CPU para preparar os triângulos para a rasterização. Os triângulos só poderiam ser processados pelo hardware gráfico se fossem previamente convertidos em trapézios degenerados, alinhados em coordenadas da tela. Figura 2.19: Jogo Carmageddon II: Carpocalypse Now (Stainless Games) em uma placa gráfica 3Dfx Voodoo, de 1998 (fonte). Outra limitação das aceleradoras gráficas nesse período era a falta de suporte adequado a uma API padrão de indústria. A arquitetura de tais placas era incompatível com aquela especificada no OpenGL e fazia com que os desenvolvedores precisassem recorrer a APIs proprietárias, como a API Glide da 3Dfx (3Dfx 1997). As placas da 3Dfx foram populares até o final da década quando então o OpenGL e a API Direct3D, da Microsoft, começaram a ser suportados de maneira eficiente pelas placas de concorrentes como a ATI Technologies (adquirida em 2006 pela AMD), Matrox e NVIDIA. Na segunda metade da década, o desenvolvimento das placas gráficas para PCs acompanhou a evolução da API Direct3D. Em 1995, a Microsoft lança o Windows 95 Games SDK, um conjunto de APIs de baixo nível para o desenvolvimento de jogos e aplicações multimídia de alto desempenho no Windows. Em 1996, o Windows 95 Games SDK muda de nome para DirectX e sua segunda e terceira versões são disponibilizadas em junho e setembro desse mesmo ano. Entre as APIs contidas no DirectX, o Direct3D é concebido como uma API para hardware gráfico compatível com o pipeline de processamento do OpenGL. Embora no início o Direct3D fosse criticado por sua arquitetura demasiadamente confusa em comparação com o OpenGL (como relatado por John Carmack, da id Software, em sua carta sobre o OpenGL), eventualmente torna-se a API mais utilizada em jogos uma vez que novas versões começam a ser distribuídas em intervalos menores que aqueles do OpenGL. A revisão do OpenGL dependia do ARB (Architecture Review Board): um consórcio formado por representantes de diversas empresas de hardware e software que se reuniam periodicamente para propor e aprovar mudanças na API. O Direct3D, por ser proprietário, respondia melhor ao rápido desenvolvimento das placas gráficas naquele momento e passou a ditar a especificação das futuras aceleradoras gráficas voltadas ao mercado de jogos. Em 1997 é anunciado o DirectX 5 (o DirectX 4 nunca chegou a ser lançado), acompanhando as primeiras placas capazes de renderizar triângulos, tais como a ATI Rage Pro e NVIDIA Riva 128 (figura 2.20). A Riva 128 não alcançava a mesma qualidade de imagem produzida pelas placas da 3Dfx, mas ultrapassava as placas Voodoo em várias medições de desempenho. Ainda assim, a aceleração de processamento de geometria era inexistente e a CPU era responsável por calcular as transformações geométricas e interpolações de atributos de vértices ao longo das arestas para cada triângulo transformado. Figura 2.20: Placa gráfica Diamond com o chip NVIDIA Riva 128, de 1997 (fonte). Em 1998 é lançado o DirectX 6 e surgem as primeiras aceleradoras gráficas capazes de interpolar atributos ao longo de arestas. Nessa geração de hardware gráfico, a CPU ainda era responsável pela transformação e iluminação de cada vértice, mas agora bastava enviar à placa gráfica os atributos de cada vértice em vez de atributos interpolados para cada aresta de cada triângulo. Um ano depois, o DirectX 7 é lançado com suporte para aceleração em hardware de transformação e iluminação (figura 2.21). As primeiras placas compatíveis com DirectX 7 surgiriam no ano seguinte. Figura 2.21: Demonstração do benchmark 3DMark2000 (UL) usando DirectX 7 com transformação de geometria e cálculo de iluminação em hardware. 2000 A década de 2000 presencia o que pode ser considerado uma revolução no uso do hardware gráfico: surgem os primeiros processadores gráficos programáveis (programmable GPUs) capazes de alterar o comportamento do pipeline de renderização sem depender da CPU. Isso torna possível a implementação de diversos novos modelos de reflexão para além do tradicional modelo de BlinnPhong disponível no pipeline de função fixa (pipeline não programável). Além disso, a capacidade de programar processadores gráficos possibilita a implementação de um incontável número de novos efeitos visuais. As GPUs programáveis tornam-se muito populares em PCs, impulsionadas pelas exigentes demandas do mercado de jogos. Ao mesmo tempo, tornam-se muito flexíveis e poderosas não só para jogos, mas também para processamento de propósito geral. O hardware gráfico programável surge no início de 2001 com o lançamento da GPU NVIDIA GeForce 3 (figura 2.22), inicialmente para o computador Apple Macintosh (Lindholm, Kilgard, and Moreton 2001). Figura 2.22: GPU NVIDIA GeForce (fonte). No início de 2001, durante o evento MacWorld Expo Tokyo, é exibido o curta metragem Luxo Jr. produzido pela Pixar em 1986. Entretanto, desta vez o filme é renderizado em tempo real em um computador equipado com uma GeForce 3. Steve Jobs, então CEO da Apple, observou: Há 15 anos, o que levava 75 horas para produzir cada segundo de vídeo, está agora sendo renderizado em tempo real na GeForce 3.  Steve Jobs (Morris 2001) Mais tarde, as potencialidades de uma GPU similar seriam exibidas durante uma demonstração de tecnologia na conferência SIGGRAPH 2001: uma versão interativa do filme Final Fantasy: The Spirits Within, de Hironobu Sakaguchi, renderizada em tempo real em uma GPU NVIDIA Quadro DCC (Sakaguchi and Aida 2001). Neste evento, a NVIDIA destacou que o desempenho em operações em ponto flutuante utilizadas para desenhar apenas um quadro do filme era superior ao poder computacional total de um supercomputador Cray (tradicional fabricante de supercomputadores, adquirida em 2019 pela Hewlett Packard Enterprise) naquele momento. Ao longo da década, as GPUs de baixo custo (na faixa de 100 a 250 dólares), produzidas por empresas como NVIDIA e ATI, desbancam as estações gráficas de alto desempenho ainda baseadas em tecnologias da década anterior. As placas gráficas para computadores pessoais ultrapassam rapidamente as capacidades computacionais de sistemas como o RealityEngine da SGI, mas ao mesmo tempo com uma redução de custo superior a 90% em comparação com esses sistemas. De acordo com a Lei de Moore, e observando a diminuição do custo das CPUs nesse período, tais placas deveriam custar muito mais, em torno de 15 mil dólares. Esse avanço expressivo das GPUs é implacável com as fabricantes de estações gráficas. Em 2009, a SGI decreta falência. As APIs Direct3D (em 2006) e OpenGL (em 2009) anunciam a descontinuidade do suporte ao pipeline de função fixa. Com isso, as aplicações migram definitivamente ao uso dos shaders: programas que modificam o comportamento das etapas programáveis do pipeline, como o processamento de geometria e fragmentos (amostras de primitivas rasterizadas). Com o aumento do conjunto de instruções suportadas nas GPUs, percebe-se que é possível usar o hardware gráfico para processamento de propósito geral em tarefas como simulação de dinâmica de fluidos, operações em bancos de dados, modelagem de dinâmica molecular, criptoanálise, entre muitas outras tarefas capazes de se beneficiar de processamento paralelo. O termo GPGPU (General-Purpose Computation on GPUs) é utilizado para se referir a esse uso. A figura 2.23 mostra um exemplo atual de aplicação de GPGPU para a modelagem de DNA. Figura 2.23: Ligante de sulco menor do DNA, modelado através de GPGPU com o software Abalone. (fonte). Em 2007, a NVIDIA lança a plataforma CUDA (Compute Unified Device Architecture), composta por um conjunto de ferramentas/bibliotecas e API de GPGPU para placas da NVIDIA. A plataforma é muito popular atualmente, impulsionada pelo crescimento das aplicações em ciência de dados e aprendizado de máquina. Influenciada pelo CUDA, surgem em 2009 outras plataformas como o DirectCompute, da Microsoft (como parte do Direct3D 11), e a especificação aberta OpenCL do Khronos Group, mesmo consórcio de indústrias que mantém o OpenGL. As primeiras oficinas e conferências sobre GPGPU, como a ACM GPGPU e a GPU Technology Conference (GTC), da NVIDIA, surgem neste período. 2010 A partir da década de 2010, a aceleração de gráficos 3D se expande e se populariza nos dispositivos móveis. O uso de multitexturização (uso de vários estágios de texturização) e de técnicas como normal mapping, cube mapping (para simulação de superfícies reflexivas) e shadow mapping (para simulação de sombras) torna-se comum em aplicações gráficas interativas. Em 2011, o Khronos Group anuncia o padrão WebGL, ampliando a possibilidade de uso de aceleração de gráficos 3D nos navegadores. Na segunda metade da década, a renderização baseada em física, do inglês physically based rendering (PBR), começa a ser empregada em jogos de computador e em consoles. O jogo Alien: Isolation (Creative Assembly), de 2014, é um dos primeiros a explorar esta tecnologia. Renderização baseada em física A renderização baseada em física, ou PBR, procura simular de forma fisicamente correta a interação da luz com os diferentes materiais de uma cena (Pharr, Jakob, and Humphreys 2016). Em PBR, os materiais são descritos por informações de detalhes de microsuperfície, frequentemente obtidas por fotogrametria e armazenadas em mapas de textura. O vídeo a seguir (em inglês) apresenta uma breve introdução ao conceito de PBR e sua implementação usando o Unity Standard Shader do motor de jogo Unity: Em 2016 são lançadas novas gerações de headsets de realidade virtual como o Oculus Rift e HTC Vive, que elevam as exigências de hardware gráfico para jogos que utilizam essa tecnologia. Também em 2016, o Khronos Group lança a API Vulkan como uma API de baixo nível com maior capacidade de explorar os recursos gráficos e de computação das novas gerações de GPUs. Em 2018 é incorporado ao Direct3D 12 o DirectX Raytracing (DXR), que introduz um novo pipeline gráfico destinado ao traçado de raios em tempo real. Ainda em 2018, as GPUs NVIDIA RTX série 20 são as primeiras a suportar essa tecnologia. 2020 Em 2020, as GPUs NVIDIA RTX série 20 são sucedidas pela série 30, ampliando ainda mais a possibilidade de uso de traçado de raios em tempo real. A AMD lança as GPUs da série Radeon RX 6000, também com suporte a traçado de raios. Além disso, uma API de traçado de raios é incorporada ao Vulkan como o conjunto de extensões Vulkan Ray Tracing. Ainda estamos no início da década, mas o aumento da capacidade de processamento e largura de banda de memória do hardware gráfico deve continuar a empurrar os limites do que é possível renderizar em tempo real. Efeitos atmosféricos, texturas de altíssima resolução e simulações de iluminação global4 em tempo real devem se popularizar nos próximos anos. O vídeo de demonstração a seguir (em inglês) exibe algumas das capacidades de processamento de iluminação global em tempo real do motor de jogo Unreal Engine 5 no console PlayStation 5. O lançamento do Unreal Engine 5 está previsto para o final de 2021: Referências "],["firstapp.html", "2.3 Primeiro programa", " 2.3 Primeiro programa Nesta seção seguiremos um passo a passo de construção de um primeiro programa com a biblioteca ABCg. Será o nosso Hello, World! similar ao exemplo da ABCg mostrado na seção 1.5, mas sem o triângulo colorido renderizado com OpenGL. Configuração inicial Faça uma cópia (ou fork) de https://github.com/hbatagelo/abcg.git. Assim você poderá modificar livremente a biblioteca e armazená-la em seu repositório pessoal. Como a ABCg já tem um projeto de exemplo chamado helloworld, vamos chamar o nosso de firstapp. Em abcg/examples, crie o subdiretório abcg/examples/firstapp. A escolha de deixar o projeto como um subdiretório de abcg/examples é conveniente pois podemos replicar a configuração de abcg/examples/helloworld. Assim, bastará construir a ABCg e o nosso projeto será automaticamente construído como um exemplo adicional da biblioteca. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(firstapp). O conteúdo ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) Dessa forma o CMake incluirá o subdiretório firstapp na busca de um script CMakeLists.txt contendo a configuração do projeto. Crie o arquivo abcg/examples/firstapp/CMakeLists.txt. Edite-o com o seguinte conteúdo: project(firstapp) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) O comando project na primeira linha define o nome do projeto. Em seguida, add_executable define que o executável terá o mesmo nome definido em project e será gerado a partir dos fontes main.cpp e openglwindow.cpp (não é necessário colocar os arquivos .h ou .hpp). Por fim, a função enable_abcg() configura o projeto para usar a ABCg. Essa função é definida em abcg/cmake/ABCg.cmake, que é um script CMake chamado a partir do CMakeLists.txt do diretório raiz. Em abcg/examples/firstapp, crie os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp. Vamos editá-los a seguir. main.cpp Em main.cpp definiremos a função main: #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(std::move(window)); return 0; } Nas duas primeiras linhas são incluídos os arquivos de cabeçalho: abcg.hpp faz parte da ABCg e dá acesso às principais classes e funções da biblioteca; openglwindow.hpp é o arquivo que acabamos de criar e que terá a definição de uma classe OpenGLWindow responsável pelo comportamento da janela da aplicação; Na linha 6 é definido um objeto app da classe abcg::Application, responsável pelo gerenciamento da aplicação; Na linha 9 é criado um ponteiro inteligente (smart pointer) window para uma instância de OpenGLWindow; Na linha 10 é definido o título da janela. setWindowSettings é uma função membro de abcg::OpenGLWindow (classe base de OpenGLWindow) e recebe uma estrutura abcg::WindowSettings contendo as configurações da janela; Na linha 13, a função membro abcg::Application::run é chamada para inicializar os subsistemas da SDL, inicializar a janela recém-criada e entrar no laço principal da aplicação. Observação Todas as classes e funções da ABCg fazem parte do namespace abcg. O código acima usa diferentes conceitos de C++ moderno: A palavra-chave auto para dedução automática do tipo de variável a partir de sua inicialização; A criação de um ponteiro inteligente com std::make_unique; O uso de inicialização uniforme com chaves; O uso de inicializadores designados para inicializar o membro title da estrutura abcg::WindowSettings diretamente através de seu nome; O uso de std::move para indicar que o ponteiro inteligente window está sendo transferido (e não copiado) para abcg::Application. Internamente a ABCg usa tratamento de exceções. As exceções são lançadas como objetos da classe abcg::Exception, derivada de std::exception. Vamos alterar um pouco o código anterior para capturar as exceções que possam ocorrer e imprimir no console a mensagem de erro correspondente. O código final de main.cpp ficará assim: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } O que mudou aqui é que o código anterior foi colocado dentro do escopo try de um bloco try...catch. No escopo catch, a função fmt::print imprime no erro padrão (stderr) a mensagem de erro associada com a exceção capturada. fmt::print faz parte da biblioteca {fmt}, incluída pelo cabeçalho fmt/core.h. Ela permite a formatação de strings usando uma sintaxe parecida com as f-strings da linguagem Python5. openglwindow.hpp No arquivo openglwindow.hpp vamos definir a classe OpenGLWindow que será responsável pelo gerenciamento do conteúdo da janela da aplicação: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow {}; #endif Observe que nossa classe OpenGLWindow é derivada de abcg::OpenGLWindow, que faz parte da ABCg. abcg::OpenGLWindow gerencia uma janela capaz de renderizar gráficos com a API OpenGL. A classe possui um conjunto de funções virtuais que podem ser substituídas pela classe derivada de modo a alterar o comportamento da janela. O comportamento padrão é desenhar a janela com fundo preto, com um contador de FPS (frames per second, ou quadros por segundo) sobreposto no canto superior esquerdo da janela, e um botão no canto inferior esquerdo para alternar entre tela cheia e modo janela (com atalho pela tecla F11). O contador e o botão são gerenciados pela biblioteca Dear ImGui (no restante do texto vamos chamá-la apenas de ImGui). Por enquanto nossa classe não faz nada de especial. Ela só deriva de abcg::OpenGLWindow e não define nenhuma função ou variável membro. Mesmo assim, já podemos construir a aplicação. Experimente fazer isso. Na linha de comando, use o script build.sh (Linux/macOS) ou build.bat (Windows). Se você estiver no Visual Studio Code, abra a pasta abcg pelo editor, use a opção de configuração do CMake e então construa o projeto (F7). O executável será gerado em abcg/build/bin/firstapp. Da forma como está, a aplicação mostrará uma janela com fundo preto e os dois controles de GUI (widgets) mencionados anteriomente. Isso acontece porque OpenGLWindow não está substituindo nenhuma das funções virtuais de abcg::OpenGLWindow. Todo o comportamento está sendo definido pela classe base: Vamos alterar o conteúdo e o comportamento dessa nossa janela OpenGLWindow. Imitaremos o comportamento do projeto helloworld que cria uma pequena janela da ImGui. Modifique openglwindow.hpp para o código a seguir: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; private: std::array&lt;float, 4&gt; m_clearColor{0.906f, 0.910f, 0.918f, 1.0f}; }; #endif initializeGL, paintGL e paintUI substituem funções virtuais de abcg::OpenGLWindow. A palavra-chave override é opcional mas é recomendável, pois deixa explícito que as funções são substituições das funções virtuais da classe base: initializeGL é onde colocaremos os comandos de inicialização do estado da janela e do OpenGL. Internamente a ABCg chama essa função apenas uma vez no início do programa, após ter inicializado os subsistemas da SDL e o OpenGL. paintGL é onde colocaremos todas as funções de desenho do OpenGL. Internamente a ABCg chama essa função continuamente no laço principal da aplicação, uma vez para cada quadro (frame) de exibição. Por exemplo, na imagem acima, paintGL estava sendo chamada a uma média de 3988.7 vezes por segundo; paintUI é onde colocaremos todas as funções de desenho de widgets da ImGui (botões, menus, caixas de seleção, etc). Internamente, paintUI é chamado sempre que paintGL é chamado; m_clearColor é um arranjo de quatro valores float entre 0 e 1. Esses valores definem a cor de fundo da janela (neste caso, um cinza claro). Observação Poderíamos ter definido m_clearColor da seguinte forma, mais familiar aos programadores em C: float m_clearColor[4] = {0.906f, 0.910f, 0.918f, 1.0f}; Entretanto, em C++ o std::array é a forma recomendada e mais segura de trabalhar com arranjos. openglwindow.cpp Em openglwindow.cpp definiremos as funções virtuais substituídas: #include &lt;fmt/core.h&gt; #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; void OpenGLWindow::initializeGL() { auto windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } void OpenGLWindow::paintGL() { // Set the clear color abcg::glClearColor(m_clearColor[0], m_clearColor[1], m_clearColor[2], m_clearColor[3]); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::paintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::paintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } No início do arquivo, observe que é incluído o cabeçalho imgui.h para o uso das funções da ImGui. Vejamos com mais atenção o trecho com a definição de OpenGLWindow::initializeGL: void OpenGLWindow::initializeGL() { auto windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } Na linha 8, windowSettings é uma estrutura abcg::WindowSettings retornada por abcg::OpenGLWindow::getWindowSettings() com as configurações da janela. Na linha 9, fmt::print imprime no console o tamanho da janela6. Observe agora o trecho com a definição de OpenGLWindow::paintGL: void OpenGLWindow::paintGL() { // Set the clear color abcg::glClearColor(m_clearColor[0], m_clearColor[1], m_clearColor[2], m_clearColor[3]); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } Aqui são chamadas duas funções do OpenGL: glClearColor e glClear. glClearColor é utilizada para determinar a cor que será usada para limpar a janela7. A função recebe quatro parâmetros do tipo float (red, green, blue, alpha), que correspondem a componentes de cor RGB e um valor adicional de opacidade (alpha). Esse formato de cor é chamado de RGBA. Os valores são fixados (clamped) no intervalo \\([0,1]\\) em ponto flutuante. glClear, usando como argumento a constante GL_COLOR_BUFFER_BIT, limpa a janela com a cor especificada na última chamada de glClearColor. Em resumo, nosso paintGL limpa a tela com a cor RGBA especificada em m_clearColor. Importante As funções do OpenGL são prefixadas com as letras gl; As constantes do OpenGL são prefixadas com GL_. A versão mais recente do OpenGL é a 4.6. A documentação de cada versão está disponível em https://www.khronos.org/registry/OpenGL/. Neste curso, usaremos as funções do OpenGL que são comuns ao OpenGL ES 3.0 de modo a manter compatibilidade com o WebGL 2.0. Assim conseguiremos fazer aplicações que rodam tanto no desktop quanto no navegador. Observação Na ABCg, podemos usar as funções gl dentro do namespace abcg de modo a rastrear erros do OpenGL com o sistema de tratamento de exceções da ABCg. Por exemplo, ao escrevermos abcg::glClear no lugar de glClear, estamos na verdade chamando uma função wrapper que verifica automaticamente se a chamada da função OpenGL é válida. Se algum erro ocorrer, uma exceção é lançada e capturada pelo catch que implementamos na função main. A mensagem de erro (a string em exception.what()) inclui a descrição do erro, o nome do arquivo, o nome da função e o número da linha do código onde o erro foi detectado. Isso pode ser muito útil para a depuração de erros do OpenGL. Por isso, sempre que possível, prefixe as funções do OpenGL com abcg::. A verificação automática de erros do OpenGL é habilitada somente quando a aplicação é compilada no modo Debug. Não há sobrecarga nas chamadas das funções do OpenGL com o namespace abcg quando a aplicação é compilada em modo Release. Agora vamos à definição de OpenGLWindow::paintUI, responsável pelo desenho da interface usando a ImGui: void OpenGLWindow::paintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::paintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Na linha 23 é chamada a função membro paintUI da classe base. A paintUI da classe base mostra o medidor de FPS e o botão para alternar a tela cheia. Na linha 28 é criada uma janela da ImGui com o título Hello, First App! A partir desta linha, até a linha 47, todas as chamadas a funções da ImGui criam widgets dentro dessa janela. Apenas para isso ficar mais explícito, todo o código está dentro do escopo delimitado pelas chaves nas linhas 26 e 48. Na linha 32 é criado um texto estático mostrando o tamanho atual da janela. Na linha 37 é criado um slider horizontal que pode variar de 0 a 1 em ponto flutuante. O valor do slider é armazenado em f. A variável f é declarada como static para que seu estado seja retido entre as chamadas de paintUI (outra opção é declarar a variável como membro da classe). Na linha 40 é criado um widget de edição de cor para alterar os valores de m_clearColor. Na linha 43 é criado mais um texto estático com informações de FPS extraídas de ImGui::GetIO().Framerate. Esse código é praticamente o mesmo do Hello, World! Construa a aplicação para ver o resultado: A seguir temos alguns exemplos de uso de outros widgets da ImGui. Experimente incluir esses trechos de código no paintUI: Botões: // 100x50 button if (ImGui::Button(&quot;Press me!&quot;, ImVec2(100, 50))) { fmt::print(&quot;Button pressed.\\n&quot;); } // Nx50 button, where N is the remaining width available ImGui::Button(&quot;Press me!&quot;, ImVec2(-1, 50)); // See also IsItemHovered, IsItemActive, etc if (ImGui::IsItemClicked()) { fmt::print(&quot;Button pressed.\\n&quot;); } Checkbox: static bool enabled{true}; ImGui::Checkbox(&quot;Some option&quot;, &amp;enabled); fmt::print(&quot;The checkbox is {}\\n&quot;, enabled ? &quot;enabled&quot; : &quot;disabled&quot;); Combo box: static std::size_t currentIndex{}; std::vector&lt;std::string&gt; comboItems{&quot;AAA&quot;, &quot;BBB&quot;, &quot;CCC&quot;}; if (ImGui::BeginCombo(&quot;Combo box&quot;, comboItems.at(currentIndex).c_str())) { for (auto index{0u}; index &lt; comboItems.size(); ++index) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; // Set the initial focus when opening the combo (scrolling + keyboard // navigation focus) if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } fmt::print(&quot;Selected combo box item: {}\\n&quot;, comboItems.at(currentIndex)); Menu (em uma janela de tamanho fixo e com o flag adicional ImGuiWindowFlags_MenuBar para permitir o uso da barra de menu): ImGui::SetNextWindowSize(ImVec2(300, 100)); auto flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize}; ImGui::Begin(&quot;Window with menu&quot;, nullptr, flags); { bool save{}; static bool showCompliment{}; // Hold state // Menu Bar if (ImGui::BeginMenuBar()) { // File menu if (ImGui::BeginMenu(&quot;File&quot;)) { ImGui::MenuItem(&quot;Save&quot;, nullptr, &amp;save); ImGui::EndMenu(); } // View menu if (ImGui::BeginMenu(&quot;View&quot;)) { ImGui::MenuItem(&quot;Show Compliment&quot;, nullptr, &amp;showCompliment); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (save) { // Save file... } if (showCompliment) { ImGui::Text(&quot;You&#39;re a beautiful person.&quot;); } } ImGui::End(); Mais sliders: static std::array pos2d{0.0f, 0.0f}; ImGui::SliderFloat2(&quot;2D position&quot;, pos2d.data(), 0.0, 50.0); static std::array pos3d{0.0f, 0.0f, 0.0f}; ImGui::SliderFloat3(&quot;3D position&quot;, pos3d.data(), -1.0, 1.0); Infelizmente, a ImGui não tem um manual com exemplos de uso de todos os widgets suportados. A melhor referência atualmente é o código da função ImGui::ShowDemoWindow em abcg/external/imgui/imgui_demo.cpp. Essa função cria uma janela de demonstração contendo uma grande variedade de exemplos de uso de widgets e recursos da ImGui. No exemplo Hello, World! tal janela é exibida quando o checkbox Show demo window está ativado. Observação A ImGui é uma biblioteca de GUI que trabalha em modo imediato, isto é, não retém o estado das janelas e widgets (o Im de ImGui vem de immediate mode). Isso significa que, sempre que paintUI é chamado, a GUI é redefinida por completo. O gerenciamento da persistência de estado deve ser feito pelo usuário, por exemplo, através de variáveis estáticas ou variáveis membros da classe. Exercício Usando o projeto firstapp como base, faça um Jogo da Velha com interface composta de widgets da ImGui: Simule o tabuleiro do jogo com um arranjo de 3x3 botões. Inicialmente deixe os botões sem texto. Cada vez que um botão for pressionado, substitua-o por um X ou O de acordo com o turno do jogador; Use um widget de texto estático para mostrar informações como o resultado do jogo e o turno atual; Inclua um menu com uma opção para reiniciar o jogo. Explore diferentes funções da biblioteca, tais como: ImGui::Columns para fazer arranjos de widgets; ImGui::Spacing para adicionar espaçamentos verticais; ImGui::SameLine para criar widgets lado-a-lado e ajustar o espaçamento horizontal; ImGui::Separator() para desenhar linhas de separação. Um exemplo é dado a seguir: Compilando para WebAssembly Para compilar para WebAssembly basta usar o script build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Apenas certifique-se de habilitar antes as variáveis de ambiente do SDK do Emscripten como fizemos na seção 1.5. Após a construção do projeto, os arquivos resultantes (firstapp.js e firstapp.wasm) estarão no subdiretório public. Para usá-los, precisamos de um arquivo HTML. Faça uma cópia de um dos arquivos HTML já existentes (helloworld.html, full_window.html ou full_window_console.html). No final do arquivo copiado, mude a string src=\"helloworld.js\" para src=\"firstapp.js\", assim: &lt;script async type=&quot;text/javascript&quot; src=&quot;firstapp.js&quot;&gt;&lt;/script&gt; Para testar, monte o servidor local com runweb.sh ou runweb.bat e abra o arquivo HTML em http://localhost:8080/. Dica Disponibilize o conteúdo web de seus projetos no GitHub Pages para formar um portfólio de atividades feitas no curso: Na sua conta do GitHub, crie um repositório com visibilidade pública. Pode ser seu próprio fork da ABCg. No diretório raiz, crie um subdiretório firstapp com os arquivos firstapp.*, mas renomeie firstapp.html para index.html; Nas configurações do repositório no GitHub, habilite o GitHub Pages informando o branch que será utilizado (por exemplo, main). O conteúdo estará disponível em https://username.github.io/reponame/firstapp/ onde username e reponame são respectivamente seu nome de usuário e o nome do repositório. Ao longo do curso, suba seus projetos nesse repositório. No diretório raiz, mantenha um index.html com a descrição do portfólio e o link para cada página de projeto. Um subconjunto da {fmt} foi incorporado à biblioteca de formatação de texto no C++20, mas ainda não há suporte equivalente ao fmt::print (impressão formatada com saída padrão). O tamanho padrão para uma aplicação desktop é 800x600. Na versão para web, a janela pode ser redimensionada de acordo com a regra CSS do elemento canvas do HTML5. Mais precisamente, glClearColor define a cor que será utilizada para limpar os buffers de cor do framebuffer. Veremos mais sobre o conceito de framebuffer nos próximos capítulos. "],["graphicssystem.html", "3 Sistemas gráficos", " 3 Sistemas gráficos Um sistema gráfico é um sistema computacional com capacidade de processar dados para gerar imagens em um dispositivo de exibição. Em sistemas interativos, a interação com os modelos de dados gráficos se dá através de um ou mais dispositivos de entrada. Assim, de maneira geral, um sistema gráfico é composto pelos seguintes componentes: Dispositivos de entrada: teclado e dispositivos apontadores como mouse, touch pad, touch screen, graphics tablet, trackball, joystick, gamepad, entre outros. Processadores: CPU (central processing unit), GPU (graphics processing unit) e seus subsistemas (controladores, memórias e barramentos) necessários ao processamento dos modelos de dados e conversão em representações visuais; Dispositivos de saída: monitores e telas de LCD (liquid-crystal display), OLED (organic light-emitting diodes), CRT (cathode-ray tube) ou plasma, entre outros dispositivos de exibição. A definição de um sistema gráfico com esses componentes é bastante geral e pode incluir tanto as estações gráficas de alto desempenho equipadas com várias GPUs, quanto os computadores pessoais sem GPU dedicada. Pode incluir também os consoles de videogames, smartphones, smartwatches, smart TVs, GPSs, entre outros dispositivos com poder computacional suficiente para produzir saída em um dispositivo de exibição (figura 3.1). Figura 3.1: Exemplos de sistemas gráficos. Em sistemas gráficos atuais, o papel principal do processador gráfico é realizar a conversão de primitivas geométricas em uma imagem digital. Por exemplo, para visualizar na tela um segmento de reta definido por dois vértices, o processador deve gerar um conjunto de pixels que aproxime o aspecto do segmento visto pelo observador. Esse processo de determinar a cor dos pixels da imagem pode ser tão simples quanto avaliar a equação de uma reta \\(y=mx+b\\) para um conjunto discreto de valores de \\(x\\) em coordenadas na tela, ou tão complexo quanto gerar uma imagem fotorrealista de uma cena virtual composta de milhares de objetos. Nos sistemas gráficos mais simples, sem aceleração de gráficos em hardware, a CPU é responsável por todo o processamento gráfico e não há distinção entre a memória da CPU e a memória de processamento gráfico. Em sistemas com GPU, a GPU pode estar localizada na placa mãe do computador, integrada com o chip da CPU (como o Intel HD Graphics comum nos computadores com processador Intel), ou situada em uma placa de vídeo com memória dedicada. Em sistemas de alto desempenho, várias GPUs dedicadas podem ser combinadas para dividir a carga de processamento usando tecnologias como a SLI da NVIDIA e CrossFire da AMD. Também é comum sistemas que combinam GPUs dedicadas e GPUs integradas, em diferentes configurações. Em todos os sistemas gráficos, a saída é uma imagem digital armazenada em uma área de memória chamada de framebuffer. Essa área de memória é utilizada pelo controlador gráfico para atualizar o dispositivo de exibição (figura 3.2). Figura 3.2: Sistema gráfico com framebuffer. O restante do capítulo está organizado como a seguir: A seção 3.1 descreve as duas principais formas de representação de gráficos utilizadas em CG: a representação vetorial e a representação matricial; A seção 3.2 apresenta conceitos e tecnologias relacionadas aos dispositivos de entrada e saída dos sistemas gráficos; A seção 3.3 descreve conceitos fundamentais do uso de framebuffers. A seção 3.4 apresenta uma atividade prática de um primeiro programa que renderiza primitivas com o OpenGL. "],["vectorxraster.html", "3.1 Vetorial x matricial", " 3.1 Vetorial x matricial Em computação gráfica, é comum trabalharmos com dois tipos de representações de gráficos: a representação vetorial, utilizada na descrição de formas 2D e 3D compostas por primitivas geométricas, e a representação matricial, utilizada em imagens digitais e definição de texturas. O processo de converter representações vetoriais em representações matriciais desempenha um papel central no pipeline de processamento gráfico, uma vez que a representação matricial é a representação final de uma imagem nos dispositivos de exibição. Essa conversão matricial, também chamada de rasterização (raster conversion ou scan conversion), é implementada em hardware nas GPUs atuais. A figura 3.3 ilustra o resultado da conversão de uma representação vetorial em representação matricial. As formas geométricas à esquerda estão representadas originalmente no formato SVG (Scalable Vector Graphics), que é o formato padrão de gráficos vetoriais nos navegadores Web. A imagem à direita é um arranjo bidimensional de valores de cor, resultado da renderização das formas SVG em uma imagem digital (neste caso, uma imagem de baixa resolução). Figura 3.3: Rasterização de um círculo e triângulo. Observação A figura 3.3 é apenas ilustrativa. Rigorosamente falando, a imagem da esquerda também está no formato matricial. O navegador converte automaticamente o código SVG em comandos da API gráfica que fazem com que a GPU renderize a imagem que vemos na tela. A rasterização ocorre durante este processamento. A imagem à direita não precisa passar pelo processo de renderização pois já é uma imagem digital em seu formato nativo. Representação vetorial Na representação vetorial, os gráficos são descritos em termos de primitivas geométricas. Por exemplo, o formato SVG é um formato de descrição de gráficos vetoriais 2D através de sequências de comandos de desenho. Uma forma 2D pode ser descrita através da definição de um caminho (path) composto por uma sequência de passos de movimentação de uma caneta virtual sobre um plano. Os principais passos utilizados são comandos do tipo MoveTo, LineTo e ClosePath: MoveTo (denotado por M ou m em SVG8) move a caneta virtual para uma nova posição na área de desenho, como se ela fosse levantada da superfície e posicionada em outro local; LineTo (L ou l) traça um segmento de reta da posição atual da caneta até uma nova posição, que passa a ser a nova posição da caneta; Em uma sequência de comandos LineTo, o comando ClosePath (Z ou z) traça um segmento de reta que fecha o caminho da posição atual da caneta ao ponto inicial. Observe o código SVG a seguir que resulta no desenho do triângulo visto mais abaixo: &lt;svg width=&quot;250&quot; height=&quot;210&quot;&gt; &lt;path d=&quot;M125 0 L0 200 L250 200 Z&quot; stroke=&quot;black&quot; fill=&quot;none&quot; /&gt; &lt;/svg&gt; No rótulo &lt;svg&gt;, os atributos width=\"250\" e height=\"210\" definem que a área de desenho tem largura 250 e altura 210. Por padrão, a origem fica no canto superior esquerdo. O eixo horizontal (\\(x\\)) é positivo para a direita, e o eixo vertical (\\(y\\)) é positivo para baixo. O atributo d do rótulo &lt;path&gt; contém os comandos de desenho do caminho. M125 0 move a caneta virtual para a posição (125,0). Em seguida, L0 200 traça um segmento da posição atual até a posição (0, 200), que passa a ser a nova posição da caneta. L250 200 traça um novo segmento até (250, 200). O comando Z fecha o caminho até a posição inicial em (125, 0), completando o triângulo. O atributo stroke=\"black\" define a cor do traço como preto, e fill=\"lightgray\" define a cor de preenchimento como cinza claro: O formato SVG também suporta a descrição de curvas, arcos, retângulos, círculos, elipses, entre outras primitivas geométricas. Comandos similares são suportados em outros formatos de gráficos vetoriais, como o EPS (Encapsulated PostScript), PDF (Portable Document Format), AI (Adobe Illustrator Artwork) e DXF (AutoCAD Drawing Exchange Format). Representação vetorial no OpenGL No OpenGL, a representação vetorial é utilizada para definir a geometria que será processada durante a renderização. Todas as primitivas geométricas são definidas a partir de vértices que representam posições no espaço, além de atributos definidos pelo programador (por exemplo, a cor do vértice). Esses vértices são armazenados em arranjos ordenados que são processados em um fluxo de vértices no pipeline de renderização especificado pelo OpenGL. Os vértices podem ser utilizados para formar diferentes primitivas. Por exemplo, o uso do identificador GL_TRIANGLES na função de renderização glDrawArrays faz com que seja formado um triângulo a cada grupo de três vértices do arranjo de vértices. Assim, se o arranjo tiver seis vértices (numa sequência de 0 a 5), serão formados dois triângulos: um triângulo com os vértices 0, 1, 2, e outro com os vértices 3, 4, 5. Para o mesmo arranjo de vértices, GL_POINTS faz com que o pipeline de renderização interprete cada vértice como um ponto separado, e GL_LINE_STRIP faz com que o pipeline de renderização forme uma sequência de segmentos (uma polilinha) conectando os vértices. A figura 3.4 ilustra a formação dessas primitivas para um arranjo de seis vértices no plano. A numeração indica a ordem dos vértices no arranjo. Figura 3.4: Formando diferentes primitivas do OpenGL com um mesmo arranjo de vértices. A figura 3.5 mostra como a geometria das primitivas pode mudar (com exceção de GL_POINTS) caso os vértices estejam em uma ordem diferente no arranjo. Figura 3.5: A ordem dos vértices no arranjo altera a geometria das primitivas. Veremos com mais detalhes o uso de primitivas no próximo capítulo quando abordaremos as diferentes etapas de processamento do pipeline de renderização do OpenGL. Observação Até a década de 2010, a maneira mais comum de renderizar primitivas no OpenGL era através de comandos do modo imediato de renderização, como a seguir (em C/C++): glColor3f(0.83f, 0.83f, 0.83f); // Light gray color glBegin(GL_TRIANGLES); glVertex2i(-1, -1); glVertex2i( 1, -1); glVertex2i( 0, 1); glEnd(); Nesse código, a função glColor3f informa que a cor dos vértices que estão prestes a ser definidos será um cinza claro, como no triângulo desenhado com SVG. O sufixo 3f de glColor3f indica que os argumentos são três valores do tipo float. Entre as funções glBegin e glEnd é definida a sequência de vértices. Cada chamada a glVertex2i define as coordenadas 2D de um vértice (o sufixo 2i indica que as coordenadas são compostas por dois números inteiros). Como há três vértices e a primitiva é identificada com GL_TRIANGLES, será desenhado um triângulo cinza similar ao triângulo desenhado com SVG, porém sem o contorno preto9. O sistema de coordenadas nativo do OpenGL não é o mesmo da área de desenho do formato SVG. No OpenGL, a origem é o centro da janela de visualização, sendo que o eixo \\(x\\) é positivo à direita e o eixo \\(y\\) é positivo para cima. Além disso, para que a primitiva possa ser vista, as coordenadas dos vértices precisam estar entre -1 e 1 (em ponto flutuante). Para desenhar o triângulo colorido do exemplo Hello, World! como visto na seção 1.5, poderíamos utilizar o seguinte código: glBegin(GL_TRIANGLES); glColor3f(1.0f, 0.0f, 0.0f); // Red glVertex2f(0.0f, 0.5f); glColor3f(1.0f, 0.0f, 1.0f); // Magenta glVertex2f(0.5f, -0.5f); glColor3f(0.0f, 0.0f, 1.0f); // Green glVertex2f(-0.5f, -0.5f); glEnd(); Observe que, antes da definição de cada vértice, é definida a sua cor. Quando o triângulo é processado na GPU, as cores em cada vértice são interpoladas bilinearmente (em \\(x\\) e em \\(y\\)) ao longo da superfície do triângulo, formando um gradiente de cores. Em nossos programas usando a ABCg, bastaria colocar esse código na função membro paintGL de nossa classe derivada de abcg::OpenGLWindow. Internamente o OpenGL utilizaria o pipeline de renderização de função fixa (pipeline não programável) para desenhar o triângulo. No entanto, se compararmos com o código atual do projeto no subdiretório abcg\\examples\\helloworld, perceberemos que não há nenhum comando glBegin, glVertex* ou glColor*. Isso acontece porque o código acima é obsoleto. As funções do modo imediato foram retiradas do OpenGL na versão 3.1 (de 2009). Ainda é possível habilitar um perfil de compatibilidade (compatibility profile) para usar funções obsoletas do OpenGL, mas esse perfil já não é suportado em vários drivers e plataformas. Por isso, não o utilizaremos neste curso. Atualmente, para desenhar primitivas com o OpenGL, o arranjo ordenado de vértices precisa ser enviada previamente à GPU juntamente com programas chamados shaders que definem como os vértices serão processados e como os pixels serão preenchidos após a rasterização. Desenhar um simples triângulo preenchido no OpenGL não é tão simples como antigamente, mas essa dificuldade é compensada pela maior eficiência e flexibilidade obtida com a possibilidade de programar o comportamento da GPU. Representação matricial Na representação matricial, também chamada de representação raster, as imagens são compostas por arranjos bidimensionais de elementos discretos e finitos chamados de pixels (picture elements). Um pixel contém uma informação de amostra de cor e corresponde ao menor elemento que compõe a imagem. A resolução da imagem é o número de linhas e colunas do arranjo bidimensional. Esse é o formato utilizado nos arquivos GIF (Graphics Interchange Format), TIFF (Tag Image File Format), PNG (Portable Graphics Format), JPEG e BMP. A figura 3.6 mostra uma imagem digital e um detalhe ampliado. Figura 3.6: Imagem digital de 300x394 pixels e detalhe ampliado de 38x38 pixels. Observação Embora os pixels ampliados da figura 3.6 sejam mostrados como pequenos quadrados coloridos, um pixel não tem necessariamente o formato de um quadrado. Um pixel é apenas uma amostra de cor e pode ser exibido em diferentes formatos de acordo com o dispositivo de exibição. Uma imagem digital pode ser armazenada como um mapa de bits (bitmap). A quantidade de cores que podem ser representadas em um pixel  a profundidade da cor (color depth)  depende do número de bits designados a cada pixel. Em uma imagem binária, cada pixel é representado por apenas 1 bit. Desse modo, a imagem só pode ter duas cores, como preto (para os bits com estado 0) e branco (para os bits com estado 1). A figura 3.7 mostra uma imagem binária em formato BMP, que é um formato simples e muito utilizado para armazenar mapas de bits. Figura 3.7: Imagem binária. A imagem da figura 3.7 foi gerada a partir de outra de maior profundidade de cor (figura 3.11) usando o algoritmo Floyd-Steinberg de dithering (Floyd and Steinberg 1976). Dithering é o processo de introduzir um ruído ou padrão de pontilhado que atenua a percepção de bandas de cor (color banding) resultantes da quantização da cor. A figura 3.8 mostra esse efeito. A imagem da esquerda é a imagem original, com 24 bits de profundidade de cor. A imagem do centro teve a profundidade de cor reduzida para 4 bits (16 cores). É possível perceber as bandas de cor no gradiente do céu. Na imagem da direita, a profundidade de cor também foi reduzida para 4 bits, mas o uso de dithering reduz a percepção das variações bruscas de tom. Figura 3.8: Redução de bandas de cor com dithering. Esquerda: imagem original de 24 bits/pixel. Centro: redução para 4 bits/pixel. Direita: redução para 4 bits/pixel usando dithering. Em imagens com profundidade de cor de 8 bits, cada pixel pode assumir um valor de 0 a 255. Esse valor pode ser interpretado como um nível de luminosidade para, por exemplo, descrever imagens monocromáticas de 256 tons de cinza (figura 3.9). Figura 3.9: Imagem monocromática de 8 bits por pixel. Uma outra possibilidade é fazer com que cada valor corresponda a um índice de uma paleta de cores que determina qual será a cor do pixel. Em imagens de 8 bits, a paleta de cores é uma tabela de 256 cores, sendo que cada cor é definida por 3 bytes, um para cada componente de cor RGB (vermelho, verde, azul). Esse formato de cor indexada foi o formato predominante em computadores pessoais na década de 1990, quando os controladores gráficos só conseguiam exibir um máximo de 256 cores simultâneas no modo VGA (Video Graphics Array). O formato GIF, criado em 1987, utiliza cores indexadas. A figura 3.10 exibe uma imagem GIF e sua paleta correspondente de 256 cores. Figura 3.10: Imagem de 8 bits com cores indexadas (esquerda) e paleta utilizada (direita). Atualmente, as imagens digitais coloridas usam o formato true color no qual cada pixel tem 24 bits (3 bytes, um para cada componente de cor RGB), sem o uso de paleta de cor (figura 3.11). Isso possibilita a exibição de \\(2^{24}\\) (16.777.216) cores simultâneas. Figura 3.11: Imagem de 24 bits por pixel. Em arquivos de imagens, também é comum o uso de 32 bits por pixel (4 bytes), sendo 3 bytes para as componentes de cor e 1 byte para definir o nível de opacidade do pixel. Geralmente, os valores de intensidade de cor de um pixel são representados por números inteiros. Entretanto, em sistemas gráficos que usam imagens HDR (high dynamic range), cada componente de cor pode ter até 32 bits em formato de ponto flutuante, permitindo alcançar uma faixa muito superior de intensidades. As GPUs atuais fornecem suporte a um variado conjunto de formatos de bits, incluindo suporte a mapas de bits compactados e tipos de dados em formato de ponto flutuante de 16 e 32 bits. Referências "],["es.html", "3.2 Dispositivos de E/S", " 3.2 Dispositivos de E/S A seguir apresentamos uma visão geral de conceitos e tecnologias relacionadas a dispositivos de entrada e saída utilizados em sistemas gráficos. Dispositivos de entrada Um sistema gráfico possui um ou mais dispositivos de entrada que permitem ao usuário interagir com os modelos de dados gráficos. O dispositivo de teclado inclui o tradicional teclado físico dos computadores de mesa e laptops (figura 3.12). O teclado produz um código (scancode) composto por um byte ou sequência de bytes que identifica cada tecla pressionada e liberada. Figura 3.12: Teclado de teclas mecânicas (fonte). O teclado virtual de um smartphone ou tablet é também um dispositivo de teclado. Embora não possua teclas físicas, o resultado dos toques na tela e o resultado da conversão de uma anotação manuscrita em texto é um conjunto de scancodes que corresponde aos mesmos caracteres de teclas de um teclado físico (figura 3.13). Figura 3.13: Teclado virtual de um smartphone (fonte). Os códigos de um dispositivo de teclado podem ser interpretados como direções de movimentação de um cursor de desenho para permitir a interação com dados gráficos. Uma forma de utilizar o teclado para a interação com cenas virtuais 3D é mapear as setas (esquerda, direita, para cima, para baixo) ou as teclas WASD a transformações de translação de uma câmera em primeira pessoa, e usar outro conjunto de teclas para alterar a orientação da câmera. Além do teclado, é comum que um sistema gráfico tenha também pelo menos um dispositivo apontador, como o mouse (figura 3.14), capaz de fornecer dados de movimentação ou posicionamento sobre uma superfície, geralmente mapeados para uma posição na tela. Figura 3.14: Mouse com dois botões e botão de rolagem. (fonte). O mouse produz dados de deslocamento em duas direções ortogonais que correspondem ao movimento horizontal (\\(x\\)) e vertical (\\(y\\)) do dispositivo10. Como os dados produzidos são apenas deslocamentos, e não posições, o mouse é considerado um dispositivo de posicionamento relativo. Entretanto, os deslocamentos em \\(x\\) e \\(y\\) podem ser interpretados como velocidades e acumulados ao longo do tempo para determinar a posição de um cursor na tela. Outros dispositivos populares de posicionamento relativo são os touch pads, trackballs, joysticks e gamepads (figura 3.15). Tais dispositivos também possuem botões que podem ser configurados da mesma forma que as teclas de um teclado. Figura 3.15: Dispositivos apontadores de posicionamento relativo. Da esquerda para a direita: trackball (fonte), joystick (fonte), gamepad (fonte). Dispositivos apontadores como a tela sensível ao toque (touch screen) e a mesa digitalizadora (graphics tablet) são capazes de fornecer dados de posicionamento absoluto (figura 3.16). Os toques produzidos com o dedo ou com uma caneta de toque (stylus pen) produzem dados que correspondem a um par de coordenadas sobre a superfície de desenho, além de um valor que corresponde à pressão aplicada. Esses dispositivos também podem ser configurados para gerar dados de posicionamento relativo e detecção de gestos de arrasto (swipe e drag and drop) através do rastreamento dos pontos de pressão. Telas sensíveis ao toque frequentemente também são capazes de detectar múltiplos toques simultâneos, permitindo a detecção de gestos mais complexos como pinça (pinch) e rotação (rotate). Figura 3.16: Mesa digitalizadora com caneta (fonte). Dispositivos de saída Um sistema gráfico possui pelo menos um dispositivo de saída para exibição de gráficos. Esses dispositivos podem ser do tipo vetorial ou matricial. Dispositivos vetoriais O primeiro dispositivo de exibição utilizado em computador foi o CRT vetorial, que é o mesmo tipo de tecnologia utilizada nos osciloscópios analógicos (figura 3.17). Figura 3.17: Osciloscópio analógico com CRT vetorial (fonte). No CRT vetorial, um canhão de elétrons emite um feixe de elétrons que incide sobre uma tela revestida por um material fotoluminescente (fósforo). Um conjunto de placas defletoras permite alterar a posição horizontal (\\(x\\)) e vertical (\\(y\\)) de incidência do feixe, de modo que gráficos de linhas e curvas podem ser traçados na tela. Em um sistema gráfico, a posição de incidência do feixe pode ser descrita por comandos do tipo MoveTo e LineTo (figuras 3.18 e 3.19). Como o brilho do fósforo tem persistência baixa, na ordem de milissegundos, é preciso redesenhar o traço continuamente. Figura 3.18: Desenhando um triângulo em um CRT vetorial. A sequência de passos de 1 a 4 precisa ser repetida continuamente para manter a imagem na tela. Figura 3.19: Jogo estilo Asteroides (Space Rocks) sendo exibido em um CRT vetorial de um antigo osciloscópio (fonte). Dispositivos de exibição vetorial não conseguem desenhar de forma adequada áreas preenchidas. Além disso, a velocidade de geração do desenho é proporcional à quantidade de primitivas e ao comprimento dos caminhos, impondo um limite à complexidade do desenho. Por essas desvantagens, CRTs vetoriais tornaram-se obsoletos e foram substituídos inteiramente pelos dispositivos matriciais. Dispositivos matriciais O primeiro dispositivo de exibição matricial utilizado em computadores também foi o CRT (Noll 1971). No CRT matricial, o feixe de elétrons é direcionado por deflexão eletromagnética e varre continuamente a tela de cima para baixo, da esquerda para direita. A cada linha percorrida, o canhão de elétrons é desligado momentaneamente e religado no início da próxima linha (retraço horizontal). Ao completar a varredura no canto inferior direito, o canhão de elétrons é desligado e direcionado para o ponto inicial, no canto superior esquerdo (retraço vertical). O feixe de elétrons é então religado e uma nova varredura é feita, iniciando uma nova imagem ou quadro de exibição. Esse processo é feito continuamente, a uma taxa que, nos televisores antigos, era sincronizada com a frequência da rede elétrica: 50 Hz ou 60 Hz11. Durante a varredura, a intensidade do feixe é controlada por um sinal analógico de vídeo. Esse sinal pode ser produzido por um conversor digital-analógico a partir de uma imagem digital, reproduzindo na tela os pontos que formam a imagem (figura 3.20). Figura 3.20: Varredura de um quadro em um CRT matricial. CRTs coloridos utilizam três canhões de elétrons, um para cada componente de cor RGB. A tela é coberta por um padrão de fósforos nessas cores, em grupos de três. Uma máscara ou grelha metálica próxima da tela (shadow mask, slot mask ou apperture grille, dependendo da tecnologia utilizada) assegura que cada tipo de fósforo recebe elétrons apenas do canhão correspondente. A figura 3.21 mostra o detalhe ampliado da tela de um CRT de TV e um CRT de computador, mostrando o padrão das tríades RGB formadas pelo slot mask (no CRT de TV) e shadow mask (no CRT de computador). Uma vez que os padrões são muito pequenos e cobrem a tela por completo, o usuário percebe a combinação das cores primárias que resultam na cor da imagem. A figura 3.22 mostra o detalhe ampliado de uma letra e exibida em um CRT de TV que usa a tecnologia de apperture grille (tecnologia Trinitron, da Sony), e o detalhe ampliado de um cursor em um CRT de computador. Figura 3.21: Padrões de fósforos RGB em CRTs. Esquerda: slot mask em um CRT de TV. Direita: shadow mask em um CRT de PC. (fonte) Figura 3.22: Detalhes ampliados de telas de CRT. Esquerda: letra e em um CRT de TV Sony Trinitron (fonte). Direita: cursor na tela de um CRT de computador (fonte). Os CRTs não são mais utilizados desde meados de 2000 e foram substituídos pelos monitores LCD (liquid-crystal display). Até a metade de 2010 eram também comuns os monitores de tela de plasma. Nessa tecnologia, tensões aplicadas em eletrodos de endereçamento de linhas e colunas energizam um gás (geralmente néon e xenônio) contido em minúsculas células envoltas em painéis de vidro. O fundo das células é coberto por fósforo nas cores RGB, de modo que cada grupo de 3 cores forma um pixel. Como em uma lâmpada fluorescente, o gás ionizado se torna um plasma emissor de luz ultravioleta que faz com que os fósforos emitam a luz visível que forma as cores da imagem (figura 3.23). Figura 3.23: Estrutura de uma tela de plasma (fonte). A tecnologia LCD é a mais utilizada nos dispositivos de exibição atuais. Uma tela de LCD é composta por um sanduíche de vários painéis (figura 3.24). Na parte de trás dos painéis, lâmpadas fluorescentes ou LEDs emitem uma luz branca que é espalhada uniformemente por um painel difusor. Essa luz incide sobre um filtro que só permite passar luz polarizada em uma direção. Na frente dos painéis há uma outra camada que só permite passar a luz polarizada na direção ortogonal ao primeiro filtro, de modo que o resultado é o bloqueio total da luz. Para controlar eletronicamente a passagem da luz, entre os dois filtros é colocado um substrato de vidro contendo uma camada de cristais líquidos e eletrodos e/ou transistores que alteram a orientação dos cristais  e com isso a polarização da luz  através de campos elétricos. Uma camada de filtros de cor divide a tela em pixels compostos de três subpixels, um para cada componente RGB, coincidentes com a camada de eletrodos. Desse modo, a passagem de luz em cada subpixel é controlada individualmente para formar a imagem final. O conteúdo da tela LCD é atualizado continuamente, geralmente a uma taxa de 60 Hz, mas em monitores mais recentes essa taxa pode chegar a 240 Hz. Figura 3.24: Estrutura de uma tela de LCD (fonte). A tecnologia OLED (organic light-emitting diodes) tem se popularizado em telas de smart TVs e smartphones e tem a promessa de substituir a tecnologia de LCD. Telas OLED não utilizam luz de fundo, pois cada subpixel emite sua própria luz: cada subpixel é um LED no qual a camada eletroluminescente é um filme de compostos orgânicos. A figura 3.25 mostra o detalhe ampliado de uma tela com tecnologia AMOLED (active-matrix organic light-emitting diode) que utiliza transistores de filme fino para manter o fluxo de corrente em cada subpixel. Figura 3.25: Detalhe da tela AMOLED de um smartphone Google Nexus One. Foto por Matthew Rollings (fonte). Telas OLED conseguem obter níveis mais profundos de preto e melhor contraste em ambientes escuros quando comparadas com as telas de LCD. A tecnologia tem ainda outras vantagens, como a possibilidade de ser utilizada em telas mais finas, flexíveis e transparentes (figura 3.26). Figura 3.26: Demonstração de uma tela OLED flexível (fonte). Referências "],["framebuffer.html", "3.3 Framebuffer", " 3.3 Framebuffer O framebuffer é uma área contígua da memória utilizada para armazenar a imagem que será mostrada no dispositivo de exibição. O controlador gráfico lê continuamente o conteúdo do framebuffer e atualiza o dispositivo de exibição, tipicamente a uma taxa de 60 Hz nos monitores de LCD. Nos primeiros PCs e em sistemas gráficos mais antigos, o framebuffer fazia parte da memória padrão que poderia ser acessada diretamente pela CPU. Nos PCs do início da década de 1990, o framebuffer podia ser acessado com um simples ponteiro para o endereço 0xA000 no chamado modo 13h do controlador VGA (um modo gráfico de cores indexadas de 8 bits com resolução de 320x200). Atualmente, o framebuffer é acessado através da GPU e está localizado na mesma placa de circuito da GPU (figura 3.27). Figura 3.27: Configuração do framebuffer em sistema gráfico com GPU dedicada. Observação Em hardware compatível com OpenGL (o que inclui todas as GPUs atuais), o framebuffer pode ser composto por vários buffers. Pelo menos um deles é um color buffer (buffer de cor) no qual cada pixel contém uma informação de cor, geralmente no formato RGB (24 bits) ou RGBA (32 bits). Um framebuffer pode ter vários buffers de cor associados. Por exemplo, em implementações que suportam visão estereoscópica, dois buffers de cor podem ser utilizados: um para a tela da visão esquerda e outro para a tela da visão direita. Na técnica de backbuffering (descrita no fim da seção), também dois buffers de cor são utilizados: o backbuffer, que é um buffer off-screen (invisível) onde a imagem é renderizada antes de ser exibida na tela, e o frontbuffer, que recebe o conteúdo do backbuffer ao fim da renderização, para exibição na tela. Em renderização estéreo, cada lado esquerdo e direito tem o seu backbuffer e frontbuffer. Além dos buffers de cor, o framebuffer também pode incluir: Um depth buffer (buffer de profundidade), no qual cada pixel contém uma informação de profundidade utilizada no teste de profundidade. O teste de profundidade faz parte da implementação da técnica de Z-buffering de determinação de superfícies visíveis. A informação de profundidade pode ser um inteiro ou ponto flutuante de 16, 24 ou 32 bits (geralmente 24 bits). Um stencil buffer (buffer de estêncil), utilizado no teste de estêncil para operações de mascaramento e composição de imagens. No buffer estêncil, cada pixel contém um inteiro sem sinal, de 1, 4, 8 ou 16 bits (geralmente 8 bits). Screen tearing A taxa de atualização do dispositivo de exibição (chamada de vertical refresh rate) é controlada pelo controlador gráfico. Entretanto, a taxa em que o framebuffer é atualizado pode ser bem maior. Essa taxa é o número de quadros por segundo (FPS) que o processador gráfico consegue renderizar. Se o framebuffer for atualizado muito rapidamente, o controlador pode começar a atualizar o dispositivo de exibição com o conteúdo de um quadro e terminar com o conteúdo de outro, mais recente. Essa quebra entre os quadros de exibição gera um defeito na imagem conhecido como screen tearing ou simplesmente tearing (figura 3.28). Figura 3.28: Screen tearing. (fonte). Vsync Para contornar o problema de screen tearing, a GPU pode sincronizar a atualização do framebuffer com a atualização do controlador gráfico, efetivamente limitando o número de FPS à frequência do monitor em Hz. Esse processo de sincronização é chamado de vertical synchronization, ou vsync. Em monitores mais recentes, compatíveis com as tecnologias G-SYNC da NVIDIA, e FreeSync da AMD, é possível fazer a sincronização na direção contrária: a frequência do monitor é ajustada pela GPU de acordo com a taxa de FPS. Backbuffering Defeitos de screen tearing também podem ocorrer quando a taxa de renderização é menor que a taxa de atualização do dispositivo de exibição. Nesse caso, o dispositivo de exibição pode mostrar o quadro de exibição antes da renderização ter sido finalizada. O resultado pode ser uma mistura de elementos do quadro atual com elementos do quadro anterior, ou a percepção de que o quadro está sendo desenhado. Uma forma de reduzir o efeito de screen tearing é o uso de dois buffers de cor: o backbuffer e o frontbuffer. O processador renderiza os gráficos apenas no backbuffer. Após o fim da renderização, o conteúdo é transferido para o frontbuffer, que é então utilizado pelo controlador gráfico para atualizar o dispositivo de exibição. Dessa forma, o controlador sempre utiliza um buffer que já contém um quadro completo. Atualmente, essa técnica de backbuffering é implementada em hardware. O backbuffer e frontbuffer podem ser páginas de memória do framebuffer que são trocadas continuamente em um processo de page flipping, sem precisar realizar transferência de dados (figura 3.29). Figura 3.29: Configuração do framebuffer com suporte a backbuffering. "],["sierpinski.html", "3.4 Triângulo de Sierpinski", " 3.4 Triângulo de Sierpinski O triângulo de Sierpinski é um fractal que pode ser gerado por um tipo de sistema dinâmico chamado de sistema de função iterativa (iterated function system, ou IFS). Esse processo pode ser implementado através de um algoritmo conhecido como jogo do caos. Para jogar o jogo do caos, começamos definindo três pontos \\(A\\), \\(B\\) e \\(C\\) não colineares. Por exemplo, \\(A=(0, 1)\\), \\(B=(-1, -1)\\) e \\(C=(1, -1)\\): Além dos pontos \\(A\\), \\(B\\) e \\(C\\), definimos mais um ponto \\(P\\) em uma posição aleatória do plano. Com \\(A\\), \\(B\\), \\(C\\) e \\(P\\) definidos, o jogo do caos consiste nos seguintes passos: Mova \\(P\\) para o ponto médio entre \\(P\\) e um dos pontos \\(A\\), \\(B\\), \\(C\\) escolhido de forma aleatória; Volte ao passo 1. Para gerar o triângulo de Sierpinski, basta desenhar \\(P\\) a cada iteração. O jogo não tem fim, mas quanto maior o número de iterações, mais pontos serão desenhados e mais detalhes terá o fractal (figura 3.30). Figura 3.30: Triângulo de Sierpinski desenhado com 1.000, 10.000 e 100.000 iterações em uma área de 210x210 pixels. Vamos implementar o jogo do caos com a ABCg, usando a estrutura da aplicação que fizemos no projeto firstapp (seção 2.3). O procedimento será simples: para cada chamada de paintGL, faremos uma iteração do jogo e desenharemos um ponto na posição \\(P\\) usando um comando de renderização do OpenGL. Os pontos desenhados serão acumulados no framebuffer e visualizaremos o fractal. Configuração inicial Repita a configuração inicial do projeto firstapp, mas mudando o nome do projeto para sierpinski. O arquivo abcg/examples/CMakeLists.txt ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) add_subdirectory(sierpinski) Para a construção não ficar muito lenta, podemos comentar as linhas de add_subdirectory dos projetos anteriores para que eles não sejam compilados. Por exemplo: #add_subdirectory(helloworld) #add_subdirectory(firstapp) add_subdirectory(sierpinski) O arquivo abcg/examples/sierpinski/CMakeLists.txt ficará assim: project(sierpinski) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Crie também os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp em abcg/examples/sierpinski. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp ficará como a seguir: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings( {.samples = 2, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Esse código é bem parecido com o main.cpp do projeto firstapp. As únicas diferenças estão nas linhas 13 a 18: window-&gt;setOpenGLSettings( {.samples = 2, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); setOpenGLSettings é uma função membro de abcg::OpenGLWindow que recebe uma estrutura abcg::OpenGLSettings com as configurações de inicialização do OpenGL. Essas configurações são usadas pela SDL no momento da criação de um contexto do OpenGL que representa o framebuffer vinculado à janela: O atributo samples = 2 faz com que o framebuffer suporte suavização de serrilhado (antialiasing) das primitivas do OpenGL; O atributo preserveWebGLDrawingBuffer = true é utilizado apenas no binário em WebAssembly. No WebGL, preserveDrawingBuffer é uma configuração de criação do contexto do OpenGL que faz com que o framebuffer vinculado ao canvas da página Web não seja apagado entre os quadros de exibição. Em setWindowSettings, utilizamos alguns atributos novos de definição de propriedades da janela. Definimos a largura (width) e altura (height) inicial da janela, e desligamos a exibição do botão de tela cheia (showFullscreenButton = false) para que o botão não obstrua o desenho do triângulo. Mesmo sem o botão, o modo janela pode ser alternado com o modo de tela cheia pela tecla F11. openglwindow.hpp Na definição da classe OpenGLWindow, vamos substituir novas funções virtuais de abcg::OpenGLWindow e vamos definir variáveis que serão utilizados para atualizar o jogo do caos e para desenhar o ponto na tela: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_vao{}; GLuint m_vboVertices{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; const std::array&lt;glm::vec2, 3&gt; m_points{glm::vec2( 0, 1), glm::vec2(-1, -1), glm::vec2( 1, -1)}; glm::vec2 m_P{}; void setupModel(); }; #endif Observe que, além de usarmos as funções initializeGL, paintGL e paintUI, estamos agora substituindo mais duas funções virtuais de abcg::OpenGLWindow: resizeGL é chamada pela ABCg sempre que o tamanho da janela é alterado. O novo tamanho é recebido pelos parâmetros width e height. Na nossa aplicação, vamos armazenar esses valores nas variáveis m_viewportWidth (linha 23) e m_viewportHeight (linha 24). Precisamos disso para fazer com que a janela de exibição (viewport) do OpenGL tenha o mesmo tamanho da janela da aplicação. O conceito de viewport será detalhado mais adiante. terminateGL é chamada pela ABCg quando a janela é destruída, no fim da aplicação. Essa é a função complementar de initializeGL, usada para liberar os recursos do OpenGL que foram alocados no initializeGL ou durante a aplicação. Da linha 19 a 31 temos a definição das variáveis da classe: GLuint m_vao{}; GLuint m_vboVertices{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; const std::array&lt;glm::vec2, 3&gt; m_points{glm::vec2( 0, 1), glm::vec2(-1, -1), glm::vec2( 1, -1)}; glm::vec2 m_P{}; m_vao, m_vboVertices e m_program são identificadores de recursos alocados pelo OpenGL (recursos geralmente armazenados na memória da GPU). Esses recursos correspondem ao arranjo ordenado de vértices utilizado para montar as primitivas geométricas no pipeline de renderização12 e os shaders que definem o comportamento da renderização. m_viewportWidth e m_viewportHeight servem para armazenar o tamanho da janela da aplicação informado pelo resizeGL. m_randomEngine é um objeto do gerador de números pseudoaleatórios do C++ (observe o uso do #include &lt;random&gt; na linha 6). Esse objeto é utilizado para sortear a posição inicial de \\(P\\) e para sortear qual ponto (\\(A\\), \\(B\\) ou \\(C\\)) será utilizado em cada iteração do jogo do caos. m_points é um arranjo que contém a posição dos pontos \\(A\\), \\(B\\) e \\(C\\). As coordenadas dos pontos são descritas por uma estrutura glm::vec2. O namespace glm contém definições da biblioteca OpenGL Mathematics (GLM) que fornece estruturas e funções de operações matemáticas compatíveis com a especificação da linguagem de shaders do OpenGL. Observe que, para usar glm::vec2, incluímos o arquivo de cabeçalho glm/vec2.hpp. m_P é a posição do ponto \\(P\\). Além da definição das variáveis, na linha 33 é definida a função membro OpenGLWindow::setupModel que cria os recursos identificados por m_vao e m_vboVertices. A função é chamada sempre que um novo ponto \\(P\\) precisa ser desenhado. openglwindow.cpp Vamos implementar primeiro a lógica do jogo do caos, sem desenhar nada na tela. Em seguida incluiremos o código que usa o OpenGL para desenhar os pontos. Vamos começar incluindo os seguintes arquivos de cabeçalho: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;chrono&gt; Em OpenGLWindow::initializeGL, iniciaremos o gerador de números pseudoaleatórios e sortearemos as coordenadas iniciais de \\(P\\) (que no código é m_P): void OpenGLWindow::initializeGL() { // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } O gerador m_randomEngine é iniciado usando como semente o tempo do sistema (para isso é preciso incluir o cabeçalho &lt;chrono&gt;). As coordenadas de m_P são iniciadas como valores sorteados de um intervalo de -1 a 1. O intervalo poderia ser qualquer outro, mas fazendo assim garantimos que o ponto inicial será visto na tela. Na configuração padrão do OpenGL, só conseguimos visualizar as primitivas gráficas que estão situadas entre as coordenadas \\((-1, -1)\\) e \\((1, 1)\\). A coordenada \\((-1, -1)\\) geralmente é mapeada ao canto inferior esquerdo da janela, e a coordenada \\((1, 1)\\) é mapeada ao canto superior direito (esse mapeamento será configurado posteriormente com a função glViewport). Vamos agora implementar o passo iterativo do jogo. Faremos isso no paintGL, de modo que cada quadro de exibição corresponderá a uma iteração13: void OpenGLWindow::paintGL() { // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Neste trecho de código, index é um índice do arranjo m_points. Assim, m_points.at(index) é um dos pontos \\(A\\), \\(B\\) ou \\(C\\) que definem os vértices do triângulo. Observe que utilizamos uma distribuição uniforme para sortear o índice. Isso é importante para que o fractal seja desenhado como esperado14. A nova posição de m_P é calculada como o ponto médio entre m_P e o ponto de m_points. O código comentado pode ser utilizado para imprimir no terminal as novas coordenadas de m_P. Basicamente isso conclui a lógica do jogo do caos. Todo o resto do código será para desenhar m_P como um ponto na tela. No OpenGL anterior à versão 3.1, isso seria tão simples quanto acrescentar o seguinte código em paintGL: glBegin(GL_POINTS); glVertex2f(m_P.x, m_P.y); glEnd(); Entretanto, como vimos na seção 3.1, esse código é obsoleto e não é mais suportado em muitos drivers e plataformas. Para desenhar um simples ponto na tela, precisaremos seguir os seguintes passos: Criar um buffer de vértices como recurso do OpenGL. Esse recurso é chamado VBO (Vertex Buffer Object) e corresponde ao arranjo ordenado de vértices utilizado pelo pipeline de renderização para montar as primitivas que serão renderizadas. No nosso caso, o buffer de vértices só precisa ter um vértice, que é a coordenada do ponto que queremos desenhar. A variável m_vboVertices é um inteiro que identifica esse recurso. Programar o comportamento do pipeline de renderização. Isso é feito compilando e ligando um par de shaders que fica armazenado na GPU como um único programa de shader, identificado pela variável m_program. No OpenGL, os shaders são escritos na linguagem GLSL (OpenGL Shading Language), que é parecida com a linguagem C, mas possui novos tipos de dados e operações. Especificar como o buffer de vértices será lido pelo programa de shader. No nosso código, o estado dessa configuração é armazenado como um objeto do OpenGL chamado VAO (Vertex Array Object), identificado pela variável m_vao. Somente após alocar e ativar esses recursos é que podemos iniciar o pipeline de renderização, chamando uma função de desenho no paintGL. Não se preocupe se tudo isso está parecendo muito complexo nesse momento. Nos próximos capítulos revisitaremos cada etapa várias vezes até nos familiarizarmos com todo o processo. Por enquanto, utilizaremos o código já pronto. Primeiro, defina a função setupModel como a seguir: void OpenGLWindow::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_vboVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program const GLint positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Esse código cria o VBO (m_vboVertices) e VAO (m_VAO) usando a posição atual de m_P. Agora, modifique initializeGL para o seguinte código final: void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } Nesta função, vertexShader e fragmentShader são strings que contêm o código-fonte dos shaders. vertexShader é o código do chamado vertex shader, que programa o processamento de vértices na GPU. fragmentShader é o código do fragment shader que programa o processamento de pixels na GPU (ou, mais precisamente, o processamento de fragmentos, que são conjuntos de atributos que representam uma amostra de geometria rasterizada). A compilação e ligação dos shaders é feita pela função createProgramFromString que faz parte de abcg::OpenGLWindow. Se acontecer algum erro de compilação, a mensagem de erro será exibida no console e uma exceção será lançada. Note que limpamos o buffer de cor com a cor preta, usando glClearColor e glClear (linhas 28 e 29). Observe o trecho de código entre as diretivas de pré-processamento: #if !defined(__EMSCRIPTEN__) glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else Esse código só será compilado quando não usarmos o Emscripten, isto é, quando o binário for compilado para desktop. No OpenGL para desktop, o comando da linha 33 é necessário para que o tamanho do ponto que será desenhado possa ser definido no vertex shader. Quando o código é compilado com o Emscripten, a definição do tamanho do ponto no vertex shader já é suportada por padrão, pois o OpenGL utilizado é o OpenGL ES (o WebGL usa um subconjunto de funções do OpenGL ES). Observe, no código do vertex shader, que o tamanho do ponto é definido com gl_PointSize = 2.0 (dois pixels). Os tamanhos válidos dependem do que é suportado pelo hardware. Para imprimir no console o tamanho mínimo e máximo, usamos glGetFloatv neste trecho de código: std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); A função glGetFloatv com o identificador GL_POINT_SIZE_RANGE (para OpenGL desktop) e GL_ALIASED_POINT_SIZE_RANGE (para OpenGL ES) preenche o arranjo sizes com os tamanhos mínimo e máximo suportados. Em seguida, fmt::print mostra os valores no console. Voltando agora à implementação de OpenGLWindow::paintGL, o código final ficará assim: void OpenGLWindow::paintGL() { // Create OpenGL buffers for the single point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); const int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Na linha 52, setupModel cria os recursos do OpenGL necessários para desenhar um ponto na posição atual de m_P. Na linha 55, glViewport configura o mapeamento entre o sistema de coordenadas no qual nossos pontos foram definidos (coordenadas normalizadas do dispositivo, ou NDC, de normalized device coordinates), e o sistema de coordenadas da janela (window coordinates), em pixels, com origem no canto inferior esquerdo da janela da aplicação. A figura 3.31 ilustra como fica configurado o mapeamento entre coordenadas em NDC para coordenadas da janela, supondo uma chamada a glViewport(x, y, w, h), onde x, y, w e h são inteiros dados em pixels da tela. Na figura, o chamado viewport do OpenGL é a janela formada pelo retângulo entre os pontos \\((x,y)\\) e \\((x+w,y+h)\\). No nosso código com glViewport(0, 0, m_viewportWidth, m_viewportHeight), o ponto \\((-1,-1)\\) em NDC é mapeado para o pixel \\((0, 0)\\) da janela (canto inferior esquerdo), e o ponto \\((1,1)\\) em NDC é mapeado para o pixel \\((0,0)\\) + (m_viewportWidth, m_viewportHeight). Isso faz com que o viewport ocupe toda a janela da aplicação. Figura 3.31: Mapeamento das coordenadas normalizadas no dispositivo (NDC) para coordenadas da janela usando glViewport(x, y, w, h). Com o viewport devidamente configurado, iniciamos o pipeline de renderização neste trecho: // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); Na linha 58, glUseProgram ativa os shaders compilados no programa m_program. Na linha 60, glBindVertexArray ativa o VAO (m_VAO), que contém as especificações de como o arranjo de vértices (VBO) será lido no vertex shader atualmente ativo. Ao ativar o VAO, também é ativado automaticamente o VBO identificado por m_VBO. Finalmente, na linha 63, glDrawArrays inicia o pipeline de renderização usando os shaders e o VBO ativo. O primeiro argumento (GL_POINTS) indica que os vértices do arranjo de vértices devem ser tratados como pontos. O segundo argumento (0) é o índice inicial dos vértices no VBO, e o terceiro argumento (1) informa quantos vértices devem ser processados. O processamento no pipeline de renderização é realizado de forma paralela e assíncrona com a CPU. Isto é, glDrawArrays retorna imediatamente, enquanto a GPU trabalha em paralelo renderizando a geometria no framebuffer15. Após o comando de renderização, as linhas 66 e 68 desativam o VAO e os shaders. Essa desativação é opcional pois, de qualquer forma, o mesmo VAO e os mesmos shaders serão utilizados na próxima chamada de paintGL. Ainda assim, é uma boa prática de programação desativá-los logo após o uso. Vamos agora definir a função membro OpenGLWindow::resizeGL, assim: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } Como vimos, resizeGL é chamada sempre que a janela da aplicação muda de tamanho. Observe que simplesmente armazenamos o tamanho da janela em m_viewportWidth e m_viewportHeight. Como essas variáveis são usadas em glViewport, garantimos que o viewport sempre ocupará toda a janela da aplicação. Observe que também chamamos glClear para apagar o buffer de cor. Dessa forma, o triângulo de Sierpinski no novo tamanho não é desenhado sobre o triângulo do tamanho anterior, o que estragaria o fractal. A função membro OpenGLWindow::terminateGL é definida da seguinte maneira: void OpenGLWindow::terminateGL() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Os comandos glDelete* liberam os recursos alocado em setupModel. Para finalizar, vamos definir paintUI usando o seguinte código: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 81)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } Na linha 83 chamamos o paintUI da classe base, responsável por mostrar o contador de FPS (lembre-se que desabilitamos a exibição do botão de tela cheia). O código nas linhas 85 a 94 cria um botão Clear window que chama glClear sempre que pressionado. Isso é tudo! O código completo de openglwindow.cpp é mostrado a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;chrono&gt; void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes[0], sizes[1]); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); // Randomly choose a pair of coordinates in the interval [-1; 1] std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } void OpenGLWindow::paintGL() { // Create OpenGL buffers for the single point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_vao); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly choose a triangle vertex index std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); const int index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to the console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 5 + 50 + 16 + 5)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); } void OpenGLWindow::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_vboVertices); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_vboVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program const GLint positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Construa a aplicação para ver o resultado: No nosso caso o arranjo de vértices contém apenas um vértice e equivale ao ponto \\(P\\) que queremos desenhar. A numeração das linhas é a mesma do código completo de openglwindow.cpp mostrado no final do capítulo. Experimente outras distribuições e observe a mudança no comportamento do fractal. A ABCg habilita a técnica de backbuffering vista na seção 3.3. Desse modo, a GPU renderiza primeiro a geometria no backbuffer. Quando a renderização é concluída, o conteúdo é enviado automaticamente para o frontbuffer, que atualiza o dispositivo de exibição. "],["pipeline.html", "4 Pipeline gráfico", " 4 Pipeline gráfico O pipeline gráfico ou pipeline de renderização é um modelo conceitual de descrição da sequência de passos que um sistema gráfico utiliza para transformar um modelo matemático de dados gráficos em uma imagem digital. O termo pipeline é utilizado porque o processamento é realizado em uma sequência linear de etapas alimentadas por um fluxo de dados, de modo que cada etapa pode processar novos dados tão logo tenha enviado a saída à etapa seguinte. O pipeline é iniciado pela aplicação na CPU. A aplicação é responsável por alimentar o pipeline com os dados gráficos que serão renderizados. Em geral, esses dados descrevem objetos de uma cena 3D. A geometria de cada objeto é formada por malhas de triângulos, e uma câmera virtual define o ponto de vista da renderização. O pipeline típico implementado nas atuais GPUs envolve etapas que compreendem o processamento geométrico, a rasterização e o processamento de fragmentos (figura 4.1): Figura 4.1: Etapas de um pipeline gráfico. Processamento geométrico: envolve operações realizadas sobre vértices, como transformações afins e transformações projetivas que serão abordadas em capítulos futuros. O processamento geométrico pode envolver também a criação de geometria e o refinamento de malhas. Ao final desse processamento é feito o recorte ou descarte das primitivas geométricas que estão fora da janela de visualização. Rasterização: compreende a conversão matricial das primitivas. O resultado é um conjunto de amostras de primitivas. Durante o processamento no pipeline, o termo fragmento é frequentemente utilizado para designar essas amostras no lugar de pixel. Cada fragmento é uma coleção de valores que inclui atributos interpolados a partir dos vértices e a posição \\((x,y,z)\\) da amostra em coordenadas da janela (o valor \\(z\\) é considerado a profundidade do fragmento). O pixel é o valor final da cor no buffer de cor, que pode ser uma combinação da cor de vários fragmentos. Processamento de fragmentos: envolve operações realizadas sobre cada fragmento para determinar sua cor e outros atributos. A cor pode ser determinada através da avaliação de modelos de iluminação que levam em conta os atributos de iluminação fornecidos pela aplicação, tais como fontes de luz, texturas e descrição de materiais de superfícies. Após essas operações são realizados testes de descarte de fragmentos e combinação de cores entre os fragmentos processados e os pixels já existentes no framebuffer. O resultado é armazenado em diferentes buffers do framebuffer: buffers de cor, buffer de profundidade e buffer de estêncil. "],["dados-gráficos.html", "4.1 Dados gráficos", " 4.1 Dados gráficos O processamento de um pipeline gráfico começa com a definição dos dados gráficos pela aplicação. Esses dados são frequentemente representações de objetos  abstrações de objetos do mundo real  dispostos em uma cena virtual tridimensional. Uma cena é tipicamente composta por: Objetos com geometria descrita através de modelos geométricos, geralmente no formato de malhas de triângulos. Em renderizadores offline, também é comum representar a geometria de objetos através de equações paramétricas ou implícitas. Propriedades dos materiais dos objetos, tais como cor, textura, transparência e refletividade. Fontes de luz descritas por informações como intensidade, direção de propagação da luz e fatores de atenuação. Uma câmera virtual descrita por informações que permitem definir um ponto de vista na cena, tais como posição da câmera, orientação e campo de visão. A câmera virtual é uma abstração de uma câmera ou observador do mundo real. A síntese de imagens pode ser considerada como o processo de gerar a fotografia digital tirada pela câmera virtual. Em uma câmera de verdade, a imagem é formada a partir da energia luminosa que atravessa as lentes e é captada pelo sensor RGB durante um certo tempo de exposição. Poderíamos tentar simular de forma precisa o comportamento de uma câmera de verdade, mas seria muito custoso simular o comportamento dos fótons passando pelo sistema de lentes e então integrá-los sobre o hemisfério de todas as possíveis direções de luz que incidem em cada subpixel RGB do sensor da câmera. Felizmente, podemos simplificar de forma significativa este processo. Para aplicações de síntese de imagens em tempo real, a cor de cada pixel pode ser uma aproximação simples da energia luminosa refletida na direção do pixel. Essa aproximação pode ser fisicamente incorreta, desde que suficiente para dar a percepção de sombreamento (shading) dos objetos. É comum considerar que a câmera virtual é uma câmera pinhole ideal (figura 4.2). A câmera pinhole é uma câmera que não possui lentes. A luz passa por um pequeno furo (chamado de centro de projeção) e incide sobre um filme ou sensor localizado no fundo da câmera (o plano de imagem). A abertura do campo de visão pode ser ajustada mudando a distância focal, que é a distância entre o centro de projeção e o plano de imagem. Na câmera pinhole ideal, a abertura do furo é infinitamente pequena e as imagens formadas são perfeitamente nítidas (isto é, em foco). Efeitos de difração são ignorados nesse modelo. Figura 4.2: Camera pinhole ideal. Para evitar ter de lidar com a imagem invertida formada no plano de imagem da câmera pinhole, podemos considerar que o plano de imagem está localizado na frente do centro de projeção, o que seria impossível de fazer numa câmera real. Podemos também ajustar arbitrariamente a distância focal sem preocupação com limitações físicas. A distância focal pode até mesmo ser infinita, se desejarmos uma projeção paralela. Por fim, podemos considerar que o plano de imagem é o framebuffer. Nessa configuração, é comum considerar que o centro de projeção corresponde ao olho do observador, como mostra a figura (figura 4.3). Figura 4.3: Câmera virtual com plano de imagem na frente do centro de projeção. Para determinar a cor de cada pixel do framebuffer para um determinado ponto de vista da câmera virtual, podemos considerar duas abordagens de renderização: ray casting e rasterização. Essas abordagens são apresentadas na seção a seguir. "],["ray-casting-x-rasterização.html", "4.2 Ray casting x rasterização", " 4.2 Ray casting x rasterização Ray casting e rasterização são duas abordagens distintas de se renderizar uma cena, e resultam em pipelines também distintos. Ray casting consiste em lançar raios que saem do centro de projeção, atravessam os pixels da tela e intersectam os objetos da cena. A rasterização faz o caminho inverso: os objetos da cena são projetados na tela na direção do centro de projeção, e são então convertidos em pixels. Neste curso usaremos apenas a rasterização, que é a forma de renderização utilizada na maioria das aplicações gráficas interativas. É também a única abordagem de renderização suportada atualmente no pipeline gráfico do OpenGL. Entretanto, é importante observar que novos pipelines baseados em traçado de raios (uma forma de ray casting) têm sido incorporados às APIs gráficas e tendem a conquistar cada vez mais espaço em síntese de imagens em tempo real. Ray casting Na sua forma mais simples, o algoritmo de ray casting (Roth 1982) consiste nos seguintes passos (figura 4.4): Figura 4.4: Ray casting. Para cada pixel do framebuffer: Calcule o raio \\(R\\) que sai do centro de projeção e passa pelo pixel. Seja \\(P\\) a interseção mais próxima (se houver) de \\(R\\) com um objeto da cena. Faça com que a cor do pixel seja a cor calculada em \\(P\\). Em ray casting, cada pixel é visitado apenas uma vez. Entretanto, para cada pixel visitado, potencialmente todos os objetos da cena podem ser consultados para calcular a interseção mais próxima. Assim, o principal custo da geração de imagem usando ray casting está relacionado ao cálculo das interseções. Estruturas de dados de subdivisão espacial como k-d tree (Bentley 1975) e octree (Meagher 1980) podem ser utilizadas para que seja possível descartar rapidamente a geometria não intersectada pelo raio e com isso diminuir o número de testes de interseção. Embora o ray casting seja conceitualmente simples, é pouco adequado para implementação em hardware, pois cada iteração do laço principal exige a manutenção de toda a cena na memória do renderizador, além da estrutura de subdivisão espacial. Essa limitação tem sido cada vez menos significativa nas GPUs mais recentes, mas ray casting ainda é pouco utilizado em síntese de imagens em tempo real. Observação Para produzir imagens fotorrealistas, a cor em \\(P\\) deve ser calculada através da integração da energia luminosa que incide sobre o ponto vindo de todas as direções da cena, e da determinação da quantidade dessa energia que é refletida na direção do pixel na tela. Isso pode ser feito de diferentes formas e em diferentes níveis de aproximação. Uma aproximação pouco acurada, mas muito eficiente, é avaliar a equação de um modelo de iluminação local como o modelo de reflexão de Phong (Phong 1973) ou Blinn-Phong (Blinn 1977) que considera que a cor em uma superfície é determinada unicamente pela luz que incide diretamente sobre a superfície, e não pela luz indireta refletida por outros objetos. Outra aproximação, mais acurada porém bem menos eficiente, é a técnica de traçado de raios recursivo (Whitted 1979) que consiste em lançar novos raios a partir de \\(P\\) (figura 4.5). Esses raios são: Um raio de sombra (shadow ray) em direção a cada fonte de luz, para saber se \\(P\\) encontra-se na sombra em relação à fonte de luz correspondente; Um raio de reflexão (reflection ray) na direção espelhada em relação ao vetor normal à superfície em \\(P\\); Um raio de refração (refraction ray) que atravessa a superfície do objeto, caso o objeto seja transparente. Figura 4.5: Traçado de raios recursivo. Os raios de reflexão e refração podem intersectar outros objetos, e novos raios podem ser gerados a partir desses pontos de interseção, recursivamente, de tal modo que a cor final refletida em \\(P\\) é formada por uma combinação da energia luminosa representada por todos os raios. Rasterização Em oposição ao ray casting, a rasterização é centrada no processamento de primitivas em vez de pixels. Cada primitiva é projetada no plano de imagem e rasterizada em seguida (figura 4.6): Figura 4.6: Rasterização. Para cada primitiva da cena: Projete a primitiva no plano de imagem. Rasterize a primitiva projetada. Modifique o framebuffer com a cor calculada em cada pixel da primitiva, exceto se o pixel do framebuffer já tiver sido preenchido anteriormente com uma primitiva mais próxima do plano de imagem. Na etapa 3, a cor do pixel é geralmente avaliada através de um modelo de iluminação local como o modelo de Blinn-Phong. Outras técnicas podem ser utilizadas para melhorar a aproximação da luz refletida no ponto amostrado, mas não há lançamento de raios ou testes de interseção como no ray casting. A rasterização é mais adequada para implementação em hardware, pois cada iteração do laço principal só precisa armazenar a primitiva que está sendo processada, juntamente com o conteúdo do framebuffer. Como resultado, o processamento de transformação geométrica de vértices e a conversão matricial podem ser paralelizados de forma massiva, como de fato ocorre nas GPUs. Referências "],["glpipeline.html", "4.3 Pipeline do OpenGL", " 4.3 Pipeline do OpenGL A figura 4.7 mostra um diagrama dos estágios de processamento do pipeline gráfico do OpenGL (fundo amarelo) e de como os dados gráficos (fundo cinza) interagem com cada estágio. As etapas programáveis são mostradas com fundo preto. No lado esquerdo há uma ilustração do resultado de cada etapa para a renderização de um triângulo colorido. Figura 4.7: Pipeline gráfico do OpenGL. Observação Para simplificar, algumas etapas do pipeline foram omitidas, tais como: O geometry shader, utilizado para o processamento de geometria após a montagem de primitivas; Os shaders de tesselação (tessellation control shader e tessellation evaluation shader), utilizados para subdivisão de primitivas; O compute shader, utilizado para processamento de propósito geral (GPGPU). Essas etapas não serão utilizadas nas atividades da disciplina pois, no momento, não fazem parte do subconjunto do OpenGL ES utilizado pelo WebGL. Entretanto, são etapas frequentemente utilizadas em aplicações para OpenGL desktop. Consulte a especificação do OpenGL 4.6 para ter acesso ao pipeline completo. Aplicação Antes de iniciar o processamento, a aplicação deve especificar o formato dos dados gráficos e enviar esses dados à memória que será acessada durante a renderização. A aplicação também deve configurar as etapas programáveis do pipeline, compostas pelo vertex shader e fragment shader. Os shaders devem ser compilados, ligados e ativados previamente. A geometria a ser processada é especificada através de um arranjo ordenado de vértices. O tipo de primitiva que será formada a partir desses vértices é determinado no comando de renderização. As primitivas suportadas pelo OpenGL são descritas a seguir e mostradas na figura 4.8: GL_POINTS: cada vértice forma um ponto que será desenhado na tela como um pixel ou como um quadrilátero centralizado no vértice. O tamanho do ponto/quadrilátero pode ser definido pelo usuário16; GL_LINES: cada grupo de dois vértices forma um segmento de reta; GL_LINE_STRIP: os vértices são conectados em ordem para formar uma polilinha; GL_LINE_LOOP: os vértices são conectados em ordem para formar uma polilinha, e o último vértice forma um segmento com o primeiro vértice, formando um laço; GL_TRIANGLES: cada grupo de três vértices forma um triângulo; GL_TRIANGLE_STRIP: os vértices formam uma faixa de triângulos com arestas compartilhadas; GL_TRIANGLE_FAN: os vértices formam um leque de triângulos de modo que todos os triângulos compartilham o primeiro vértice. Figura 4.8: Primitivas do OpenGL. Cada vértice do arranjo de vértices de entrada é composto por um conjunto de atributos definidos pela aplicação. Cada atributo pode ser um único valor ou um conjunto de valores. A forma como esses valores são interpretados depende exclusivamente do que é definido no vertex shader. Geralmente, considera-se que cada vértice tem pelo menos uma posição 2D \\((x,y)\\) ou 3D \\((x,y,z)\\). Outros atributos comuns para cada vértice são o vetor normal, cor e coordenadas de textura. Para ser utilizado pelo pipeline, o arranjo de vértices deve ser armazenado na memória como um recurso chamado Vertex Buffer Object (VBO). Cada atributo de vértice pode ser armazenado como um VBO separado, mas também é possível deixar todos os atributos em um único VBO (interleaved data). Cabe à aplicação especificar o formato dos dados de cada VBO e como eles serão lidos pelo vertex shader. Isso deve ser feito sempre antes da chamada do comando de renderização, para todos os VBOs. Alternativamente, essa configuração pode ser feita apenas uma vez e armazenada em um Vertex Array Object (VAO), bastando então ativar o VAO antes de cada renderização. Além da criação dos VBOs, a aplicação pode criar variáveis globais, chamadas de variáveis uniformes (uniform variables), que podem ser lidas pelo vertex shader e fragment shader. Essa é uma outra forma de enviar dados ao pipeline. As variáveis uniformes contêm dados apenas de leitura e que não variam de vértice para vértice, por isso o nome uniforme. Por exemplo, uma matriz de transformação geométrica pode ser armazenada como uma variável uniforme pois todos os vértices serão transformados por essa matriz durante o processamento no vertex shader (isto é, a matriz de transformação é a mesma para todos os vértices). Também é possível criar buffers de dados uniformes (Uniform Buffer Objects, ou UBOs) para enviar arranjos de dados. A especificação do OpenGL garante ser possível enviar pelo menos 16KB de dados no formato de UBOs, mas é comum os drivers oferecerem suporte a até 64KB. A aplicação também pode enviar dados ao pipeline usando buffers de texturas (buffer textures). Os valores dos texels dessas texturas podem ser lidos no vertex shader e no fragment shader como se fossem valores de arranjos unidimensionais. Esses valores podem ser interpretados como cores RGBA normalizadas entre 0 e 1 ou como valores arbitrários em ponto flutuante de até 32 bits. Uma forma mais recente e flexível de enviar dados uniformes é através dos Shader Storage Buffer Objects (SSBOs). O tamanho de um SSBO pode ser de até 128MB segundo a especificação, mas na maioria das implementações pode ser tão grande quanto a memória de vídeo disponível. Além disso, esse recurso pode ser utilizado tanto para leitura quanto escrita. Há muitas formas de enviar e receber dados da GPU. Entretanto, para deixarmos as coisas mais simples, usaremos neste curso apenas os recursos mais básicos, como VBOs, VAOs e variáveis uniformes. Vertex shader Os shaders do OpenGL são programas escritos na linguagem OpenGL Shading Language (GLSL). GLSL é similar à linguagem C, mas utiliza novas palavras-chave, novos tipos de dados, qualificadores e operações. O vertex shader processa cada vértice individualmente. Entretanto, esse processamento é paralelizado de forma massiva na GPU. Cada execução de um vertex shader acessa apenas os atributos do vértice que está sendo processado. Não há como compartilhar o estado do processamento de um vértice com os demais vértices. A entrada do vertex shader é um conjunto de atributos de vértice definidos pelo usuário. Esses atributos são alimentados pelo pipeline de acordo com os VBOs atualmente ativos. A saída do vertex shader é também um conjunto de atributos de vértice definidos pelo usuário. Esses atributos podem ser diferentes dos atributos de entrada. Além de escrever o resultado nos atributos de saída, é esperado (mas não obrigatório) que o vertex shader preencha uma variável embutida gl_Position com a posição final do vértice em um sistema de coordenadas homogêneas 4D \\((x, y, z, w)\\) chamado de espaço de recorte (clip space). Nos próximos estágios, a geometria das primitivas será determinada com bases nessas coordenadas. Veremos mais detalhes sobre os diferentes sistemas de coordenadas do OpenGL em capítulos futuros. Exemplo A seguir é exibido o código-fonte de um vertex shader: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition.x, inPosition.y * 1.5, 0, 1); fragColor = inColor / 2; } A primeira linha do vertex shader é a diretiva de pré-processamento #version que identifica a versão da especificação GLSL utilizada. Neste exemplo, 410 corresponde à especificação GLSL 4.10. Podemos substituir por #version 300 es para restringir os comandos do GLSL à especificação GLSL 3.0 ES compatível com o WebGL 2.0. Na verdade isso é necessário se quisermos rodar a aplicação em WebAssembly. Felizmente não precisamos nos preocupar com isso pois a ABCg faz esse ajuste automaticamente para nós. Em todos os exemplos que veremos nesta disciplina, usaremos funções do OpenGL/GLSL 4.1 que também são compatíveis com OpenGL ES 3.0. Nas linhas 3 e 4 são definidas as variáveis que receberão os atributos de entrada. Essas variáveis são identificadas com o qualificador in17: inPosition é uma tupla de dois elementos (vec2) que recebe uma posição 2D (a posição do vértice). inColor é uma tupla de quatro elementos (vec4) que recebe componentes de cor RGBA (a cor do vértice). O vertex shader tem apenas um atributo de saída, definido na linha 6 através da variável fragColor com o qualificador out. A função main é chamada para cada vértice processado. Para cada chamada, inPosition e inColor recebem os atributos do vértice. Na linha 9, a variável embutida gl_Position é preenchida com \\((x, \\frac{3}{2}y,0,1)\\), onde \\(x\\) e \\(y\\) são as coordenadas da posição 2D de entrada. Isso significa que a geometria sofrerá uma escala não uniforme: será esticada verticalmente. Na linha 10, a variável de saída recebe a cor de entrada com a intensidade de cada componente RGBA dividida por dois. Isso significa que a cor de saída terá a metade da intensidade da cor de entrada. Montagem de primitivas A montagem de primitivas recebe os atributos de vértices processados pelo vertex shader e monta as primitivas de acordo com o que é informado na chamada do comando de renderização. As primitivas geradas são formadas por pontos, segmentos ou triângulos. As primitivas da figura 4.8 são sempre decompostas em uma dessas três primitivas básicas. Por exemplo, se a primitiva informada pela aplicação é GL_LINE_STRIP, a polilinha será desmembrada em uma sequência de segmentos individuais. Recorte Na etapa de recorte, as primitivas que estão fora do volume de visão (fora do viewport) são descartadas ou recortadas. Por exemplo, se a ponta de um triângulo estiver fora do volume de visão, o triângulo será recortado e formará um quadrilátero, que é então decomposto em dois triângulos. Os atributos dos vértices a mais gerados no recorte são obtidos através da interpolação linear dos atributos dos vértices originais. O recorte também pode operar sobre planos de recorte definidos pelo usuário no vertex shader. Após o recorte, ocorre a divisão perspectiva, que consiste na conversão das coordenadas homogêneas 4D \\((x, y, z, w)\\) em coordenadas cartesianas 3D \\((x, y, z)\\). Isso é feito dividindo \\(x\\), \\(y\\) e \\(z\\) por \\(w\\). O sistema de coordenadas resultante é chamado de coordenadas normalizadas do dispositivo (normalized device coordinates, ou NDC). Em NDC, todas as primitivas após o recorte estão situadas dentro de um volume de visão canônico: um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Ainda nesta etapa, as componentes \\(x\\) e \\(y\\) das coordenadas em NDC são mapeadas para o sistema de coordenadas da janela (chamado de espaço da janela, ou window space), em pixels. Esse mapeamento é configurado pelo comando glViewport. O valor \\(z\\) é mapeado de \\([-1, 1]\\) para \\([0, 1]\\) por padrão, mas isso pode ser configurado com glDepthRange. Rasterização Todas as primitivas contidas no volume de visão canônico passam por uma conversão matricial, na ordem em que foram processadas nas etapas anteriores. O resultado da rasterização de cada primitiva é um conjunto de fragmentos que representam amostras da primitiva no espaço da tela. Um fragmento pode ser interpretado como um pixel em potencial. A cor final de cada pixel no framebuffer poderá ser determinada por um fragmento ou pela combinação de vários fragmentos. Cada fragmento é descrito por dados como: Posição \\((x, y, z)\\) em coordenadas da janela18, sendo que \\(z\\) é a profundidade do fragmento (por padrão, um valor no intervalo \\([0, 1]\\)). Como cada fragmento tem uma profundidade, é possível determinar qual fragmento está mais na frente quando vários fragmentos são mapeados para a mesma posição \\((x, y)\\) da janela. Assim, a cor do pixel pode ser determinada apenas pelo fragmento mais próximo. Os demais podem ser descartados pois estão sendo escondidos pelo fragmento mais próximo. Atributos interpolados a partir dos vértices da primitiva. Isso inclui todos os atributos definidos na saída do vertex shader. Por exemplo, se a saída do vertex shader devolve um atributo de cor RGB para cada vértice (uma tupla de três valores), então cada fragmento terá também uma cor RGB, com valores obtidos através da interpolação (geralmente linear) dos atributos definidos nos vértices. Fragment shader O fragment shader é um programa que processa cada fragmento individualmente após a rasterização. A entrada do fragment shader é o mesmo conjunto de atributos definidos pelo usuário na saída do vertex shader. É possível acessar também outros atributos pré-definidos que compõem o conjunto de dados de cada fragmento. Por exemplo, a posição do fragmento pode ser acessada através de uma variável embutida chamada gl_FragCoord. A saída do fragment shader geralmente é uma cor em formato RGBA (uma tupla de quatro valores), mas é possível produzir também mais de uma cor caso o pipeline tenha sido configurado para renderizar simultaneamente em vários buffers de cor. O fragment shader também pode alterar as propriedades do fragmento através de variáveis embutidas. Por exemplo, a profundidade pode ser modificada através de gl_FragDepth. Exemplo A seguir é exibido o código-fonte do fragment shader que acompanha o vertex shader mostrado no exemplo anterior: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = vec4(fragColor.r, fragColor.r, fragColor.r, 1); } Esse fragment shader só tem um atributo de entrada, definido na linha 3 pela variável fragColor. O atributo de entrada é a cor RGBA correspondente ao atributo de saída do vertex shader. A saída do fragment shader também é uma cor RGBA, definida pela variável outColor. A função main é chamada para cada fragmento processado. Para cada chamada, fragColor recebe o atributo do fragmento, que é o atributo de saída do vertex shader, mas interpolado entre os vértices da primitiva. Por exemplo, se a primitiva é um segmento formado por um vértice de cor RGB branca \\((1,1,1)\\) e outro vértice de cor preta \\((0,0,0)\\), o fragmento produzido no ponto médio do segmento terá a cor cinza \\((0.5, 0.5, 0.5)\\). Na linha 8, outColor recebe uma cor RGBA na qual as componentes RGB são uma replicação da componente R da cor de entrada. Isso significa que a cor resultante é um tom de cinza que corresponde à intensidade de vermelho da cor original. Se esse fragment shader e o vertex shader do exemplo anterior fossem utilizados no projeto Hello, World! da ABCg (seção 1.5), o triângulo resultante seria igual ao mostrado à direita na figura 4.9. Observe o efeito da mudança de escala da geometria (feita no vertex shader) e modificação das cores (intensidade reduzida pela metade no vertex shader, e conversão para tons de cinza no fragment shader). Figura 4.9: Renderização do triângulo do projeto Hello, World! com os shaders originais (esquerda) e shaders utilizados nos exemplos (direita). Operações de fragmentos Após o processamento no fragment shader, cada fragmento passa por uma sequência de testes que podem resultar em seu descarte. Se o fragmento falhar em algum desses testes, ele será ignorado e não contribuirá para a cor do pixel final. O teste de propriedade de pixel (pixel ownership test) verifica se o fragmento corresponde a um pixel do framebuffer que está de fato visível no sistema de janelas. Por exemplo, se uma outra janela estiver sobrepondo a janela do OpenGL, os fragmentos mapeados para a área sobreposta serão descartados. O teste de tesoura (scissor test), quando ativado com glEnable, descarta fragmentos que estão fora de um retângulo definido no espaço da janela pela função glScissor. Por exemplo, usando o código a seguir, o teste de tesoura será ativado e serão descartados todos os fragmentos que estiverem fora do retângulo definido pelas coordenadas \\((50,30)\\) a \\((250,130)\\) pixels no espaço da janela (o pixel de coordenada \\((0,0)\\) corresponde ao canto inferior esquerdo da janela): glEnable(GL_SCISSOR_TEST); glScissor(50, 30, 200, 100); O teste de estêncil (stencil test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação entre um valor de estêncil do fragmento (um número inteiro, geralmente de 8 bits) e o valor de estêncil do buffer de estêncil (stencil buffer), que é um dos buffers do framebuffer. Por exemplo, no código a seguir, glStencilFunc estabelece que o teste deve comparar se o valor de estêncil do fragmento é maior que 5. Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_STENCIL_TEST); glStencilFunc(GL_GREATER, 5, 0xFF) O teste de profundidade (depth test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação do valor de profundidade do fragmento (valor \\(z\\) no espaço da janela) com o valor de profundidade armazenado atualmente no buffer de profundidade (depth buffer). Com o teste de profundidade é possível fazer com que só os fragmentos mais próximos sejam exibidos. Por exemplo, no código a seguir, glDepthFunc faz com que o teste de profundidade compare se o valor de profundidade do fragmento é menor que o valor do buffer de profundidade (GL_LESS é a comparação padrão). Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LESS); Muitos desses testes podem ser realizados antes do fragment shader, em uma otimização chamada de early per-fragment test, suportada pela maioria das GPUs atuais. Por exemplo, se o fragment shader não modificar gl_FragDepth, é possível fazer o teste de profundidade logo após a rasterização, evitando o processamento de um fragmento que já se sabe que não contribuirá para a formação da imagem. Se o fragmento passou por todos os testes e não foi descartado, sua cor será utilizada para modificar o pixel correspondente no(s) buffer(s) de cor. Mesmo que o fragmento não tenha passado por todos os testes, é possível que o buffer de estêncil e buffer de profundidade sejam modificados. Esse comportamento pode ser determinado pela aplicação. Também é possível usar operações de mascaramento para permitir, por exemplo, que somente as componentes RG da cor RGB sejam escritas no buffer de cor. Antes do buffer de cor ser modificado, também é possível fazer com que a cor do fragmento seja misturada com a cor atual do buffer de cor, em uma operação de mistura de cor (color blending). Por exemplo, considere o código a seguir: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND) habilita o modo de mistura de cor. As funções glBlendEquation e glBlendFunc configuram a mistura de cor para que cada componente de cor RGBA do buffer de cor seja calculada como \\(C=C_sF_s + C_dF_d\\), onde: \\(C_s\\) é a componente de cor do fragmento (origem); \\(C_d\\) é a componente de cor do buffer de cor (destino); \\(F_s\\) é um fator de mistura de cor, que nesse caso é a componente A da cor do fragmento (GL_SRC_ALPHA); \\(F_d\\) é um fator de mistura de cor, que nesse caso é 1 menos a componente A da cor do fragmento (GL_ONE_MINUS_SRC_ALPHA). O resultado é a combinação da cor do fragmento com a cor atual do buffer de cor, usando a componente A do fragmento como fator de opacidade (1=totalmente opaco, 0=totalmente transparente). O tamanho do ponto pode ser definido através da função glPointSize ou pela variável embutida (built-in) gl_PointSize no vertex shader. Neste exemplo, o nome das variáveis de entrada também começa com o prefixo in, mas isso é só uma convenção. A posição de cada fragmento também inclui o valor recíproco da coordenada \\(w\\) no espaço de recorte. "],["coloredtriangles.html", "4.4 Triângulos coloridos", " 4.4 Triângulos coloridos Na seção 3.4, renderizamos pontos (GL_POINTS) para gerar o Triângulo de Sierpinski. Neste projeto, desenharemos triângulos (GL_TRIANGLES). Para cada quadro de exibição, renderizaremos um triângulo colorido com coordenadas 2D aleatórias dentro da janela de exibição. O resultado ficará como a seguir: Ao longo da atividade veremos com mais detalhes os comandos do OpenGL utilizados para especificar os dados gráficos e configurar o pipeline. Configuração inicial Repita a configuração inicial dos projetos anteriores e mude o nome do projeto para coloredtriangles. O arquivo abcg/examples/CMakeLists.txt ficará assim (com a compilação desabilitada para os projetos anteriores): #add_subdirectory(helloworld) #add_subdirectory(firstapp) #add_subdirectory(sierpinski) add_subdirectory(coloredtriangles) O arquivo abcg/examples/coloredtriangles/CMakeLists.txt ficará assim: project(coloredtriangles) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Como nos projetos anteriores, crie os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp em abcg/examples/coloredtriangles. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é bem similar ao utilizado no projeto sierpinski e nos projetos anteriores: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings( {.samples = 2, .vsync = true, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Colored Triangles&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Em setOpenGLSettings, .vsync = true habilita a sincronização vertical (vsync), que está desabilitada por padrão. Assim, OpenGLWindow::paintGL será chamada na mesma taxa de atualização do monitor (geralmente 60 Hz). openglwindow.hpp A definição da classe OpenGLWindow também é parecida com aquela do projeto sierpinski: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;array&gt; #include &lt;glm/vec4.hpp&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_vao{}; GLuint m_vboPositions{}; GLuint m_vboColors{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec4, 3&gt; m_vertexColors{glm::vec4{0.36f, 0.83f, 1.00f, 1.0f}, glm::vec4{0.63f, 0.00f, 0.61f, 1.0f}, glm::vec4{1.00f, 0.69f, 0.30f, 1.0f}}; void setupModel(); }; #endif No projeto anterior utilizamos apenas um VBO (m_vboVertices). Agora há dois VBOs: um para a posição dos vértices (m_vboPositions) e outro para as cores (m_vboColors). O arranjo m_vertexColors contém as cores RGBA que serão copiadas para m_vboColors. São três cores, uma para cada vértice do triângulo. openglwindow.cpp Primeiramente, vamos incluir os arquivos de cabeçalho: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;glm/vec3.hpp&gt; #include &quot;abcg.hpp&quot; Agora vamos à definição da função membro OpenGLWindow::initializeGL: void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window glClearColor(0, 0, 0, 1); glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } O código das linhas 35 a 44 é praticamente idêntico ao do projeto anterior. Vamos nos concentrar nos códigos dos shaders. Observe o conteúdo da string em vertexShader: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Este vertex shader define dois atributos de entrada: inPosition, que recebe a posição 2D do vértice, e inColor que recebe a cor RGBA. A saída, fragColor, é também uma cor RGBA. Na função main, a posição \\((x,y)\\) do vértice é repassada sem modificações para gl_Position. A conversão de \\((x,y)\\) em coordenadas cartesianas para \\((x,y,0,1)\\) em coordenadas homogêneas preserva a geometria do triângulo19. A cor do atributo de entrada também é repassada sem modificações para o atributo de saída. Vejamos agora o fragment shader: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } O fragment shader é ainda mais simples. O atributo de entrada (fragColor) é copiado sem modificações para o atributo de saída (outColor). A compilação e ligação dos shaders é feita pela chamada a abcg::OpenGLWindow::createProgramFromString na linha 36. Consulte a definição dessa função em abcg/abcg/abcg_openglwindow.cpp para ver quais funções do OpenGL são utilizadas. O resultado de createProgramFromString é m_program, um número inteiro que identifica o programa de shader composto pelo par de vertex/fragment shader. Para ativar o programa no pipeline, devemos chamar glUseProgram(m_program). Para desativá-lo, podemos ativar outro programa (se existir) ou chamar glUseProgram(0). A função OpenGLWindow::paintGL() é definida assim: void OpenGLWindow::paintGL() { setupModel(); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Novamente, o código é similar ao utilizado no projeto sierpinski. A função de renderização, glDrawArrays, dessa vez usa GL_TRIANGLES e 3 vértices, sendo que o índice inicial dos vértices no arranjo é 0. Isso significa que o pipeline desenhará apenas um triângulo. Em OpenGLWindow::paintUI(), usaremos controles de interface da ImGui para criar uma pequena janela de edição das três cores dos vértices: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { auto widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_vertexColors[0].x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_vertexColors[1].x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_vertexColors[2].x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } As funções ImGui::SetNextWindowPos e ImGui::SetNextWindowSize definem a posição e tamanho da janela da ImGui que está prestes a ser criada na linha 70. A janela é inicializada com alguns flags para que ela não possa ser redimensionada (ImGuiWindowFlags_NoResize) e não tenha a barra de título (ImGuiWindowFlags_NoTitleBar). Os controles ImGui::ColorEdit3 também são criados com flags para desabilitar o color picker (ImGuiColorEditFlags_NoPicker) e os tooltips (ImGuiColorEditFlags_NoTooltip), pois eles podem atrapalhar o desenho dos triângulos. A definição de OpenGLWindow::resizeGL é idêntica à do projeto sierpinski. A definição de OpenGLWindow::terminateGL também é bem semelhante e libera os recursos do pipeline: void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Vamos agora definir a função membro OpenGLWindow::setupModel e detalhar as funções do OpenGL que são utilizadas. A definição completa é dada abaixo, mas em seguida faremos uma análise mais detalhada de cada trecho: void OpenGLWindow::setupModel() { abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } As linhas 100 a 102 liberam os VBOs e o VAO, caso tenham sido criados anteriormente: abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); É importante fazer isso, pois a função setupModel é chamada continuamente em paintGL. Se não liberarmos os recursos, em algum momento eles consumirão toda a memória da GPU e CPU20. As linhas 104 a 114 criam arranjos com os dados que serão copiados para os VBOs: // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); Observe que as coordenadas das posições dos vértices são números pseudoaleatórios do intervalo \\([-1.5, 1.5]\\). Vimos no projeto anterior que, para uma primitiva ser vista no viewport, ela precisa ser especificada entre \\([-1, -1]\\) e \\([1, 1]\\). Logo, nossos triângulos terão partes que ficarão para fora da janela. O pipeline se encarregará de recortar os triângulos e mostrar apenas os fragmentos que estão dentro do viewport. Nas linhas 116 a 128 são criados os VBOs (um para as posições 2D, outro para as cores RGBA): // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); glGenBuffers cria o identificador de um objeto de buffer (buffer object). Um objeto de buffer é um arranjo de dados alocado pelo OpenGL, geralmente na memória da GPU. glBindBuffer com o argumento GL_ARRAY_BUFFER vincula o objeto de buffer a um buffer de atributos de vértices. Isso define o objeto de buffer como um objeto de buffer de vértice (VBO). O objeto de buffer pode ser desvinculado com glBindBuffer(0), ou vinculando outro objeto de buffer. glBufferData aloca a memória e inicializa o buffer com o conteúdo copiado de um ponteiro alocado na CPU. O primeiro parâmetro indica o tipo de objeto de buffer utilizado. O segundo parâmetro é o tamanho do buffer em bytes. O terceiro parâmetro é um ponteiro para os dados que serão copiados, na quantidade de bytes correspondente ao tamanho do buffer. O quarto parâmetro é uma dica ao driver de vídeo de como o buffer será usado. GL_STATIC_DRAW significa que o buffer será modificado apenas uma vez, potencialmente será utilizado muitas vezes, e que os dados serão usados para renderizar algo no framebuffer. Após a cópia dos dados com o glBufferData, o arranjo de origem não é mais necessário e pode ser destruído. No nosso código, positions e colors estão alocados na pilha e são liberados no fim do escopo. As linhas 130 a 132 usam glGetAttribLocation para pegar a localização de cada atributo de entrada do vertex shader de m_program: // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; O resultado de positionAttribute será 0, pois o vertex shader define inPosition com layout(location = 0). Da mesma forma, colorAttribute será 1, pois o vertex shader define inColor com layout(location = 1). Poderíamos omitir esse código e usar os valores diretamente no trecho a seguir, mas é sempre preferível fazer a consulta da localização com glGetAttribLocation do que usar valores hard-coded. Agora que sabemos a localização dos atributos inPosition e inColor no vertex shader, podemos especificar ao OpenGL como os dados de cada VBO serão mapeados para esses atributos. Isso é feito nas linhas 134 a 153 a seguir: // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); Na linha 135, glGenVertexArray cria um VAO que, como vimos no projeto sierpinski, armazena o estado da especificação de vinculação dos VBOs com o vertex shader. Neste projeto, essa especificação é feita nas linhas 140 a 150. Em paintGL, antes de chamar glDrawArrays, quando vinculamos o VAO com glBindVertexArray, o estado da configuração dos VBOs com o programa de shader é recuperado automaticamente (isto é, é como se as linhas 140 a 150 fossem executadas). Na nossa aplicação isso não parece tão útil. As linhas 140 a 150 já são executadas para todo quadro de exibição, pois chamamos setupModel logo antes de glDrawArrays. Mas, em aplicações futuras, chamaremos setupModel apenas uma vez (por exemplo, em initializeGL). Geralmente, o modelo geométrico é definido apenas uma vez e não é mais alterado (ou é raramente alterado). Nesse caso, o VAO é útil para que não tenhamos de configurar manualmente a ligação dos VBOs com os atributos do vertex shader para todo quadro de exibição. O código completo de openglwindow.cpp é mostrado a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;glm/vec2.hpp&gt; #include &lt;glm/vec3.hpp&gt; #include &quot;abcg.hpp&quot; void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window glClearColor(0, 0, 0, 1); glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator auto seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } void OpenGLWindow::paintGL() { setupModel(); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { auto widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_vertexColors[0].x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_vertexColors[1].x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_vertexColors[2].x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); } void OpenGLWindow::setupModel() { abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Create vertex positions std::uniform_real_distribution&lt;float&gt; rd(-1.5f, 1.5f); std::array positions{glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine)), glm::vec2(rd(m_randomEngine), rd(m_randomEngine))}; // Create vertex colors std::vector&lt;glm::vec4&gt; colors(0); colors.emplace_back(m_vertexColors[0]); colors.emplace_back(m_vertexColors[1]); colors.emplace_back(m_vertexColors[2]); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec4), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Dica Experimente habilitar o modo de mistura de cor usando o código mostrado na seção 4.3. Inclua o código a seguir em OpenGLWindow::initializeGL: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); Além disso, mude a componente A das cores RGBA de m_vertexColors. Por exemplo, com a definição a seguir, os triângulos ficarão 50% transparentes: std::array&lt;glm::vec4, 3&gt; m_vertexColors{glm::vec4{0.36f, 0.83f, 1.00f, 0.5f}, glm::vec4{0.63f, 0.00f, 0.61f, 0.5f}, glm::vec4{1.00f, 0.69f, 0.30f, 0.5f}}; Exercício Modifique o projeto coloredtriangles para suportar novas funcionalidades: Geração de cores aleatórias nos vértices; Possibilidade de desenhar cada triângulo com uma cor sólida; Ajuste do intervalo de tempo entre a renderização de cada triângulo. Um exemplo é dado a seguir: O conceito de coordenadas homogêneas será abordado futuramente, quando trabalharmos com transformações geométricas 3D. Em geral, destruir e criar os VBOs a cada quadro de exibição não é muito eficiente. É preferível criar o VBO apenas uma vez e, se necessário, modificá-lo com glBufferData a cada quadro. Em nossa aplicação, optamos por chamar setupModel a cada paintGL apenas para manter o código mais simples. "],["game.html", "5 Desenvolvendo um jogo 2D", " 5 Desenvolvendo um jogo 2D Neste capítulo, usaremos o pipeline gráfico do OpenGL e funções das bibliotecas auxiliares da ABCg (GLM, ImGui, SDL) para desenvolver um jogo com gráficos 2D. Veremos como especificar a geometria de objetos de cena usando diferentes formatos de Vertex Buffer Objects (VBOs), como trabalhar com mais de um par de vertex/fragment shaders, e como usar variáveis uniformes (uniform variables) para modificar a cor, posição e orientação dos objetos sem precisar reconstruir os VBOs. Também trabalharemos com aspectos não relacionados ao pipeline, mas que são necessários para dar animação e interação ao jogo: Como responder a eventos do mouse e teclado; Como calcular a colisão entre os objetos do jogo; Como usar temporizadores para animar os objetos. O resultado será um jogo estilo Asteroids, como mostrado a seguir. Para simplificar, o jogo não terá efeitos sonoros21. A nave pode ser controlada pelo teclado ou mouse: A orientação é ajustada pela posição do mouse, setas para os lados ou teclas A e D; A aceleração é ativada com o botão direito do mouse, seta para cima ou tecla W; Os tiros são disparados com o botão esquerdo do mouse ou barra de espaço. Observação Como a página do jogo está embutida neste livro online, só é possível controlar a nave pelo mouse. Para a aplicação ter o foco do teclado, abra o link original. Antes de começar a desenvolver o jogo, acompanharemos na seção 5.1 o passo a passo de desenvolvimento de um projeto mais simples, que desenha polígonos regulares coloridos em posições aleatórias da janela. O projeto explora conceitos que serão aplicados no jogo. Em particular, os polígonos regulares servirão de base para criarmos os asteroides e os tiros da espaçonave. Também usaremos temporizadores e variáveis uniformes de forma muito parecida com o que veremos nesse primeiro projeto. É possível incluir sons usando as funções de áudio da SDL, mas neste curso não utilizaremos tais funcionalidades. "],["regularpolygons.html", "5.1 Polígonos regulares", " 5.1 Polígonos regulares Este projeto é um aprimoramento do projeto coloredtriangles da seção 4.4. No lugar de desenharmos triângulos (GL_TRIANGLES), desenharemos polígonos regulares 2D formados por leques de triângulos (GL_TRIANGLE_FAN). Para cada quadro de exibição, será renderizado um polígono regular colorido em uma posição aleatória do viewport. O número de lados de cada polígono também será escolhido aleatoriamente. A aplicação ficará como a seguir: Configuração inicial A configuração inicial é a mesma dos projetos anteriores. Apenas mude o nome do projeto para regularpolygons e inclua a linha add_subdirectory(regularpolygons) em abcg/examples/CMakeLists.txt. O arquivo abcg/examples/regularpolygons/CMakeLists.txt ficará assim: project(regularpolygons) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Este projeto também terá os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp. main.cpp O conteúdo de main.cpp é praticamente idêntico ao do projeto coloredtriangles: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings( {.samples = 2, .preserveWebGLDrawingBuffer = true}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Regular Polygons&quot;}); // Run application app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } openglwindow.hpp Aqui também há poucas mudanças em relação ao projeto anterior: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;random&gt; #include &quot;abcg.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_vao{}; GLuint m_vboPositions{}; GLuint m_vboColors{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; int m_delay{200}; abcg::ElapsedTimer m_elapsedTimer; void setupModel(int sides); }; #endif Observe que há novamente dois VBOs: um para a posição e outro para a cor dos vértices (linhas 18 e 19). Na linha 27, a variável m_delay é utilizada para especificar o intervalo de tempo, em milissegundos, entre a renderização dos polígonos. Na linha 28, m_elapsedTimer, da classe abcg::ElapsedTimer, é um temporizador simples usando funções da biblioteca std::chrono. A contagem de tempo inicia quando o objeto é criado. Só há duas funções membro disponíveis: double abcg::ElapsedTimer::elapsed() retorna o tempo, em segundos, desde a criação do objeto, ou desde a última chamada a abcg::ElapsedTimer::restart(); double abcg::ElapsedTimer::restart() reinicia a contagem de tempo. Usaremos m_elapsedTimer junto com m_delay para definir a frequência de desenho dos polígonos. openglwindow.cpp Antes de qualquer coisa, vamos incluir os seguintes arquivos de cabeçalho: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &quot;abcg.hpp&quot; A definição de OpenGLWindow::initializeGL é a mesma do projeto coloredtriangles. Apenas o conteúdo do vertex shader será modificado. A definição completa fica assim: void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); } Compare o código do vertex shader na string vertexShader com o vertex shader do projeto anterior. No projeto anterior (coloredtriangles), o vertex shader estava assim: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Agora o vertex shader ficará assim: #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } A principal mudança é o uso de duas variáveis uniformes, identificadas pela palavra-chave uniform. São elas: translation: um fator de translação (deslocamento) da geometria; scale: um fator de escala da geometria. O conteúdo de translation e scale é definido em paintGL antes de cada renderização. Lembre-se que variáveis uniformes são variáveis globais do shader que não mudam de valor de um vértice para outro, ao contrário do que ocorre com as variáveis inPosition e inColor. Observe o conteúdo da função main: void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } A posição original do vértice (inPosition) é multiplicada por scale e somada com translation para gerar uma nova posição (newPosition), que é a posição final do vértice passada para gl_Position. Na expressão inPosition * scale + translation, inPosition * scale resulta na aplicação do fator de escala nas coordenadas \\(x\\) e \\(y\\) do vértice. Como isso é feito para cada vértice da geometria, o resultado será a mudança do tamanho do objeto. Se o fator de escala for 1, não haverá mudança de escala. Se for 0.5, o tamanho do objeto será reduzido pela metade em \\(x\\) e em \\(y\\). Se for 2.0, o tamanho será dobrado em \\(x\\) e em \\(y\\). O resultado de inPosition * scale é somado com translation. Isso significa que, após a mudança de escala, a geometria será deslocada pelas coordenadas \\((x,y)\\) da translação. Ao aplicar a escala e a translação do vertex shader, podemos usar um mesmo VBO para renderizar o objeto em posições e escalas diferentes, como mostra a figura 5.1: Figura 5.1: Resultado da transformação dos vértices de um triângulo usando diferentes fatores de escala e translação. Observação O uso de variáveis uniformes e transformações geométricas no vertex shader pode reduzir em muito o consumo de memória dos dados gráficos. Suponha que queremos renderizar uma cena estilo Minecraft composta por 100.000 cubos. A estratégia mais ingênua para renderizar essa cena é criar um único VBO contendo os vértices dos 100.000 cubos. Se usarmos GL_TRIANGLES, cada lado do cubo terá de ser renderizado como 2 triângulos, isto é, precisaremos de 6 vértices. Como um cubo tem 6 lados, teremos então 36 vértices por cubo. Logo, nosso VBO de 100.000 cubos terá 3.600.000 vértices22. Ao usar variáveis uniformes, podemos criar um VBO para apenas um cubo e renderizar esse cubo 100.000 vezes, cada um com um fator de escala e translação diferente. No fim, o número de vértices processados será igual, mas o uso de memória terá uma redução de 5 ordens de magnitude! Vamos agora à definição de OpenGLWindow::paintGL(): void OpenGLWindow::paintGL() { // Check whether to render the next polygon if (m_elapsedTimer.elapsed() &lt; m_delay / 1000.0) return; m_elapsedTimer.restart(); // Create a regular polygon with a number of sides in the range [3,20] std::uniform_int_distribution&lt;int&gt; intDist(3, 20); const auto sides{intDist(m_randomEngine)}; setupModel(sides); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); // Choose a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution&lt;float&gt; rd1(-1.0f, 1.0f); const glm::vec2 translation{rd1(m_randomEngine), rd1(m_randomEngine)}; const GLint translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Choose a random scale factor (1% to 25%) std::uniform_real_distribution&lt;float&gt; rd2(0.01f, 0.25f); const auto scale{rd2(m_randomEngine)}; const GLint scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); // Render abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 52, o tempo contado por m_elapsedTimer é comparado com m_delay. Se o tempo ainda não atingiu m_delay, a função retorna. Caso contrário, o temporizador é reiniciado na linha 53 e a execução continua nas linhas seguintes. Na linha 58, setupModel(sides) é chamada para criar o VBO de um polígono regular de sides lados. O número de lados é escolhido aletoriamente do intervalo \\([3,20]\\). Nas linhas 64 a 75 são definidos os valores das variáveis uniformes do shader: // Choose a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution&lt;float&gt; rd1(-1.0f, 1.0f); const glm::vec2 translation{rd1(m_randomEngine), rd1(m_randomEngine)}; const GLint translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Choose a random scale factor (1% to 25%) std::uniform_real_distribution&lt;float&gt; rd2(0.01f, 0.25f); const auto scale{rd2(m_randomEngine)}; const GLint scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); Na linha 66, translation contém coordenadas 2D aleatórias no intervalo \\([-1,1]\\). Na linha 73, scale é um fator de escala aleatório no intervalo \\([0.01, 0.25]\\). Nas linhas 67 e 74, translationLocation e scaleLocation contêm os identificadores de localização das variáveis uniformes do shader. Esse valores são obtidos com glGetUniformLocation passando o identificador do programa de shader como primeiro argumento (m_program) e uma string com o nome da variável uniforme como segundo argumento. A atribuição dos valores das variáveis uniformes é feita nas linhas 69 e 75. As funções glUniform* têm como primeiro parâmetro a localização da variável uniforme que será modificada, seguida de uma lista de parâmetros que depende do sufixo no fim de glUniform: Em glUniform2fv, 2fv significa que a variável uniforme é um arranjo de tuplas de dois valores float, isto é, um arranjo de vec2. Nesse caso, o segundo argumento é a quantidade de vec2 que serão copiados. O argumento é 1 porque translation não é apenas um vec2. O terceiro argumento é o endereço do primeiro elemento do conjunto de dados que serão copiados. Em glUniform1f, 1f significa que a variável uniforme é apenas um valor float. Nesse caso, o segundo argumento é simplesmente o valor float que será copiado. Observação O formato geral de glUniform é glUniform{1|2|3|4}{f|i|ui}[v]: {1|2|3|4} define o número de componentes do tipo de dado: 1 para float, int, unsigned int e bool; 2 para vec2, ivec2, uvec2, bvec2; 3 para vec3, ivec3, uvec3, bvec3; 4 para vec4, ivec4, uvec4, bvec4. {f|i|ui} define o tipo de dado de cada componente: f para float, vec2, vec3, vec4; i para int, ivec2, ivec3, ivec4; ui para unsigned int, uvec2, uvec3, uvec4. Tanto f, i e ui podem ser usados para copiar dados para variáveis uniformes booleanas (bool, bvec2, bvec3, bvec4). Nesse caso, true é qualquer valor diferente de zero. Se o v final não é especificado, então {1|2|3|4} é também o número de parâmetros após o identificador de localização. Por exemplo: // Variável uniform é um float ou bool glUniform1f(loc, 3.14f); // Variável uniform é um unsigned int ou bool glUniform1ui(loc, 42); // Variável uniform é um vec2 ou bvec2 glUniform2f(loc, 0.0f, 10.5f); // Variável uniform é um ivec4 ou bvec4 glUniform4i(loc, -1, 2, 10, 3); Se o v é especificado, o segundo parâmetro é o número de elementos do arranjo, e o terceiro parâmetro é o ponteiro para os dados. Por exemplo: // Variável uniform é um float ou bool float pi{3.14f}; glUniform1fv(loc, 1, &amp;pi); // Variável uniform é um unsigned int ou bool unsigned int answer{42}; glUniform1uiv(loc, 1, &amp;answer); // Variável uniform é um vec2 ou bvec2 glm::vec2 foo{0.0f, 10.5f}; glUniform2fv(loc, 1, &amp;foo.x); // Variável uniform é um ivec4[2] ou bvec4[2] std::array bar{glm::ivec4{-1, 2, 10, 3}, glm::ivec4{7, -5, 1, 90}}; glUniform4iv(loc, 2, &amp;bar.at(0).x); Nas linhas 77 a 80 temos a chamada à função de renderização: // Render abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); O VAO é vinculado na linha 78 e automaticamente ativa e configura a ligação dos VBOs com o programa de shader. O comando de renderização é chamado na linha 79. Observe o uso da constante GL_TRIANGLE_FAN. O número de vértices é sides + 2 porque vamos definir nossos polígonos de tal modo que o número de vértices será sempre o número de lados mais dois, como mostra a figura 5.2 para a definição de um pentágono: Figura 5.2: Pentágono formado por um leque de sete vértices. No pentágono, o vértice de índice 6 tem a mesma posição do vértice de índice 1 para fechar o leque de triângulos. Na verdade, o leque poderia definir um pentágono com apenas cinco vértices, como mostra a figura 5.3: Figura 5.3: Pentágono formado por um leque de cinco vértices. A escolha de manter o vértice de índice 0 no centro é proposital pois permite simular um efeito de gradiente de cor parecido com um gradiente radial. Para isto, basta atribuir uma cor ao vértice 0, e outra cor aos demais vértices. Como os atributos dos vértices são interpolados linearmente pelo rasterizador para cada fragmento gerado, o resultado será um gradiente de cor. A figura 5.4 mostra um exemplo usando amarelo no vértice central e azul nos demais vértices: Figura 5.4: Pentágono com gradiente de cor formado através da interpolação do atributo de cor dos vértices. Continuando com a definição das funções membro de OpenGLWindow, definiremos OpenGLWindow::paintUI() usando o código a seguir. Ele é bem parecido com o do projeto anterior. A diferença é que, no lugar de ImGui::ColorEdit3, criaremos um slider para controlar o valor de m_delay e criaremos um botão para limpar a janela: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { const auto widgetSize{ImVec2(200, 72)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); const auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); ImGui::PushItemWidth(140); ImGui::SliderInt(&quot;Delay&quot;, &amp;m_delay, 0, 200, &quot;%d ms&quot;); ImGui::PopItemWidth(); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(-1, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } A definição de OpenGLWindow::resizeGL e OpenGLWindow::terminateGL é idêntica à do projeto coloredtriangles. Vamos agora à definição da função membro OpenGLWindow::setupModel. O código completo é mostrado abaixo, mas analisaremos cada trecho em seguida: void OpenGLWindow::setupModel(int sides) { // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Select random colors for the radial gradient std::uniform_real_distribution&lt;float&gt; rd(0.0f, 1.0f); const glm::vec3 color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; const glm::vec3 color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions(0); std::vector&lt;glm::vec3&gt; colors(0); // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices const auto step{M_PI * 2 / sides}; for (const auto angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec3), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program const auto positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; const auto colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } No início da função, os VBOs e o VAO são liberados caso tenham sido criados anteriormente: // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); Em seguida temos o código que cria os vértices do polígono regular (arranjos positions e colors): // Select random colors for the radial gradient std::uniform_real_distribution&lt;float&gt; rd(0.0f, 1.0f); const glm::vec3 color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; const glm::vec3 color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions(0); std::vector&lt;glm::vec3&gt; colors(0); // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices const auto step{M_PI * 2 / sides}; for (const auto angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); Duas cores RGB são sorteadas nas linhas 132 e 134. color1 é utilizada na definição do vértice do centro (linhas 144 e 145), e color2 é utilizada para os demais vértices. Nas linhas 148 a 152, a posição dos vértices é calculada com a equação paramétrica de um círculo unitário: \\[ \\begin{eqnarray} x&amp;=&amp;cos(t),\\\\ y&amp;=&amp;sin(t), \\end{eqnarray} \\] onde \\(t\\) é o ângulo (angle) que varia de \\(0\\) a \\(2\\pi\\) usando um tamanho do passo (step) igual à divisão de \\(2\\pi\\) pelo número de lados do polígono. A definição dos VBOs é semelhante à forma utilizada no projeto anterior. Nas linhas 183 a 193 é definido como os dados dos VBOs serão mapeados para a entrada do vertex shader. Vamos nos concentrar na definição do mapeamento de m_vboPositions (o mapeamento de m_vboColors é similar): abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); Na linha 183, glEnableVertexAttribArray habilita o atributo de posição do vértice (inPosition) para ser utilizado durante a renderização. Em seguida, glBindBuffer vincula o VBO m_vboPositions, que contém os dados das posições dos vértices. Na linha 185, glVertexAttribPointer define como os dados do VBO serão mapeados para o atributo. Lembre-se que o VBO é apenas um arranjo linear de bytes copiados pela função glBufferData. Com glVertexAttribPointer, informamos ao OpenGL como esses bytes devem ser mapeados para uma variável de atributo de entrada do vertex shader. A assinatura de glVertexAttribPointer é a seguinte: void glVertexAttribPointer(GLuint index, GLint size, GLenum type, GLboolean normalized, GLsizei stride, const void * pointer); Os parâmetros são descritos a seguir: index: índice do atributo que será modificado. No nosso caso (linha 180) é positionAttribute. size: número de componentes do atributo. No nosso caso é 2 pois inPosition é um vec2, isto é, um atributo de dois componentes. type: tipo de dado de cada valor do VBO. Usamos GL_FLOAT pois cada coordenada \\(x\\) e \\(y\\) do VBO de posições é um float. normalized: flag que indica se valores inteiros devem ser normalizados para \\([-1,1]\\) (para valores com sinal) ou \\([0,1]\\) (para valores sem sinal) quando forem enviados ao atributo. Usamos GL_FALSE porque nossas coordenadas são valores do tipo float; stride: é o número de bytes entre o início do atributo de um vértice e o início do atributo do próximo vértice. O argumento 0 indica que não há bytes extras entre uma posição \\((x,y)\\) e a posição \\((x,y)\\) do vértice seguinte. pointer: apesar do nome, não é um ponteiro, mas um deslocamento em bytes que informa qual é a posição do primeiro componente do atributo. Usamos nullptr, que corresponde a zero, pois não há bytes extras no início do VBO antes da primeira posição \\((x,y)\\). Observação Os parâmetros stride e pointer de glVertexAttribPointer podem ser utilizados para especificar o mapeamento de VBOs que contém dados intercalados (interleaved data). Nosso m_vboPositions não usa dados intercalados. O arranjo contém apenas posições \\((x,y)\\) em sequência. Assim, para um triângulo (três vértices), o VBO é um arranjo no formato: \\[[x\\; y\\; x\\; y\\; x\\; y],\\] onde cada grupo de \\((x, y)\\) é a posição de um vértice, e tanto \\(x\\) quanto \\(y\\) são do tipo float. Da mesma forma, m_vboColors não usa dados intercalados. Para a definição das cores dos vértices de um triângulo, o arranjo tem o formato: \\[[r\\; g\\; b\\; r\\; g\\; b\\; r\\; g\\; b],\\] onde cada grupo de \\((r,g,b)\\) define a cor de um vértice, e \\(r\\), \\(g\\) e \\(b\\) também são do tipo float. Quando os dados não são intercalados, podemos especificar 0 como argumento de stride, que é o que fizemos. Além disso, pointer também é 0. Suponha agora que os dados tenham sido intercalados em um único VBO no seguinte formato: \\[[x\\; y\\; r\\; g\\; b\\; x\\; y\\; r\\; g\\; b\\; x\\; y\\; r\\; g\\; b].\\] Agora, o atributo de posição \\((x,y)\\) tem um stride que corresponde à quantidade de bytes contida em \\((x,y,r,g,b)\\). Esse valor é 20 se cada float tiver 4 bytes (5*4=20 bytes). pointer continua sendo 0, pois não há deslocamento no início do arranjo. O atributo de cor \\((r,g,b)\\) também tem um stride de 20 bytes. Entretanto, pointer precisa ser 8, pois \\(x\\) e \\(y\\) formam 8 bytes antes do início do primeiro grupo de \\((r,g,b)\\). Suponha agora um único VBO no formato a seguir: \\[[x\\; y\\; x\\; y\\; x\\; y\\; r\\; g\\; b\\; r\\; g\\; b\\; r\\; g\\; b].\\] O stride da posição pode ser 0, pois após um grupo de \\((x,y)\\) há imediatamente outro \\((x,y)\\)23. O stride da cor também pode ser 0 pelo mesmo raciocínio. Entretanto, o pointer para o atributo de cor precisa ser 24 (8*3=24 bytes), pois o primeiro grupo de \\((r,g,b)\\) ocorre apenas depois de três grupos de \\((x,y)\\). Com todas essas opções de formatação de VBOs, não há uma forma mais certa ou mais recomendada de organizar os dados. É possível que algum driver use algum formato de forma mais eficiente, mas isso só pode ser determinado através de medição de tempo. Na prática, use o formato que melhor fizer sentido para o caso de uso. Para simplificar, fizemos as contas supondo 4 bytes por float, mas lembre-se sempre de usar sizeof(float) pois o tamanho de um float pode variar dependendo da arquitetura. O código completo de openglwindow.cpp é mostrado a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &quot;abcg.hpp&quot; void OpenGLWindow::initializeGL() { const auto *vertexShader{R&quot;gl( #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } )gl&quot;}; const auto *fragmentShader{R&quot;gl( #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; // Create shader program m_program = createProgramFromString(vertexShader, fragmentShader); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); } void OpenGLWindow::paintGL() { // Check whether to render the next polygon if (m_elapsedTimer.elapsed() &lt; m_delay / 1000.0) return; m_elapsedTimer.restart(); // Create a regular polygon with a number of sides in the range [3,20] std::uniform_int_distribution&lt;int&gt; intDist(3, 20); const auto sides{intDist(m_randomEngine)}; setupModel(sides); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); // Choose a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution&lt;float&gt; rd1(-1.0f, 1.0f); const glm::vec2 translation{rd1(m_randomEngine), rd1(m_randomEngine)}; const GLint translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Choose a random scale factor (1% to 25%) std::uniform_real_distribution&lt;float&gt; rd2(0.01f, 0.25f); const auto scale{rd2(m_randomEngine)}; const GLint scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); // Render abcg::glBindVertexArray(m_vao); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { const auto widgetSize{ImVec2(200, 72)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, m_viewportHeight - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); const auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); ImGui::PushItemWidth(140); ImGui::SliderInt(&quot;Delay&quot;, &amp;m_delay, 0, 200, &quot;%d ms&quot;); ImGui::PopItemWidth(); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(-1, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); } void OpenGLWindow::setupModel(int sides) { // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_vboPositions); abcg::glDeleteBuffers(1, &amp;m_vboColors); abcg::glDeleteVertexArrays(1, &amp;m_vao); // Select random colors for the radial gradient std::uniform_real_distribution&lt;float&gt; rd(0.0f, 1.0f); const glm::vec3 color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; const glm::vec3 color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions(0); std::vector&lt;glm::vec3&gt; colors(0); // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices const auto step{M_PI * 2 / sides}; for (const auto angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vboPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_vboColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec3), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program const auto positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; const auto colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vboColors); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } O código completo do projeto pode ser baixado deste link. Agora que vimos como usar variáveis uniformes para fazer transformações geométricas no vertex shader e como organizar os dados de um VBO de diferentes maneiras, vamos ao jogo! Como veremos posteriomente, é possível reduzir esse número com o uso de geometria indexada, mas ainda assim o consumo de memória seria alto para este caso. Na verdade, o stride nesse caso é de 8 bytes, mas o argumento 0 serve para indicar que os atributos estão agrupados de forma apertada. "],["asteroids.html", "5.2 Asteroids", " 5.2 Asteroids A cena de nosso Asteroids será composta pelos seguintes objetos: Uma nave espacial, formada por GL_TRIANGLES; Asteroides, formados por GL_TRIANGLE_FAN; Tiros, formados por GL_TRIANGLE_FAN; Estrelas de fundo, formadas por GL_POINTS. Como nas aplicações feitas até agora, trabalharemos somente com gráficos 2D. As coordenadas de todos os objetos do jogo serão especificadas no chamado NDC (espaço normalizado do dispositivo). Como vimos na seção 4.3, para que as primitivas sejam renderizadas, as coordenadas em NDC devem estar dentro do volume de visão canônico, que é um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Também vimos que coordenadas em NDC são mapeadas para o espaço da janela, de modo que o ponto \\((-1,-1)\\) é mapeado para o canto inferior esquerdo do viewport, e \\((1,1)\\) é mapeado para o canto superior direito, de acordo com o especificado em glViewport. A figura 5.5 ilustra o posicionamento de objetos da cena recortados pela região visível do NDC. Figura 5.5: Objetos de cena na região visível do NDC. No jogo Asteroids original, a nave se movimenta pela tela enquanto a câmera virtual permanece fixa. Quando a nave sai dos limites da tela, reaparece no lado oposto. No nosso projeto, a nave se manterá fixa no centro da tela, enquanto todo o resto se moverá ao seu redor. O espaço será finito como no Asteroids original, e terá o tamanho da região que vai de \\((-1,-1)\\) a \\((1,1)\\). Se um asteroide sair do lado esquerdo da tela, reaparecerá no lado direito (observe que isso acontece na figura 5.5). Um truque simples para obter o efeito de replicação do espaço é renderizar a cena nove vezes, uma vez para célula de uma grade 3x3 na qual apenas a célula do meio corresponde à região de \\((-1,-1)\\) a \\((1,1)\\). Isso é ilustrado na figura 5.6: Figura 5.6: Replicando a cena em torno da região visível do NDC. Não é necessário replicar os objetos que não saem da tela, como a nave. No nosso caso, os tiros também não serão replicados e deixarão de existir assim que saírem da tela. Embora esse truque de replicação de cena funcione bem para este jogo simples, em cenas mais complexas é recomendável fazer testes de proximidade para descartar os objetos que estão fora da área visível. Isso evita processamento desnecessário no pipeline gráfico. Organização do projeto Nosso jogo possui vários objetos de cena, e portanto possui vários VBOs, VAOs e variáveis de propriedades desses objetos. Precisamos pensar bem em como organizar tudo isso. O código pode ficar bastante confuso se definirmos tudo na classe OpenGLWindow como fizemos nos projetos anteriores. Classes Para organizar melhor o projeto, separaremos os elementos de cena do jogo nas seguintes classes: Ship: classe que representa a nave espacial (VAO, VBO e atributos como translação, orientação e velocidade). StarLayers: classe que gerencia as camadas de estrelas usadas para fazer o efeito de paralaxe24 de fundo. StarLayers contém um arranjo de objetos do tipo StarLayer, sendo que cada StarLayer define o VBO de pontos de uma camada de estrelas. Bullets: classe que gerencia os tiros. A classe contém uma lista de instâncias de uma estrutura Bullet, sendo que cada Bullet representa as propriedades de um tiro (translação, velocidade, etc). Todos os tiros compartilham um mesmo VBO definido em Bullets. Asteroids: classe que gerencia os asteroides. Asteroids contém uma lista de instâncias de uma estrutura Asteroid, sendo que cada Asteroid define o VBO e propriedades de um asteroide. As classes Ship, StarLayers, Bullets e Asteroids contêm suas próprias funções membro initializeGL, paintGL e terminateGL que serão chamadas nas funções membro respectivas de OpenGLWindow. Definiremos também uma classe GameData para permitir o compartilhamento de dados de estado do jogo entre OpenGLWindow e as outras classes. Arquivos O diretório de projeto abcg/examples/asteroids terá a seguinte estrutura: asteroids/  asteroids.cpp  asteroids.hpp  bullets.cpp  bullets.hpp  CMakeLists.txt  gamedata.hpp  main.cpp  openglwindow.hpp  openglwindow.cpp  ship.cpp  ship.hpp  starlayers.cpp  starlayers.hpp  assets/  Inconsolata-Medium.ttf  objects.frag  objects.vert  stars.frag  stars.vert O subdiretório assets contém arquivos de recursos utilizados no jogo: O arquivo Inconsolata-Medium.ttf é a fonte Inconsolata utilizada na mensagem Game Over e You Win. O arquivo pode ser baixado ou copiado de abcg/abcg/assets (ou substitua por sua fonte favorita!). Os arquivos stars.vert e stars.frag contêm o código-fonte do vertex shader e fragment shader utilizados para renderizar as estrelas. Os arquivos objects.vert e objects.frag contêm o código-fonte do vertex shader e fragment shader utilizados em todos os outros objetos: nave, asteroides e tiros. Poderíamos continuar definindo os shaders através de strings, mas o projeto fica mais organizado desta nova forma. Importante Sempre que um projeto da ABCg é configurado pelo CMake, o diretório assets (se existir) é copiado para build/bin/proj, onde proj é o nome do projeto. Em todas as vezes que um arquivo de assets for modificado, é necessário limpar o diretório build para forçar a cópia de assets para build/bin/proj na próxima compilação. Isso pode ser feito das seguintes maneiras: Removendo o diretório build antes da compilação; No Visual Studio Code, usando o comando CMake: Clean Rebuild da paleta de comandos (Ctrl+Shift+P) antes da compilação; Construindo o projeto através de build.sh/build.bat. Se você precisar editar um shader várias vezes, deixe-o como uma string como fizemos nos projetos anteriores. Transforme-o em um asset apenas quando o shader estiver pronto e não for mais editado. Observação Quando o projeto é compilado para WebAssembly, o conteúdo de assets é transformado em um arquivo .data no diretório public. Assim, os arquivos resultantes de um projeto chamado proj serão: proj.data: arquivo de recursos (assets); proj.js: arquivo JavaScript que deve ser chamado pelo html; proj.wasm: binário WebAssembly. Configuração inicial Em abcg/examples, crie o subdiretório asteroids. No arquivo abcg/examples/CMakeLists.txt, inclua a linha add_subdirectory(asteroids). Crie o arquivo abcg/examples/asteroids/CMakeLists.txt com o seguinte conteúdo: project(asteroids) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp asteroids.cpp bullets.cpp ship.cpp starlayers.cpp) enable_abcg(${PROJECT_NAME}) Crie todos os arquivos .cpp e .hpp (de asteroids.cpp até starlayers.cpp). Por enquanto os arquivos ficarão vazios. Crie o subdiretório assets e baixe/copie a fonte .ttf. Crie também os arquivos .frag e .vert. Vamos editá-los em seguida. main.cpp Não há nada de realmente novo no conteúdo de main.cpp. Apenas desativaremos o contador de FPS e o botão de tela cheia. O código ficará assim: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings({.samples = 4}); window-&gt;setWindowSettings({.width = 600, .height = 600, .showFPS = false, .showFullscreenButton = false, .title = &quot;Asteroids&quot;}); app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } gamedata.hpp Neste arquivo definiremos uma estrutura GameData que descreve o estado atual do jogo e o estado dos dispositivos de entrada: #ifndef GAMEDATA_HPP_ #define GAMEDATA_HPP_ #include &lt;bitset&gt; enum class Input { Right, Left, Down, Up, Fire }; enum class State { Playing, GameOver, Win }; struct GameData { State m_state{State::Playing}; std::bitset&lt;5&gt; m_input; // [fire, up, down, left, right] }; #endif m_state pode ser: State::Playing: quando a aplicação está em modo de jogo, com a nave respondendo aos comandos do jogador; State::GameOver: quando o jogador perdeu. Nesse caso a nave não é exibida e não responde aos comandos do jogador; State::Win: quando o jogador ganhou. A nave também não é exibida nesse estado. m_input é uma máscara de bits de eventos de estado dos dispositivos de entrada. Por exemplo, o bit 0 corresponde a Input::Right e está setado enquando o usuário pressiona a seta para a direita, ou a tecla D. Esse estado é atualizado pela função membro OpenGLWindow::handleEvent que veremos adiante. A classe OpenGLWindow manterá uma instância de GameData que será compartilhada com outras classes (Ship, Bullets, Asteroids, etc) sempre que elas precisarem ler ou modificar o estado do jogo. objects.vert Esse é o shader utilizado na renderização da nave, asteroides e tiros. O conteúdo será como a seguir: #version 410 layout(location = 0) in vec2 inPosition; uniform vec4 color; uniform float rotation; uniform float scale; uniform vec2 translation; out vec4 fragColor; void main() { float sinAngle = sin(rotation); float cosAngle = cos(rotation); vec2 rotated = vec2(inPosition.x * cosAngle - inPosition.y * sinAngle, inPosition.x * sinAngle + inPosition.y * cosAngle); vec2 newPosition = rotated * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = color; } Observe que os vértices só possuem um atributo inPosition do tipo vec2. Esse atributo corresponde à posição \\((x,y)\\) do vértice. A saída do vertex shader é uma cor RGBA definida pela variável uniforme color. Isso significa que, usando esse shader, todos os vértices terão a mesma cor. O código de main é similar ao do vertex shader do projeto regularpolygons, mas dessa vez a posição é modificada não apenas por um fator de escala e translação, mas também por uma rotação. As linhas 13 a 16 fazem com que a posição inPosition seja rodada pelo ângulo rotation (em radianos) no sentido anti-horário. O resultado é uma nova posição rotated que é então transformada pela escala e translação. Em capítulos futuros, veremos a teoria das transformações geométricas e os passos necessários para se chegar à expressão das linhas 15 e 16. Observação Todos os objetos do jogo são desenhados em tons de cinza, mas não há nada nos shaders que impeça que utilizemos cores. O aspecto preto e branco do jogo é só uma escolha artística para lembrar o antigo Asteroids do arcade. objects.frag O conteúdo desse fragment shader que acompanha objects.vert é o mesmo dos projetos anteriores. A cor de entrada é copiada para a cor de saída: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } Nave Para simplificar, faremos primeiramente o código para renderizar e animar a nave. Em seguida incluiremos o código das estrelas, asteroides, e por fim os tiros e a detecção de colisões. openglwindow.hpp O conteúdo de openglwindow.hpp ficará como a seguir: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;imgui.h&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; #include &quot;asteroids.hpp&quot; #include &quot;bullets.hpp&quot; #include &quot;ship.hpp&quot; #include &quot;starlayers.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void handleEvent(SDL_Event&amp; event) override; void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_objectsProgram{}; int m_viewportWidth{}; int m_viewportHeight{}; GameData m_gameData; Ship m_ship; abcg::ElapsedTimer m_restartWaitTimer; ImFont* m_font{}; std::default_random_engine m_randomEngine; void restart(); void update(); }; #endif m_objectsProgram é o identificador do par de shaders objects.vert e objects.frag. m_gameData é a instância de GameData com o estado do jogo e dos dispositivos de entrada. m_ship é a instância da classe Ship que gerencia a nave. m_restartWaitTimer é um temporizador utilizado para fazer com que o jogo seja reiniciado em cinco segundos após o fim de jogo. m_font representa a fonte que será utilizada pela ImGui para escrever Game Over e Win na tela. Observe que há uma nova função membro handleEvent que substitui uma função virtual da classe base. handleEvent é chamada pela ABCg sempre que ocorre algum evento da SDL, incluindo os eventos dos dispositivos de entrada. Neste projeto, vamos usar handleEvent para atualizar o bitset m_input de m_gameData. Na linha 39 há a declaração de uma função membro restart. Essa função será chamada sempre que quisermos reiniciar o estado do jogo. Na linha 40 é declarada a função membro update. Essa função será chamada em paintGL antes de cada renderização e atualizará a posição dos objetos da cena para torná-la animada. openglwindow.cpp Em openglwindow.cpp, começaremos com a definição de OpenGLWindow::handleEvent: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &quot;abcg.hpp&quot; void OpenGLWindow::handleEvent(SDL_Event &amp;event) { // Keyboard events if (event.type == SDL_KEYDOWN) { if (event.key.keysym.sym == SDLK_SPACE) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Fire)); if (event.key.keysym.sym == SDLK_UP || event.key.keysym.sym == SDLK_w) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Up)); if (event.key.keysym.sym == SDLK_DOWN || event.key.keysym.sym == SDLK_s) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Down)); if (event.key.keysym.sym == SDLK_LEFT || event.key.keysym.sym == SDLK_a) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Left)); if (event.key.keysym.sym == SDLK_RIGHT || event.key.keysym.sym == SDLK_d) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Right)); } if (event.type == SDL_KEYUP) { if (event.key.keysym.sym == SDLK_SPACE) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Fire)); if (event.key.keysym.sym == SDLK_UP || event.key.keysym.sym == SDLK_w) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Up)); if (event.key.keysym.sym == SDLK_DOWN || event.key.keysym.sym == SDLK_s) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Down)); if (event.key.keysym.sym == SDLK_LEFT || event.key.keysym.sym == SDLK_a) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Left)); if (event.key.keysym.sym == SDLK_RIGHT || event.key.keysym.sym == SDLK_d) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Right)); } // Mouse events if (event.type == SDL_MOUSEBUTTONDOWN) { if (event.button.button == SDL_BUTTON_LEFT) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Fire)); if (event.button.button == SDL_BUTTON_RIGHT) m_gameData.m_input.set(static_cast&lt;size_t&gt;(Input::Up)); } if (event.type == SDL_MOUSEBUTTONUP) { if (event.button.button == SDL_BUTTON_LEFT) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Fire)); if (event.button.button == SDL_BUTTON_RIGHT) m_gameData.m_input.reset(static_cast&lt;size_t&gt;(Input::Up)); } if (event.type == SDL_MOUSEMOTION) { glm::ivec2 mousePosition; SDL_GetMouseState(&amp;mousePosition.x, &amp;mousePosition.y); glm::vec2 direction{glm::vec2{mousePosition.x - m_viewportWidth / 2, mousePosition.y - m_viewportHeight / 2}}; direction.y = -direction.y; m_ship.setRotation(std::atan2(direction.y, direction.x) - M_PI_2); } } Os dados do evento são descritos pelo parâmetro event que é uma estrutura SDL_Event da SDL. Os eventos do teclado são divididos em eventos de pressionamento de teclas (SDL_KEYDOWN), tratados nas linhas 9 a 20, e liberação de teclas (SDL_KEYUP), nas linhas 21 a 32. De acordo com as teclas pressionadas/liberadas, os bits de m_gameData.m_input são setados/resetados. Os eventos do mouse são divididos em pressionamento (SDL_MOUSEBUTTONDOWN) e liberação (SDL_MOUSEBUTTONUP) dos botões, e movimentação do mouse (SDL_MOUSEMOTION). Quando ocorre a movimentação do mouse, as coordenadas \\((x,y)\\) do cursor são lidas com SDL_GetMouseState (linha 49) e convertidas para um vetor direction (linhas 51 e 52) definido pelo ponto que sai do centro da janela e vai até a posição do cursor. A coordenada \\(y\\) é invertida na linha 53 porque, no sistema de coordenadas do mouse, o eixo \\(y\\) é positivo para baixo, enquanto que no OpenGL é positivo para cima. Na linha 54, o ângulo de rotação da nave é definido como o ângulo subentendido pelo vetor direction. A função atan2 retorna o ângulo entre o vetor direction e o eixo \\(x\\) positivo. Desse valor é subtraído \\(\\pi/2\\) (90 graus) pois a orientação inicial da nave já é de 90 graus (a nave está olhando na direção do eixo \\(y\\) e não na direção do eixo \\(x\\)). Vamos agora à definição de OpenGLWindow::initializeGL: void OpenGLWindow::initializeGL() { // Load a new font ImGuiIO &amp;io{ImGui::GetIO()}; auto filename{getAssetsPath() + &quot;Inconsolata-Medium.ttf&quot;}; m_font = io.Fonts-&gt;AddFontFromFileTTF(filename.c_str(), 60.0f); if (m_font == nullptr) { throw abcg::Exception{abcg::Exception::Runtime(&quot;Cannot load font file&quot;)}; } // Create program to render the other objects m_objectsProgram = createProgramFromFile(getAssetsPath() + &quot;objects.vert&quot;, getAssetsPath() + &quot;objects.frag&quot;); abcg::glClearColor(0, 0, 0, 1); #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); #endif // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); restart(); } Nas linhas 59 a 65 é carregada a fonte TrueType da pasta assets. O tamanho da fonte é definido como 60.0f na chamada à função AddFontFromFileTTF da ImGui. Internamente, a ImGui renderiza cada letra em uma textura que pode ser utilizada posteriormente em paintUI para produzir texto com essa fonte e tamanho. Esses dados ficam armazenados em m_font. Na linha 68 é chamada a função createProgramFromFile da ABCg para compilar e ligar os shaders objects.vert e objects.frag. O restante do código de OpenGLWindow::initializeGL contém funções que já usamos em projetos anteriores. Na linha 81 é chamada a função membro restart que reinicia o estado do jogo. A definição de OpenGLWindow::restart ficará como a seguir: void OpenGLWindow::restart() { m_gameData.m_state = State::Playing; m_ship.initializeGL(m_objectsProgram); } A função membro OpenGLWindow::update será definida como segue: void OpenGLWindow::update() { float deltaTime{static_cast&lt;float&gt;(getDeltaTime())}; // Wait 5 seconds before restarting if (m_gameData.m_state != State::Playing &amp;&amp; m_restartWaitTimer.elapsed() &gt; 5) { restart(); return; } m_ship.update(m_gameData, deltaTime); } Na linha 91, getDeltaTime é uma função membro de abcg::OpenGLWindow que retorna o tempo que se passou, em segundos, desde a última chamada a paintGL (é o delta de tempo entre os quadros de exibição). Na linha 100, esse tempo é passado para a função update da nave, junto com o estado do jogo em m_gameData para atualizar a animação da nave. Importante O valor retornado por abcg::OpenGLWindow::getDeltaTime deve ser utilizado para fazer com que a animação dos objetos seja sincronizada pelo tempo e não pela velocidade do computador. Suponha que queremos animar o deslocamento de um asteroide, de x = 0 para a coordenada x = 10 em dois segundos. Para fazer isso, poderíamos incluir a seguinte expressão em update: x += (10.0 / 2.0) * getDeltaTime(); Isso faz com que x cresça a uma taxa de 5 unidades por segundo, independentemente da velocidade do computador. Em um computador lento, que renderiza a uma taxa de, digamos, 10 FPS, getDeltaTime retornará 0.1 a cada quadro. Em um computador com GPU mais rápida, que renderiza a uma taxa de 100 FPS, getDeltaTime retornará 0.01 a cada quadro. Nos dois casos, a integração de getDeltaTime durante um segundo será exatamante 1. Em resumo, sempre utilize getDeltaTime para atualizar os parâmetros de uma animação. Nunca suponha que a velocidade de renderização será fixa. A definição de OpenGLWindow::paintGL ficará assim: void OpenGLWindow::paintGL() { update(); abcg::glClear(GL_COLOR_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); m_ship.paintGL(m_gameData); } Observe que a função update é chamada logo no início. Em seguida o buffer de cor é limpo com glClear e a nave é renderizada usando a função paintGL que implementaremos em Ship. Em OpenGLWindow::paintUI, mostraremos o texto de Game Over e Win de acordo com o estado do jogo: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { const auto size{ImVec2(300, 85)}; const auto position{ImVec2((m_viewportWidth - size.x) / 2.0f, (m_viewportHeight - size.y) / 2.0f)}; ImGui::SetNextWindowPos(position); ImGui::SetNextWindowSize(size); ImGuiWindowFlags flags{ImGuiWindowFlags_NoBackground | ImGuiWindowFlags_NoTitleBar | ImGuiWindowFlags_NoInputs}; ImGui::Begin(&quot; &quot;, nullptr, flags); ImGui::PushFont(m_font); if (m_gameData.m_state == State::GameOver) { ImGui::Text(&quot;Game Over!&quot;); } else if (m_gameData.m_state == State::Win) { ImGui::Text(&quot;*You Win!*&quot;); } ImGui::PopFont(); ImGui::End(); } } Nas linhas 121 a 123, definimos os flags da janela da ImGui, usando ImGuiWindowFlags_NoBackground para a janela ficar invisível e ImGuiWindowFlags_NoInputs para não receber entrada do teclado. O conteúdo da janela continua visível, de modo que ImGui::Text mostrada o texto sobreposto na tela. Todo o texto exibido entre ImGui::PushFont(m_font) (linha 125) e ImGui::PopFont() (linha 133), usa a fonte m_font, que é a fonte Inconsolata de tamanho 60 criada em OpenGLWindow::initializeGL. As funções membro OpenGLWindow::resizeGL e OpenGLWindow::terminateGL ficarão como a seguir: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; abcg::glClear(GL_COLOR_BUFFER_BIT); } void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_objectsProgram); m_ship.terminateGL(); } Não há nada de novo em OpenGLWindow::resizeGL. É o mesmo código já utilizado em outros projetos. Em OpenGLWindow::terminateGL, m_objectsProgram é liberado e a função membro terminateGL de m_ship é chamada para liberar os recursos da nave. ship.h A definição da classe Ship ficará como a seguir: #ifndef SHIP_HPP_ #define SHIP_HPP_ #include &quot;abcg.hpp&quot; #include &quot;gamedata.hpp&quot; class Asteroids; class Bullets; class OpenGLWindow; class StarLayers; class Ship { public: void initializeGL(GLuint program); void paintGL(const GameData &amp;gameData); void terminateGL(); void update(const GameData &amp;gameData, float deltaTime); void setRotation(float rotation) { m_rotation = rotation; } private: friend Asteroids; friend Bullets; friend OpenGLWindow; friend StarLayers; GLuint m_program{}; GLint m_translationLoc{}; GLint m_colorLoc{}; GLint m_scaleLoc{}; GLint m_rotationLoc{}; GLuint m_vao{}; GLuint m_vbo{}; GLuint m_ebo{}; glm::vec4 m_color{1}; float m_rotation{}; float m_scale{0.125f}; glm::vec2 m_translation{glm::vec2(0)}; glm::vec2 m_velocity{glm::vec2(0)}; abcg::ElapsedTimer m_trailBlinkTimer; abcg::ElapsedTimer m_bulletCoolDownTimer; }; #endif Nas linhas 7 a 10 há declarações antecipadas de classes25 que são especificadas como friend de Ship (linhas 22 a 25), isto é, elas podem acessar membros privados de Ship. As variáveis nas linhas 28 a 31 receberão o resultado de glGetUniformLocation para as variáveis uniformes do vertex shader. Isso será feito em Ship::initializeGL. As propriedades da nave são definidas nas linhas 37 a 38: cor, ângulo de rotação, fator de escala, translação e vetor de velocidade. Os temporizadores definidos nas linhas 43 e 44 são utilizados para controlar o tempo de piscagem do fogo do foguete (quando a nave está acelerando), e o tempo de espera entre um tiro e outro. ship.cpp Aqui definiremos Ship::initializeGL, Ship::paintGL, Ship::terminateGL e Ship::update. A definição de Ship::initializeGL contém o código que cria o VBO e VAO da nave: #include &quot;ship.hpp&quot; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; #include &lt;glm/gtx/rotate_vector.hpp&gt; void Ship::initializeGL(GLuint program) { terminateGL(); m_program = program; m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); m_rotation = 0.0f; m_translation = glm::vec2(0); m_velocity = glm::vec2(0); std::array&lt;glm::vec2, 24&gt; positions{ // Ship body glm::vec2{-02.5f, +12.5f}, glm::vec2{-15.5f, +02.5f}, glm::vec2{-15.5f, -12.5f}, glm::vec2{-09.5f, -07.5f}, glm::vec2{-03.5f, -12.5f}, glm::vec2{+03.5f, -12.5f}, glm::vec2{+09.5f, -07.5f}, glm::vec2{+15.5f, -12.5f}, glm::vec2{+15.5f, +02.5f}, glm::vec2{+02.5f, +12.5f}, // Cannon left glm::vec2{-12.5f, +10.5f}, glm::vec2{-12.5f, +04.0f}, glm::vec2{-09.5f, +04.0f}, glm::vec2{-09.5f, +10.5f}, // Cannon right glm::vec2{+09.5f, +10.5f}, glm::vec2{+09.5f, +04.0f}, glm::vec2{+12.5f, +04.0f}, glm::vec2{+12.5f, +10.5f}, // Thruster trail (left) glm::vec2{-12.0f, -07.5f}, glm::vec2{-09.5f, -18.0f}, glm::vec2{-07.0f, -07.5f}, // Thruster trail (right) glm::vec2{+07.0f, -07.5f}, glm::vec2{+09.5f, -18.0f}, glm::vec2{+12.0f, -07.5f}, }; // Normalize for (auto &amp;position : positions) { position /= glm::vec2{15.5f, 15.5f}; } const std::array indices{0, 1, 3, 1, 2, 3, 0, 3, 4, 0, 4, 5, 9, 0, 5, 9, 5, 6, 9, 6, 8, 8, 6, 7, // Cannons 10, 11, 12, 10, 12, 13, 14, 15, 16, 14, 16, 17, // Thruster trails 18, 19, 20, 21, 22, 23}; // Generate VBO abcg::glGenBuffers(1, &amp;m_vbo); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vbo); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate EBO abcg::glGenBuffers(1, &amp;m_ebo); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_ebo); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vbo); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_ebo); // End of binding to current VAO abcg::glBindVertexArray(0); } Observe que terminateGL é chamada logo no início da função para liberar o VBO e VAO anterior, caso exista. Nas linhas 10 a 13, a localização das variáveis uniformes é salva para ser utilizada posteriormente na renderização. O arranjo positions (linhas 19 a 44) contém a posição dos vértices da nave. São 24 vértices (de 0 a 23), como mostra a figura 5.7. Esses vértices são ligados para formar 14 triângulos. A imagem foi dividida em duas para facilitar a visualização, pois há sobreposição de triângulos: Os vértices de 0 a 9 definem o corpo da nave; Os vértices de índices 10 a 17 definem os canhões de tiro; Os vértices de 18 a 23 definem dois triângulos que representam o fogo dos foguetes. Esses dois triângulos só serão desenhados quando a nave estiver acelerando. Figura 5.7: Geometria da nave. As coordenadas \\(x\\) em positions estão no intervalo \\([-15.5, 15.5]\\). Nas linhas 46 a 49, esse intervalo é mapeado para \\([-1,1]\\) de modo que a nave fique dentro da região visível do viewport. Após a normalização, a nave já poderá ser vista no viewport, mas ainda está muito grande. Para chegar ao tamanho final, a escala é ajustada no shader usando m_scale, que é 0.125 (valor definido na linha 39 de ship.h). Na figura 5.7, todos os vértices ligados por linhas pontilhadas fazem parte de mais de um triângulo. Por exemplo, o vértice de índice 0 é compartilhado por quatro triângulos. Para evitar ter de criar quatro vértices na mesma posição, um para cada triângulo, vamos usar o conceito de geometria indexada. Nas linhas 51 a 56 é definido um arranjo de índices aos vértices. Cada grupo de três índices define um triângulo (compare com a figura 5.7). O VBO que contém os dados de positions é criado nas linhas 68 a 73. Nas linhas 75 a 80 é criado um element buffer object (EBO), que é o buffer de índices do VBO. O EBO armazena os dados de indices. Quando o comando de renderização for chamado com GL_TRIANGLES, cada sequência de três índices do EBO será utilizada para criar um triângulo. No restante do código de initializeGL, o VAO é criado para salvar a vinculação do VBO e o mapeamento do VBO para o atributo de entrada do vertex shader. Note que o VAO também salva a vinculação do EBO na linha 97. Quando o VAO for utilizado no paintGL, toda a configuração entre as linhas 89 a 100 do initializeGL será aplicada ao pipeline. O código de Ship::paintGL ficará como a seguir: void Ship::paintGL(const GameData &amp;gameData) { if (gameData.m_state != State::Playing) return; abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glUniform1f(m_scaleLoc, m_scale); abcg::glUniform1f(m_rotationLoc, m_rotation); abcg::glUniform2fv(m_translationLoc, 1, &amp;m_translation.x); // Restart thruster blink timer every 100 ms if (m_trailBlinkTimer.elapsed() &gt; 100.0 / 1000.0) m_trailBlinkTimer.restart(); if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Up)]) { // Show thruster trail during 50 ms if (m_trailBlinkTimer.elapsed() &lt; 50.0 / 1000.0) { abcg::glEnable(GL_BLEND); abcg::glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); // 50% transparent abcg::glUniform4f(m_colorLoc, 1, 1, 1, 0.5f); abcg::glDrawElements(GL_TRIANGLES, 14 * 3, GL_UNSIGNED_INT, nullptr); abcg::glDisable(GL_BLEND); } } abcg::glUniform4fv(m_colorLoc, 1, &amp;m_color.r); abcg::glDrawElements(GL_TRIANGLES, 12 * 3, GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Note que paintGL retorna imediatamente caso o jogo não esteja no estado State::Playing. A escala, ângulo de rotação e translação da nave são enviadas às variáveis uniformes nas linhas 110 a 112. Na linha 115, o temporizador m_trailBlinkTimer é reiniciado a cada 100 milissegundos. Ele é utilizado quando a nave está acelerando, de modo a mostrar o rastro de fogo do foguete durante 50 milissegundos, intercalado com uma pausa também de 50 milissegundos. Para desenhar o rastro de fogo, todos os 14 triângulos da geometria da nave são exibidos usando transparência de 50% (linhas 120 a 128). Isso é necessário pois não é possível desenhar só os dois triângulos finais. Para isso precisaríamos ter de criar um outro VBO, o que daria mais trabalho. Na linha 133 é renderizado o corpo da nave sem o rastro de fogo, isto é, só são renderizados os primeiros 12 triângulos. Nessa renderização, a nave é renderizada com cor opaca, sobrepondo a renderização anterior da nave 50% transparente. Note que a função de renderização é glDrawElements. Essa função deve ser utilizada no lugar de glDrawArrays sempre que quisermos renderizar geometria indexada. A função membro OpenGLWindow::terminateGL contém apenas a liberação do VBO, EBO e VAO: void Ship::terminateGL() { abcg::glDeleteBuffers(1, &amp;m_vbo); abcg::glDeleteBuffers(1, &amp;m_ebo); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Na função membro OpenGLWindow::update é onde são atualizados os atributos de orientação e velocidade da nave de acordo com o estado de GameData::m_input: void Ship::update(const GameData &amp;gameData, float deltaTime) { // Rotate if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Left)]) m_rotation = glm::wrapAngle(m_rotation + 4.0f * deltaTime); if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Right)]) m_rotation = glm::wrapAngle(m_rotation - 4.0f * deltaTime); // Apply thrust if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Up)] &amp;&amp; gameData.m_state == State::Playing) { // Thrust in the forward vector glm::vec2 forward = glm::rotate(glm::vec2{0.0f, 1.0f}, m_rotation); m_velocity += forward * deltaTime; } } A rotação (linhas 147 a 151) é feita a uma taxa de 4 radianos por segundo (note o uso do deltaTime). A função glm::wrapAngle é utilizada para fazer com que o valor sempre fique no intervalo \\([0,2\\pi]\\). A aceleração é atualizada no if da linha 154. Na linha 157 é calculado um vetor que indica a direção para onde a nave está apontando (vetor forward). Isso é obtido aplicando uma rotação no vetor \\([0\\; 1]\\)26 usando o ângulo atual da nave. Uma vez calculado o vetor forward, ele é adicionado ao vetor de velocidade m_velocity na linha 158. Observe novamente o uso do deltaTime. Como forward é um vetor unitário, a velocidade será incrementada em uma unidade por segundo. Visualmente, a nave ficará presa no centro do viewport, mas os outros objetos da cena (asteroides, estrelas e tiros) usarão m_velocity (na verdade, -m_velocity) para serem deslocados. Se o código for construído neste momento, o resultado será como mostrado a seguir (use o link original caso queria controlar a nave pelo teclado): O código do projeto pode ser baixado deste link. Estrelas As estrelas serão desenhadas como pontos (GL_POINTS) e usarão os shaders stars.vert e stars.frag definidos a seguir. stars.vert #version 410 layout(location = 0) in vec2 inPosition; layout(location = 1) in vec3 inColor; uniform vec2 translation; uniform float pointSize; out vec4 fragColor; void main() { gl_PointSize = pointSize; gl_Position = vec4(inPosition.xy + translation, 0, 1); fragColor = vec4(inColor, 1); } Os atributos de entrada são uma posição \\((x,y)\\) (inPosition) e uma cor RGB (inColor). Em main, a cor de entrada é copiada para o atributo de saída (fragColor) como uma cor RGBA onde A é 1. A posição do ponto é deslocada por translation, e o tamanho do ponto é definido por pointSize. stars.frag #version 410 in vec4 fragColor; out vec4 outColor; void main() { float intensity = 1.0 - length(gl_PointCoord - vec2(0.5)) * 2.0; outColor = fragColor * intensity; } O processamento principal deste shader ocorre na definição da variável intensity. Para compreendermos o que está acontecendo, lembre-se primeiro que o tamanho de um ponto (gl_PointSize) é dado em pixels, e tamanhos maiores que 1 fazem com que o pipeline renderize um quadrado centralizado na posição de cada ponto. O fragment shader explora esse fato para exibir um gradiente radial no quadrado de modo a simular o formato circular de uma estrela. A variável embutida gl_PointCoord contém as coordenadas do fragmento dentro do quadrado. Na configuração padrão, \\((0,0)\\) é o canto superior esquerdo, e \\((1,1)\\) é o canto inferior direito (figura 5.8). Figura 5.8: Quadrado gerado em torno de um ponto de GL_POINTS, e coordenadas de gl_PointCoord dentro do quadrado formado. A expressão length(gl_PointCoord - vec2(0.5)) calcula a distância euclidiana até o centro do quadrado. Na direção em \\(x\\) e \\(y\\), essa distância está no intervalo \\([0,0.5]\\). A distância é convertida em uma intensidade de luz armazenada em intensity, sendo que a intensidade é máxima (1) no centro do quadrado. A cor de saída é multiplicada por essa intensidade. Se o quadrado for branco, o resultado será como o mostrado na figura 5.9). Figura 5.9: Gradiente radial produzido por stars.frag no quadrado de um ponto definido com GL_POINTS. Atualizando openglwindow.hpp Para a implementação das estrelas, precisamos definir em OpenGLWindow o identificador dos shaders m_starsProgram e a instância de StarLayers. O código atualizado ficará como a seguir: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;imgui.h&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; #include &quot;asteroids.hpp&quot; #include &quot;bullets.hpp&quot; #include &quot;ship.hpp&quot; #include &quot;starlayers.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void handleEvent(SDL_Event&amp; event) override; void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_starsProgram{}; GLuint m_objectsProgram{}; int m_viewportWidth{}; int m_viewportHeight{}; GameData m_gameData; Ship m_ship; StarLayers m_starLayers; abcg::ElapsedTimer m_restartWaitTimer; ImFont* m_font{}; std::default_random_engine m_randomEngine; void restart(); void update(); }; #endif Atualizando openglwindow.cpp Precisamos atualizar também as funções membro de OpenGLWindow: Em OpenGLWindow::initializeGL, inclua o seguinte código para compilar os novos shaders: // Create program to render the stars m_starsProgram = createProgramFromFile(getAssetsPath() + &quot;stars.vert&quot;, getAssetsPath() + &quot;stars.frag&quot;); Em OpenGLWindow::restart, inclua a chamada a initializeGL de StarLayers junto com a chamada de initializeGL de Ship: m_starLayers.initializeGL(m_starsProgram, 25); m_ship.initializeGL(m_objectsProgram); Em OpenGLWindow::update, chame a função update de StarLayers depois da chamada de update de Ship, assim: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); Em OpenGLWindow::paintGL, chame paintGL de StarLayers antes de paintGL de Ship, assim: m_starLayers.paintGL(); m_ship.paintGL(m_gameData); Por fim, modifique OpenGLWindow::terminateGL da seguinte forma: void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_starsProgram); abcg::glDeleteProgram(m_objectsProgram); m_ship.terminateGL(); m_starLayers.terminateGL(); } starlayers.hpp A definição da classe StarLayers ficará assim: #ifndef STARLAYERS_HPP_ #define STARLAYERS_HPP_ #include &lt;array&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class OpenGLWindow; class StarLayers { public: void initializeGL(GLuint program, int quantity); void paintGL(); void terminateGL(); void update(const Ship &amp;ship, float deltaTime); private: friend OpenGLWindow; GLuint m_program{}; GLint m_pointSizeLoc{}; GLint m_translationLoc{}; struct StarLayer { GLuint m_vao{}; GLuint m_vbo{}; float m_pointSize{}; int m_quantity{}; glm::vec2 m_translation{glm::vec2(0)}; }; std::array&lt;StarLayer, 5&gt; m_starLayers; std::default_random_engine m_randomEngine; }; #endif Nas linhas 28 a 35 é definida StarLayer. A estrutura contém o VBO e VAO dos pontos que formam uma camada de estrelas, o tamanho (m_pointSize) e quantidade (m_quantity) de pontos, e um fator de translação (m_translation) utilizado para deslocar todos os pontos da camada (isto é, todos os vértices do VBO). Na linha 37 é definido um arranjo de cinco instâncias de StarLayer, pois renderizaremos cinco camadas de estrelas. starlayers.cpp O arquivo começa com a definição de StarLayers::initializeGL: #include &quot;starlayers.hpp&quot; #include &lt;cppitertools/itertools.hpp&gt; void StarLayers::initializeGL(GLuint program, int quantity) { terminateGL(); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); m_program = program; m_pointSizeLoc = abcg::glGetUniformLocation(m_program, &quot;pointSize&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); auto &amp;re{m_randomEngine}; std::uniform_real_distribution&lt;float&gt; distPos(-1.0f, 1.0f); std::uniform_real_distribution&lt;float&gt; distIntensity(0.5f, 1.0f); for (auto &amp;&amp;[index, layer] : iter::enumerate(m_starLayers)) { layer.m_pointSize = 10.0f / (1.0f + index); layer.m_quantity = quantity * (static_cast&lt;int&gt;(index) + 1); layer.m_translation = glm::vec2(0); std::vector&lt;glm::vec3&gt; data(0); for ([[maybe_unused]] auto i : iter::range(0, layer.m_quantity)) { data.emplace_back(distPos(re), distPos(re), 0); data.push_back(glm::vec3(1) * distIntensity(re)); } // Generate VBO abcg::glGenBuffers(1, &amp;layer.m_vbo); abcg::glBindBuffer(GL_ARRAY_BUFFER, layer.m_vbo); abcg::glBufferData(GL_ARRAY_BUFFER, data.size() * sizeof(glm::vec3), data.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; GLint colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;layer.m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(layer.m_vao); abcg::glBindBuffer(GL_ARRAY_BUFFER, layer.m_vbo); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, sizeof(glm::vec3) * 2, nullptr); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(glm::vec3) * 2, reinterpret_cast&lt;void *&gt;(sizeof(glm::vec3))); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } } O laço da linha 20 itera sobre cada elemento de m_starLayers. A expressão na linha 21 faz com que os pontos tenham tamanho 10 na 1ª camada, 5 na 2ª camada, 2.5 na 3ª camada, e assim sucessivamente. Na linha 22, a quantidade de pontos é dobrada a cada camada. Na linha 25 é criado um arranjo data com dados dos pontos da camada. Os dados ficarão intercalados no formato \\(\\{x,y,0,r,g,b,x,y,0,r,g,b,\\dots\\}\\), onde \\((x,y,0)\\) é a posição do ponto, e \\((r,g,b)\\) é a cor do ponto. Dentro do laço, as coordenadas \\(x\\) e \\(y\\) de cada ponto são escolhidas de forma aleatória dentro do intervalo \\([-1,1]\\). A cor de cada ponto é um tom de cinza escolhido aleatoriamente do intervalo \\([0.5,1]\\). Os dados de data são copiados para o VBO através de glBufferData na linha 34. Observe nas linhas 48 a 56 como é feito o mapeamento do VBO com os atributos inPosition (do tipo vec2) e inColor (do tipo vec4) do vertex shader. O stride do VBO é sizeof(glm::vec3) * 2 (isto é, dois vec3). Na linha 55, o deslocamento no início do VBO é sizeof(glm::vec3) (isto é, apenas um vec3). O cast de tipo é necessário porque o parâmetro de deslocamento é do tipo const void * (é assim por razões históricas). A definição de StarLayers::paintGL ficará como a seguir: void StarLayers::paintGL() { abcg::glUseProgram(m_program); abcg::glEnable(GL_BLEND); abcg::glBlendFunc(GL_ONE, GL_ONE); for (const auto &amp;layer : m_starLayers) { abcg::glBindVertexArray(layer.m_vao); abcg::glUniform1f(m_pointSizeLoc, layer.m_pointSize); for (const auto i : {-2, 0, 2}) { for (const auto j : {-2, 0, 2}) { abcg::glUniform2f(m_translationLoc, layer.m_translation.x + j, layer.m_translation.y + i); abcg::glDrawArrays(GL_POINTS, 0, layer.m_quantity); } } abcg::glBindVertexArray(0); } abcg::glDisable(GL_BLEND); abcg::glUseProgram(0); } Observe que os pontos são desenhados com o modo de mistura de cor habilitado. Na linha 67, a definição da função de mistura com fatores GL_ONE faz com que as cores produzidas pelo fragment shader sejam somadas com as cores atuais do framebuffer. Isso produz um efeito cumulativo de intensidade da luz quando estrelas de camadas diferentes são renderizadas na mesma posição. Os laços aninhados nas linhas 73 e 74 produzem índices i e j que são usados em layer.m_translation para replicar o desenho das estrelas em uma grade 3x3 em torno da região visível do NDC, como vimos no início da seção. Na linha 85, o modo de mistura de cor é desabilitado para não afetar a renderização dos outros objetos de cena que são totalmente opacos. Em StarLayers::terminateGL são liberados os VBOs e VAOs de todas as instâncias de StarLayer: void StarLayers::terminateGL() { for (auto &amp;layer : m_starLayers) { abcg::glDeleteBuffers(1, &amp;layer.m_vbo); abcg::glDeleteVertexArrays(1, &amp;layer.m_vao); } } Em StarLayers::update, a translação (m_translation) de cada camada é atualizada de acordo com a velocidade da nave. Se a nave está indo para a frente, então a camada de estrelas deve ir para trás: por isso a subtração de ship.m_velocity. A velocidade é multiplicada por um fator de escala layerSpeedScale para fazer com que a primeira camada seja mais rápida que a segunda, e assim sucessivamente para produzir o efeito de paralaxe. Nas linhas 103 a 106 há uma série de condicionais que testam se os pontos saíram dos limites da região visível do NDC. Se sim, são deslocados para o lado oposto. void StarLayers::update(const Ship &amp;ship, float deltaTime) { for (auto &amp;&amp;[index, layer] : iter::enumerate(m_starLayers)) { const auto layerSpeedScale{1.0f / (index + 2.0f)}; layer.m_translation -= ship.m_velocity * deltaTime * layerSpeedScale; // Wrap-around if (layer.m_translation.x &lt; -1.0f) layer.m_translation.x += 2.0f; if (layer.m_translation.x &gt; +1.0f) layer.m_translation.x -= 2.0f; if (layer.m_translation.y &lt; -1.0f) layer.m_translation.y += 2.0f; if (layer.m_translation.y &gt; +1.0f) layer.m_translation.y -= 2.0f; } } Nesse momento, o jogo ficará como a seguir (link original): O código pode ser baixado deste link. Asteroides Para incluir a implementação dos asteroides, vamos primeiramente atualizar OpenGLWindow. Atualizando openglwindow.hpp Adicione a definição de m_asteroids junto às definições dos outros objetos (m_ship e m_starLayers), assim: Asteroids m_asteroids; Ship m_ship; StarLayers m_starLayers; Atualizando openglwindow.cpp Em OpenGLWindow::restart, chame o initializeGL de m_asteroids junto com a chamada de initializeGL dos objetos anteriores: m_starLayers.initializeGL(m_starsProgram, 25); m_ship.initializeGL(m_objectsProgram); m_asteroids.initializeGL(m_objectsProgram, 3); Em OpenGLWindow::update, chame o update de m_asteroids após o update de m_ship: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); m_asteroids.update(m_ship, deltaTime); Em OpenGLWindow::paintGL, chame o paintGL de m_asteroids logo após o paintGL de m_starLayers: m_starLayers.paintGL(); m_asteroids.paintGL(); m_ship.paintGL(m_gameData); Em OpenGLWindow::terminateGL, chame o terminateGL de m_asteroids junto com o terminateGL dos outros objetos: m_asteroids.terminateGL(); m_ship.terminateGL(); m_starLayers.terminateGL(); Observação A ordem em que a função paintGL de cada objeto é chamada é importante porque o objeto renderizado por último será desenhado sobre os anteriores que já foram desenhados antes no framebuffer. Essa forma de renderizar os objetos na ordem do mais distante para o mais próximo é chamada de algoritmo do pintor pois é similar ao modo como um pintor desenha sobre uma tela: primeiro é desenhado o fundo (elemento mais distante) e então sobre ele são desenhados os elementos mais próximos. asteroids.hpp A definição da classe Asteroids ficará assim: #ifndef ASTEROIDS_HPP_ #define ASTEROIDS_HPP_ #include &lt;list&gt; #include &lt;random&gt; #include &quot;abcg.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class OpenGLWindow; class Asteroids { public: void initializeGL(GLuint program, int quantity); void paintGL(); void terminateGL(); void update(const Ship &amp;ship, float deltaTime); private: friend OpenGLWindow; GLuint m_program{}; GLint m_colorLoc{}; GLint m_rotationLoc{}; GLint m_translationLoc{}; GLint m_scaleLoc{}; struct Asteroid { GLuint m_vao{}; GLuint m_vbo{}; float m_angularVelocity{}; glm::vec4 m_color{1}; bool m_hit{false}; int m_polygonSides{}; float m_rotation{}; float m_scale{}; glm::vec2 m_translation{glm::vec2(0)}; glm::vec2 m_velocity{glm::vec2(0)}; }; std::list&lt;Asteroid&gt; m_asteroids; std::default_random_engine m_randomEngine; std::uniform_real_distribution&lt;float&gt; m_randomDist{-1.0f, 1.0f}; Asteroids::Asteroid createAsteroid(glm::vec2 translation = glm::vec2(0), float scale = 0.25f); }; #endif Entre as linhas 30 a 42 é definida a estrutura Asteroid. Cada asteroide tem seu próprio VAO e VBO. Além disso possui uma velocidade angular, uma cor, número de lados, ângulo de rotação, escala, translação, vetor de velocidade, e um flag m_hit que indica se o asteroide foi acertado por um tiro. Na linha 44 é definida uma lista de Asteroid. O número de elementos dessa lista será modificado de acordo com os asteroides que forem acertados pelos tiros. Cada vez que um asteroide for acertado, ele será retirado da lista. Entretanto, se o asteroide for grande, simularemos que ele foi quebrado em vários pedaços e então asteroides menores serão inseridos na lista. A função membro createAsteroid declarada nas linhas 49 e 50 será utilizada para criar um novo asteroide para ser inserido na lista m_asteroids. O fator de escala (parâmetro scale) permitirá configurar o tamanho do novo asteroide. asteroids.cpp O arquivo começa com a definição de Asteroids::initializeGL: #include &quot;asteroids.hpp&quot; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; void Asteroids::initializeGL(GLuint program, int quantity) { terminateGL(); // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); m_program = program; m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); // Create asteroids m_asteroids.clear(); m_asteroids.resize(quantity); for (auto &amp;asteroid : m_asteroids) { asteroid = createAsteroid(); // Make sure the asteroid won&#39;t collide with the ship do { asteroid.m_translation = {m_randomDist(m_randomEngine), m_randomDist(m_randomEngine)}; } while (glm::length(asteroid.m_translation) &lt; 0.5f); } } Na linha 21, a lista de asteroides é iniciada com uma quantidade quantity de objetos do tipo Asteroid. Essa lista é então iterada no laço das linhas 23 a 31 e o conteúdo de cada asteroide é modificado por Asteroids::createAsteroid. No laço da linha 27 é escolhida uma posição aleatória para o asteroide, mas que esteja longe o suficiente da nave: não queremos que o jogo comece com o asteroide colidindo com a nave! A definição de Asteroids::paintGL ficará como a seguir: void Asteroids::paintGL() { abcg::glUseProgram(m_program); for (const auto &amp;asteroid : m_asteroids) { abcg::glBindVertexArray(asteroid.m_vao); abcg::glUniform4fv(m_colorLoc, 1, &amp;asteroid.m_color.r); abcg::glUniform1f(m_scaleLoc, asteroid.m_scale); abcg::glUniform1f(m_rotationLoc, asteroid.m_rotation); for (auto i : {-2, 0, 2}) { for (auto j : {-2, 0, 2}) { abcg::glUniform2f(m_translationLoc, asteroid.m_translation.x + j, asteroid.m_translation.y + i); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, asteroid.m_polygonSides + 2); } } abcg::glBindVertexArray(0); } abcg::glUseProgram(0); } A lista m_asteroids é iterada e cada asteroide é renderizado 9 vezes (em uma grade 3x3), como fizemos com as estrelas. Em Asteroids::terminateGL são liberados os VBOs e VAOs dos asteroides: void Asteroids::terminateGL() { for (auto asteroid : m_asteroids) { abcg::glDeleteBuffers(1, &amp;asteroid.m_vbo); abcg::glDeleteVertexArrays(1, &amp;asteroid.m_vao); } } Vamos agora à definição de Asteroids::update: void Asteroids::update(const Ship &amp;ship, float deltaTime) { for (auto &amp;asteroid : m_asteroids) { asteroid.m_translation -= ship.m_velocity * deltaTime; asteroid.m_rotation = glm::wrapAngle( asteroid.m_rotation + asteroid.m_angularVelocity * deltaTime); asteroid.m_translation += asteroid.m_velocity * deltaTime; // Wrap-around if (asteroid.m_translation.x &lt; -1.0f) asteroid.m_translation.x += 2.0f; if (asteroid.m_translation.x &gt; +1.0f) asteroid.m_translation.x -= 2.0f; if (asteroid.m_translation.y &lt; -1.0f) asteroid.m_translation.y += 2.0f; if (asteroid.m_translation.y &gt; +1.0f) asteroid.m_translation.y -= 2.0f; } } Na linha 68, a translação (m_translation) de cada asteroide é modificada pelo vetor de velocidade da nave, como fizemos com as estrelas. Na linha 69, a rotação é atualizada de acordo com a velocidade angular. Na linha 71, a translação do asteroide é modificada novamente, mas agora considerando a velocidade do próprio asteroide. As condicionais das linhas 74 a 77 fazem com que as coordenadas de m_translation permaneçam no intervalo circular de -1 a 1. Em Asteroids::createAsteroid é criada uma nova instância de Asteroid: Asteroids::Asteroid Asteroids::createAsteroid(glm::vec2 translation, float scale) { Asteroid asteroid; auto &amp;re{m_randomEngine}; // Shortcut // Randomly choose the number of sides std::uniform_int_distribution&lt;int&gt; randomSides(6, 20); asteroid.m_polygonSides = randomSides(re); // Choose a random color (actually, a grayscale) std::uniform_real_distribution&lt;float&gt; randomIntensity(0.5f, 1.0f); asteroid.m_color = glm::vec4(1) * randomIntensity(re); asteroid.m_color.a = 1.0f; asteroid.m_rotation = 0.0f; asteroid.m_scale = scale; asteroid.m_translation = translation; // Choose a random angular velocity asteroid.m_angularVelocity = m_randomDist(re); // Choose a random direction glm::vec2 direction{m_randomDist(re), m_randomDist(re)}; asteroid.m_velocity = glm::normalize(direction) / 7.0f; // Create geometry std::vector&lt;glm::vec2&gt; positions(0); positions.emplace_back(0, 0); const auto step{M_PI * 2 / asteroid.m_polygonSides}; std::uniform_real_distribution&lt;float&gt; randomRadius(0.8f, 1.0f); for (const auto angle : iter::range(0.0, M_PI * 2, step)) { const auto radius{randomRadius(re)}; positions.emplace_back(radius * std::cos(angle), radius * std::sin(angle)); } positions.push_back(positions.at(1)); // Generate VBO abcg::glGenBuffers(1, &amp;asteroid.m_vbo); abcg::glBindBuffer(GL_ARRAY_BUFFER, asteroid.m_vbo); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;asteroid.m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(asteroid.m_vao); abcg::glBindBuffer(GL_ARRAY_BUFFER, asteroid.m_vbo); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); return asteroid; } Na linha 101 é escolhida uma velocidade angular aleatória do intervalo \\([-1, 1]\\) (em radianos por segundo). Na linha 105 é escolhido um vetor unitário aleatório para definir a velocidade do asteroide. As componentes do vetor são divididas por 7 de modo que cada asteroide inicie com uma velocidade de 1/7 unidades de espaço por segundo. O restante do código cria a geometria do asteroide. O código é bem parecido com o que foi utilizado para criar o polígono regular no projeto regularpolygons. A diferença é que agora usamos a equação paramétrica do círculo com raio \\(r\\) \\[ \\begin{eqnarray} x&amp;=&amp;r cos(t),\\\\ y&amp;=&amp;r sin(t), \\end{eqnarray} \\] e selecionamos um \\(r\\) aleatório do intervalo \\([0.8, 1]\\) para cada vértice do polígono. Nesse momento, o jogo ficará como a seguir (link original): O código pode ser baixado deste link. Tiros e colisões Neste momento o jogo ainda não tem detecção de colisões e tiros. É isso que implementaremos agora. Atualizando openglwindow.hpp Adicione a definição de m_bullets junto à definição dos outros objetos: Asteroids m_asteroids; Bullets m_bullets; Ship m_ship; StarLayers m_starLayers; Adicione também a declaração das seguintes funções membro adicionais de OpenGLWindow: void checkCollisions(); void checkWinCondition(); checkCollisions será utilizada para verificar as colisões; checkWinCondition será utilizada para verificar se o jogador ganhou (isto é, se não há mais asteroides). Atualizando openglwindow.cpp Em OpenGLWindow::restart, inclua a chamada à função initializeGL de m_bullets junto com initializeGL dos objetos anteriores, assim: m_starLayers.initializeGL(m_starsProgram, 25); m_ship.initializeGL(m_objectsProgram); m_asteroids.initializeGL(m_objectsProgram, 3); m_bullets.initializeGL(m_objectsProgram); Em OpenGLWindow::update, chame update de m_bullets em qualquer lugar após a chamada de update de m_ship. Por exemplo: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); m_asteroids.update(m_ship, deltaTime); m_bullets.update(m_ship, m_gameData, deltaTime); Além disso, inclua a seguinte condicional após os updates para calcular as colisões e verificar a condição de vitória: if (m_gameData.m_state == State::Playing) { checkCollisions(); checkWinCondition(); } Em OpenGLWindow::paintGL, chame paintGL de m_bullets logo após a chamada de paintGL de m_asteroids: m_starLayers.paintGL(); m_asteroids.paintGL(); m_bullets.paintGL(); m_ship.paintGL(m_gameData); Em OpenGLWindow::terminateGL, chame terminateGL de m_bullets junto com terminateGL dos outros objetos: m_asteroids.terminateGL(); m_bullets.terminateGL(); m_ship.terminateGL(); m_starLayers.terminateGL(); Vamos agora definir OpenGLWindow::checkCollisions como a seguir: void OpenGLWindow::checkCollisions() { // Check collision between ship and asteroids for (const auto &amp;asteroid : m_asteroids.m_asteroids) { const auto asteroidTranslation{asteroid.m_translation}; const auto distance{ glm::distance(m_ship.m_translation, asteroidTranslation)}; if (distance &lt; m_ship.m_scale * 0.9f + asteroid.m_scale * 0.85f) { m_gameData.m_state = State::GameOver; m_restartWaitTimer.restart(); } } // Check collision between bullets and asteroids for (auto &amp;bullet : m_bullets.m_bullets) { if (bullet.m_dead) continue; for (auto &amp;asteroid : m_asteroids.m_asteroids) { for (const auto i : {-2, 0, 2}) { for (const auto j : {-2, 0, 2}) { const auto asteroidTranslation{asteroid.m_translation + glm::vec2(i, j)}; const auto distance{ glm::distance(bullet.m_translation, asteroidTranslation)}; if (distance &lt; m_bullets.m_scale + asteroid.m_scale * 0.85f) { asteroid.m_hit = true; bullet.m_dead = true; } } } } // Break asteroids marked as hit for (const auto &amp;asteroid : m_asteroids.m_asteroids) { if (asteroid.m_hit &amp;&amp; asteroid.m_scale &gt; 0.10f) { std::uniform_real_distribution&lt;float&gt; m_randomDist{-1.0f, 1.0f}; std::generate_n(std::back_inserter(m_asteroids.m_asteroids), 3, [&amp;]() { const glm::vec2 offset{m_randomDist(m_randomEngine), m_randomDist(m_randomEngine)}; return m_asteroids.createAsteroid( asteroid.m_translation + offset * asteroid.m_scale * 0.5f, asteroid.m_scale * 0.5f); }); } } m_asteroids.m_asteroids.remove_if( [](const Asteroids::Asteroid &amp;a) { return a.m_hit; }); } } Nas linhas 173 a 183 é feita a detecção de colisão entre a nave e cada asteroide: // Check collision between ship and asteroids for (auto &amp;asteroid : m_asteroids.m_asteroids) { auto asteroidTranslation{asteroid.m_translation}; auto distance{glm::distance(m_ship.m_translation, asteroidTranslation)}; if (distance &lt; m_ship.m_scale * 0.9f + asteroid.m_scale * 0.85f) { m_gameData.m_state = State::GameOver; m_restartWaitTimer.restart(); } } A detecção de colisão é feita através da comparação da distância euclidiana (glm::distance) entre as coordenadas de translação dos objetos. Essas coordenadas podem ser consideradas como a posição do centro dos objetos na cena (como ilustrado pelos pontos \\(P_s\\) e \\(P_a\\) na figura 5.10: Figura 5.10: Detecção de colisão entre nave e asteroide através da comparação de distância entre círculos. \\(P_s\\) e \\(P_a\\) também podem ser considerados como centros de círculos. O fator de escala de cada objeto corresponde ao raio do círculo (\\(r_s\\) e \\(r_a\\)). Assim, podemos detectar a colisão através de uma simples comparação da distância \\(|P_s-P_a|\\) com a soma dos fatores de escala. Só há colisão se a distância for menor ou igual a \\(r_s+r_a\\). Esse tipo de teste é bem mais simples e eficiente (embora menos preciso) do que comparar a interseção entre os triângulos que formam os objetos. Note, na linha 179, que \\(r_s\\) e \\(r_a\\) são de fato os fatores m_scale de cada objeto, mas multiplicados por 0.9f (para a nave) e 0.85f (para o asteroide). Isso é feito para diminuir um pouco o raio dos círculos e fazer com que exista uma tolerância de sobreposição antes de ocorrer a colisão. Veja, na figura 5.10, que dessa forma os objetos não ficam inscritos nos círculos. Os valores 0.9 e 0.85 foram determinados empiricamente. Nas linhas 185 a 203 é feita a detecção de colisão entre os tiros e os asteroides: // Check collision between bullets and asteroids for (auto &amp;bullet : m_bullets.m_bullets) { if (bullet.m_dead) continue; for (auto &amp;asteroid : m_asteroids.m_asteroids) { for (const auto i : {-2, 0, 2}) { for (const auto j : {-2, 0, 2}) { const auto asteroidTranslation{asteroid.m_translation + glm::vec2(i, j)}; const auto distance{ glm::distance(bullet.m_translation, asteroidTranslation)}; if (distance &lt; m_bullets.m_scale + asteroid.m_scale * 0.85f) { asteroid.m_hit = true; bullet.m_dead = true; } } } } A interseção é calculada novamente através da comparação da distância entre círculos. Note que os testes de distância são feitos dentro de laços aninhados parecidos com os que foram utilizados para replicar a renderização dos asteroides na grade 3x3 em torno da região visível do viewport. De fato, o teste de colisão de um tiro com um asteroide precisa considerar essa replicação, pois um asteroide que está saindo à esquerda do viewport pode ser atingido por um tiro no lado oposto, à direita. Se um tiro acertou um asteroide, o m_hit do asteroide e o m_dead do tiro tornam-se true. Observe agora as linhas 205 a 217: // Break asteroids marked as hit for (const auto &amp;asteroid : m_asteroids.m_asteroids) { if (asteroid.m_hit &amp;&amp; asteroid.m_scale &gt; 0.10f) { std::uniform_real_distribution&lt;float&gt; m_randomDist{-1.0f, 1.0f}; std::generate_n(std::back_inserter(m_asteroids.m_asteroids), 3, [&amp;]() { const glm::vec2 offset{m_randomDist(m_randomEngine), m_randomDist(m_randomEngine)}; return m_asteroids.createAsteroid( asteroid.m_translation + offset * asteroid.m_scale * 0.5f, asteroid.m_scale * 0.5f); }); } } Nesse código, os asteroides com m_hit == true são testados para verificar se são suficientemente grandes (m_scale &gt; 0.10f). Se sim, três novos asteroides menores são criados e inseridos na lista m_asteroids.m_asteroids. Nas linhas 219 a 220, os asteroides que estavam com m_hit == true são removidos da lista: m_asteroids.m_asteroids.remove_if( [](const Asteroids::Asteroid &amp;a) { return a.m_hit; }); Isso é tudo para a detecção de colisão. Vamos agora à definição de OpenGLWindow::checkWinCondition, que ficará como a seguir: void OpenGLWindow::checkWinCondition() { if (m_asteroids.m_asteroids.empty()) { m_gameData.m_state = State::Win; m_restartWaitTimer.restart(); } } A vitória ocorre quando a lista de asteroides está vazia. Nesse caso, o estado do jogo é modificado para State::Win e o temporizador m_restartWaitTimer é reiniciado. Como resultado, o jogo será reiniciado após cinco segundos (essa verificação é feita em OpenGLWindow::update). Enquanto isso, paintUI exibirá o texto de vitória. bullets.hpp A definição da classe Bullets ficará como a seguir: #ifndef BULLETS_HPP_ #define BULLETS_HPP_ #include &lt;list&gt; #include &quot;abcg.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class OpenGLWindow; class Bullets { public: void initializeGL(GLuint program); void paintGL(); void terminateGL(); void update(Ship &amp;ship, const GameData &amp;gameData, float deltaTime); private: friend OpenGLWindow; GLuint m_program{}; GLint m_colorLoc{}; GLint m_rotationLoc{}; GLint m_translationLoc{}; GLint m_scaleLoc{}; GLuint m_vao{}; GLuint m_vbo{}; struct Bullet { bool m_dead{}; glm::vec2 m_translation{glm::vec2(0)}; glm::vec2 m_velocity{glm::vec2(0)}; }; float m_scale{0.015f}; std::list&lt;Bullet&gt; m_bullets; }; #endif Nas linhas 32 a 36 é definida a estrutura Bullet. Observe que o VAO e VBO não está em Bullet, mas em Bullets, pois todos os tiros utilizarão o mesmo VBO. Na linha 38 é definido o fator de escala de cada tiro, e na linha 40 é definida a lista de tiros atualmente na cena. O número de elementos da lista será alterado de acordo com a quantidade de tiros visíveis. bullets.cpp O arquivo começa com a definição de Bullets::initializeGL: #include &quot;bullets.hpp&quot; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/rotate_vector.hpp&gt; void Bullets::initializeGL(GLuint program) { terminateGL(); m_program = program; m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); m_bullets.clear(); // Create regular polygon const auto sides{10}; std::vector&lt;glm::vec2&gt; positions(0); positions.emplace_back(0, 0); const auto step{M_PI * 2 / sides}; for (const auto angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); } positions.push_back(positions.at(1)); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_vbo); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vbo); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program const GLint positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_vao); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_vao); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_vbo); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } A lista de tiros é inicializada como vazia na linha 15. O restante do código é para criar o VBO que será compartilhado por todos os tiros. O VBO contém vértices de um polígono regular de 10 lados e usa o mesmo código que utilizamos no projeto regularpolygons. A definição de Bullets::paintGL ficará como a seguir: void Bullets::paintGL() { abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_vao); abcg::glUniform4f(m_colorLoc, 1, 1, 1, 1); abcg::glUniform1f(m_rotationLoc, 0); abcg::glUniform1f(m_scaleLoc, m_scale); for (const auto &amp;bullet : m_bullets) { abcg::glUniform2f(m_translationLoc, bullet.m_translation.x, bullet.m_translation.y); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, 12); } abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Todos os tiros têm a mesma cor (linha 59), ângulo de rotação (linha 60) e fator de escala (linha 61). A lista de tiros é iterada no laço das linhas 63 a 68. Cada tiro é renderizado como um GL_TRIANGLE_FAN. Em Bullets::terminateGL é liberado o VBO e VAO: void Bullets::terminateGL() { abcg::glDeleteBuffers(1, &amp;m_vbo); abcg::glDeleteVertexArrays(1, &amp;m_vao); } Vamos agora à definição de Bullets::update: void Bullets::update(Ship &amp;ship, const GameData &amp;gameData, float deltaTime) { // Create a pair of bullets if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Fire)] &amp;&amp; gameData.m_state == State::Playing) { // At least 250 ms must have passed since the last bullets if (ship.m_bulletCoolDownTimer.elapsed() &gt; 250.0 / 1000.0) { ship.m_bulletCoolDownTimer.restart(); // Bullets are shot in the direction of the ship&#39;s forward vector glm::vec2 forward{glm::rotate(glm::vec2{0.0f, 1.0f}, ship.m_rotation)}; glm::vec2 right{glm::rotate(glm::vec2{1.0f, 0.0f}, ship.m_rotation)}; const auto cannonOffset{(11.0f / 15.5f) * ship.m_scale}; const auto bulletSpeed{2.0f}; Bullet bullet{.m_dead = false, .m_translation = ship.m_translation + right * cannonOffset, .m_velocity = ship.m_velocity + forward * bulletSpeed}; m_bullets.push_back(bullet); bullet.m_translation = ship.m_translation - right * cannonOffset; m_bullets.push_back(bullet); // Moves ship in the opposite direction ship.m_velocity -= forward * 0.1f; } } for (auto &amp;bullet : m_bullets) { bullet.m_translation -= ship.m_velocity * deltaTime; bullet.m_translation += bullet.m_velocity * deltaTime; // Kill bullet if it goes off screen if (bullet.m_translation.x &lt; -1.1f) bullet.m_dead = true; if (bullet.m_translation.x &gt; +1.1f) bullet.m_dead = true; if (bullet.m_translation.y &lt; -1.1f) bullet.m_dead = true; if (bullet.m_translation.y &gt; +1.1f) bullet.m_dead = true; } // Remove dead bullets m_bullets.remove_if([](const Bullet &amp;p) { return p.m_dead; }); } Um par de tiros é criado a cada disparo. O temporizador m_bulletCoolDownTimer é utilizado para fazer com que os disparos ocorram em intervalos de no mínimo 250 milissegundos. Observe, na linha 103, que subtraímos da velocidade da nave o vetor de direção dos tiros. Isso produz um efeito de recuo da nave. Quanto mais tiros são disparados, mais a nave será deslocada para trás. Nas linhas 107 a 116 são atualizadas as coordenadas de translação de cada tiro. Nas linhas 108 e 109, os tiros são atualizados de acordo com a velocidade da nave e a velocidade do próprio tiro. Nas linhas 111 a 115, verifica-se se o tiro saiu da tela. Se sim, o flag m_dead torna-se true. A comparação é feita com -1.1f/+1.1f no lugar de -1.0f/+1.0f para ter certeza que todo o polígono do tiro saiu da tela. Na linha 119, todos os tiros com m_dead == true são removidos da lista. Isso é tudo! Eis o jogo completo (link original): O código do projeto completo pode ser baixado deste link. As estrelas das camadas superiores se moverão mais rapidamente do que as estrelas das camadas inferiores, dando a sensação de profundidade do espaço. As declarações antecipadas são utilizadas no lugar do #include para evitar inclusões recursivas. Por exemplo, openglwindow.hpp inclui ship.hpp, então ship.hpp não pode incluir openglwindow.hpp. O vetor é \\([0\\; 1]\\) pois a nave está alinhada ao eixo \\(y\\) positivo em sua orientação original. "],["geometry.html", "6 Espaços e geometria", " 6 Espaços e geometria Em computação gráfica, utilizamos escalares, pontos e vetores para representar quantidades, posições e direções. Através da combinação de operações sobre esses elementos podemos representar objetos geométricos e realizar o processamento necessário para a síntese de imagens. Este capítulo faz uma revisão dos conceitos fundamentais relacionados aos espaços abstratos que definem operações sobre pontos e vetores: Espaço vetorial (seção 6.1). Espaço afim (seção 6.2). Espaço euclidiano (seção 6.3). Tais conceitos formarão a base teórica para o uso de transformações geométricas nas atividades dos próximos capítulos. O final do capítulo (seção 6.4) contém uma atividade de prática de programação: veremos como carregar modelos geométricos tridimensionais formados por malhas de triângulos no formato Wavefront OBJ. Veremos também como pré-processar esses modelos de modo a assegurar que eles estejam contidos no volume de visão. "],["vector.html", "6.1 Espaço vetorial", " 6.1 Espaço vetorial Um espaço vetorial contém um conjunto de escalares e um conjunto de vetores. Usaremos letras minúsculas \\((a, b, \\dots)\\) para denotar escalares, e letras em negrito \\((\\mathbf{u}, \\mathbf{v}, \\dots)\\) para denotar vetores. Um escalar é um número real ou complexo que representa uma quantidade ou medida. Um vetor é um ente matemático abstrato de um conjunto \\(V\\) fechado para as seguintes operações: Adição de vetor com vetor: \\[ \\mathbf{u}+\\mathbf{v}. \\] Essa operação é comutativa e associativa: \\[ \\begin{align} \\mathbf{u}+\\mathbf{v} &amp;= \\mathbf{v}+\\mathbf{u},\\\\ \\mathbf{u}+(\\mathbf{v}+\\mathbf{w}) &amp;= (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}. \\end{align} \\] Podemos definir a subtração de dois vetores através da adição: \\[ \\begin{align} \\mathbf{u}-\\mathbf{v} &amp;= \\mathbf{u}+(-1)\\mathbf{v}. \\end{align} \\] Multiplicação de escalar com vetor: \\[ a \\mathbf{v}. \\] Essa operação é distributiva e associativa: \\[ \\begin{align} a (\\mathbf{u} + \\mathbf{v}) &amp;= a \\mathbf{u} + a \\mathbf{v},\\\\ (a + b) \\mathbf{u} &amp;= a \\mathbf{u} + b \\mathbf{u},\\\\ a(b \\mathbf{u}) &amp;= (a b) \\mathbf{u},\\\\ ab\\mathbf{u} &amp;= ba\\mathbf{u}. \\end{align} \\] No espaço vetorial há um vetor nulo denotado por \\(\\mathbf{0}\\). O vetor nulo é o elemento neutro na adição de vetor com vetor: \\[ \\begin{align} \\mathbf{v}+\\mathbf{0} &amp;= \\mathbf{v}, \\quad \\forall \\mathbf{v} \\in V. \\end{align} \\] Todo vetor possui um elemento inverso aditivo \\((-1)\\mathbf{v}\\), denotado por \\(-\\mathbf{v}\\), de modo que: \\[\\mathbf{v}+(-\\mathbf{v}) = \\mathbf{0}.\\] Além disso, \\[0\\mathbf{v} = \\mathbf{0}.\\] Combinação e independência linear Uma combinação linear de \\(n\\) vetores \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) é um vetor na forma \\[\\mathbf{v}=a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2 + \\cdots + a_n\\mathbf{v}_n.\\] Em uma combinação linear, os vetores \\(\\{\\mathbf{v}_i\\}\\) são linearmente independentes se \\[a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2 + \\cdots + a_n\\mathbf{v}_n = \\mathbf{0}\\] se e somente se \\[a_1 = a_2 = \\cdots = a_n = 0.\\] Dimensão e base A dimensão de um espaço vetorial corresponde ao maior número de vetores linearmente independentes encontrados naquele espaço. Em um espaço vetorial de dimensão \\(n\\), qualquer conjunto de \\(n\\) vetores linearmente independentes forma uma base. Seja \\(\\{\\mathbf{v}_i\\}\\) uma base em \\(V\\). Então, qualquer vetor \\(\\mathbf{v} \\in V\\) pode ser expresso unicamente como uma combinação linear \\[\\mathbf{v}=a_1\\mathbf{v_1} + a_2\\mathbf{v_2} + \\dots + a_n\\mathbf{v_n}.\\] Os escalares \\(\\{a_i\\}\\) formam uma representação de \\(\\mathbf{v}\\) com relação à base \\(\\{\\mathbf{v}_i\\}\\). Vetores geométricos Até agora, definimos vetores apenas como entidades abstratas do espaço vetorial. Em computação gráfica, estamos geralmente interessados em vetores geométricos, isto é, vetores que representam uma direção e uma magnitude (comprimento) e que podem ser representados visualmente como segmentos direcionados em um espaço coordenado de números reais como no \\(\\mathbb{R}^2\\) (figura 6.1). Figura 6.1: Vetores como segmentos direcionados. Nessa representação em que vetor é sinônimo de segmento direcionado, a adição de vetor com vetor (\\(\\mathbf{u}+\\mathbf{v}\\)) pode ser feita graficamente pela chamada regra do polígono como mostra a figura 6.2: desenhamos \\(\\mathbf{v}\\) partindo do fim de \\(\\mathbf{u}\\), e obtemos a soma traçando o segmento que parte do início de \\(\\mathbf{u}\\) e vai até o fim de \\(\\mathbf{v}\\). Figura 6.2: Adição entre vetores geométricos. A multiplicação de escalar com vetor também tem uma interpretação geométrica. O comprimento de \\(a\\mathbf{u}\\) (\\(a \\neq 0\\), \\(\\mathbf{u}\\neq\\mathbf{0}\\)) é igual ao comprimento de \\(\\mathbf{u}\\) multiplicado pelo módulo de \\(a\\). Assim, se \\(|a| \\in (0, 1)\\), o vetor diminui de comprimento. Se \\(|a|=1\\), o comprimento se mantém. Se \\(|a|&gt;1\\), o comprimento aumenta. Em todos os casos, se o sinal do escalar é negativo, o vetor muda de sentido. A figura 6.3 ilustra a multiplicação de escalar com vetor para um exemplo com \\(a&gt;1\\) e \\(a=-1\\). Figura 6.3: Multiplicação de escalar com vetor geométrico. Frequentemente, precisaremos usar vetores para representar deslocamentos de um ponto a outro do espaço. Entretanto, no espaço vetorial não existe o conceito de ponto no espaço. Sem uma noção de localização, os vetores agrupados no lado esquerdo da figura 6.4 são indistinguíveis dos vetores exibidos no lado direito. Os vetores são os mesmos, pois são caracterizados unicamente por sua direção e comprimento. Figura 6.4: Vetores idênticos no espaço vetorial. Para representarmos um vetor como um deslocamento a partir de um ponto no espaço, precisamos de um espaço afim. Além disso, para podermos usar o conceito de comprimento ou distância, precisamos de um espaço afim que use uma estrutura métrica, como o espaço euclidiano. "],["affine.html", "6.2 Espaço afim", " 6.2 Espaço afim Um espaço afim contém, além do espaço vetorial de mesma dimensão, um conjunto de pontos. Usararemos letras maiúsculas \\((P, Q, \\dots)\\) para denotar pontos. Além das operações com vetores, é definida a operação de diferença entre pontos, que resulta em um vetor: \\[\\mathbf{u}=P-Q.\\] O vetor \\(\\mathbf{u}\\) representa o deslocamento (translação) de \\(Q\\) para \\(P\\) (figura 6.5). Figura 6.5: Vetor como deslocamento de um ponto \\(Q\\) para um ponto \\(P\\). A partir da diferença entre pontos podemos definir a operação de adição de ponto e vetor, que resulta em um ponto deslocado pelo vetor: \\[P=Q+\\mathbf{u}.\\] Combinação afim Com as operações de diferença entre pontos e adição de ponto e vetor, podemos fazer combinações entre pontos para formar novos pontos. Por exemplo, considere a seguinte operação entre os pontos \\(P\\) e \\(Q\\): \\[R=Q+a(P-Q).\\] Se \\(a=0\\), então \\(R=Q\\); Se \\(a=1\\), então \\(R=P\\). Valores de \\(a \\in [0, 1]\\) produzem pontos entre o segmento que conecta \\(P\\) e \\(Q\\) (figura 6.6). As setas representam os diferentes vetores \\(a(P-Q)\\). Figura 6.6: Pontos produzidos com diferentes valores de \\(a\\) em \\(R=Q+a(P-Q)\\). Podemos usar essa expressão para definir uma nova e importante operação básica sobre pontos. Primeiro, note que \\[R=Q+a(P-Q)\\] é equivalente a \\[R=a P+(1-a)Q.\\] Uma combinação afim entre pontos \\(P\\) e \\(Q\\) é definida como \\[P=a P + b Q,\\] onde \\[a + b = 1.\\] De forma mais geral, uma combinação afim entre os pontos \\(P_1, P_2, \\dots, P_n\\) é qualquer soma na forma \\[a_1 P_1 + a_2 P_2 + \\dots + a_n P_n,\\] onde \\[a_1 + a_2 + \\dots + a_n = 1.\\] Se restringirmos os escalares \\(a_i\\) de uma combinação afim para o intervalo \\([0, 1]\\), temos uma combinação convexa. A união de todas as combinações convexas de um conjunto de pontos é chamada de casco convexo ou fecho convexo (figura 6.7). Figura 6.7: Casco convexo de um conjunto de pontos. Se \\(P_1\\), \\(P_2\\) e \\(P_3\\) forem quaisquer três pontos não colineares, então a combinação afim \\[P=a_1 P_1 + a_2 P_2 + a_3 P_3\\] pode ser usada para gerar qualquer ponto no plano que contém \\(P_1\\), \\(P_2\\) e \\(P_3\\). A união de todas as combinações convexas de \\(P_1\\), \\(P_2\\) e \\(P_3\\) é o conjunto de pontos do triângulo \\(\\triangle P_1 P_2 P_3\\). Se \\(a_1, a_2,a_3 \\geq 0\\) e \\(a_1+a_2+a_3=1\\), esses três escalares são chamados de coordenadas baricêntricas. Um ponto no triângulo pode ser unicamente determinado pela combinação de quaisquer duas coordenadas baricêntricas. Como ilustrado na figura 6.8, as coordenadas baricêntricas representam razões entre as áreas de subtriângulos e a área do triângulo. No pipeline de renderização, o rasterizador usa coordenadas baricêntricas para fazer a interpolação dos atributos de vértices para os fragmentos do triângulo. Figura 6.8: Coordenadas baricêntricas de um triângulo. "],["euclidean.html", "6.3 Espaço euclidiano", " 6.3 Espaço euclidiano O espaço euclidiano é um espaço afim que inclui a operação de produto escalar, também chamada de produto interno. O produto escalar produz um escalar a partir de dois vetores. Com isso é possível definir conceitos como distância e ângulo. O espaço euclidiano inclui o espaço vetorial de números reais \\(\\mathbb{R}^n\\). Seus elementos são \\(n\\)-tuplas de números reais \\((a_1, a_2, \\dots, a_n)\\) que denotam coordenadas do sistema de coordenadas cartesiano. A figura 6.9 ilustra a representação de um ponto em coordenadas cartesianas no espaço euclidiano de três dimensões (espaço 3D). Figura 6.9: Representação de um ponto no espaço 3D. As operações do espaço vetorial são definidas como: \\[ \\begin{align} \\mathbf{u}+\\mathbf{v}&amp;=(u_1 + v_1, u_2 + v_2, \\dots, u_n + v_n),\\\\ a \\mathbf{v}&amp;=(av_1, av_2, \\dots, av_n). \\end{align} \\] As operações afins são definidas como: \\[ \\begin{align} P-Q &amp;= (p_1 - q_1, p_2 - q_2, \\dots, p_n - q_n),\\\\ P+\\mathbf{u} &amp;= (p_1 + u_1, p_2 + u_2, \\dots, p_n + u_n). \\end{align} \\] Frames Assim como no mundo físico não existe um ponto de referência absoluto (uma origem no universo), no espaço euclidiano qualquer ponto pode ser considerado como a origem. Desse modo, para definir unicamente vetores e pontos, precisamos de uma base e um ponto de referência. Um frame cumpre esse papel27. Um frame de um espaço euclidiano de \\(n\\) dimensões é composto por: Um ponto de referência \\(P_0\\); Uma base composta por \\(n\\) vetores linearmente independentes \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\). Dado um frame \\(\\{P_0, \\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\), um vetor \\[\\mathbf{u}=(u_1, \\dots, u_n)\\] pode ser escrito unicamente como \\[\\mathbf{u}=u_1 \\mathbf{v}_1 + u_2 \\mathbf{v}_2 + \\cdots + u_n \\mathbf{v}_n.\\] Um ponto \\[P=(p_1, \\dots, p_n)\\] pode ser escrito unicamente como \\[P=P_0 + p_1\\mathbf{v}_1 + p_2\\mathbf{v}_2 + \\cdots + p_n\\mathbf{v}_n.\\] O ponto de referência \\(P_0\\) representa a origem do frame. No frame cartesiano padrão, \\[P_0=(0, \\dots, 0),\\] e a base é formada pelo conjunto de tuplas \\[ \\begin{align} \\mathbf{e}_1 &amp;= (1, 0, 0, \\dots, 0),\\\\ \\mathbf{e}_2 &amp;= (0, 1, 0, \\dots, 0),\\\\ &amp;\\;\\;\\vdots\\\\ \\mathbf{e}_n &amp;= (0, 0, 0, \\dots, 1),\\\\ \\end{align} \\] No \\(\\mathbb{R}^3\\), a base é frequentemente denotada por vetores \\(\\hat{\\mathbf{i}}\\), \\(\\hat{\\mathbf{j}}\\), \\(\\hat{\\mathbf{k}}\\): \\[ \\begin{align} \\hat{\\mathbf{i}} &amp;= (1, 0, 0),\\\\ \\hat{\\mathbf{j}} &amp;= (0, 1, 0),\\\\ \\hat{\\mathbf{k}} &amp;= (0, 0, 1). \\end{align} \\] O conceito de frame é importante em computação gráfica pois é comum que os modelos geométricos 3D sejam representados originalmente em um frame local no qual a origem é o centro ou a base do objeto. Esses modelos podem ser dispostos em uma cena virtual 3D que usa outro frame de referência. Além disso, durante o processamento geométrico do pipeline de renderização, os objetos da cena podem ser expressos em relação ao frame da câmera virtual, cuja origem é frequentemente o centro de projeção. Veremos futuramente como realizar essas mudanças de representação através de matrizes de transformação. Produto escalar Sejam \\(\\mathbf{u}=(u_1, \\dots, u_n)\\) e \\(\\mathbf{v}=(v_1, \\dots, v_n)\\) dois vetores do \\(\\mathbb{R}^n\\). O produto escalar, denotado por \\(\\mathbf{u} \\cdot \\mathbf{v}\\), é a soma da multiplicação componente a componente das tuplas. O resultado é, portanto, um escalar: \\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\\] As seguintes propriedades se aplicam: \\(\\mathbf{u} \\cdot \\mathbf{v}= \\mathbf{v} \\cdot \\mathbf{u}\\). \\((a \\mathbf{u} + b \\mathbf{v}) \\cdot \\mathbf{w} = a \\mathbf{u} \\cdot \\mathbf{w} + b \\mathbf{v} \\cdot \\mathbf{w}\\). \\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\), e \\(\\mathbf{v} \\cdot \\mathbf{v} = 0\\) se e somente se \\(\\mathbf{v}=\\mathbf{0}\\). \\(\\mathbf{0} \\cdot \\mathbf{0} = 0\\). Ortogonalidade Se \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\), então \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) são ortogonais, isto é, os vetores são perpendiculares entre si. Quando todos os vetores de uma base são ortogonais, temos uma base ortogonal. A base padrão \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_n\\) de \\(\\mathbb{R}^n\\) é um exemplo de base ortogonal, pois \\(\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0\\) para \\(i \\neq j\\). Comprimento e distância No espaço euclidiano, a magnitude ou comprimento de um vetor \\(\\mathbf{u}=(u_1, u_2, \\dots, u_n)\\) é definida pela norma euclidiana: \\[ \\begin{align} |\\mathbf{u}| &amp;= \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\\\ &amp; = \\sqrt{u_1^2 + u_2^2 + \\cdots + u_n^2}. \\end{align} \\] A norma euclidiana permite calcular a distância entre pontos. Como \\(P - Q\\) é um vetor de deslocamento de \\(Q=(q_1, \\dots, q_n)\\) para \\(P=(p_1, \\dots, p_n)\\), a distância entre os dois pontos pode ser calculada como \\[ \\begin{align} |P - Q| &amp;= \\sqrt{(P-Q) \\cdot (P-Q)}\\\\ &amp;= \\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}. \\end{align} \\] A figura 6.10 ilustra a distância euclidiana entre dois pontos \\(P=(p_x, p_y)\\) e \\(Q=(q_x, q_y)\\). Observe sua relação com o teorema de Pitágoras. Figura 6.10: Distância entre dois pontos no plano. Normalização Se \\(|\\mathbf{v}|=1\\), dizemos que \\(\\mathbf{v}\\) é um vetor unitário. Podemos transformar qualquer vetor \\(\\mathbf{v}\\) não nulo em um vetor unitário na mesma direção, denotado por \\(\\hat{\\mathbf{v}}\\), se dividirmos todos os elementos de \\(\\mathbf{v}\\) por seu comprimento: \\[\\hat{\\mathbf{v}}=\\frac{\\mathbf{v}}{|\\mathbf{v}|}.\\] A figura 6.11 ilustra o resultado da normalização de vetores no \\(\\mathbb{R}^2\\). Observe que os vetores normalizados desenhados a partir da origem ficam inscritos em um círculo unitário. Figura 6.11: Um conjunto de vetores (esquerda) e seus correspondentes vetores normalizados (direita). Sempre que possível trabalharemos com vetores unitários. O uso de vetores unitários simplifica o cálculo do sombreamento e iluminação de superfícies. Quando dois vetores unitários são ortogonais, dizemos que os vetores são ortonormais. A base padrão \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_n\\) do \\(\\mathbb{R}^n\\) é uma base ortonormal pois possui vetores de base unitários (isto é, \\(|\\mathbf{e}_i|=1\\)) e ortogonais entre si. Ângulo entre vetores O produto escalar entre dois vetores \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) não nulos é proporcional ao cosseno do ângulo \\(\\theta\\) formado entre esses vetores: \\[\\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos \\theta.\\] Logo, \\[\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}||\\mathbf{v}|}.\\] Se os vetores são ortogonais, \\(\\cos \\theta = 0\\). Se os vetores são paralelos e na mesma direção, \\(\\cos \\theta = 1\\). O menor ângulo não negativo entre dois vetores (\\(\\theta \\in [0, \\pi]\\)) pode ser calculado como: \\[\\theta = \\cos^{-1} \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}||\\mathbf{v}|} \\right).\\] Note que, para vetores unitários, a expressão é mais simples: \\[\\cos \\theta = \\mathbf{u} \\cdot \\mathbf{v}\\] e \\[\\theta = \\cos^{-1}(\\mathbf{u} \\cdot \\mathbf{v}).\\] A relação entre \\(\\theta\\) e \\(\\mathbf{u} \\cdot \\mathbf{v}\\) é como segue: \\[ \\begin{align} \\nonumber \\textbf{u} \\cdot \\textbf{v} \\begin{cases} &gt;0, \\quad \\text{para }0 \\leq \\theta &lt; \\frac{\\pi}{2} \\\\[4pt] =0, \\quad \\text{para }\\theta = \\frac{\\pi}{2} \\\\[4pt] &lt;0, \\quad \\text{para } \\frac{\\pi}{2} &lt; \\theta \\leq \\pi \\end{cases} \\end{align} \\] A figura 6.12 mostra exemplos dos diferentes valores do produto escalar usando vetores no plano. Figura 6.12: Valor do produto escalar e ângulo entre vetores. Projeção ortogonal Dado um vetor \\(\\mathbf{w}\\) e um vetor \\(\\mathbf{v}\\) não nulo, podemos decompor \\(\\mathbf{w}\\) como uma soma de dois vetores, sendo um paralelo a \\(\\mathbf{v}\\) e outro ortogonal a \\(\\mathbf{v}\\) (figura 6.13): \\[\\mathbf{w} = a\\mathbf{v} + \\mathbf{u},\\] \\(a \\mathbf{v}\\) é o vetor paralelo, chamado de projeção de \\(\\mathbf{w}\\) sobre \\(\\mathbf{v}\\), sendo que \\[a = \\frac{\\mathbf{w} \\cdot \\mathbf{v}}{\\mathbf{v} \\cdot \\mathbf{v}}.\\] \\(\\mathbf{u}\\) é o vetor ortogonal a \\(\\mathbf{v}\\) (isto é, \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\)) e \\[\\mathbf{u}=\\mathbf{w}-a \\mathbf{v}.\\] Figura 6.13: Projeção ortogonal de um vetor sobre outro. Note que, se \\(\\mathbf{v}\\) é um vetor unitário, \\[a=\\mathbf{w} \\cdot \\hat{\\mathbf{v}}=|\\mathbf{w}|\\cos \\theta,\\] onde \\(\\theta\\) é o ângulo entre \\(\\mathbf{w}\\) e \\(\\hat{\\mathbf{v}}\\). Produto vetorial Sejam \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) dois vetores do \\(\\mathbb{R}^3\\). O produto vetorial ou produto externo de \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) é definido como \\[\\mathbf{u} \\times \\mathbf{v} = |\\mathbf{u}| |\\mathbf{v}| \\sin(\\theta) \\hat{\\mathbf{n}},\\] onde \\(\\hat{\\mathbf{n}}\\) é um vetor unitário ortogonal a \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\), e \\(\\theta\\) é o ângulo entre \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\). Assim, \\(\\mathbf{u} \\times \\mathbf{v}\\) é um vetor ortogonal aos dois vetores, com magnitude \\(|\\mathbf{u} \\times \\mathbf{v}| = |\\mathbf{u}| |\\mathbf{v}| |\\sin \\theta|\\) como mostra a figura 6.14. Figura 6.14: Produto vetorial. A direção do vetor ortogonal é dada pela regra da mão direita: usando a mão direita, se o indicador apontar na direção de \\(\\mathbf{u}\\) e o dedo médio apontar na direção de \\(\\mathbf{v}\\), o vetor ortogonal \\(\\mathbf{u} \\times \\mathbf{v}\\) apontará na direção do dedão (figura 6.15). Figura 6.15: Direção do produto vetorial segundo a regra da mão direita (imagem modificada do original). O produto vetorial é anticomutativo, isto é, \\[\\mathbf{u} \\times \\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u}).\\] Assim, se a ordem dos operandos for invertida, o vetor ortogonal apontará para a direção oposta, como mostra a figura 6.16 (pela regra da mão direita, o dedão apontará para baixo). Figura 6.16: A ordem dos operandos determina a direção do vetor ortogonal. O produto vetorial é calculado como \\[ \\mathbf{u} \\times \\mathbf{v} = (u_y v_z - u_z v_y, u_z v_x - u_x v_z, u_x v_y - u_y v_x). \\] Para memorizar mais facilmente, podemos expressar o produto vetorial como um determinante de ordem 3: \\[ \\mathbf{u} \\times \\mathbf{v} = \\begin{vmatrix} \\hat{\\mathbf{i}} &amp; \\hat{\\mathbf{j}} &amp; \\hat{\\mathbf{k}} \\\\ u_x &amp; u_y &amp; u_z \\\\ v_x &amp; v_y &amp; v_z \\end{vmatrix}. \\] Usando expansão de cofatores: \\[ \\begin{align} \\mathbf{u} \\times \\mathbf{v} &amp;= \\begin{vmatrix} u_y &amp; u_z \\\\ v_y &amp; v_z \\end{vmatrix}\\hat{\\mathbf{i}} - \\begin{vmatrix} u_x &amp; u_z \\\\ v_x &amp; v_z \\end{vmatrix}\\hat{\\mathbf{j}} + \\begin{vmatrix} u_x &amp; u_y \\\\ v_x &amp; v_y \\end{vmatrix}\\hat{\\mathbf{k}}\\\\ &amp;= (u_y v_z - u_z v_y)\\hat{\\mathbf{i}}-(u_x v_z - u_z v_x)\\hat{\\mathbf{j}}+(u_x v_y - u_y v_x)\\hat{\\mathbf{k}}\\\\ &amp;= (u_y v_z - u_z v_y, u_z v_x - u_x v_z, u_x v_y - u_y v_x) \\end{align} \\] Uma vez que o vetor ortogonal tem tamanho proporcional ao seno do ângulo entre os vetores, o produto vetorial de dois vetores paralelos é o vetor nulo: \\[ \\begin{align} \\mathbf{u} \\times \\mathbf{u} &amp;= \\mathbf{0},\\\\ -\\mathbf{u} \\times \\mathbf{u} &amp;= \\mathbf{0}. \\end{align} \\] Outras propriedades são dadas a seguir: \\(\\mathbf{u} \\times (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u}\\times\\mathbf{v})+(\\mathbf{u}\\times\\mathbf{w})\\). \\((a\\mathbf{u}) \\times \\mathbf{v} = \\mathbf{u} \\times (a \\mathbf{v}) = a (\\mathbf{u} \\times \\mathbf{v})\\). \\(\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w}) = (\\mathbf{u} \\times \\mathbf{v}) \\cdot \\mathbf{w}\\). \\(\\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u} \\cdot \\mathbf{w})\\mathbf{v} - (\\mathbf{u} \\cdot \\mathbf{v})\\mathbf{w}\\). Além disso, \\[ \\begin{align} \\hat{\\mathbf{i}} \\times \\hat{\\mathbf{j}} = \\hat{\\mathbf{k}},\\\\ \\hat{\\mathbf{j}} \\times \\hat{\\mathbf{k}} = \\hat{\\mathbf{i}},\\\\ \\hat{\\mathbf{k}} \\times \\hat{\\mathbf{i}} = \\hat{\\mathbf{j}}. \\end{align} \\] Vetor normal Um vetor normal, ou simplesmente normal, é um vetor perpendicular ao plano que tangencia uma superfície em um dado ponto. Em computação gráfica, vetores normais são essenciais para o cálculo correto do sombreamento e iluminação de superfícies. Se considerarmos a superfície de uma esfera de raio \\(r&gt;0\\) dada pela equação \\[x^2+y^2+z^2=r^2,\\] a normal de um ponto \\(P=(x,y,z)\\) sobre essa esfera é o vetor \\(\\mathbf{n}=(2x, 2y, 2z)\\) (figura 6.17). O vetor no sentido oposto, \\(-\\mathbf{n}\\), também é um vetor normal. Entretanto, em superfícies fechadas como a esfera, geralmente estamos interessados nas normais que apontam para fora da superfície. Figura 6.17: Vetor normal em um ponto na superfície de uma esfera. O cálculo do vetor normal em superfícies suaves frequentemente exige o uso de ferramentas de geometria diferencial. Por exemplo, em uma superfície definida implicitamente como uma função level set \\(f(x,y,z)=c\\), o vetor normal é calculado através do gradiente \\[ \\begin{align} \\mathbf{n} = \\nabla f(x,y,z) = \\frac{\\partial f}{\\partial x}\\hat{\\mathbf{i}} + \\frac{\\partial f}{\\partial y}\\hat{\\mathbf{j}} + \\frac{\\partial f}{\\partial z}\\hat{\\mathbf{k}}. \\end{align} \\] De fato, para a esfera centralizada na origem, \\[f(x,y,z)=x^2+y^2+z^2\\] e \\[ \\begin{align} \\mathbf{n} = \\nabla f(x,y,z) &amp;= 2x\\,\\hat{\\mathbf{i}} + 2y\\,\\hat{\\mathbf{j}} + 2z\\,\\hat{\\mathbf{k}}\\\\ &amp;= (2x, 2y, 2z). \\end{align} \\] Entretanto, neste curso não trabalharemos com superfícies implícitas. Utilizaremos apenas superfícies formadas por malhas de triângulos, uma vez que o pipeline gráfico do OpenGL trabalha apenas com pontos, segmentos e triângulos. Se quisermos renderizar uma esfera, teremos de usar uma malha triangular que aproxime essa esfera. A figura 6.18 ilustra uma malha que aproxima uma esfera. Essa malha pode ser descrita unicamente por triângulos, pois cada quadrilátero pode ser formado por dois triângulos. Figura 6.18: Vetor normal em uma malha que aproxima a superfície de uma esfera. Em geral, dada uma malha de triângulos, não temos acesso à representação implícita ou paramétrica da superfície que a malha tenta aproximar. Assim, no caso geral, a normal \\(n\\) mostrada na figura 6.18 precisa ser calculada utilizando unicamente os triângulos que formam a malha. Figura 6.19: Calculando o vetor normal de um triângulo. Para calcular a normal de um triângulo \\(\\triangle ABC\\), basta definir dois vetores sobre o plano do triângulo, e então calcular o produto vetorial desses vetores. Os dois vetores podem ser obtidos através da subtração dos vértices que formam quaisquer duas arestas do triângulo (figura 6.19): \\[ \\begin{align} \\mathbf{u}&amp;=A-C,\\\\ \\mathbf{v}&amp;=B-C,\\\\ \\mathbf{n}&amp;=\\mathbf{u} \\times \\mathbf{v}.\\\\ \\end{align} \\] Em geral, desejaremos trabalhar com normais unitárias. Nesse caso, a normal será calculada como \\[ \\begin{align} \\hat{\\mathbf{n}}&amp;=\\frac{\\mathbf{u} \\times \\mathbf{v}}{|\\mathbf{u} \\times \\mathbf{v}|}.\\\\ \\end{align} \\] Como o triângulo é uma superfície planar, o vetor normal é o mesmo para todos os pontos do triângulo. Entretanto, isso evidencia um problema com o cálculo do vetor normal em vértices de uma malha que aproxima uma superfície suave. Observe na figura 6.20 o detalhe ampliado da esfera da figura 6.18. Cada face, formada por dois triângulos (mostrados pelo tracejado), é uma superfície planar. Portanto, cada face tem o mesmo vetor normal para todos os pontos. Por outro lado, o ponto \\(P\\) é compartilhado por quatro faces (seis triângulos). Qual das normais (\\(\\mathbf{n}_1\\), \\(\\mathbf{n}_2\\), \\(\\mathbf{n}_3\\), \\(\\mathbf{n}_4\\)) deve ser utilizada em \\(\\mathbf{n}_p\\)? Figura 6.20: Detalhe ampliado de uma esfera aproximada por uma malha. Como a malha de triângulos aproxima uma superfície suave, podemos calcular um vetor normal em \\(P\\) como uma média dos vetores normais de todos os \\(n\\) triângulos que usam \\(P\\). Uma forma simples de fazer isso é através da normalização da soma dessas normais \\[ \\begin{align} \\hat{\\mathbf{n}}_p&amp;=\\frac{\\sum_{i=1}^n \\mathbf{n}_i}{|\\sum_{i=1}^n \\mathbf{n}_i|},\\\\ \\end{align} \\] onde \\(\\mathbf{n}_i\\) é o vetor normal do \\(n\\)-ésimo triângulo que usa \\(P\\). O resultado é um vetor normalizado chamado de normal de vértice. A figura 6.21 ilustra, em um corte bidimensional, como a normal do vértice \\(P\\) aproxima a superfície suave mostrada no tracejado. Figura 6.21: Normal de vértice como aproximação da normal da superfície suave. Podemos utilizar este método sempre que soubermos que a malha aproxime uma superfície suave. Veremos nas próximas seções que esse é o método ideal para ser utilizado com geometria indexada, que é a geometria em que os atributos de um vértice são compartilhados com todas as faces adjacentes. Entretanto, se a malha representar um objeto com quinas, tal como um cubo ou pirâmide, então cada face precisará ser renderizada com vértices não compartilhados, pois queremos evidenciar a descontinuidade da superfície. Se esse fosse o caso da geometria ilustrada na figura 6.20, quatro vértices teriam de ser utilizados em \\(P\\): um para cada face (quadrilátero formado por dois triângulos). Os vértices teriam a mesma posição de \\(P\\), mas cada um usaria a normal da face correspondente. Nesse contexto, frame é um quadro de referência (frame of reference) de um sistema de coordenadas e não tem relação com o termo frame utilizado para descrever uma imagem renderizada. "],["objmodel.html", "6.4 Lendo um modelo 3D", " 6.4 Lendo um modelo 3D Nos capítulos anteriores vimos como usar o pipeline gráfico do OpenGL para renderizar primitivas formadas a partir de arranjos ordenados de vértices. Em particular, conseguimos representar diferentes formas no plano através de malhas de triângulos. Tomemos, como exemplo, o polígono da figura 6.22: Figura 6.22: Um polígono de seis lados. O polígono pode ser convertido em uma malha de triângulos através de uma triangulação. Uma possível triangulação é mostrada na figura 6.23: Figura 6.23: Triangulação de um polígono de seis lados. Os triângulos \\(T_1\\) a \\(T_4\\) podem ser renderizados com glDrawArrays(GL_TRIANGLES, ...) usando um VBO formado por um arranjo de posições de vértices: std::array positions{// Triângulo T_1 glm::vec2{-3, 3}, glm::vec2{-4,-2}, glm::vec2{ 1,-4}, // Triângulo T_2 glm::vec2{-3, 3}, glm::vec2{ 1,-4}, glm::vec2{ 4,-1}, // Triângulo T_3 glm::vec2{-3, 3}, glm::vec2{ 4,-1}, glm::vec2{ 4, 2}, // Triângulo T_4 glm::vec2{-3, 3}, glm::vec2{ 4, 2}, glm::vec2{ 1, 4}}; ... glDrawArrays(GL_TRIANGLES, 0, 12); Uma desvantagem dessa representação é que há repetição de coordenadas. Por exemplo, a coordenada \\((-3,3)\\) é repetida quatro vezes, uma para cada triângulo. Felizmente, como a triangulação forma um leque, podemos usar GL_TRIANGLE_FAN com um arranjo mais compacto: std::array positions{glm::vec2{-3, 3}, glm::vec2{-4,-2}, glm::vec2{ 1,-4}, glm::vec2{ 4,-1}, glm::vec2{ 4, 2}, glm::vec2{ 1, 4}}; ... glDrawArrays(GL_TRIANGLE_FAN, 0, 6); Outra possibilidade é usar geometria indexada. A figura 6.24 mostra uma possível indexação dos vértices do polígono da figura 6.23. Os índices são numerados de 0 a 5: Figura 6.24: Geometria indexada. Na geometria indexada, um arranjo ordenado de posições de vértices é armazenado no VBO, e um arranjo de números inteiros que representam os índices para esses vértices é armazenado no EBO: std::array positions{glm::vec2{-3, 3}, // Vértice 0 glm::vec2{-4,-2}, // Vértice 1 glm::vec2{ 1,-4}, // Vértice 2 glm::vec2{ 4,-1}, // Vértice 3 glm::vec2{ 4, 2}, // Vértice 4 glm::vec2{ 1, 4}}; // Vértice 5 std::array indices{0, 1, 2, // Triângulo T_1 0, 2, 3, // Triângulo T_2 0, 3, 4, // Triângulo T_3 0, 4, 5}; // Triângulo T_4 ... glDrawElements(GL_TRIANGLES, 12, GL_UNSIGNED_INT, nullptr); A geometria indexada é o formato mais utilizado para descrever malhas de triângulos. É, por exemplo, o formato utilizado nos modelos OBJ que utilizaremos nas atividades da disciplina. Até agora, só utilizamos formas no plano. Entretanto, não há qualquer limitação no OpenGL que nos impeça de representar geometria no espaço. Basta especificarmos a posição dos vértices como coordenadas \\((x,y,z)\\) do espaço euclidiano usando tuplas de três elementos com glm::vec3. Nesta seção descreveremos um passo a passo de construção de uma aplicação de leitura de modelos 3D no formato OBJ. Neste formato, os dados são gravados em formato texto e são fáceis de serem lidos. Veja a seguir o conteúdo de um arquivo OBJ contendo a definição de um cubo unitário centralizado na origem. Observe que há inicialmente a definição dos 8 vértices do cubo, e então a definição dos índices das 12 faces. Neste arquivo, cada face é um triângulo (o cubo tem seis 6 lados e cada lado é formado por 2 faces coplanares). # object Box v -0.5000 -0.5000 0.5000 v -0.5000 -0.5000 -0.5000 v 0.5000 -0.5000 -0.5000 v 0.5000 -0.5000 0.5000 v -0.5000 0.5000 0.5000 v 0.5000 0.5000 0.5000 v 0.5000 0.5000 -0.5000 v -0.5000 0.5000 -0.5000 # 8 vertices o Box g Box f 1 3 2 f 3 1 4 f 5 7 6 f 7 5 8 f 1 6 4 f 6 1 5 f 4 7 3 f 7 4 6 f 3 8 2 f 8 3 7 f 2 5 1 f 5 2 8 # 12 faces Antes de iniciarmos o passo a passo de leitura do modelo 3D, veremos o conceito de orientação de triângulos e como isso pode afetar a renderização. Se os triângulos de um modelo 3D estiverem com orientação diferente do esperado, o modelo pode ser renderizado de forma incorreta. Orientação e face culling A direção do vetor normal pode ser utilizada para definir a orientação de uma superfície, isto é, qual é o lado da frente da superfície. Entretanto, reservaremos o uso dos vetores normais apenas para o cálculo da iluminação de superfícies. Há uma forma mais simples de determinar a orientação de uma superfície quando polígonos são utilizados, que é o caso do pipeline do OpenGL. No OpenGL, a orientação de um triângulo é definida pela ordem em que seus vértices estão ordenados no arranjo de vértices quando o triângulo é visto de frente. Só há duas orientações possíveis (figura 6.25): Sentido horário (clockwise ou CW): os vértices estão orientados no sentido horário quando o triângulo é visto de frente; Sentido anti-horário (counterclockwise ou CCW): os vértices estão orientados no sentido anti-horário quando o triângulo é visto de frente. Figura 6.25: Sentidos de orientação de um triângulo. Volte a observar o exemplo anterior de geometria indexada (figura 6.24) e veja como os índices de cada triângulo \\(T_1\\) a \\(T_4\\) estão ordenados no sentido anti-horário (CCW). Por padrão, o OpenGL considera que o lado da frente é o lado orientado no sentido anti-horário (CCW). Isso pode ser modificado com a função glFrontFace, usando o argumento GL_CW ou GL_CCW, como a seguir: glFrontFace(GL_CW): para indicar que o lado da frente tem vértices no sentido horário; glFrontFace(GL_CCW): para indicar que o lado da frente tem vértices no sentido anti-horário (padrão). CCW é a orientação padrão do OpenGL porque essa é também a orientação padrão utilizada na matemática. Por exemplo, os ângulos medidos no plano cartesiano são medidos no sentido anti-horário e seguem a regra da mão direita (figura 6.26): \\(0\\) radianos (\\(0^{\\circ}\\)) aponta para o eixo \\(x\\) positivo (para a direita); \\(\\frac{\\pi}{2}\\) radianos (\\(90^{\\circ}\\)) aponta para o eixo \\(y\\) positivo (para cima); \\(\\pi\\) radianos (\\(180^{\\circ}\\)) aponta para o eixo \\(x\\) negativo (para a esquerda). Figura 6.26: Direção convencional dos ângulos em um eixo de rotação (fonte). Há algumas vantagens em saber qual é o lado da frente de um triângulo: Podemos desenhar cada lado com uma cor ou efeito diferente. No fragment shader, a variável embutida gl_FrontFacing é uma variável booleana que é true sempre que o fragmento pertencer a um triângulo virado de frente, e false caso contrário. Podemos fazer uso da técnica de face culling, também chamada de back-face culling. Face culling Face culling é uma técnica que consiste em descartar todas as faces (triângulos no OpenGL) que não estão de frente para a câmera. O uso de face culling pode aumentar de forma considerável a eficiência da renderização, pois os triângulos podem ser removidos antes da rasterização. Se a malha de triângulos formar um sólido opaco e fechado, então o face culling pode remover cerca de metade dos triângulos. Esse é o caso da malha de triângulos que aproxima uma esfera. Na figura 6.27, parte das faces da frente da malha foram deslocadas para revelar as faces voltadas para trás, em vermelho. Essas faces em vermelho podem ser removidas completamente se a esfera estiver fechada. Figura 6.27: Faces voltadas para trás (em vermelho). O descarte de primitivas usando face culling pode ser feito automaticamente pelo pipeline gráfico do OpenGL, após a etapa de recorte de primitivas, e imediatamente antes da rasterização. Podemos ativar o face culling através de glEnable(GL_CULL_FACE) e desativá-lo através de glDisable(GL_CULL_FACE). Por padrão, o face culling está desativado. A função glCullFace pode ser utilizada para especificar qual lado deve ser descartado quando o face culling estiver habilitado. Por exemplo: glCullFace(GL_FRONT): para descartar os triângulos que estão de frente quando projetados no viewport; glCullFace(GL_BACK): para descartar os triângulos que estão voltados para trás quando projetados no viewport (padrão). glCullFace(GL_FRONT_AND_BACK): para descartar todos os triângulos, mas ainda renderizar pontos e segmentos. Configuração inicial A configuração inicial do nosso leitor de arquivos OBJ é semelhante a dos projetos anteriores. No arquivo abcg/examples/CMakeLists.txt, inclua a linha: add_subdirectory(loadmodel) Crie o subdiretório abcg/examples/loadmodel e o arquivo abcg/examples/loadmodel/CMakeLists.txt, que ficará assim: project(loadmodel) add_executable(${PROJECT_NAME} main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Crie os arquivos main.cpp, openglwindow.cpp e openglwindow.hpp. Crie o subdiretório abcg/examples/loadmodel/assets. Dentro dele, crie os arquivos loadmodel.frag e loadmodel.vert. Baixe o arquivo bunny.zip e descompacte-o em assets. O conteúdo é o arquivo bunny.obj que contém o modelo 3D de um coelho: o Stanford Bunny (figura 6.28) processado e simplificado no MeshLab. Figura 6.28: Renderização do Stanford Bunny (fonte). O conteúdo de abcg/examples/loadmodel ficará com a seguinte estrutura: loadmodel/  CMakeLists.txt  main.cpp  openglwindow.hpp  openglwindow.cpp  assets/  bunny.obj  loadmodel.frag  loadmodel.vert main.cpp O conteúdo de main.cpp é bem similar ao dos projetos anteriores. Não há nada de realmente novo aqui: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings({.samples = 4}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Load Model&quot;}); app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } loadmodel.vert O vertex shader ficará como a seguir: #version 410 core layout(location = 0) in vec3 inPosition; uniform float angle; void main() { float sinAngle = sin(angle); float cosAngle = cos(angle); gl_Position = vec4( inPosition.x * cosAngle + inPosition.z * sinAngle, inPosition.y, inPosition.z * cosAngle - inPosition.x * sinAngle, 1.0); } Só há um atributo de entrada, inPosition, que é a posição \\((x,y,z)\\) do vértice. Observe que não há atributo de saída. A cor dos fragmentos será determinada unicamente no fragment shader. A variável uniforme angle (linha 5) é um ângulo (em radianos) que será incrementado continuamente para produzir uma animação de rotação. No código de main, gl_Position recebe inPosition transformado através de uma operação de rotação em torno do eixo \\(y\\) pelo ângulo angle. A transformação de inPosition (posição \\(x,y,z\\)) para gl_Position (posição \\(x&#39;,y&#39;,z&#39;,1\\)) é como segue: \\[ \\begin{align} x&#39; &amp;= x \\cos(\\theta) + z \\sin(\\theta),\\\\ y&#39; &amp;= y,\\\\ z&#39; &amp;= z \\cos(\\theta) - x \\sin(\\theta). \\end{align} \\] A derivação da transformação de rotação será vista no próximo capítulo. Veremos também que poderemos simplificar esse código através do uso de matrizes de transformação. loadmodel.frag O conteúdo do fragment shader ficará assim: #version 410 out vec4 outColor; void main() { float i = 1.0 - gl_FragCoord.z; if (gl_FrontFacing) { outColor = vec4(i, i, i, 1); } else { outColor = vec4(i, 0, 0, 1); } } Na linha 6, a variável i é um valor de intensidade de cor. Esse valor é calculado a partir da componente z da variável embutida gl_FragCoord. gl_FragCoord é um vec4 que contém a posição do fragmento no espaço da janela. As componentes \\(x\\) e \\(y\\) são a posição do fragmento na janela, em pixels. A componente \\(z\\) é a profundidade do fragmento, que varia de 0 (mais perto) a 1 (mais distante). Lembre-se que esse é o valor \\(z\\) que estava no intervalo \\([-1, 1]\\) em coordenadas normalizadas do dispositivo (NDC) e que, após a rasterização, foi mapeado para \\([0, 1]\\) no espaço da janela (o mapeamento pode ser controlado com glDepthRange). A cor de saída depende do valor da variável embutida gl_FrontFacing, que indica se o fragmento pertence a uma face de frente ou de trás. Se é true (frente), a cor de saída é um tom de cinza dado pelo valor de i. Caso contrário (trás), a cor de saída é um tom de vermelho. O resultado será um modelo renderizado em tons de cinza (frente) ou vermelho (trás). Quanto maior for a profundidade do fragmento, menor será sua intensidade. Com isso conseguiremos distinguir melhor a forma e o volume do objeto. openglwindow.hpp A definição da classe OpenGLWindow ficará assim: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;vector&gt; #include &quot;abcg.hpp&quot; struct Vertex { glm::vec3 position; bool operator==(const Vertex&amp; other) const { return position == other.position; } }; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_VAO{}; GLuint m_VBO{}; GLuint m_EBO{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; float m_angle{}; int m_verticesToDraw{}; std::vector&lt;Vertex&gt; m_vertices; std::vector&lt;GLuint&gt; m_indices; void loadModelFromFile(std::string_view path); void standardize(); }; #endif Primeiro, observe a estrutura Vertex definida nas linhas 8 a 14: struct Vertex { glm::vec3 position; bool operator==(const Vertex&amp; other) const { return position == other.position; } }; Essa estrutura define os atributos que compõem um vértice. Há apenas uma posição \\((x,y,z)\\) e a definição de um operador de igualdade (==) que verifica se um dado vértice other é igual ao vértice atual. As instâncias dessa estrutura serão os elementos de uma tabela hash implementada com std::unordered_map. Durante a leitura do modelo OBJ, a tabela hash será utilizada para verificar se há algum vértice com posição repetida (por isso o operador de igualdade). Através disso conseguiremos criar uma geometria indexada o mais compacta possível. Veremos mais sobre isso na implementação da função de leitura do modelo OBJ. A variável m_angle (linha 33) é o ângulo de rotação que será enviado à variável uniforme do vertex shader. A variável m_verticesToDraw (linha 34) é a quantidade de vértices do VBO que será processada pela função de renderização, glDrawElements. O valor de m_verticesToDraw será controlado por um slider da ImGui. Assim conseguiremos controlar quantos triângulos queremos renderizar. Nas linhas 36 e 37, m_vertices e m_indices são os arranjos de vértices e índices lidos do arquivo OBJ. Esses são os dados que serão enviados ao VBO (m_VBO) e EBO (m_EBO). O carregamento do arquivo OBJ será feito pela função OpenGLWindow::loadModelFromFile (linha 37). A função OpenGLWindow::standardize (linha 40) será chamada após OpenGLWindow::loadModelFromFile e servirá para centralizar o modelo na origem e normalizar as coordenadas de todos os vértices no intervalo \\([-1, 1]\\). openglwindow.cpp O início de openglwindow.cpp começa como a seguir: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;tiny_obj_loader.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; #include &lt;glm/gtx/hash.hpp&gt; #include &lt;unordered_map&gt; // Explicit specialization of std::hash for Vertex namespace std { template &lt;&gt; struct hash&lt;Vertex&gt; { size_t operator()(Vertex const&amp; vertex) const noexcept { const std::size_t h1{std::hash&lt;glm::vec3&gt;()(vertex.position)}; return h1; } }; } // namespace std Observe que incluímos tiny_obj_loader.h (linha 5). Esse é o cabeçalho da biblioteca TinyObjLoader que fará o parsing do conteúdo do arquivo OBJ. As funções de TinyObjLoader serão utilizadas em OpenGLWindow::loadModelFromFile. Nas linhas 12 a 21 há uma especialização explícita de std::hash para a nossa estrutura Vertex. Isso é necessário para que possamos usar Vertex como chave de uma tabela hash (como std::unordered_map). Geralmente, uma tabela hash utiliza tipos de dados mais simples como chave, como char, int e float. Internamente esses dados são convertidos para um valor de hashing do tipo std::size_t. Como queremos usar um Vertex, precisamos definir como o valor de hashing será gerado. Isso é feito através da sobrecarga do operador de chamada de função () na linha 16. O valor de hashing é o h1 da linha 17, gerado a partir da posição do vértice. Para isso funcionar, a biblioteca GLM implementa sua própria especialização de std::hash para glm::vec3 (definido no cabeçalho glm/gtx/hash.cpp). Na verdade, para este projeto poderíamos ter usado glm::vec3 diretamente no lugar de Vertex. Entretanto, em projetos futuros ampliaremos o número de atributos de Vertex e nossas chaves serão mais complexas. Essa é uma boa oportunidade para entendermos agora esse código, que só ficará maior nos próximos projetos. Vamos agora à definição de OpenGLWindow::initializeOpenGL: void OpenGLWindow::initializeGL() { abcg::glClearColor(0, 0, 0, 1); // Enable depth buffering abcg::glEnable(GL_DEPTH_TEST); // Create program m_program = createProgramFromFile(getAssetsPath() + &quot;loadmodel.vert&quot;, getAssetsPath() + &quot;loadmodel.frag&quot;); // Load model loadModelFromFile(getAssetsPath() + &quot;bunny.obj&quot;); standardize(); m_verticesToDraw = m_indices.size(); // Generate VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_vertices[0]) * m_vertices.size(), m_vertices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate EBO abcg::glGenBuffers(1, &amp;m_EBO); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(m_indices[0]) * m_indices.size(), m_indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); // End of binding to current VAO abcg::glBindVertexArray(0); } Na linha 27, o teste de profundidade é habilitado. Isso faz com que os fragmentos sejam descartados durante a renderização com base na comparação de sua profundidade com o valor atual do buffer de profundidade. Precisamos usar o teste de profundidade pois, em cenas 3D, geralmente é inviável ordenar e renderizar os triângulos do mais longe para o mais perto, como fizemos com os objetos do projeto asteroids (seção 5.2) usando o algoritmo do pintor. Na linha 34 carregamos o arquivo bunny.obj. Internamente, OpenGLWindow::loadModelFromFile armazena os vértices e índices em m_vertices e m_indices. Na linha 35, OpenGLWindow::standardize modifica esses dados para fazer com que a geometria caiba no volume de visão do pipeline gráfico, que é o cubo de tamanho \\(2 \\times 2 \\times 2\\) centralizado em \\((0,0,0)\\) no espaço normalizado do dispositivo (figura 6.29). Figura 6.29: Volume de visão em coordenadas normalizadas do dispositivo. O restante do código de OpenGLWindow::initializeGL contém a criação do VAO, VBO e EBO usando os dados de m_vertices e m_indices. A definição de OpenGLWindow::loadModelFromFile será como a seguir: void OpenGLWindow::loadModelFromFile(std::string_view path) { tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data())) { if (!reader.Error().empty()) { throw abcg::Exception{abcg::Exception::Runtime( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error()))}; } throw abcg::Exception{ abcg::Exception::Runtime(fmt::format(&quot;Failed to load model {}&quot;, path))}; } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } const auto&amp; attrib{reader.GetAttrib()}; const auto&amp; shapes{reader.GetShapes()}; m_vertices.clear(); m_indices.clear(); // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (const auto&amp; shape : shapes) { // Loop over indices for (const auto offset : iter::range(shape.mesh.indices.size())) { // Access to vertex const tinyobj::index_t index{shape.mesh.indices.at(offset)}; // Vertex position const int startIndex{3 * index.vertex_index}; const float vx{attrib.vertices.at(startIndex + 0)}; const float vy{attrib.vertices.at(startIndex + 1)}; const float vz{attrib.vertices.at(startIndex + 2)}; Vertex vertex{}; vertex.position = {vx, vy, vz}; // If hash doesn&#39;t contain this vertex if (hash.count(vertex) == 0) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } } A variável reader (linha 74) é um objeto da classe tinyobj::ObjReader, responsável pela leitura e parsing do arquivo. O resultado é um conjunto de malhas (shapes) e um conjunto de atributos de vértices (attrib). Cada malha pode ser um objeto, ou um pedaço de um objeto. No nosso caso, leremos tudo como um objeto só. Para mais detalhes sobre a estrutura utilizada pelo TinyObjLoader, consulte a documentação. Na linha 96 definimos a tabela hash que será utilizada para fazer a consulta de vértices não repetidos. Embora o formato OBJ utilize geometria indexada, durante a leitura do modelo é possível que tenhamos vértices em uma mesma posição, embora com índices diferentes. Isso acontece porque os vértices podem diferir em relação a outros atributos além da posição. Por exemplo, dois vértices podem ter a mesma posição no espaço, mas cada um pode ter uma cor diferente. Neste projeto, cada vértice só contém o atributo de posição. Podemos então simplificar o modelo mantendo apenas um índice para cada posição de vértice. Cada vértice lido do modelo OBJ será inserido na tabela hash usando a posição \\((x,y,z)\\) como chave, e a ordem de leitura do vértice como valor associado à chave. Se em algum momento verificarmos que um vértice de mesma posição já existe na tabela hash, usaremos o valor da chave como índice do vértice. No fim, teremos uma lista de vértices não repetidos (em m_vertices), que será o nosso VBO, e uma lista de índices (em m_indices), que será o EBO. No laço da linha 99, o conjunto de malhas (shapes) é iterado para ler todos os triângulos e vértices. A posição de cada vértice é lida nas linhas 107 a 109, nas variáveis vx, vy e vz, e utilizada para criar o vértice vertex nas linhas 111 e 112. Na linha 115 é verificado se o vértice atual existe na tabela hash. Se não existir, ele é incluído na tabela e em m_vertices. Na linha 122, o índice do vértice atual é inserido em m_indices. O índice é o valor da tabela hash para a chave de vertex. A definição da função OpenGLWindow::standardize será como segue: void OpenGLWindow::standardize() { // Center to origin and normalize largest bound to [-1, 1] // Get bounds glm::vec3 max(std::numeric_limits&lt;float&gt;::lowest()); glm::vec3 min(std::numeric_limits&lt;float&gt;::max()); for (const auto&amp; vertex : m_vertices) { max.x = std::max(max.x, vertex.position.x); max.y = std::max(max.y, vertex.position.y); max.z = std::max(max.z, vertex.position.z); min.x = std::min(min.x, vertex.position.x); min.y = std::min(min.y, vertex.position.y); min.z = std::min(min.z, vertex.position.z); } // Center and scale const auto center{(min + max) / 2.0f}; const auto scaling{2.0f / glm::length(max - min)}; for (auto&amp; vertex : m_vertices) { vertex.position = (vertex.position - center) * scaling; } } As maiores e menores coordenadas \\(x\\), \\(y\\) e \\(z\\) dos vértices são calculadas no laço da linha 133. Esses valores determinam os limites de uma caixa envoltória que contém o modelo geométrico. O centro dessa caixa é calculado na linha 143. Na linha 144 é calculado um fator de escala que reduz o comprimento da maior dimensão da caixa para o comprimento 2. No laço da linha 145 essas variáveis são utilizadas para centralizar o modelo na origem e mudar sua escala de modo que o modelo caiba no volume de visão em NDC, isto é, todos os vértices terão coordenadas no intervalo \\([-1, 1]\\). Definiremos OpenGLWindow::paintGL como a seguir: void OpenGLWindow::paintGL() { // Animate angle by 15 degrees per second const float deltaTime{static_cast&lt;float&gt;(getDeltaTime())}; m_angle = glm::wrapAngle(m_angle + glm::radians(15.0f) * deltaTime); // Clear color buffer and depth buffer abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); // Update uniform variable const GLint angleLoc{abcg::glGetUniformLocation(m_program, &quot;angle&quot;)}; abcg::glUniform1f(angleLoc, m_angle); // Draw triangles abcg::glDrawElements(GL_TRIANGLES, m_verticesToDraw, GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 153, o valor de m_angle é incrementado a uma taxa de 15 graus por segundo (veja que o ângulo é convertido para radianos). Observe, na linha 156, que glClear agora usa GL_DEPTH_BUFFER_BIT além de GL_COLOR_BUFFER_BIT. Isso é necessário para limpar o buffer de profundidade antes de renderizar o quadro atual. O restante do código é similar ao que já usamos em projetos anteriores. Vamos agora à definição de OpenGLWindow::paintUI: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); // Create window for slider { ImGui::SetNextWindowPos(ImVec2(5, m_viewportHeight - 94)); ImGui::SetNextWindowSize(ImVec2(m_viewportWidth - 10, -1)); ImGui::Begin(&quot;Slider window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); // Create a slider to control the number of rendered triangles { // Slider will fill the space of the window ImGui::PushItemWidth(m_viewportWidth - 25); static int n{m_verticesToDraw / 3}; ImGui::SliderInt(&quot;&quot;, &amp;n, 0, m_indices.size() / 3, &quot;%d triangles&quot;); m_verticesToDraw = n * 3; ImGui::PopItemWidth(); } ImGui::End(); } // Create a window for the other widgets { const auto widgetSize{ImVec2(172, 62)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, 5)); ImGui::SetNextWindowSize(widgetSize); ImGui::Begin(&quot;Widget window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); static bool faceCulling{}; ImGui::Checkbox(&quot;Back-face culling&quot;, &amp;faceCulling); if (faceCulling) { abcg::glEnable(GL_CULL_FACE); } else { abcg::glDisable(GL_CULL_FACE); } // CW/CCW combo box { static std::size_t currentIndex{}; const std::vector&lt;std::string&gt; comboItems{&quot;CW&quot;, &quot;CCW&quot;}; ImGui::PushItemWidth(70); if (ImGui::BeginCombo(&quot;Front face&quot;, comboItems.at(currentIndex).c_str())) { for (const auto index : iter::range(comboItems.size())) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); if (currentIndex == 0) { abcg::glFrontFace(GL_CW); } else { abcg::glFrontFace(GL_CCW); } } ImGui::End(); } } Na linhas 185 a 194 é definido o slider que controla o número de triângulos que será renderizado. Na linha 207 é criada uma caixa de seleção (checkbox) de ativação do back-face culling (estamos usando o padrão do glCullFace, que é GL_BACK). O resultado da variável booleana faceCulling é utilizado para ativar ou desativar o face culling nas linhas 210 a 212. Uma caixa de combinação (combo box) com as opções CW e CCW é definida a partir da linha 216. Nas linhas 234 e 236, a função glFrontFace é chamada com GL_CW ou GL_CCW de acordo com o que foi selecionado pelo usuário. Como a opção CW é a primeira opção da caixa, a aplicação iniciará com GL_CW. O conteúdo restante de openglwindow.cpp é similar ao utilizado nos projetos anteriores: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; } void OpenGLWindow::terminateGL() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Baixe o código completo do projeto a partir deste link. Outros modelos OBJ estão disponíveis neste link. "],["transformations.html", "7 Matrizes e transformações", " 7 Matrizes e transformações Este capítulo dá sequência à introdução de conceitos utilizados no processamento geométrico para síntese de imagens. A seção 7.1 é uma revisão sobre os conceitos de matrizes e transformações lineares. A seção 7.2 faz uma introdução ao sistema de coordenadas homogêneas. Coordenadas homogêneas possibilitam representar de forma unificada transformações afins (transformações lineares e deslocamento de pontos) através de operações matriciais. A seção 7.3 mostra como podemos fazer concatenação de transformações através de produtos matriciais. Junto com a representação em coordenadas homogêneas, a concatenação de transformações como produtos entre matrizes simplifica e melhora a eficiência do processamento geométrico. Na seção 7.4 são apresentadas as principais matrizes de transformação que serão utilizadas no curso. Tais matrizes representam transformações entre espaços que podem ser utilizadas para posicionar objetos em uma cena e definir câmeras virtuais. A seção 7.5 apresenta os principais espaços utilizados no pipeline do OpenGL. A seção 7.6 apresenta o conceito de câmera LookAt, que é uma forma de criar o frame de uma câmera virtual a partir de dois pontos (posição da câmera e posição para onde a câmera está olhando) e um vetor de referência para indicar a direção para cima. Este e outros conceitos abordados neste capítulo são colocados em prática no projeto lookat da seção 7.7. "],["matrix.html", "7.1 Matrizes", " 7.1 Matrizes Uma matriz é um arranjo de \\(n \\times m\\) escalares, onde \\(n\\) é o número de linhas e \\(m\\) é o número de colunas. No exemplo a seguir, \\(\\mathbf{A}\\) é uma matriz de tamanho \\(3 \\times 4\\), isto é, é uma matriz de 3 linhas e 4 colunas: \\[ \\mathbf{A}= \\begin{bmatrix} \\phantom{-.1}9 &amp; \\phantom{-}2 &amp; \\phantom{.2} 10 &amp; \\phantom{.2} 4 \\\\ \\phantom{-.1}4 &amp; -2 &amp; 0.25 &amp; \\phantom{.2} 1 \\\\ -8.1 &amp; \\phantom{-}6 &amp; \\phantom{.2} 42 &amp; \\phantom{.2} 0 \\\\ \\end{bmatrix}. \\] Cada elemento de \\(\\mathbf{A}\\) é denotado por \\(a_{ij}\\), onde \\(i\\) é a linha, e \\(j\\) é a coluna do elemento da matriz: \\[ \\mathbf{A}= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nm} \\end{bmatrix}. \\] Matriz transposta A transposta de uma matriz \\(\\mathbf{A}\\) de tamanho \\(n \\times m\\), denotada por \\(\\mathbf{A}^T\\), é uma matriz de tamanho \\(m \\times n\\) obtida trocando as linhas pelas colunas. Por exemplo, se \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix}, \\] então \\[ \\mathbf{A}^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix}. \\] A transposta da transposta é a matriz original, isto é, \\[{(\\mathbf{A}^T)}^T = \\mathbf{A}.\\] Matriz quadrada Uma matriz quadrada é uma matriz na qual o número de linhas é igual ao número de colunas (\\(n=m\\)). Se \\(\\mathbf{A}\\) é uma matriz quadrada de \\(n\\) linhas e colunas, dizemos que \\(\\mathbf{A}\\) é uma matriz de ordem \\(n\\). Por exemplo, \\[ \\mathbf{A}= \\begin{bmatrix} 5 &amp; 1 &amp; 4 \\\\ 7 &amp; 0 &amp; 2 \\\\ 3 &amp; 3 &amp; 9 \\\\ \\end{bmatrix} \\] é uma matriz de ordem 3. Matriz linha/coluna Uma tupla de \\(n\\) elementos pode ser representada por uma matriz \\(1 \\times n\\) (matriz linha) ou \\(n \\times 1\\) (matriz coluna). Para matrizes linha e matrizes coluna, usaremos a notação de letra minúscula em negrito \\((\\mathbf{a}, \\mathbf{b}, \\dots)\\), como na notação de vetor. Para trabalharmos com transformações matriciais, usaremos matrizes coluna para escrever a representação de vetores em uma base conhecida. Por exemplo, a representação de um vetor no \\(\\mathbb{R}^3\\) pode ser escrita pela matriz de componentes \\[ \\mathbf{v} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\] sendo que \\[\\mathbf{v}=x\\hat{\\mathbf{i}} + y\\hat{\\mathbf{j}} + z\\hat{\\mathbf{k}}.\\] Matriz identidade A matriz identidade, denotada por \\(\\mathbf{I}\\), é uma matriz quadrada na qual todos os elementos de uma das diagonais  elementos \\(a_{ij}\\) em que \\(i=j\\), chamada de diagonal principal  são \\(1\\), e todos os outros elementos são \\(0\\). \\[ \\mathbf{I}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix}. \\] Por exemplo, a matriz identidade de ordem 3 é a matriz \\[ \\mathbf{I}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] A matriz identidade é o elemento neutro da multiplicação entre matrizes: \\[ \\mathbf{A}\\mathbf{I} = \\mathbf{I}\\mathbf{A} = \\mathbf{A}. \\] Operações Há três operações básicas com matrizes: Adição de matriz com matriz: \\[\\mathbf{A} + \\mathbf{B}\\] tem como resultado uma matriz que contém a soma elemento a elemento de \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\), sendo que \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) precisam ter o mesmo tamanho. Por exemplo, se \\[ \\begin{align} \\mathbf{A} &amp;= \\begin{bmatrix} \\phantom{-}1 &amp; -2 &amp; \\phantom{-}3 \\\\ -4 &amp; \\phantom{-}5 &amp; \\phantom{-}6 \\\\ \\end{bmatrix},\\\\ \\mathbf{B} &amp;= \\begin{bmatrix} -1 &amp; -3 &amp; \\phantom{-}6 \\\\ \\phantom{-}0 &amp; \\phantom{-}5 &amp; \\phantom{-}2 \\\\ \\end{bmatrix}, \\end{align} \\] então \\[ \\mathbf{A}+\\mathbf{B} = \\begin{bmatrix} \\phantom{-}0 &amp; -5 &amp; \\phantom{-}9 \\\\ -4 &amp; 10 &amp; \\phantom{-}8 \\\\ \\end{bmatrix}. \\] Essa operação é comutativa e associativa: \\[ \\begin{align} \\mathbf{A}+\\mathbf{B} &amp;= \\mathbf{B}+\\mathbf{A},\\\\ \\mathbf{A}+(\\mathbf{B}+\\mathbf{C}) &amp;= (\\mathbf{A}+\\mathbf{B})+\\mathbf{C}. \\end{align} \\] Multiplicação de escalar com matriz: \\[a\\mathbf{A}\\] tem como resultado uma matriz na qual cada elemento da matriz \\(\\mathbf{A}\\) é multiplicado pelo escalar \\(a\\). Por exemplo, se \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix}, \\] então \\[ 2\\mathbf{A} = \\begin{bmatrix} 2 &amp; 4 &amp; 6 \\\\ 8 &amp; 10 &amp; 12 \\\\ \\end{bmatrix}. \\] Essa operação é distributiva e associativa: \\[ \\begin{align} a(\\mathbf{A}+\\mathbf{B}) &amp;= a\\mathbf{A}+a\\mathbf{B},\\\\ (a+b)\\mathbf{A} &amp;= a\\mathbf{A}+b\\mathbf{A},\\\\ a(b\\mathbf{A}) &amp;= (ab)\\mathbf{A},\\\\ ab\\mathbf{A} &amp;= ba\\mathbf{A}. \\end{align} \\] Multiplicação de matriz com matriz: \\[\\mathbf{A}\\mathbf{B}\\] ou \\[\\mathbf{A}.\\mathbf{B}\\] é definida apenas quando o número de colunas de \\(\\mathbf{A}\\) é igual ao número de linhas de \\(\\mathbf{B}\\). Se \\(\\mathbf{A}\\) é uma matriz \\(n \\times l\\), e \\(\\mathbf{B}\\) é uma matriz \\(l \\times m\\), então \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) é uma matriz de tamanho \\(n \\times m\\) tal que \\[ c_{ij}=\\sum_{k=1}^l a_{ik}b_{kj}. \\] Desse modo, o elemento \\(c_{ij}\\) (elemento da \\(i\\)-ésima linha e \\(j\\)-ésima coluna de \\(\\mathbf{C}\\)) será o somatório da multiplicação elemento a elemento da \\(i\\)-ésima linha de \\(\\mathbf{A}\\) com a \\(j\\)-ésima coluna de \\(\\mathbf{B}\\). Por exemplo, se \\[ \\begin{align} \\mathbf{A} &amp;= \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix},\\\\ \\mathbf{B} &amp;= \\left[ \\begin{array}{rr} -1 &amp; 0 \\\\ -3 &amp; 5 \\\\ 6 &amp; 2 \\end{array} \\right], \\end{align} \\] então \\[ \\begin{align} \\mathbf{C} &amp;=\\mathbf{A}\\mathbf{B} \\\\ &amp;= \\left[ \\begin{array}{rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{array} \\right] \\left[ \\begin{array}{rr} -1 &amp; 0 \\\\ -3 &amp; 5 \\\\ 6 &amp; 2 \\end{array} \\right]\\\\ &amp;= \\left[ \\begin{array}{rr} (1 \\times -1)+(2 \\times -3)+(3 \\times 6) &amp; (1 \\times 0)+(2 \\times 5)+(3 \\times 2) \\\\ (4 \\times -1)+(5 \\times -3)+(6 \\times 6) &amp; (4 \\times 0)+(5 \\times 5)+(6 \\times 2) \\end{array} \\right]\\\\ &amp;= \\left[ \\begin{array}{rr} 11 &amp; 16 \\\\ 17 &amp; 37 \\end{array} \\right]. \\end{align} \\] Embora a multiplicação entre matrizes seja associativa, \\[ \\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\] na maioria das vezes não é comutativa, isto é, \\[ \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. \\] Os casos em que \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}\\) são casos especiais como, por exemplo, quando \\(\\mathbf{A}\\mathbf{B} = \\mathbf{I}\\) (veja definição de matriz inversa a seguir), ou quando a matriz é multiplicada pela identidade. Se \\(\\mathbf{a}\\) e \\(\\mathbf{b}\\) são representações de vetores \\[ \\begin{align} \\mathbf{a} = \\left[ \\begin{array}{} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{array} \\right],\\qquad \\mathbf{b} = \\left[ \\begin{array}{} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{array} \\right], \\end{align} \\] a multiplicação da transposta de \\(\\mathbf{a}\\) por \\(\\mathbf{b}\\) tem como resultado o produto escalar entre os vetores: \\[ \\begin{align} \\mathbf{a}^T\\mathbf{b} &amp;= \\left[ \\begin{array}{} a_1 &amp; a_2 &amp; \\dots &amp; a_n \\end{array} \\right] \\left[ \\begin{array}{} b_1 \\\\ b_2 \\\\ \\vdots \\\\ a_n \\end{array} \\right]\\\\ &amp;= a_1b_1 + a_2b_2 + \\dots + a_nb_n\\\\ &amp;= \\mathbf{a} \\cdot \\mathbf{b}. \\end{align} \\] Da mesma forma, podemos definir a norma euclidiana de \\(\\mathbf{a}\\) como \\[ \\begin{align} |\\mathbf{a}| &amp;= \\sqrt{\\mathbf{a}^T \\mathbf{a}} \\end{align}. \\] Matriz inversa Se, para uma dada matriz quadrada \\(\\mathbf{A}\\), existir uma matriz \\(\\mathbf{B}\\) tal que \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A} = \\mathbf{I}\\), então \\(\\mathbf{B}\\) é a inversa de \\(\\mathbf{A}\\), denotada por \\(\\mathbf{A}^{-1}\\): \\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}. \\] A inversa da inversa é a matriz original, isto é, \\[{(\\mathbf{A}^{-1})}^{-1} = \\mathbf{A}.\\] Se uma matriz possui uma inversa, dizemos que a matriz é inversível ou não singular. Caso contrário, dizemos que a matriz é singular. Uma matriz \\(\\mathbf{A}\\) é inversível apenas se seu determinante, denotado por \\(|\\mathbf{A}|\\) ou \\(\\det(\\mathbf{A}),\\) for diferente de zero. O determinante de uma matriz de ordem \\(n\\) pode ser definido recursivamente como uma soma ponderada de \\(n\\) determinantes de submatrizes de ordem \\(n-1\\) (teorema de Laplace): \\[ |\\mathbf{A}|=\\sum^n_{i=1}(-1)^{i+j}a_{ij}M_{ij}, \\] onde \\(M_{ij}\\), chamado de determinante menor de \\(\\mathbf{A}\\), é o determinante da matriz de ordem \\(n-1\\) obtida através da remoção da linha \\(i\\) e coluna \\(j\\) de \\(\\mathbf{A}\\). Nessa expressão, \\((-1)^{i+j}M_{ij}\\) é chamado de cofator do elemento \\(a_{ij}\\) e é denotado por \\(C_{ij}\\). Para matrizes de ordem 2 e 3, o procedimento é suficientemente simples e é mostrado a seguir: Para uma matriz de ordem 2, \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}, \\] \\[ \\begin{align} |\\mathbf{A}| &amp;= \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix}\\\\ &amp;= ad - bc. \\end{align} \\] Para uma matriz de ordem 3, \\[ \\begin{align} \\mathbf{A} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix}, \\end{align} \\] \\[ \\begin{align} |\\mathbf{A}| &amp;= \\begin{vmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{vmatrix}\\\\ &amp;= a \\begin{vmatrix} e &amp; f \\\\ h &amp; i \\end{vmatrix} -b \\begin{vmatrix} d &amp; f \\\\ g &amp; i \\end{vmatrix} +c \\begin{vmatrix} d &amp; e \\\\ g &amp; h \\end{vmatrix}\\\\ &amp;= aei + bfg + cdh - ceg -bdi - afh. \\end{align} \\] A complexidade de tempo para calcular determinante por este método é \\(O(n!)\\). Isso só é rápido para matrizes de ordem até 5. Há soluções mais eficientes para matrizes de ordem mais elevada, como o uso de decomposição LU ou eliminação de Gauss, com complexidade \\(O(n^3)\\). Entretanto, como usaremos matrizes de ordem até 4, o método anterior é suficiente. A matriz inversa de uma matriz \\(\\mathbf{A}\\) inversível pode ser calculada como \\[ \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} . \\textrm{adj}(\\mathbf{A}), \\] onde \\(\\textrm{adj}(\\mathbf{A})\\), chamada de matriz adjunta de \\(\\mathbf{A}\\), é a transposta da matriz de cofatores de \\(\\mathbf{A}\\). Isto é, \\[ \\textrm{adj}(\\mathbf{A})= \\begin{bmatrix} C_{ij} \\end{bmatrix}^T. \\] Em computação gráfica, frequentemente podemos evitar o uso de métodos numéricos de inversão de matrizes pois muitas matrizes representam transformações inversíveis através de um raciocínio geométrico. Por exemplo: Se uma matriz representa uma rotação de \\(45\\) graus em torno do eixo \\(x\\), então sua inversa é uma matriz que representa uma rotação de \\(-45\\) graus em torno do eixo \\(x\\); Se uma matriz representa uma translação pelo vetor \\((x,y,z)\\), então sua inversa é uma matriz que representa uma translação pelo vetor \\((-x,-y,-z)\\). Se uma matriz representa uma mudança de escala por um fator \\(s \\neq 0\\), então sua inversa é uma matriz de mudança de escala por um fator \\(1/s\\). Abordaremos essas e outras matrizes de transformação na seção 7.4. Matriz ortogonal Uma matriz \\(\\mathbf{A}\\) é ortogonal se \\[\\mathbf{A}\\mathbf{A}^T=\\mathbf{A}^T\\mathbf{A}=\\mathbf{I},\\] isto é, sua transposta é também a sua inversa: \\[\\mathbf{A}^T=\\mathbf{A}^{-1}.\\] As linhas e colunas de uma matriz ortogonal formam uma base ortonormal. Por exemplo, as matrizes \\[ \\begin{align} \\mathbf{A} &amp;= \\begin{bmatrix} \\hat{\\mathbf{i}} &amp; \\phantom{-}\\hat{\\mathbf{j}} &amp; \\phantom{-}\\hat{\\mathbf{k}} \\\\ \\end{bmatrix},\\\\ \\mathbf{B} &amp;= \\begin{bmatrix} \\hat{\\mathbf{i}} &amp; -\\hat{\\mathbf{j}} &amp; \\phantom{-}\\hat{\\mathbf{k}} \\\\ \\end{bmatrix},\\\\ \\mathbf{C} &amp;= \\begin{bmatrix} \\hat{\\mathbf{k}} &amp; \\phantom{k}\\hat{\\mathbf{i}} &amp; \\phantom{-.}\\hat{\\mathbf{j}} \\\\ \\end{bmatrix}, \\end{align} \\] com colunas formadas pelos seguintes vetores da base padrão do \\(\\mathbb{R}^3\\), \\[ \\begin{align} \\hat{\\mathbf{i}} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\qquad \\hat{\\mathbf{j}} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\qquad \\hat{\\mathbf{k}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\end{align} \\] são matrizes ortogonais. o determinante de uma matriz ortogonal é sempre \\(1\\) ou \\(-1\\). Propriedades As seguintes propriedades se aplicam: \\((\\mathbf{A}+\\mathbf{B})^T=\\mathbf{A}^T + \\mathbf{B}^T\\). \\((\\mathbf{A}^T)^{-1}=(\\mathbf{A}^{-1})^T\\). \\((\\mathbf{A}\\mathbf{B})^T=\\mathbf{B}^T\\mathbf{A}^T\\). Observe como a ordem dos fatores é trocada. No caso geral, \\((\\mathbf{A}_1\\mathbf{A}_2\\dots\\mathbf{A}_k)^T=\\mathbf{A}_k^T\\dots\\mathbf{A}_2^T\\mathbf{A}_1^T\\). \\((\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}\\). No caso geral, para um conjunto \\(\\{\\mathbf{A}_i\\}\\) de matrizes inversíveis, \\((\\mathbf{A}_1\\mathbf{A}_2\\dots\\mathbf{A}_k)^{-1}=\\mathbf{A}_k^{-1}\\dots\\mathbf{A}_2^{-1}\\mathbf{A}_1^{-1}\\). Transformação Seja \\(\\mathbf{v}\\) uma matriz coluna que representa um vetor no \\(\\mathbb{R}^n\\). A transformação \\(T(\\mathbf{v})\\), que transforma \\(\\mathbf{v}\\) em uma nova representação \\(\\mathbf{v}&#39;\\), pode ser escrita como \\[\\mathbf{v}&#39;=\\mathbf{A}\\mathbf{v},\\] onde \\(\\mathbf{A}\\) é uma matriz quadrada de ordem \\(n\\) chamada de matriz de transformação. Toda matriz de transformação de ordem \\(n\\) representa uma transformação linear em \\(\\mathbb{R}^n\\). Uma transformação linear é um mapeamento entre dois espaços vetoriais de tal modo que as operações de adição de vetor com vetor e multiplicação de vetor com escalar são preservadas, isto é, \\[ T(a\\mathbf{u}+b\\mathbf{v})=aT(\\mathbf{u})+bT(\\mathbf{v}) \\] para quaisquer vetores \\(\\mathbf{u}, \\mathbf{v} \\in V\\), e quaisquer escalares \\(a\\) e \\(b\\). Como resultado, a transformação do vetor representado por \\(\\mathbf{v}\\) é equivalente à combinação linear da transformação da base do vetor: \\[\\mathbf{v}&#39;=T(\\mathbf{v})=v_1T(\\mathbf{e}_1) + v_2T(\\mathbf{e}_2) + \\cdots + v_nT(\\mathbf{e}_n).\\] Se \\(\\mathbf{v} \\in \\mathbb{R}^3\\), temos \\[\\mathbf{v}&#39;=T(\\mathbf{v})=xT(\\hat{\\mathbf{i}}) + yT(\\hat{\\mathbf{j}}) + zT(\\hat{\\mathbf{k}}).\\] Essa expressão pode ser representada na forma matricial: \\[ \\mathbf{v}&#39;=\\mathbf{A}\\mathbf{v}\\\\ \\begin{bmatrix}x&#39;\\\\y&#39;\\\\z&#39;\\end{bmatrix}= \\begin{bmatrix} T(\\hat{\\mathbf{i}}) &amp; T(\\hat{\\mathbf{j}}) &amp; T(\\hat{\\mathbf{k}}) \\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}. \\] Logo, a matriz que transforma \\(\\mathbf{v}\\), da base \\(\\{\\hat{\\mathbf{i}}, \\hat{\\mathbf{j}}, \\hat{\\mathbf{k}}\\}\\), para \\(\\mathbf{v}&#39;\\), da base \\(\\{T(\\hat{\\mathbf{i})}, T(\\hat{\\mathbf{j})}, T(\\hat{\\mathbf{k}})\\}\\), é uma matriz de mudança de base, e é construída fazendo com que suas colunas sejam os vetores da base transformada. Transformações lineares e matrizes O vídeo a seguir, do canal 3Blue1Brown do Youtube, apresenta de forma visual o conceito de transformação linear e sua relação com a operação matricial. O áudio está em inglês, mas há legendas em português. Este vídeo faz parte de uma excelente série de vídeos sobre fundamentos de álgebra linear. O conteúdo permite compreender de forma intuitiva os principais conceitos vistos até agora. A maioria dos vídeos possui legendas em português. Além de transformações lineares, matrizes podem representar também outras transformações, tais como: Transformação afim: para a translação de pontos; Transformação projetiva: para projeção perspectiva. Essas transformações podem ser representadas por transformações lineares em um espaço de dimensão adicional. Por exemplo, uma transformação afim no \\(\\mathbb{R}^3\\) pode ser representada por uma matriz de transformação linear no \\(\\mathbb{R}^4\\). Veremos como fazer isso na seção 7.2. "],["homogeneous.html", "7.2 Coordenadas homogêneas", " 7.2 Coordenadas homogêneas A expressão \\[\\mathbf{v}&#39;=\\mathbf{A}\\mathbf{v},\\] onde \\[ \\mathbf{v}&#39;=\\begin{bmatrix}x&#39;\\\\y&#39;\\\\z&#39;\\end{bmatrix},\\qquad \\mathbf{A}=\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix}, \\qquad \\mathbf{v}=\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}, \\] representa uma transformação linear do vetor \\(\\mathbf{v} \\in \\mathbb{R}^3\\). Podemos considerar que \\(\\mathbf{v}\\) também representa um ponto no frame padrão do espaço euclidiano tridimensional: é o ponto resultante do deslocamento da origem pelo vetor \\(\\mathbf{v}\\). Com isso podemos aplicar transformações lineares sobre pontos. Entretanto, essa notação não permite diferenciar o que é a representação de um ponto e o que é a representação de um vetor. Além disso, a matriz de transformação linear no \\(\\mathbb{R}^3\\) não é capaz de representar transformações que envolvem deslocamento de pontos. A simples operação \\[\\mathbf{p&#39;}=\\mathbf{p}+\\mathbf{t},\\] onde \\(\\mathbf{p}\\) é um ponto e \\(\\mathbf{t}\\) é um vetor, não pode ser representada como uma matriz de transformação linear na forma \\[ \\mathbf{p}&#39;=\\mathbf{A}\\mathbf{p}\\\\ \\\\ \\begin{bmatrix}x&#39;\\\\y&#39;\\\\z&#39;\\end{bmatrix}= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}. \\] Felizmente, podemos contornar essas dificuldades se representarmos pontos e vetores do \\(\\mathbb{R}^3\\) em um sistema de coordenadas homogêneas no espaço \\(\\mathbb{R}^4\\). Como vimos na definição de combinação afim (seção 6.2), podemos considerar que é válido multiplicar um escalar \\(a\\) por um ponto \\(P\\), de modo que \\[ 0 P = \\mathbf{0},\\\\ 1 P = P. \\] Um ponto \\(P=(x,y,z)\\) no frame \\(\\{P_0, \\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) é definido unicamente como \\[ P = x\\mathbf{v}_1 + y\\mathbf{v}_2 + z\\mathbf{v}_3 + P_0. \\] Em coordenadas homogêneas, \\(P\\) pode ser representado pela matriz coluna de coeficientes \\[ \\begin{align} \\mathbf{p}&amp;=\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}, \\end{align} \\] uma vez que \\[ \\begin{align} P &amp;= x\\mathbf{v}_1 + y\\mathbf{v}_2 + z\\mathbf{v}_3 + P_0\\\\ &amp;= \\begin{bmatrix}x &amp; y &amp; z &amp; 1\\end{bmatrix} \\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\mathbf{v}_3 \\\\ P_0\\end{bmatrix}. \\end{align} \\] De forma semelhante, um vetor \\(\\mathbf{v}\\) no mesmo frame, \\[ \\mathbf{v} = x\\mathbf{v}_1 + y\\mathbf{v}_2 + z\\mathbf{v}_3, \\] pode ser definido em coordenadas homogêneas pela matriz coluna de coeficientes \\[ \\begin{align} \\mathbf{v}&amp;=\\begin{bmatrix}x\\\\y\\\\z\\\\0\\end{bmatrix}, \\end{align} \\] uma vez que \\[ \\begin{align} \\mathbf{v} &amp;= x\\mathbf{v}_1 + y\\mathbf{v}_2 + z\\mathbf{v}_3\\\\ &amp;= \\begin{bmatrix}x &amp; y &amp; z &amp; 0\\end{bmatrix} \\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\mathbf{v}_3 \\\\ P_0\\end{bmatrix}. \\end{align} \\] Na expressão \\[\\mathbf{p}&#39;=\\mathbf{A}\\mathbf{p},\\] onde \\[ \\mathbf{p}&#39;=\\begin{bmatrix}x&#39;\\\\y&#39;\\\\z&#39;\\\\w&#39;\\end{bmatrix},\\qquad \\mathbf{A}=\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}x\\\\y\\\\z\\\\w\\end{bmatrix}, \\] a matriz \\(\\mathbf{A}\\) representa uma transformação linear no \\(\\mathbb{R}^4\\) e uma transformação afim no \\(\\mathbb{R}^3\\). A transformação afim preserva a operação de combinação afim, isto é, \\[ T(aP+(1-a)Q)=aT(P)+(1-a)T(Q) \\] para quaisquer pontos \\(P\\) e \\(Q\\), e qualquer escalar \\(a \\in [0,1]\\). Assim como a transformação linear transforma espaços vetoriais, a transformação afim de um ponto \\(\\mathbf{p}\\) equivale à transformação do frame de \\(\\mathbf{p}\\) em outro frame (frame de \\(\\mathbf{p}&#39;\\)). Com a matriz de transformação afim, conseguimos representar tanto as transformações de espaços vetoriais quanto as transformações que envolvem deslocamento de pontos. Como mostra a figura 7.1, a parte \\(3 \\times 3\\) superior de \\(\\mathbf{A}\\) representa uma ou mais transformações lineares em \\(\\mathbb{R}^3\\) (como a rotação, escala e reflexão, abordadas na seção 7.4). A parte \\(3 \\times 1\\) da última coluna (\\(\\begin{bmatrix}a_{14} &amp; a_{24} &amp; a_{34}\\end{bmatrix}^T\\)) representa uma translação (deslocamento de ponto). Figura 7.1: Parte de transformação linear e translação em uma matriz na notação homogênea. Agora podemos representar o deslocamento \\(\\mathbf{p&#39;}=\\mathbf{p}+\\mathbf{t}\\) através de uma operação matricial: \\[ \\mathbf{p}&#39;=\\mathbf{A}\\mathbf{p}\\\\ \\\\ \\begin{bmatrix}x+t_x\\\\y+t_y\\\\z+tz\\\\1\\end{bmatrix}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; 0 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}. \\] Observe como, de fato, os fatores de deslocamento estão na parte \\(3 \\times 1\\) da última coluna. Em \\(\\mathbf{p}=\\begin{bmatrix}x &amp; y &amp; z &amp; w\\end{bmatrix}^T\\), a coordenada \\(w\\) é chamada de coordenada homogênea. A escolha de fazer \\(w=1\\) para pontos e \\(w=0\\) para vetores permite diferenciar, sem ambiguidades, as operações de adição de vetor com vetor, diferença entre pontos, e adição de ponto com vetor: Na adição de vetor com vetor, o resultado é um vetor (coordenada \\(w=0\\)): \\[ \\begin{align} \\mathbf{u}+\\mathbf{v}&amp;=\\mathbf{w},\\\\ \\\\ \\begin{bmatrix}u_x\\\\u_y\\\\u_z\\\\0\\end{bmatrix}+ \\begin{bmatrix}v_x\\\\v_y\\\\v_z\\\\0\\end{bmatrix}&amp;= \\begin{bmatrix}u_x+v_x\\\\u_y+v_y\\\\u_z+v_z\\\\0\\end{bmatrix}. \\end{align} \\] Na diferença entre pontos, o resultado é um vetor (coordenada \\(w=0\\)): \\[ \\begin{align} \\mathbf{p}-\\mathbf{q}&amp;=\\mathbf{u},\\\\ \\\\ \\begin{bmatrix}p_x\\\\p_y\\\\p_z\\\\1\\end{bmatrix}- \\begin{bmatrix}q_x\\\\q_y\\\\q_z\\\\1\\end{bmatrix}&amp;= \\begin{bmatrix}p_x-q_x\\\\p_y-q_y\\\\p_z-q_z\\\\0\\end{bmatrix}. \\end{align} \\] Na adição de ponto com vetor, o resultado é um ponto (coordenada \\(w=1\\)): \\[ \\begin{align} \\mathbf{p}+\\mathbf{u}&amp;=\\mathbf{q},\\\\ \\\\ \\begin{bmatrix}p_x\\\\p_y\\\\p_z\\\\1\\end{bmatrix}+ \\begin{bmatrix}u_x\\\\u_y\\\\u_z\\\\0\\end{bmatrix}&amp;= \\begin{bmatrix}p_x+u_x\\\\p_y+u_y\\\\p_z+u_z\\\\1\\end{bmatrix}. \\end{align} \\] Coordenadas homogêneas \\((x,y,z,w)\\) podem ser convertidas de volta para coordenadas cartesianas \\((x&#39;,y&#39;,z&#39;)\\) do espaço euclidiano 3D através da divisão de \\(x\\), \\(y\\), \\(z\\) por \\(w\\): \\[ (x&#39;, y&#39;, z&#39;) = \\left(\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}\\right). \\] Assim, um ponto \\(\\mathbf{p}\\) em coordenadas homogêneas, \\[ \\mathbf{p}=\\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}, \\] corresponde, em coordenadas cartesianas, ao ponto \\[ \\mathbf{p}&#39;=\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix}=\\begin{bmatrix}x/1 \\\\ y/1 \\\\ z/1\\end{bmatrix}. \\] Em transformações afins expressas de forma homogênea, o valor de \\(w\\) será sempre \\(0\\) ou \\(1\\). Entretanto, em matrizes de transformação projetiva, \\(w\\) poderá assumir outros valores. Transformações projetivas serão abordadas no próximo capítulo. Entretanto, perceba que a divisão por \\(w\\) faz com que um ponto \\((x&#39;,y&#39;,z&#39;)\\) do espaço euclidiano 3D corresponda a infinitos pontos no espaço de dimensão extra (4D). Em particular, os pontos \\((sx,sy,sz,sw)\\), onde \\(s \\neq 0\\) e \\(w \\neq 0\\), correspondem ao mesmo ponto \\((x&#39;,y&#39;,z&#39;)\\) do espaço euclidiano: \\[ (x&#39;, y&#39;, z&#39;) = \\left(\\frac{sx}{sw}, \\frac{sy}{sw}, \\frac{sz}{sw}\\right) = \\left(\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}\\right). \\] Em outras palavras, os pontos \\((sx,sy,sz,sw)\\) do \\(\\mathbb{R}^4\\) são projetados em um mesmo ponto \\((x&#39;,y&#39;,z&#39;)\\) do \\(\\mathbb{R}^3\\). Esse comportamento será útil para simular o efeito de diminuição do tamanho de objetos em uma projeção perspectiva. Em particular, note que: Se um objeto é formado por pontos com coordenada homogênea \\(w &gt; 1\\), o objeto diminui de tamanho após a conversão para o espaço euclidiano; Se um objeto é formado por pontos com coordenadas homogênea \\(0 \\leq w &lt; 1\\), o objeto aumenta de tamanho após a conversão para o espaço euclidiano. "],["concat.html", "7.3 Concatenação de transformações", " 7.3 Concatenação de transformações Podemos expressar uma sequência, composição ou concatenação de transformações através de um produto matricial. Por exemplo, a transformação de um ponto (ou vetor) \\(\\mathbf{p}\\) por \\(\\mathbf{A}\\), e em seguida por \\(\\mathbf{B}\\), e então por \\(\\mathbf{C}\\), pode ser escrita como: \\[\\mathbf{p}&#39;=\\mathbf{C}(\\mathbf{B}(\\mathbf{A}\\mathbf{p})),\\] Como a multiplicação entre matrizes é associativa, podemos remover os parênteses: \\[\\mathbf{p}&#39;=\\mathbf{C}\\mathbf{B}\\mathbf{A}\\mathbf{p}.\\] Observação Lembre-se que, em geral, o produto matricial não é comutativo: \\[\\mathbf{CBA}\\neq\\mathbf{ABC}.\\] Na expressão \\(\\mathbf{p}&#39;=\\mathbf{C}\\mathbf{B}\\mathbf{A}\\mathbf{p}\\), a ordem de aplicação das transformações é determinada pela leitura da direita para a esquerda: Em 1º lugar, aplica-se a transformação representada pela matriz \\(\\mathbf{A}\\). Em 2º lugar, aplica-se a transformação representada pela matriz \\(\\mathbf{B}\\). Por último, a transformação representada pela matriz \\(\\mathbf{C}\\). Se quisermos ler a ordem das transformações da esquerda para a direita, precisamos antes calcular a transposta da expressão: \\[ \\begin{align} \\mathbf{p}&#39;^T&amp;=(\\mathbf{C}\\mathbf{B}\\mathbf{A}\\mathbf{p})^T\\\\ &amp;=\\mathbf{p}^T\\mathbf{A}^T\\mathbf{B}^T\\mathbf{C}^T. \\end{align} \\] Observe que, neste caso, \\(\\mathbf{p}^T\\) é uma matriz linha multiplicada à esquerda (pré-multiplicação) da matriz de transformação. Como nossa convenção neste curso é usar representações de pontos e vetores como matrizes coluna multiplicadas à direita (pós-multiplicação), usaremos sempre a expressão original \\(\\mathbf{p}&#39;=\\mathbf{C}\\mathbf{B}\\mathbf{A}\\mathbf{p}\\). No caso geral, se um ponto ou vetor é representado por uma matriz coluna \\(\\mathbf{p}\\), então a expressão \\[\\mathbf{p}&#39;=\\mathbf{A}_k \\mathbf{A}_{k-1}\\dots\\mathbf{A}_1 \\mathbf{p}\\] representa a transformação do ponto/vetor por uma sequência de transformações na ordem \\[\\mathbf{A}_1, \\mathbf{A}_2, \\dots, \\mathbf{A}_k.\\] Se for necessário aplicar uma mesma sequência de transformações a diferentes pontos ou vetores, é desejável primeiramente armazenar o resultado da multiplicação das matrizes de transformação em uma única matriz. Por exemplo, \\[ \\mathbf{M}=\\mathbf{A}_k \\mathbf{A}_{k-1}\\dots\\mathbf{A}_1, \\] onde \\(\\mathbf{M}\\) representa as transformações na ordem \\(\\mathbf{A}_1, \\mathbf{A}_2, \\dots, \\mathbf{A}_k\\) e pode ser utilizada para transformar quantos pontos/vetores forem necessários: \\[ \\mathbf{v}_1&#39;=\\mathbf{M}\\mathbf{v}_1,\\\\ \\mathbf{v}_2&#39;=\\mathbf{M}\\mathbf{v}_2,\\\\ \\vdots \\] A malha de triângulos de um modelo geométrico pode ser composta por milhares de vértices, e a transformação de um modelo exige a transformação da posição de todos os seus vértices. Assim, a concatenação de transformações em uma única matriz aumenta a eficiência do processamento geométrico. "],["transforms.html", "7.4 Transformações", " 7.4 Transformações Nesta seção definiremos as matrizes de transformação em coordenadas homogêneas que correspondem às principais transformações afins utilizadas no pipeline de renderização. Identidade A transformação de identidade é a transformação que mapeia um ponto a ele mesmo: \\[ \\mathbf{p}&#39;=\\mathbf{p}. \\] É representada por uma matriz identidade: \\[ \\mathbf{I}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Assim, \\[ \\mathbf{p}&#39;=\\mathbf{I}\\mathbf{p}=\\mathbf{p}. \\] Translação Translação é a transformação afim de deslocamento de um ponto \\(\\mathbf{p}\\) por um vetor \\(\\mathbf{t}\\): \\[ \\mathbf{p}&#39;=\\mathbf{p}+\\mathbf{t} \\] \\[ \\begin{bmatrix} x&#39; \\\\ y&#39; \\\\ z&#39; \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}+ \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\\\ 0 \\end{bmatrix}. \\] A figura 7.2 ilustra o resultado da translação de todos os pontos de um cubo centralizado na origem. Figura 7.2: Translação. A matriz de translação é definida como \\[ \\mathbf{T}=\\mathbf{T}(\\mathbf{t})= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; 0 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Assim, \\[ \\mathbf{p}&#39;=\\mathbf{T}\\mathbf{p} \\] \\[ \\begin{bmatrix} x+t_x\\\\ y+t_y\\\\ z+t_z\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; 0 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\end{bmatrix}. \\] A inversa da translação é o deslocamento de \\(\\mathbf{p}\\) no sentido inverso do vetor de deslocamento: \\[ \\mathbf{T}^{-1}(\\mathbf{t})=\\mathbf{T}(-\\mathbf{t}). \\] Note que a transformação de translação não altera o formato do objeto. O objeto deslocado continua com as mesmas proporções. Uma transformação que, como a translação, preserva a distância entre todos os pontos do objeto transformado, é chamada de transformação de corpo rígido. Escala A escala é um tipo de transformação linear que faz com que um objeto aumente ou diminua de tamanho de acordo com fatores de escala em \\(x\\), \\(y\\) e \\(z\\). Desse modo, a escala não é uma transformação de corpo rígido. A transformação de escala é obtida multiplicando cada coordenada \\((x,y,z)\\) de um ponto \\(\\mathbf{p}\\) pelos escalares \\((s_x, s_y, s_z)\\): \\[ \\begin{align} x&#39; &amp;= s_x x\\\\ y&#39; &amp;= s_y y\\\\ z&#39; &amp;= s_z z. \\end{align} \\] Se os fatores de escala forem todos iguais (\\(s_x=s_y=s_z\\)), temos uma escala uniforme pois o objeto é redimensionado por igual em todas direções. A figura 7.3 ilustra o resultado da escala uniforme de cada ponto de um cubo centralizado na origem. Como \\(s_x=s_y=s_z=2\\), o objeto dobra de tamanho em cada dimensão. Figura 7.3: Escala uniforme sobre um objeto centralizado na origem. A origem do frame é o ponto fixo da transformação, isto é, é o ponto que não é afetado pela escala. Observe, na figura 7.4, a escala uniforme de um cubo que não está centralizado na origem. O vértice que coincide com a origem é o único ponto que permanece inalterado. Pontos na origem continuam na origem após a escala. Figura 7.4: Escala uniforme sobre um objeto não centralizado na origem. Antes da aplicação da transformação de escala, é frequentemente desejável centralizar o objeto na origem ou fazer com que pelo menos a base do objeto fique centralizada na origem, como no modelo mostrado na figura 7.5. Figura 7.5: Escala em objeto com base centralizada na origem. Se o objeto não estiver centralizado na origem, a escala poderá deslocar o objeto de forma indireta. Por exemplo, se um cubo unitário estiver centralizado no ponto \\((10,10,10)\\), a escala com \\(s_x=s_y=s_z=2\\) dobrará o tamanho do cubo em cada dimensão, como esperado. Porém, o cubo agora estará centralizado no ponto \\((20,20,20)\\), o que provavelmente não é o que se quer. Se os fatores de escala estão no intervalo \\([0, 1)\\), o objeto diminui de tamanho. A figura 7.6 mostra o resultado de uma escala uniforme que diminui o tamanho do objeto. Como \\(s_x=s_y=s_z=0.5\\), o objeto resultante tem \\(50\\%\\) do tamanho original em cada dimensão. Figura 7.6: Escala uniforme com fatores de escala menores que 1. Se os fatores de escala não são iguais, o resultado é uma escala não uniforme. Na figura 7.7, o cubo triplica de tamanho na direção \\(x\\) (\\(s_x=3\\)), mantém o tamanho na direção \\(y\\) (\\(s_y=1\\)), e diminui o tamanho pela metade em \\(z\\) (\\(s_z=0.5\\)). Figura 7.7: Escala não uniforme. Em resumo, se \\(s\\) é um fator de escala (\\(s_x\\), \\(s_y\\) ou \\(s_z\\)): \\(0 \\leq s &lt; 1\\) faz o objeto diminuir de tamanho na direção correspondente. \\(s = 1\\) mantém o tamanho do objeto. \\(s &gt; 1\\) faz o objeto aumentar de tamanho na direção correspondente. Se o sinal de \\(s\\) é negativo, o resultado é uma reflexão na direção correspondente. A figura 7.8 ilustra o resultado da reflexão em \\(x\\) e \\(y\\) sem alteração do tamanho do objeto (\\(|s|=1\\)). Figura 7.8: Reflexão. A matriz de escala é definida como \\[ \\mathbf{S}=\\mathbf{S}(s_x,s_y,s_z)= \\begin{bmatrix} s_x &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; s_y &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; s_z &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Assim, \\[ \\mathbf{p}&#39;=\\mathbf{S}\\mathbf{p} \\] \\[ \\begin{bmatrix} s_x x\\\\ s_y y\\\\ s_z z\\\\ 1\\end{bmatrix} = \\begin{bmatrix} s_x &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; s_y &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; s_z &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\end{bmatrix}. \\] A inversa é a escala pelo valor recíproco dos fatores de escala: \\[ \\mathbf{S}^{-1}(s_x,s_y,s_z) = \\mathbf{S}\\left(\\frac{1}{s_x},\\frac{1}{s_y},\\frac{1}{s_z}\\right) \\] Assim, se um objeto teve seu tamanho dobrado (\\(s=2\\)), para voltar ao tamanho original devemos diminuir seu tamanho pela metade (\\(s=1/2=0.5\\)). Rotação A rotação é um tipo de transformação linear que produz uma movimentação radial em torno de um ponto ou eixo de rotação. A rotação é uma transformação de corpo rígido, pois não altera as distâncias entre os pontos do objeto rodado. Para simplificar, vamos primeiramente definir uma rotação no plano cartesiano e depois estender o conceito para o espaço tridimensional. Um ponto \\(P=(x,y)\\) é rodado em torno da origem por um ângulo \\(\\theta\\) no sentido anti-horário, resultando em um ponto \\(P&#39;=(x&#39;,y&#39;)\\) como mostra a figura 7.9. Figura 7.9: Rotação no plano. Podemos determinar a transformação que leva \\(P\\) a \\(P&#39;\\) através da representação dos pontos em coordenadas polares: \\[ \\begin{align} x &amp;= r \\cos \\phi,\\\\ y &amp;= r \\sin \\phi,\\\\ \\\\ x&#39; &amp;= r \\cos (\\phi + \\theta),\\\\ y&#39; &amp;= r \\sin (\\phi + \\theta).\\\\ \\end{align} \\] Usando a soma de cossenos, \\[ \\begin{align} x&#39; &amp;= r \\cos (\\phi + \\theta)\\\\ &amp;= r \\cos \\phi \\cos \\theta - r\\sin\\phi\\sin\\theta,\\\\ y&#39; &amp;= r \\sin (\\phi + \\theta)\\\\ &amp;= r \\sin \\phi \\cos \\theta + r\\cos\\phi\\sin\\theta. \\end{align} \\] Substituindo por \\(x\\) e \\(y\\), obtemos o resultado: \\[ \\begin{align} x&#39; &amp;= x \\cos \\theta - y \\sin \\theta,\\\\ y&#39; &amp;= y \\cos \\theta + x \\sin \\theta. \\end{align} \\] Em notação matricial: \\[ \\begin{bmatrix} x&#39;\\\\ y&#39;\\end{bmatrix} = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\phantom{-}\\cos \\theta \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\end{bmatrix}. \\] A matriz \\[ \\mathbf{R}(\\theta) = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\phantom{-}\\cos \\theta \\end{bmatrix} \\] representa a transformação de rotação em torno da origem por um ângulo \\(\\theta\\) no sentido anti-horário. Rotação 3D No espaço tridimensional, a rotação ocorre em torno de um eixo de referência. Vamos derivar as transformações de rotação em torno dos três eixos do sistema de coordenadas cartesiano orientado segundo a regra da mão direita (figura 7.10). Figura 7.10: Sistema de coordenadas baseado na regra da mão direita. Um ponto \\(P=(x,y,z)\\) é rodado em torno do eixo \\(z\\) por um ângulo \\(\\theta\\) no sentido anti-horário, resultando em um ponto \\(P&#39;=(x&#39;,y&#39;,z&#39;)\\), como mostra a figura 7.11. Figura 7.11: Rotação em torno de \\(z\\). Pela regra da mão direita, é como se o eixo \\(z\\) positivo na figura 7.11 estivesse saindo da tela. Assim, as coordenadas \\(x\\) e \\(y\\) mudam como na rotação 2D, e a coordenação \\(z\\) não é modificada: \\[ \\begin{align} x&#39; &amp;= x \\cos \\theta - y \\sin \\theta,\\\\ y&#39; &amp;= y \\cos \\theta + x \\sin \\theta,\\\\ z&#39; &amp;= z. \\end{align} \\] A figura 7.12 ilustra o resultado da rotação em \\(\\pi/6\\) radianos (\\(30^{\\circ}\\)) em torno do eixo \\(z\\), aplicada sobre um cubo centralizado na origem. Como o ângulo é positivo, o objeto é rodado no sentido anti-horário. Figura 7.12: Rotação sobre um objeto centralizado na origem. A figura 7.13 mostra a rotação em \\(\\pi/6\\) radianos em torno do eixo \\(z\\) sobre um cubo não centralizado na origem. Observe que todos os pontos ao longo do eixo \\(z\\) permanecem inalterados. De fato, na rotação 3D, todos os pontos ao longo do eixo de rotação são pontos fixos. Figura 7.13: Rotação sobre um objeto não centralizado na origem. Podemos obter a rotação em torno dos outros eixos através da substituição cíclica das coordenadas \\(x\\), \\(y\\) e \\(z\\) nas expressões de rotação em torno de \\(z\\), segundo a ordem mostrada na figura 7.14. Figura 7.14: Ordem cíclica de substituição das coordenadas. Assim, a rotação em torno do eixo \\(x\\) resultará em: \\[ \\begin{align} y&#39; &amp;= y \\cos \\theta - z \\sin \\theta,\\\\ z&#39; &amp;= z \\cos \\theta + y \\sin \\theta,\\\\ x&#39; &amp;= x. \\end{align} \\] A rotação em torno do eixo \\(y\\) é obtida através de mais uma substituição cíclica das coordenadas nas expressões acima: \\[ \\begin{align} z&#39; &amp;= z \\cos \\theta - x \\sin \\theta,\\\\ x&#39; &amp;= x \\cos \\theta + z \\sin \\theta,\\\\ y&#39; &amp;= y. \\end{align} \\] As matrizes de rotação em torno dos eixos \\(x\\), \\(y\\) e \\(z\\) ficam como a seguir: \\[ \\mathbf{R_x}(\\theta)= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos \\theta &amp; -\\sin \\theta &amp; 0 \\\\ 0 &amp; \\sin \\theta &amp; \\phantom{-}\\cos \\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix},\\\\ \\mathbf{R_y}(\\theta)= \\begin{bmatrix} \\cos \\theta &amp; 0 &amp; \\sin \\theta &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ -\\sin \\theta &amp; 0 &amp; \\cos \\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix},\\\\ \\mathbf{R_z}(\\theta)= \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta &amp; 0 &amp; 0 \\\\ \\sin \\theta &amp; \\phantom{-}\\cos \\theta &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] A matriz inversa de uma rotação por um ângulo \\(\\theta\\) é simplesmente a matriz de rotação pelo mesmo eixo, mas pelo ângulo \\(-\\theta\\): \\[ \\mathbf{R_x}(\\theta)^{-1}=\\mathbf{R_x}(-\\theta),\\\\ \\mathbf{R_y}(\\theta)^{-1}=\\mathbf{R_y}(-\\theta),\\\\ \\mathbf{R_z}(\\theta)^{-1}=\\mathbf{R_z}(-\\theta). \\] Além disso, as matrizes de rotação são matrizes ortogonais. Desse modo, sua inversa é sua transposta: \\[ \\mathbf{R_x}(\\theta)^{-1}=\\mathbf{R_x}(\\theta)^T,\\\\ \\mathbf{R_y}(\\theta)^{-1}=\\mathbf{R_y}(\\theta)^T,\\\\ \\mathbf{R_z}(\\theta)^{-1}=\\mathbf{R_z}(\\theta)^T. \\] "],["glspaces.html", "7.5 Espaços do OpenGL", " 7.5 Espaços do OpenGL Vimos que, no pipeline gráfico do OpenGL, as primitivas só são renderizadas se estiverem contidas em um volume de visão formado por um cubo centralizado na origem, com coordenadas que vão de \\(-1\\) a \\(1\\). As coordenadas desse espaço tridimensional são chamadas de coordenadas normalizadas do dispositivo, ou NDC (figura 7.15). Figura 7.15: Volume de visão em coordenadas normalizadas do dispositivo. Observe que os eixos do NDC seguem a regra da mão esquerda, isto é, o eixo \\(z\\) positivo aponta para dentro da tela. Por uma questão de simplicidade, até agora representamos todos os objetos diretamente no NDC. Entretanto, essa abordagem é pouco flexível para a maioria das aplicações. Com frequência precisamos trabalhar com coordenadas em intervalos maiores em um sistema orientado segundo a regra da mão direita. Felizmente, podemos representar mudanças de pontos entre diferentes espaços intermediários através de transformações matriciais, da forma que considerarmos mais conveniente. Só precisamos garantir que, no vertex shader, essas coordenadas serão finalmente mapeadas para o espaço homogêneo de recorte, que é o sistema de coordenadas utilizado na variável embutida gl_Position, ou diretamente para o NDC, fazendo com que a coordenada homogênea (\\(w\\)) seja sempre 1. Usaremos os seguintes sistemas de coordenadas, nessa ordem: Espaço do objeto (local space ou object space). Espaço do mundo (world space). Espaço da câmera (view space, camera space ou eye space). Além desses, há os espaços utilizados durante o processamento do pipeline gráfico do OpenGL: Espaço de recorte (clip space). Espaço normalizado do dispositivo (NDC space). Espaço da janela (window space). Um modelo 3D está inicialmente no espaço do objeto. Esse é o espaço das coordenadas dos atributos dos vértices armazenados no VBO. No pipeline de renderização, os pontos/vetores do espaço local são convertidos para os espaços subsequentes através de multiplicações com matrizes de transformação (matrizes \\(4 \\times 4\\), como visto na seção anterior). A figura 7.16 mostra a sequência de transformações de pontos/vetores, do espaço do objeto ao espaço da janela. Figura 7.16: Etapas de transformação entre sistemas de coordenadas no pipeline gráfico. Nesta seção, serão abordados apenas os três primeiros espaços, que são os espaços definidos pelo usuário: Espaço do objeto; Espaço do mundo; Espaço da câmera. Os espaços restantes serão detalhados no próximo capítulo. Espaço do objeto É o espaço local utilizado na definição do modelo geométrico original. Por exemplo, o modelo Stanford Bunny do arquivo bunny.obj (seção 6.4) foi definido em um sistema de coordenadas que segue a regra da mão direita (RHS, de right-handed coordinate system), sendo que o eixo \\(y\\) positivo aponta para cima (figura 7.17). A origem desse sistema de coordenadas é o centro da base do coelho. Além disso, as coordenadas do modelo estão na faixa \\(x \\in [-1, 0.7]\\), \\(y \\in [0, 1.6]\\), \\(z \\in [-0.5, 0.7]\\). Figura 7.17: Espaço do objeto do modelo do arquivo bunny.obj. O cubo definido no arquivo box.obj (seção 6.4) usa um sistema de coordenadas que segue a regra da mão esquerda (LHS, de left-handed coordinate system). A origem é o centro do cubo e as coordenadas variam de \\(-0.5\\) a \\(0.5\\) em cada dimensão. Figura 7.18: Espaço do objeto do modelo do arquivo box.obj. Observação Um triângulo definido em um espaço RHS tem sua orientação invertida quando renderizado sem modificações em um espaço LHS, como o NDC. No espaço RHS, o lado da frente de um triângulo no plano \\(xy\\) é o lado com orientação CCW para um observador olhando na direção de \\(z\\) negativo. Entretanto, se esse mesmo triângulo é utilizado sem modificação no NDC, o lado da frente ficará orientado no sentido CW após o mapeamento no espaço da janela, como se o observador agora olhasse na direção de \\(z\\) negativo do espaço LHS (figura 7.19). Figura 7.19: Mudança de orientação de um sistema RHS para LHS. Esse problema pode ser resolvido facilmente através da negação de cada coordenada \\(z\\) do modelo. Faremos isso na matriz de projeção, pois sempre consideraremos que o espaço do objeto é RHS. Espaço do mundo O espaço do mundo é o frame utilizado para dispor os diferentes objetos em um cenário virtual. A origem do espaço do mundo pode ser um ponto que indica o centro do cenário, a posição inicial de um personagem virtual, ou simplesmente um ponto arbitrário no espaço. Em nossas aplicações, adotaremos como convenção que o espaço do mundo é um espaço euclidiano tridimensional RHS, sendo que o eixo \\(x\\) positivo aponta para o lado, e o eixo \\(y\\) positivo aponta para cima. O chão pode ser considerado como o plano \\(xz\\) com altura \\(y=0\\). A figura 7.20 ilustra a composição de uma cena através da disposição de objetos sobre o plano \\(y=0\\) do espaço do mundo. Figura 7.20: Cena no espaço do mundo. Em uma cena composta por vários objetos, a transformação que posiciona e orienta um objeto na cena é definida através de uma matriz de modelo (model matrix). A matriz de modelo é uma concatenação de transformações de rotação, escala e translação que representa uma mudança de frame: do espaço do objeto para o espaço do mundo. Cada objeto de uma cena deve ter a sua própria matriz de modelo. Na cena da figura 7.20, as bases cuboides compartilham o mesmo modelo de um cubo mostrado na figura 7.21, mas cada um usa uma matriz de modelo diferente, pois cada cuboide está numa posição/orientação/escala diferente na cena. Figura 7.21: Cubo unitário centralizado na origem. Por exemplo, para o objeto cuboide que serve de base para o coelho, a matriz de modelo é a seguinte concatenação de transformações: \\[\\mathbf{M}=\\mathbf{T}\\left(-3,\\frac{1}{2},-\\frac{1}{2}\\right).\\mathbf{R}_y\\left(\\frac{7\\pi}{36}\\right).\\mathbf{S}\\left(\\frac{5}{4}, \\frac{1}{2}, \\frac{5}{4}\\right). \\] A matriz representa a aplicação das transformações na ordem de leitura da direita para a esquerda (figura 7.22): Escala de \\(s_x=1.25\\), \\(s_y=0.5\\), \\(s_z=1.25\\); Rotação por \\(7\\pi/36\\) radianos (\\(35^{\\circ}\\)) em torno de \\(y\\); Translação por \\(t_x=-3\\), \\(t_y=0.5\\), \\(t_z=-0.5\\). Figura 7.22: Sequência de transformações de uma matriz de modelo. Se a matriz de modelo é a matriz que converte pontos no frame local para pontos no frame do mundo, então a inversa da matriz de modelo faz o mapeamento inverso, isto é, do frame do mundo para o frame local. No caso da base cuboide, essa transformação inversa é \\[ \\begin{align} \\mathbf{M}^{-1}&amp;=\\left(\\mathbf{T}\\left(-3,\\frac{1}{2},-\\frac{1}{2}\\right).\\mathbf{R}_y\\left(\\frac{7\\pi}{36}\\right).\\mathbf{S}\\left(\\frac{5}{4}, \\frac{1}{2}, \\frac{5}{4}\\right)\\right)^{-1}\\\\ &amp;=\\mathbf{S}\\left(\\frac{5}{4}, \\frac{1}{2}, \\frac{5}{4}\\right)^{-1}.\\mathbf{R}_y\\left(\\frac{7\\pi}{36}\\right)^{-1}.\\mathbf{T}\\left(-3,\\frac{1}{2},-\\frac{1}{2}\\right)^{-1}\\\\ &amp;=\\mathbf{S}\\left(\\frac{4}{5}, 2, \\frac{4}{5}\\right).\\mathbf{R}_y\\left(-\\frac{7\\pi}{36}\\right).\\mathbf{T}\\left(3,-\\frac{1}{2},\\frac{1}{2}\\right). \\end{align} \\] Note que as transformações concatenadas da matriz inversa desfazem cada uma das transformações da figura 7.22, no sentido contrário (da etapa 3 até o modelo original). Espaço da câmera O espaço da câmera representa o ponto de vista da câmera virtual posicionada no mundo, isto é, o ponto de vista de um observador em primeira pessoa dentro da cena. No espaço da câmera, a câmera está posicionada na origem, olhando na direção do eixo \\(z\\) negativo. As coordenadas de todos os objetos da cena são descritos em relação à câmera. A figura 7.23 ilustra o frame de uma câmera em relação ao mundo. A direção de visão é representada pela seta tracejada, que é a direção do eixo \\(z\\) negativo do frame da câmera. Figura 7.23: Frame da câmera em relação ao frame do mundo. A figura 7.24 ilustra como a cena é vista a partir da câmera, e como a câmera vê o frame do mundo. Figura 7.24: Frame do mundo do ponto de vista da câmera. A transformação que transforma pontos do espaço do mundo para o espaço da câmera é definida através de uma matriz de visão (view matrix). Essa matriz é uma concatenação de rotações e uma translação. Para construir uma matriz de visão, precisamos de uma posição e uma orientação. A posição é a localização da câmera no espaço do mundo. Do ponto de vista da câmera, é a origem (ponto \\((0,0,0)\\)) de seu frame. Essa informação fica armazenada na parte de translação da matriz de visão (parte \\(3 \\times 1\\) da quarta coluna) A orientação é uma base ortonormal, e corresponde à parte \\(3 \\times 3\\) superior da matriz de visão (mudança de base). Descreveremos na seção 7.6 o processo de construção de uma matriz de visão a partir da posição da câmera, a posição para onde a câmera está olhando, e um vetor de direção para cima, que geralmente é o vetor \\(\\hat{\\mathbf{j}}=\\begin{bmatrix}0&amp;1&amp;0\\end{bmatrix}^T\\) do \\(\\mathbb{R}^3\\). Concatenação das matrizes de modelo e visão Sempre que um objeto for renderizado, a posição de cada um de seus vértices precisa ser transformada. Isso pode ser feito em duas etapas: Conversão do espaço local para o espaço do mundo: \\[\\mathbf{p}&#39;=\\mathbf{M}_{\\textrm{model}}.\\mathbf{p},\\] onde \\(\\mathbf{M}_{\\textrm{model}}\\) é a matriz de modelo do objeto que está sendo renderizado. Conversão do espaço do mundo para o espaço da câmera: \\[\\mathbf{p}&#39;&#39;=\\mathbf{M}_{\\textrm{view}}.\\mathbf{p}&#39;,\\] onde \\(\\mathbf{M}_{\\textrm{view}}\\) é a matriz de visão. A matriz de visão deve ser a mesma para todos os objetos da cena renderizada. É comum combinar as duas transformações em uma só matriz modelo-visão: \\[ \\mathbf{M}_{\\textrm{modelview}}=\\mathbf{M}_{\\textrm{view}}.\\mathbf{M}_{\\textrm{model}}. \\] Assim, no vertex shader basta uma multiplicação matricial para transformar a posição do vértice pela matriz modelo-visão. O resultado será um ponto no espaço da câmera. Observação Após a transformação do ponto para o espaço da câmera, é necessário aplicar ainda uma transformação projetiva através de uma matriz de projeção \\(\\mathbf{M}_{\\textrm{proj}}\\). A matriz de projeção converte um ponto do espaço da câmera para o espaço homogêneo de recorte, que é o espaço esperado pela variável embutida gl_Position no vertex shader. A transformação completa fica como a seguir: \\[ \\mathbf{p}&#39;=\\mathbf{M}_{\\textrm{proj}}.\\mathbf{M}_{\\textrm{view}}.\\mathbf{M}_{\\textrm{model}}.\\mathbf{p}, \\] onde \\(\\mathbf{p}\\) é a entrada do vertex shader, isto é, a posição do vértice no formato \\(\\begin{bmatrix}x&amp;y&amp;z&amp;1\\end{bmatrix}^T\\); \\(\\mathbf{p}&#39;\\) é a posição transformada no formato \\(\\begin{bmatrix}x&#39;&amp;y&#39;&amp;z&#39;&amp;w\\end{bmatrix}^T\\). Essa é a posição que será copiada para gl_Position. Os conceitos sobre transformações de projeção e o processo de construir a matriz \\(\\mathbf{M}_{\\textrm{proj}}\\) serão abordados no próximo capítulo. "],["lookat.html", "7.6 Câmera LookAt", " 7.6 Câmera LookAt Câmera LookAt é o nome dado ao frame de câmera virtual \\(\\{P_{\\textrm{eye}}, \\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\) construído a partir das seguintes informações: Um ponto \\(P_\\textrm{eye}\\) que corresponde à posição da câmera no espaço do mundo; Um ponto \\(P_\\textrm{at}\\) que corresponde à posição aonde a câmera está olhando, também no espaço do mundo28. Um vetor \\(\\mathbf{v}_\\textrm{up}\\) utilizado para indicar a direção para cima da câmera. Geralmente esse vetor é a direção \\((0,1,0)\\). A figura 7.25 ilustra esses elementos, incluindo os vetores \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\) que formam a base ortonormal da câmera. Figura 7.25: Frame da câmera, representado em relação ao mundo. O sistema de coordenadas da câmera segue a regra da mão direita. Note que a câmera está olhando na direção \\(-\\hat{\\mathbf{n}}\\) no espaço do mundo, que corresponde à direção do eixo \\(z\\) negativo da câmera. Inicialmente, não temos a base ortonormal \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\). Só temos as seguintes informações (ilustradas na figura 7.26): A posição da câmera, \\(P_\\textrm{eye}\\); A posição para onde a câmera deve ser direcionada, \\(P_\\textrm{at}\\); O vetor \\(\\mathbf{v}_\\textrm{up}\\), que vamos considerar como sendo o vetor \\((0,1,0)\\). Figura 7.26: Parâmetros de uma câmera LookAt. Através dessas informações construiremos a base \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\). Com a base e o ponto de referência (\\(P_{\\textrm{eye}}\\)) temos o frame completo para criar a matriz de visão \\(\\mathbf{M}_{\\textrm{view}}\\). Como vimos anteriormente, a matriz de visão representa uma mudança de frame: do espaço do mundo para o espaço da câmera. Construindo o vetor n Para construir a base ortonormal, primeiro fazemos \\(P_{\\textrm{eye}}-P_{\\textrm{at}}\\) para obter o vetor que aponta na direção contrária da direção de visão. Esse vetor é então normalizado para obter \\(\\hat{\\mathbf{n}}\\) (figura 7.27): \\[ \\hat{\\mathbf{n}}=\\frac{P_{\\textrm{eye}}-P_{\\textrm{at}}}{|P_{\\textrm{eye}}-P_{\\textrm{at}}|}. \\] Figura 7.27: Construção do vetor n da câmera LookAt. Note que \\(\\hat{\\mathbf{n}}\\) está sendo representado em coordenadas do espaço do mundo. Em relação à câmera, \\(\\hat{\\mathbf{n}}\\) torna-se o vetor \\(\\hat{\\mathbf{k}}=(0,0,1)\\), isto é, a direção do eixo \\(z\\) positivo da câmera (direção para trás da câmera). Construindo o vetor u Agora que temos \\(\\hat{\\mathbf{n}}\\), o segundo passo é calcular o produto vetorial \\(\\mathbf{v}_{\\textrm{up}} \\times \\hat{\\mathbf{n}}\\) e normalizar o resultado. Com isso obtemos o vetor \\(\\hat{\\mathbf{u}}\\) perpendicular ao plano formado por \\(\\mathbf{v}_{\\textrm{up}}\\) e \\(\\hat{\\mathbf{n}}\\) (figura 7.28): \\[ \\hat{\\mathbf{u}}=\\frac{\\mathbf{v}_{\\textrm{up}} \\times \\hat{\\mathbf{n}}}{|\\mathbf{v}_{\\textrm{up}} \\times \\hat{\\mathbf{n}}|}. \\] Figura 7.28: Construção do vetor u da câmera LookAt. No frame da câmera, \\(\\hat{\\mathbf{u}}\\) corresponde ao vetor \\(\\hat{\\mathbf{i}}=(1,0,0)\\), isto é, a direção do eixo \\(x\\) da câmera (direção à direita). Construindo o vetor v Embora \\(\\hat{\\mathbf{u}}\\) seja perpendicular a \\(\\hat{\\mathbf{n}}\\) e a \\(\\mathbf{v}_{\\textrm{up}}\\), ainda não temos uma base ortonormal pois \\(\\mathbf{v}_{\\textrm{up}}\\) não é necessariamente perpendicular a \\(\\hat{\\mathbf{n}}\\). Na figura 7.28, \\(\\mathbf{v}_{\\textrm{up}}\\) e \\(\\hat{\\mathbf{n}}\\) formam um ângulo menor que \\(90^{\\circ}\\). Para obter um vetor que seja mutuamente ortogonal a \\(\\hat{\\mathbf{n}}\\) e \\(\\hat{\\mathbf{u}}\\), basta calcularmos o produto vetorial \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{u}}\\). O resultado é \\(\\hat{\\mathbf{v}}\\) (figura 7.29) que já está normalizado pois \\(\\hat{\\mathbf{n}}\\) e \\(\\hat{\\mathbf{u}}\\) também têm comprimento 1. \\[ \\hat{\\mathbf{v}}=\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{u}}. \\] Figura 7.29: Construção do vetor v da câmera LookAt. Note que, em relação à câmera, \\(\\hat{\\mathbf{v}}\\) corresponde ao vetor \\(\\hat{\\mathbf{j}}=(0,1,0)\\), isto é, o eixo \\(y\\) da câmera (direção para cima). Os vetores \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\) formam a base ortonormal da câmera, representados em relação ao espaço do mundo. Construindo a matriz de visão Para a construção da matriz de mudança de frame, vamos relembrar primeiro a matriz de mudança de base. A matriz com colunas formadas pelos vetores \\(\\{T(\\mathbf{\\hat{\\mathbf{i}}}),T(\\mathbf{\\hat{\\mathbf{j}}}),T(\\mathbf{\\hat{\\mathbf{k}}})\\}\\) representa uma mudança da base \\(\\{\\hat{\\mathbf{i}}, \\hat{\\mathbf{j}}, \\hat{\\mathbf{k}}\\}\\) para a base transformada. A transformação \\(T\\) é uma composição de rotações (por exemplo, \\(\\mathbf{R}_z\\mathbf{R}_y\\mathbf{R}_x\\)), que tem o efeito de rodar a base original para a nova. O que temos atualmente é a base \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\). Esses vetores estão representados em relação ao espaço do mundo. Se estivessem representados em relação ao espaço da câmera, a base seria \\(\\{\\hat{\\mathbf{i}}, \\hat{\\mathbf{j}}, \\hat{\\mathbf{k}}\\}\\). Então, se construirmos a matriz \\(\\mathbf{R}\\) de mudança de base, \\[ \\mathbf{R}= \\begin{bmatrix} u_{11} &amp; v_{12} &amp; n_{13} &amp; 0 \\\\ u_{21} &amp; v_{22} &amp; n_{23} &amp; 0 \\\\ u_{31} &amp; v_{32} &amp; n_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\] tal matriz representa a mudança do espaço da câmera para o espaço do mundo. Não é bem o que queremos. Gostaríamos da matriz que faz a transformação inversa, isto é, que converte coordenadas do mundo para a câmera. Entretanto, vamos prosseguir com \\(\\mathbf{R}\\) da forma como está. Ao final poderemos calcular a matriz inversa da transformação completa, para finalmente obter \\(\\mathbf{M}_{\\textrm{view}}\\). Com a matriz \\(\\mathbf{R}\\), a base \\(\\{\\hat{\\mathbf{i}}, \\hat{\\mathbf{j}}, \\hat{\\mathbf{k}}\\}\\) no espaço da câmera é transformada por rotações para resultar na base \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\) representada no espaço do mundo. Isso é ilustrado na figura 7.30. Figura 7.30: Rotação da base representada no espaço da câmera, para a base representada no espaço do mundo. Além da base, um frame também precisa de um ponto de referência. Esse ponto de referência é o próprio \\(P_\\textrm{eye}\\), que representa a origem \\(O\\) no espaço da câmera. \\(P_\\textrm{eye}\\) é o deslocamento necessário para mover a origem do espaço da câmera para sua posição no espaço do mundo. Em outras palavras, temos uma transformação de translação que pode ser representada pela matriz \\[ \\mathbf{T}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; P_{\\textrm{eye}_x} \\\\ 0 &amp; 1 &amp; 0 &amp; P_{\\textrm{eye}_y} \\\\ 0 &amp; 0 &amp; 1 &amp; P_{\\textrm{eye}_z} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Fazendo a composição das transformações, temos \\[ \\mathbf{M} = \\mathbf{T} \\mathbf{R} \\] \\[ \\mathbf{M}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; P_{\\textrm{eye}_x} \\\\ 0 &amp; 1 &amp; 0 &amp; P_{\\textrm{eye}_y} \\\\ 0 &amp; 0 &amp; 1 &amp; P_{\\textrm{eye}_z} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} u_{11} &amp; v_{12} &amp; n_{13} &amp; 0 \\\\ u_{21} &amp; v_{22} &amp; n_{23} &amp; 0 \\\\ u_{31} &amp; v_{32} &amp; n_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] A figura 7.31 ilustra como a matriz de transformação \\(\\mathbf{M}\\) converte coordenadas do espaço da câmera para o espaço do mundo, que é o equivalente a rodar a base \\(\\{\\hat{\\mathbf{i}}, \\hat{\\mathbf{j}}, \\hat{\\mathbf{k}}\\}\\) para \\(\\{\\hat{\\mathbf{u}}, \\hat{\\mathbf{v}}, \\hat{\\mathbf{n}}\\}\\) (matriz de rotação \\(\\mathbf{R}\\)), e então transladar a origem \\(O\\) para \\(P_{\\textrm{eye}}\\) (matriz de translação \\(\\mathbf{T}\\)). Figura 7.31: Mudança do espaço da câmera para o espaço do mundo. Para obter \\(\\mathbf{M}_{\\textrm{view}}\\), basta calcularmos a inversa de \\(\\mathbf{M}\\). Lembre-se que a inversa de uma matriz de rotação é a sua transposta, e a inversa da translação por \\(P_{\\textrm{eye}}\\) é a translação por \\(-P_{\\textrm{eye}}\\). Portanto, \\[ \\begin{align} \\mathbf{M}_{\\textrm{view}} &amp;= \\mathbf{M}^{-1}\\\\ &amp;= (\\mathbf{T} \\mathbf{R})^{-1}\\\\ &amp;= \\left( \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; P_{\\textrm{eye}_x} \\\\ 0 &amp; 1 &amp; 0 &amp; P_{\\textrm{eye}_y} \\\\ 0 &amp; 0 &amp; 1 &amp; P_{\\textrm{eye}_z} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} u_{11} &amp; v_{12} &amp; n_{13} &amp; 0 \\\\ u_{21} &amp; v_{22} &amp; n_{23} &amp; 0 \\\\ u_{31} &amp; v_{32} &amp; n_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\right)^{-1}\\\\ &amp;= \\begin{bmatrix} u_{11} &amp; v_{12} &amp; n_{13} &amp; 0 \\\\ u_{21} &amp; v_{22} &amp; n_{23} &amp; 0 \\\\ u_{31} &amp; v_{32} &amp; n_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; P_{\\textrm{eye}_x} \\\\ 0 &amp; 1 &amp; 0 &amp; P_{\\textrm{eye}_y} \\\\ 0 &amp; 0 &amp; 1 &amp; P_{\\textrm{eye}_z} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}^{-1}\\\\ &amp;= \\begin{bmatrix} u_{11} &amp; u_{21} &amp; u_{23} &amp; 0 \\\\ v_{12} &amp; v_{22} &amp; v_{33} &amp; 0 \\\\ n_{13} &amp; v_{23} &amp; n_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -P_{\\textrm{eye}_x} \\\\ 0 &amp; 1 &amp; 0 &amp; -P_{\\textrm{eye}_y} \\\\ 0 &amp; 0 &amp; 1 &amp; -P_{\\textrm{eye}_z} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} u_{11} &amp; u_{21} &amp; u_{23} &amp; -\\hat{\\mathbf{u}}\\cdot P_{\\textrm{eye}} \\\\ v_{12} &amp; v_{22} &amp; v_{33} &amp; -\\hat{\\mathbf{v}}\\cdot P_{\\textrm{eye}} \\\\ n_{13} &amp; v_{23} &amp; n_{33} &amp; -\\hat{\\mathbf{n}}\\cdot P_{\\textrm{eye}} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\end{align} \\] A biblioteca GLM possui a função glm::lookAt, definida em cabeçalho glm/gtc/matrix_transform.hpp: glm::mat4 glm::lookAt(glm::vec3 const&amp; eye, glm::vec3 const&amp; center, glm::vec3 const&amp; up); glm::dmat4 glm::lookAt(glm::dvec3 const&amp; eye, glm::dvec3 const&amp; center, glm::dvec3 const&amp; up); glm::lookAt gera a matriz \\(\\mathbf{M}_{\\textrm{view}}\\) de uma câmera LookAt, dados os parâmetros \\(P_{\\textrm{eye}}\\) (eye), \\(P_{\\textrm{at}}\\) (center) e \\(\\mathbf{v}_\\textrm{up}\\) (up). Internamente, a função chama glm::lookAtRH para gerar o frame baseado na regra da mão direita. O conteúdo dessa função é dado a seguir: template&lt;typename T, qualifier Q&gt; GLM_FUNC_QUALIFIER mat&lt;4, 4, T, Q&gt; lookAtRH(vec&lt;3, T, Q&gt; const&amp; eye, vec&lt;3, T, Q&gt; const&amp; center, vec&lt;3, T, Q&gt; const&amp; up) { vec&lt;3, T, Q&gt; const f(normalize(center - eye)); vec&lt;3, T, Q&gt; const s(normalize(cross(f, up))); vec&lt;3, T, Q&gt; const u(cross(s, f)); mat&lt;4, 4, T, Q&gt; Result(1); Result[0][0] = s.x; Result[1][0] = s.y; Result[2][0] = s.z; Result[0][1] = u.x; Result[1][1] = u.y; Result[2][1] = u.z; Result[0][2] =-f.x; Result[1][2] =-f.y; Result[2][2] =-f.z; Result[3][0] =-dot(s, eye); Result[3][1] =-dot(u, eye); Result[3][2] = dot(f, eye); return Result; } Na linha 4, f (vetor forward) é equivalente ao nosso \\(-\\hat{\\mathbf{n}}\\). Na linha 5, s (vetor side) é o nosso vetor \\(\\hat{\\mathbf{u}}\\), calculado como \\(-\\hat{\\mathbf{n}} \\times \\mathbf{v}_\\textrm{up}\\), que é o mesmo que \\(\\mathbf{v}_\\textrm{up} \\times \\hat{\\mathbf{n}}\\), seguido de uma normalização. Na linha 6, u é o nosso vetor \\(\\hat{\\mathbf{v}}\\), calculado como \\(\\hat{\\mathbf{u}} \\times -\\hat{\\mathbf{n}}\\), que é o mesmo que \\(\\hat{\\mathbf{n}} \\times \\hat{\\mathbf{u}}\\). Nas linhas 9 a 21 é montada a matriz Result, que é a matriz \\(\\mathbf{M}_{\\textrm{view}}\\). Internamente, a GLM armazena as matrizes no formato column-major, o que significa que o primeiro índice é a coluna, e o segundo índice é a linha. Levando isso em consideração, observe que a matriz resultante é de fato: \\[ \\begin{align} \\mathbf{M}_{\\textrm{view}} &amp;= \\begin{bmatrix} u_{11} &amp; u_{21} &amp; u_{23} &amp; -\\hat{\\mathbf{u}}\\cdot P_{\\textrm{eye}} \\\\ v_{12} &amp; v_{22} &amp; v_{33} &amp; -\\hat{\\mathbf{v}}\\cdot P_{\\textrm{eye}} \\\\ n_{13} &amp; v_{23} &amp; n_{33} &amp; -\\hat{\\mathbf{n}}\\cdot P_{\\textrm{eye}} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\end{align} \\] O ponto at também é chamado de center ou target. "],["lookatproject.html", "7.7 LookAt na prática", " 7.7 LookAt na prática Nesta seção, seguiremos o passo a passo de desenvolvimento de uma aplicação que renderiza uma cena 3D do ponto de vista de uma câmera LookAt. A cena 3D será composta por quatro instâncias do modelo Stanford Bunny dispostos sobre o plano \\(xz\\) do espaço do mundo. A câmera LookAt simulará um observador em primeira pessoa. A figura 7.32 mostra o posicionamento dos objetos e a localização inicial da câmera. Figura 7.32: Objetos e elementos de cena dispostos no espaço do mundo. Na figura acima, os vetores \\(\\hat{\\mathbf{i}}\\),\\(\\hat{\\mathbf{j}}\\),\\(\\hat{\\mathbf{k}}\\) correspondem às direções dos eixos \\(x\\),\\(y\\),\\(z\\) do espaço euclidiano, e \\(\\hat{\\mathbf{u}}\\),\\(\\hat{\\mathbf{v}}\\),\\(\\hat{\\mathbf{n}}\\) são os vetores do frame da câmera. A câmera está localizada em \\((0, 0.5, 2.5)\\) e está olhando na direção do eixo \\(z\\) negativo. O coelho vermelho está na posição \\((0,0,0)\\) e tem escala de \\(10\\%\\) do tamanho original. O coelho cinza está na posição \\((-1,0,0)\\) e está rodado em \\(90^{\\circ}\\) em torno de seu eixo \\(y\\) local. O coelho azul está na posição \\((1,0,0)\\) e está rodado em \\(-90^{\\circ}\\) em torno de seu eixo \\(y\\) local. O coelho amarelo está na posição \\((0,0,-1)\\) e está com sua orientação original. A posição e orientação da câmera pode ser modificada através do teclado: As setas para cima/baixo (ou W/S) fazem a câmera ir para a frente e para trás ao longo da direção de visão (direção de \\(\\pm\\hat{\\mathbf{n}}\\)). Esse movimento de câmera é conhecido como dolly no jargão da cinematografia. As setas para os lados (ou A/D) fazem a câmera girar em torno de seu eixo \\(y\\) (vetor \\(\\hat{\\mathbf{v}}\\)). Esse movimento é chamado de pan. As teclas Q/E fazem a câmera deslizar para os lados (direção de \\(\\pm\\hat{\\mathbf{u}}\\)). Esse movimento é chamado de truck ou dolly lateral. Neste exemplo, a altura da câmera permanecerá sempre em \\(y=0.5\\). O resultado ficará como a seguir: Observação Para controlar a câmera usando o teclado é necessário abrir o link original e clicar na área de desenho. Desse modo a aplicação terá o foco do teclado. Configuração inicial No arquivo abcg/examples/CMakeLists.txt, inclua a linha: add_subdirectory(lookat) Crie o subdiretório abcg/examples/lookat e o arquivo abcg/examples/lookat/CMakeLists.txt com o seguinte conteúdo: project(lookat) add_executable(${PROJECT_NAME} camera.cpp ground.cpp main.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Crie os arquivos camera.cpp, camera.hpp, ground.cpp, ground.hpp, main.cpp, openglwindow.cpp e openglwindow.hpp. Crie o subdiretório abcg/examples/lookat/assets. Dentro dele, crie os arquivos lookat.frag e lookat.vert. Além disso, baixe o arquivo bunny.zip e descompacte-o em assets. A estrutura de abcg/examples/lookat ficará assim: lookat/  CMakeLists.txt  camera.hpp  camera.cpp  ground.hpp  ground.cpp  main.cpp  openglwindow.hpp  openglwindow.cpp  assets/  bunny.obj  lookat.frag  lookat.vert main.cpp Exceto pelo título da janela, o conteúdo de main.cpp é o mesmo do projeto anterior: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings({.samples = 4}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;LookAt Camera&quot;}); app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } lookat.vert O vertex shader ficará como a seguir: #version 410 layout(location = 0) in vec3 inPosition; uniform vec4 color; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; out vec4 fragColor; void main() { vec4 posEyeSpace = viewMatrix * modelMatrix * vec4(inPosition, 1); float i = 1.0 - (-posEyeSpace.z / 5.0); fragColor = vec4(i, i, i, 1) * color; gl_Position = projMatrix * posEyeSpace; } O atributo de entrada, inPosition, é a posição \\((x,y,z)\\) do vértice. Vamos considerar que estas coordenadas estão no espaço do objeto. O atributo de saída, fragColor, é uma cor RGBA. As variáveis uniformes são utilizadas para determinar a cor do objeto (color, na linha 5) e as matrizes \\(4 \\times 4\\) de transformação geométrica (linhas 6 a 8): Matriz de modelo: modelMatrix; Matriz de visão: viewMatrix; Matriz de projeção: projMatrix. Embora ainda não tenhamos visto o conteúdo teórico sobre a construção de uma matriz de projeção, vamos utilizar essa matriz desde já. Ela será necessária para obter o efeito de perspectiva e assim manter a ilusão de que a câmera LookAt é um observador dentro de um cenário 3D. No código de main, a linha 13 transforma a posição de entrada usando as matrizes de modelo e visão. Para entendermos a ordem das transformações, temos de ler os operandos da linha 13 da direita para a esquerda: vec4 posEyeSpace = viewMatrix * modelMatrix * vec4(inPosition, 1); Primeiro, vec4(inPosition, 1) produz a posição \\((x,y,z,1)\\), isto é, o ponto/vértice em coordenadas homogêneas que corresponde à posição \\((x,y,z)\\) no espaço do objeto. Esse vértice é transformado, através do produto matricial, pela matriz de modelo modelMatrix. A transformação pela matriz de modelo converte coordenadas do espaço do objeto para o espaço do mundo. Em seguida há uma transformação pela matriz de visão viewMatrix. A matriz de visão converte coordenadas do espaço do mundo para coordenadas do espaço da câmera. Assim, o resultado armazenado em posEyeSpace é a posição do vértice no espaço da câmera. Na linha 15, calculamos um valor i de intensidade de cor a partir da coordenada \\(z\\) do vértice no espaço da câmera: float i = 1.0 - (-posEyeSpace.z / 5.0); Lembre-se que, no espaço da câmera, a câmera está olhando na direção de seu eixo \\(z\\) negativo. Logo, do ponto de vista da câmera, todos os objetos à sua frente têm valor \\(z\\) negativo. Ao fazermos -PosEyeSpace.z, tornamos esse valor positivo, correspondendo à distância entre o vértice e a câmera ao longo do eixo \\(z\\). A ideia aqui é transformar essa distância em um valor de intensidade de cor. A intensidade será máxima (1) se o objeto estiver o mais próximo possível da câmera (isto é, se estiver na mesma posição da câmera), e mínima (0) se estiver a 5 ou mais unidades de distância na direção de visão. Na linha 16, esse valor de intensidade é utilizado para multiplicar as componentes RGB da cor color. fragColor = vec4(i, i, i, 1) * color; Assim, quanto mais longe o objeto estiver da câmera, mais escuro ele ficará. A partir da distância 5, a intensidade fica negativa, mas nesse caso o OpenGL fixa automaticamente o valor de cor para zero (não existe intensidade negativa de cor). Na linha 18, projMatrix * posEyeSpace faz com que as coordenadas no espaço da câmera sejam convertidas para o espaço de recorte. É esse o resultado final em gl_Position: gl_Position = projMatrix * posEyeSpace; lookat.frag O conteúdo do fragment shader ficará assim: #version 410 in vec4 fragColor; out vec4 outColor; void main() { if (gl_FrontFacing) { outColor = fragColor; } else { outColor = fragColor * 0.5; } } Se o triângulo estiver orientado de frente para a câmera, a cor final do fragmento será a cor de entrada (fragColor). Caso contrário, a cor terá metade da intensidade original (a cor RGB é multiplicada por 0.5). Assim, se a câmera estiver dentro de um objeto, os triângulos serão desenhados com uma cor mais escura, pois estaremos vendo o lado de trás da malha triangular. camera.hpp Neste arquivo definiremos a classe Camera que gerenciará a câmera LookAt. O conteúdo ficará como a seguir: #ifndef CAMERA_HPP_ #define CAMERA_HPP_ #include &lt;glm/mat4x4.hpp&gt; #include &lt;glm/vec3.hpp&gt; class OpenGLWindow; class Camera { public: void computeViewMatrix(); void computeProjectionMatrix(int width, int height); void dolly(float speed); void truck(float speed); void pan(float speed); private: friend OpenGLWindow; glm::vec3 m_eye{glm::vec3(0.0f, 0.5f, 2.5f)}; // Camera position glm::vec3 m_at{glm::vec3(0.0f, 0.5f, 0.0f)}; // Look-at point glm::vec3 m_up{glm::vec3(0.0f, 1.0f, 0.0f)}; // &quot;up&quot; direction // Matrix to change from world space to camera soace glm::mat4 m_viewMatrix; // Matrix to change from camera space to clip space glm::mat4 m_projMatrix; }; #endif Observe, nas linhas 21 a 23, que a classe tem todos os atributos necessários para criar o frame de uma câmera LookAt: m_eye: posição da câmera \\((0, 0.5, 2.5)\\). m_at: posição para onde a câmera está olhando \\((0, 0.5, 0)\\). m_up: vetor de direção para cima \\((0, 1, 0)\\). Na linha 26 temos a matriz de visão (m_viewMatrix) que será calculada pela função Camera::computeViewMatrix declarada na linha 11. Na linha 29 temos a matriz de projeção (m_projMatrix) que será calculada pela função Camera::computeProjectionMatrix declarada na linha 12. As funções Camera::dolly, Camera::truck e Camera::pan serão chamadas a partir de OpenGLWindow em resposta à entrada do teclado. Internamente, essas funções modificarão as variáveis m_eye e m_at, fazendo a câmera mudar de posição e orientação. camera.cpp A definição das funções membro de Camera ficará como a seguir: #include &quot;camera.hpp&quot; #include &lt;glm/gtc/matrix_transform.hpp&gt; void Camera::computeProjectionMatrix(int width, int height) { m_projMatrix = glm::mat4(1.0f); const auto aspect{static_cast&lt;float&gt;(width) / static_cast&lt;float&gt;(height)}; m_projMatrix = glm::perspective(glm::radians(70.0f), aspect, 0.1f, 5.0f); } void Camera::computeViewMatrix() { m_viewMatrix = glm::lookAt(m_eye, m_at, m_up); } void Camera::dolly(float speed) { // Compute forward vector (view direction) const glm::vec3 forward{glm::normalize(m_at - m_eye)}; // Move eye and center forward (speed &gt; 0) or backward (speed &lt; 0) m_eye += forward * speed; m_at += forward * speed; computeViewMatrix(); } void Camera::truck(float speed) { // Compute forward vector (view direction) const glm::vec3 forward{glm::normalize(m_at - m_eye)}; // Compute vector to the left const glm::vec3 left{glm::cross(m_up, forward)}; // Move eye and center to the left (speed &lt; 0) or to the right (speed &gt; 0) m_at -= left * speed; m_eye -= left * speed; computeViewMatrix(); } void Camera::pan(float speed) { glm::mat4 transform{glm::mat4(1.0f)}; // Rotate camera around its local y axis transform = glm::translate(transform, m_eye); transform = glm::rotate(transform, -speed, m_up); transform = glm::translate(transform, -m_eye); m_at = transform * glm::vec4(m_at, 1.0f); computeViewMatrix(); } No próximo capítulo, quando tivermos visto o conteúdo teórico sobre matrizes de projeção, descreveremos o funcionamento da função Camera::computeProjectionMatrix. Por enquanto, basta sabermos que ela calcula uma matriz de projeção perspectiva. Em Camera::computeViewMatrix, chamamos a função lookAt da GLM usando os atributos da câmera: void Camera::computeViewMatrix() { m_viewMatrix = glm::lookAt(m_eye, m_at, m_up); } Camera::computeViewMatrix será chamada sempre que houver alguma alteração em m_eye ou m_at. Em Camera::dolly, os pontos m_eye e m_at são deslocados para a frente ou para trás ao longo da direção de visão (vetor forward): void Camera::dolly(float speed) { // Compute forward vector (view direction) const glm::vec3 forward{glm::normalize(m_at - m_eye)}; // Move eye and center forward (speed &gt; 0) or backward (speed &lt; 0) m_eye += forward * speed; m_at += forward * speed; computeViewMatrix(); } Veja que, ao final, Camera::computeViewMatrix é chamada para reconstruir a matriz de visão. Camera::truck funciona de forma parecida com Camera::dolly. Os pontos m_eye e m_at são deslocados nas laterais de acordo com a direção do vetor left. O vetor left é o produto vetorial entre o vetor up e o vetor forward. void Camera::truck(float speed) { // Compute forward vector (view direction) const glm::vec3 forward{glm::normalize(m_at - m_eye)}; // Compute vector to the left const glm::vec3 left{glm::cross(m_up, forward)}; // Move eye and center to the left (speed &lt; 0) or to the right (speed &gt; 0) m_at -= left * speed; m_eye -= left * speed; computeViewMatrix(); } Camera::pan faz o movimento de girar a câmera em torno de seu eixo \\(y\\). Isso é feito alterando apenas o ponto m_at: void Camera::pan(float speed) { glm::mat4 transform{glm::mat4(1.0f)}; // Rotate camera around its local y axis transform = glm::translate(transform, m_eye); transform = glm::rotate(transform, -speed, m_up); transform = glm::translate(transform, -m_eye); m_at = transform * glm::vec4(m_at, 1.0f); computeViewMatrix(); } Após a linha 45, a matriz transform representa uma concatenação de transformações na forma: \\[ \\mathbf{M}=\\mathbf{I}.\\mathbf{T}(\\mathbf{p}_{\\textrm{eye}}).\\mathbf{R}_y(\\theta).\\mathbf{T}(-\\mathbf{p}_{\\textrm{eye}}). \\] A ordem de aplicação das transformações é obtida lendo a expressão acima da direita para a esquerda (no código, lemos de baixo para cima, da linha 45 à linha 40): \\(\\mathbf{T}(-\\mathbf{p}_{\\textrm{eye}})\\) (linha 45) tem o efeito de transladar a câmera para a origem do mundo, isto é, faz o ponto \\(\\mathbf{p}_{\\textrm{eye}}\\) virar a origem \\(O\\). \\(\\mathbf{R}_y(\\theta)\\) (linha 44) roda a câmera em torno do eixo \\(y\\) do mundo. Como a câmera agora está na origem, é como se a câmera fosse girada em torno de seu próprio eixo \\(y\\). \\(\\mathbf{T}(\\mathbf{p}_{\\textrm{eye}})\\) (linha 43) é a transformação inversa da primeira, isto é, faz a câmera voltar à sua posição original (mas note que, por causa do passo anterior, a orientação da câmera não é mais a orientação original). \\(\\mathbf{I}\\) é a matriz identidade (criada na linha 40). A linha 47 transforma m_at por transform. O resultado é rodar m_at em torno do eixo \\(y\\) local da câmera. Observação As operações da linha 40 até a linha 45 em Camera::pan são equivalentes ao pseudocódigo: transform = I; transform = transform * T(m_eye); transform = transform * Ry(-speed); transform = transform * T(-m_eye); que é o mesmo que transform = I * T(m_eye) * Ry(-speed) * T(-m_eye); onde I, Ry e T são as matrizes de transformação identidade, rotação em \\(y\\), e translação. openglwindow.hpp Deixaremos a definição da classe OpenGLWindow como a seguir: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;vector&gt; #include &quot;abcg.hpp&quot; #include &quot;camera.hpp&quot; #include &quot;ground.hpp&quot; struct Vertex { glm::vec3 position; bool operator==(const Vertex&amp; other) const { return position == other.position; } }; class OpenGLWindow : public abcg::OpenGLWindow { protected: void handleEvent(SDL_Event&amp; ev) override; void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_VAO{}; GLuint m_VBO{}; GLuint m_EBO{}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; Camera m_camera; float m_dollySpeed{0.0f}; float m_truckSpeed{0.0f}; float m_panSpeed{0.0f}; Ground m_ground; std::vector&lt;Vertex&gt; m_vertices; std::vector&lt;GLuint&gt; m_indices; void loadModelFromFile(std::string_view path); void update(); }; #endif O código é semelhante ao do projeto loadmodel visto no capítulo anterior. As diferenças estão nas seguintes linhas: Linhas 7 e 8: inclusão dos cabeçalhos camera.hpp e ground.hpp; Linha 20: declaração da função handleEvent para tratar os eventos do teclado; Linha 36: definição de um objeto da classe Camera para controlar a câmera LookAt; Linhas 37 a 39: definição de variáveis de controle de velocidade de dolly, truck e pan; Linha 41: definição de um objeto da classe Ground para desenhar o chão. Linha 47: definição de uma função update que será chamada em paintGL. Algumas coisas foram removidas em relação ao projeto loadmodel, como a variável que controlava o número de triângulos exibidos e a função OpenGLWindow::standardize que normalizava e centralizava o modelo no NDC. Dessa vez, o modelo armazenado no VBO será o modelo sem modificações, isto é, o modelo lido diretamente do arquivo. Para mudar a escala e posição do modelo, usaremos a matriz de modelo. openglwindow.cpp O início de openglwindow.cpp é exatamente o mesmo do projeto anterior: #include &quot;openglwindow.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;imgui.h&gt; #include &lt;tiny_obj_loader.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; #include &lt;glm/gtx/hash.hpp&gt; #include &lt;unordered_map&gt; // Explicit specialization of std::hash for Vertex namespace std { template &lt;&gt; struct hash&lt;Vertex&gt; { size_t operator()(Vertex const&amp; vertex) const noexcept { const std::size_t h1{std::hash&lt;glm::vec3&gt;()(vertex.position)}; return h1; } }; } // namespace std A definição de OpenGLWindow::handleEvent vem a seguir: void OpenGLWindow::handleEvent(SDL_Event&amp; ev) { if (ev.type == SDL_KEYDOWN) { if (ev.key.keysym.sym == SDLK_UP || ev.key.keysym.sym == SDLK_w) m_dollySpeed = 1.0f; if (ev.key.keysym.sym == SDLK_DOWN || ev.key.keysym.sym == SDLK_s) m_dollySpeed = -1.0f; if (ev.key.keysym.sym == SDLK_LEFT || ev.key.keysym.sym == SDLK_a) m_panSpeed = -1.0f; if (ev.key.keysym.sym == SDLK_RIGHT || ev.key.keysym.sym == SDLK_d) m_panSpeed = 1.0f; if (ev.key.keysym.sym == SDLK_q) m_truckSpeed = -1.0f; if (ev.key.keysym.sym == SDLK_e) m_truckSpeed = 1.0f; } if (ev.type == SDL_KEYUP) { if ((ev.key.keysym.sym == SDLK_UP || ev.key.keysym.sym == SDLK_w) &amp;&amp; m_dollySpeed &gt; 0) m_dollySpeed = 0.0f; if ((ev.key.keysym.sym == SDLK_DOWN || ev.key.keysym.sym == SDLK_s) &amp;&amp; m_dollySpeed &lt; 0) m_dollySpeed = 0.0f; if ((ev.key.keysym.sym == SDLK_LEFT || ev.key.keysym.sym == SDLK_a) &amp;&amp; m_panSpeed &lt; 0) m_panSpeed = 0.0f; if ((ev.key.keysym.sym == SDLK_RIGHT || ev.key.keysym.sym == SDLK_d) &amp;&amp; m_panSpeed &gt; 0) m_panSpeed = 0.0f; if (ev.key.keysym.sym == SDLK_q &amp;&amp; m_truckSpeed &lt; 0) m_truckSpeed = 0.0f; if (ev.key.keysym.sym == SDLK_e &amp;&amp; m_truckSpeed &gt; 0) m_truckSpeed = 0.0f; } } Os eventos de teclado são tratados de forma separada para as teclas pressionadas (SDL_KEYDOWN, linhas 24 a 35) e para as teclas liberadas (SDL_KEYUP, linhas 36 a 51). Quando uma tecla é pressionada (seta ou QWEASD), a velocidade de dolly, pan ou truck é modificada para +1 ou -1. Quando a tecla é liberada, a velocidade correspondente volta para 0. Vamos agora à definição de OpenGLWindow::initializeOpenGL, que também é bem parecida com a do projeto loadmodel: void OpenGLWindow::initializeGL() { abcg::glClearColor(0, 0, 0, 1); // Enable depth buffering abcg::glEnable(GL_DEPTH_TEST); // Create program m_program = createProgramFromFile(getAssetsPath() + &quot;lookat.vert&quot;, getAssetsPath() + &quot;lookat.frag&quot;); m_ground.initializeGL(m_program); // Load model loadModelFromFile(getAssetsPath() + &quot;bunny.obj&quot;); // Generate VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_vertices[0]) * m_vertices.size(), m_vertices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate EBO abcg::glGenBuffers(1, &amp;m_EBO); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(m_indices[0]) * m_indices.size(), m_indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); const GLint positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); // End of binding to current VAO abcg::glBindVertexArray(0); resizeGL(getWindowSettings().width, getWindowSettings().height); } Em relação ao projeto anterior, modificamos o nomes dos shaders lidos (linhas 61 a 62), chamamos Ground::initializeGL na linha 64 (para inicializar o VAO/VBO do chão) e incluímos a chamada a OpenGLWindow::resizeGL na linha 103. A função Camera::computeProjectioMatrix é chamada dentro de OpenGLWindow::resizeGL para reconstruir a matriz de projeção. Os valores da matriz dependem do tamanho atual da janela. Assim, ao chamarmos OpenGLWindow::resizeGL em OpenGLWindow::initializeGL, garantimos que a aplicação começará com uma matriz de projeção válida para as dimensões da janela. A definição de OpenGLWindow::loadModelFromFile é a mesma do projeto anterior. Vamos à definição de OpenGLWindow::paintGL: void OpenGLWindow::paintGL() { update(); // Clear color buffer and depth buffer abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); // Get location of uniform variables (could be precomputed) const GLint viewMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;viewMatrix&quot;)}; const GLint projMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;projMatrix&quot;)}; const GLint modelMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;modelMatrix&quot;)}; const GLint colorLoc{abcg::glGetUniformLocation(m_program, &quot;color&quot;)}; // Set uniform variables for viewMatrix and projMatrix // These matrices are used for every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_camera.m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_camera.m_projMatrix[0][0]); abcg::glBindVertexArray(m_VAO); // Draw white bunny glm::mat4 model{1.0f}; model = glm::translate(model, glm::vec3(-1.0f, 0.0f, 0.0f)); model = glm::rotate(model, glm::radians(90.0f), glm::vec3(0, 1, 0)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 1.0f, 1.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); // Draw yellow bunny model = glm::mat4(1.0); model = glm::translate(model, glm::vec3(0.0f, 0.0f, -1.0f)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 0.8f, 0.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); // Draw blue bunny model = glm::mat4(1.0); model = glm::translate(model, glm::vec3(1.0f, 0.0f, 0.0f)); model = glm::rotate(model, glm::radians(-90.0f), glm::vec3(0, 1, 0)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 0.0f, 0.8f, 1.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); // Draw red bunny model = glm::mat4(1.0); model = glm::scale(model, glm::vec3(0.1f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 0.25f, 0.25f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); // Draw ground m_ground.paintGL(); abcg::glUseProgram(0); } A função OpenGLWindow::update é chamada logo do início (linha 161) para atualizar a posição e orientação da câmera LookAt. Nas linhas 179 e 184, o conteúdo das matrizes de visão e projeção é enviado às variáveis uniformes no shader: // Set uniform variables for viewMatrix and projMatrix // These matrices are used for every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_camera.m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_camera.m_projMatrix[0][0]); Observe o uso da função glUniformMatrix4fv. Essa função tem a assinatura void glUniformMatrix4fv(GLint location, GLsizei count, GLboolean transpose, const GLfloat *value); onde location é o identificador de localização da variável uniforme no shader; count é o número de matrizes que queremos transferir à variável uniforme; transpose é um valor booleano que indica se queremos enviar a transposta da matriz; value é o ponteiro para o primeiro elemento do arranjo de elementos da matriz. A renderização do coelho branco é configurada nas linhas 188 a 197: // Draw white bunny glm::mat4 model{1.0f}; model = glm::translate(model, glm::vec3(-1.0f, 0.0f, 0.0f)); model = glm::rotate(model, glm::radians(90.0f), glm::vec3(0, 1, 0)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 1.0f, 1.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); Nas linhas 189 a 192 é criada a concatenação de transformações que forma a matriz de modelo (model). Para o coelho branco, essa concatenação é \\[ \\mathbf{M}_{\\textrm{model}}=\\mathbf{I}.\\mathbf{T}(-1,0,0).\\mathbf{R}_y\\left(\\frac{\\pi}{2}\\right).\\mathbf{S}(0.5, 0.5, 0.5). \\] Essas transformações servem para posicionar o modelo do coelho no mundo. Inicialmente o modelo está na posição e orientação definida no arquivo bunny.obj: na origem, sobre o plano \\(y=0\\), como vimos na figura 7.17. As transformações são aplicadas da seguinte forma: Transformação de escala para reduzir o tamanho do coelho para \\(50\\%\\) de seu tamanho original (linha 192); Rotação em \\(90^{\\circ}\\) em torno do eixo \\(y\\) do espaço do objeto, que é o mesmo eixo \\(y\\) do espaço do mundo (linha 191); Translação pelo vetor \\((-1,0,0)\\), que posiciona o coelho em sua posição final na cena (linha 190); Transformação identidade (linha 189). Na linha 194, a matriz de modelo é enviada à variável uniforme m_modelMatrix no vertex shader. Na linha 195, a variável uniforme color é definida com \\((1,1,1,1)\\) (branco) no vertex shader. Finalmente, na linha 196 é feita a chamada ao comando de renderização. Observe como um procedimento semelhante é feito para os outros coelhos. Mudam apenas as transformações que serão usadas para criar a matriz model, e o valor de cor definido na variável uniforme color. Para o coelho amarelo: // Draw yellow bunny model = glm::mat4(1.0); model = glm::translate(model, glm::vec3(0.0f, 0.0f, -1.0f)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 0.8f, 0.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); Para o coelho azul: // Draw blue bunny model = glm::mat4(1.0); model = glm::translate(model, glm::vec3(1.0f, 0.0f, 0.0f)); model = glm::rotate(model, glm::radians(-90.0f), glm::vec3(0, 1, 0)); model = glm::scale(model, glm::vec3(0.5f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 0.0f, 0.8f, 1.0f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); Para o pequeno coelho vermelho: // Draw red bunny model = glm::mat4(1.0); model = glm::scale(model, glm::vec3(0.1f)); abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 0.25f, 0.25f, 1.0f); abcg::glDrawElements(GL_TRIANGLES, m_indices.size(), GL_UNSIGNED_INT, nullptr); Note que todos os modelos foram renderizados com o mesmo VAO (linha 186), pois todos compartilham o mesmo VBO. É a matriz de modelo que faz com que cada coelho tenha uma transformação diferente no cenário 3D. No final de OpenGLWindow::paintGL, temos o seguinte código: abcg::glBindVertexArray(0); // Draw ground m_ground.paintGL(); abcg::glUseProgram(0); Na linha 229, o VAO dos coelhos deixa de ser usado. Em seguida, na linha 232, o chão é desenhado. O chão tem seu próprio VAO, mas usa os mesmos shaders dos coelhos. É por isso que os shaders só são desabilitados na linha 234 com a chamada a glUseProgram(0). A definição de OpenGLWindow::terminateGL ficará como a seguir: void OpenGLWindow::terminateGL() { m_ground.terminateGL(); abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Não há nada de muito novo nesse código, exceto a chamada a Ground::terminateGL para liberar o VAO e VBO do chão. Finalmente, a definição de OpenGLWindow::update ficará como a seguir: void OpenGLWindow::update() { const float deltaTime{static_cast&lt;float&gt;(getDeltaTime())}; // Update LookAt camera m_camera.dolly(m_dollySpeed * deltaTime); m_camera.truck(m_truckSpeed * deltaTime); m_camera.pan(m_panSpeed * deltaTime); } Aqui, as funções de movimentação da câmera são chamadas usando as variáveis de velocidade que tiveram seus valores determinados em OpenGLWindow::handleEvent de acordo com as teclas pressionadas. ground.hpp A classe Ground é responsável pelo desenho do chão. Embora não seja uma classe derivada de abcg::OpenGLWindow, os nomes de funções são os mesmos (initializeGL, paintGL e terminateGL). Como vimos anteriormente, essas funções são chamadas nas respectivas funções de OpenGLWindow: #ifndef GROUND_HPP_ #define GROUND_HPP_ #include &quot;abcg.hpp&quot; class Ground { public: void initializeGL(GLuint program); void paintGL(); void terminateGL(); private: GLuint m_VAO{}; GLuint m_VBO{}; GLint m_modelMatrixLoc{}; GLint m_colorLoc{}; }; #endif Ground::initializeGL recebe como parâmetro o identificador de um programa de shader já existente. Assim, o chão pode usar os mesmos shaders dos coelhos. Em Ground::paintGL, veremos que o chão é desenhado como um padrão de xadrez. Como é um padrão composto por quadriláteros, o VBO não precisa ser a malha geométrica do chão inteiro. O VBO é apenas um quadrilátero de tamanho unitário. Em Ground::paintGL, esse quadrilátero será desenhado várias vezes para formar um ladrilho com padrão de xadrez. ground.cpp Vamos começar com a definição de Ground::initializeGL: #include &quot;ground.hpp&quot; #include &lt;cppitertools/itertools.hpp&gt; void Ground::initializeGL(GLuint program) { // Unit quad on the xz plane std::array vertices{glm::vec3(-0.5f, 0.0f, 0.5f), glm::vec3(-0.5f, 0.0f, -0.5f), glm::vec3( 0.5f, 0.0f, 0.5f), glm::vec3( 0.5f, 0.0f, -0.5f)}; // Generate VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Create VAO and bind vertex attributes abcg::glGenVertexArrays(1, &amp;m_VAO); abcg::glBindVertexArray(m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); const GLint posAttrib{abcg::glGetAttribLocation(program, &quot;inPosition&quot;)}; abcg::glEnableVertexAttribArray(posAttrib); abcg::glVertexAttribPointer(posAttrib, 3, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindVertexArray(0); // Save location of uniform variables m_modelMatrixLoc = abcg::glGetUniformLocation(program, &quot;modelMatrix&quot;); m_colorLoc = abcg::glGetUniformLocation(program, &quot;color&quot;); } No início da função, definimos os vértices de um quadrilátero de tamanho unitário centralizado no plano \\(xz\\). Em seguida, criamos o VBO e fazemos a ligação do VBO com o atributo inPosition do shader program. Por fim, salvamos a localização das variáveis uniformes que serão utilizadas em Ground::paintGL. A propósito, eis o código de Ground::paintGL: void Ground::paintGL() { // Draw a grid of tiles centered on the xz plane const int N{5}; abcg::glBindVertexArray(m_VAO); for (const auto z : iter::range(-N, N + 1)) { for (const auto x : iter::range(-N, N + 1)) { // Set model matrix glm::mat4 model{1.0f}; model = glm::translate(model, glm::vec3(x, 0.0f, z)); abcg::glUniformMatrix4fv(m_modelMatrixLoc, 1, GL_FALSE, &amp;model[0][0]); // Set color (checkerboard pattern) const float gray{(z + x) % 2 == 0 ? 1.0f : 0.5f}; abcg::glUniform4f(m_colorLoc, gray, gray, gray, 1.0f); abcg::glDrawArrays(GL_TRIANGLE_STRIP, 0, 4); } } abcg::glBindVertexArray(0); } Aqui, desenhamos uma grade de 11x11 quadriláteros (variando \\(z\\) e \\(x\\) de -5 a 5). Cada quadrilátero é transladado através de uma matriz de modelo e então desenhado com glDrawArrays usando a primitiva GL_TRIANGLE_STRIP. A cor utilizada (configurada pela variável uniforme do shader) é modificada de acordo com a paridade das coordenadas da grade de modo a formar o padrão de xadrez. Em Ground::terminateGL, apenas o VBO e o VAO são liberados: void Ground::terminateGL() { abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Como o programa de shader é o mesmo dos coelhos, o responsável pela liberação dos shaders é OpenGLWindow, como vimos em OpenGLWindow::terminateGLP. Isso conclui o projeto lookat. Baixe o código completo a partir deste link. "],["projections.html", "8 Projeções e trackball virtual", " 8 Projeções e trackball virtual Em síntese de imagens, utilizamos técnicas de projeção gráfica para converter objetos tridimensionais (3D) em representações bidimensionais (2D). Em particular, estamos interessados em projeções planares, pois desejamos projetar objetos 3D sobre o plano de imagem da câmera virtual. No capítulo 4, vimos que é comum supor que a câmera virtual seja baseada em uma câmera pinhole ideal. Nesse modelo, o plano de imagem está posicionado à frente da câmera, e a imagem é formada através da projeção dos objetos tridimensionais sobre o plano, na direção da posição da câmera/olho (figura 8.1). Figura 8.1: Projeção de um triângulo no plano de imagem de uma câmera virtual. As linhas tracejadas que saem de cada vértice do objeto 3D e atravessam o plano de imagem são chamadas de projetores. Quando os projetores convergem em um centro de projeção, o resultado é uma projeção perspectiva (figura 8.2). Figura 8.2: Projeção perspectiva. A projeção perspectiva produz o efeito de diminuição do tamanho aparente dos objetos mais distantes, que é o que percebemos no mundo real. Assim, a projeção perspectiva é o tipo de projeção mais adequada para simular a visão de um observador em um cenário virtual. Quando os projetores são paralelos, temos uma projeção paralela (figura 8.3). Na projeção paralela, é como se o centro de projeção estivesse a uma distância infinita do plano de projeção. Figura 8.3: Projeção paralela. Projeções paralelas são frequentemente utilizadas em desenhos técnicos, pois segmentos paralelos em 3D continuam paralelos quando projetados. Projeções paralelas podem ser classificadas em dois grandes grupos segundo o ângulo formado entre os projetores e o plano de projeção. Se os projetores formam um ângulo de \\(90^{\\circ}\\) com o plano de projeção, temos uma projeção ortográfica. Caso contrário, temos uma projeção oblíqua. Projeções ortográficas e oblíquas podem ainda ser classificadas segundo os ângulos formados entre os eixos principais após a projeção, e a forma como as distâncias são preservadas. A seguir é apresentada uma classificação hierárquica das principais projeções planares. Exemplos correspondentes são mostrados na figura 8.4: Projeção perspectiva: projetores convergem em um centro de projeção. Projeção paralela: projetores são paralelos entre si. Projeção ortográfica: projetores são perpendiculares ao plano de projeção. Projeção multivista: é o uso de diferentes pontos de vista de projeção ortográfica, cada um em uma direção paralela a um dos eixos \\(x\\), \\(y\\) e \\(z\\), compondo um conjunto de imagens chamadas de planos e elevações. Na projeção multivista, é comum desenhar o objeto de frente (elevação frontal), de lado (elevação lateral) e de cima (plano). Projeção axonométrica: projeção ortográfica em uma vista que não é paralela a nenhum dos eixos principais. Isométrica: os três eixos projetados formam um ângulo comum de \\(120^{\\circ}\\) entre si. Dimétrica: dois dos eixos projetados formam um ângulo comum com outro eixo. Trimétrica: não há ângulo em comum entre os eixos projetados. Projeção oblíqua: projetores não são perpendiculares ao plano de projeção. Cavalier: dois dos eixos projetados formam um ângulo comum entre si, e as distâncias são preservadas nas três direções. Cabinet: similiar à projeção cavalier, mas a distância na profundidade é metade da distância nas outras direções. Outras: variações da projeção cavalier. Figura 8.4: Exemplos de projeções planares (fonte). Neste capítulo, abordaremos a construção de duas matrizes de projeção no OpenGL: Matriz de projeção perspectiva (seção 8.1); Matriz de projeção ortográfica (seção 8.2); Essas matrizes serão utilizadas para converter coordenadas do espaço da câmera para coordenadas do espaço de recorte no pipeline de renderização. A partir da matriz de projeção perspectiva e de projeção ortográfica é possível gerar qualquer tipo de projeção planar da classificação anterior. Nosso foco será no uso da projeção perspectiva. Entretanto, descreveremos primeiramente a projeção ortográfica por ser mais simples e por ser a projeção padrão adotada pelo pipeline quando a matriz de projeção não é utilizada. Observação As projeções multivista e axonométricas utilizam a mesma matriz de projeção ortográfica que iremos construir. A diferença entre cada subtipo de projeção é determinada pela orientação da câmera, isto é, pela matriz de visão. Por exemplo, para que tenhamos uma projeção isométrica, podemos usar uma matriz de projeção ortográfica associada com uma matriz de visão criada através de uma câmera LookAt com parâmetros tais que \\[ \\begin{align} P_{\\textrm{at}}-P_{\\textrm{eye}}&amp;=(\\pm c,\\pm c, \\pm c). \\end{align} \\] com \\(c \\neq 0\\). Em um exemplo mais concreto: \\[ \\begin{align} c &amp;= \\sqrt{1/3},\\\\ P_{\\textrm{eye}}&amp;=(c, c, c),\\\\ P_{\\textrm{at}}&amp;=(0, 0, 0),\\\\ \\mathbf{v}_{\\textrm{up}}&amp;=(0,1,0) \\end{align} \\] resulta em uma visão isométrica com a câmera posicionada a uma unidade de distância da origem. Projeções oblíquas podem ser obtidas através da concatenação de uma matriz adicional de transformação de cisalhamento à matriz de projeção ortográfica. Uma matriz de transformação de cisalhamento tem a forma \\[ \\mathbf{H(\\theta,\\phi)}= \\begin{bmatrix} 1 &amp; 0 &amp; -\\cot \\theta &amp; 0\\\\ 0 &amp; 0 &amp; -\\cot \\phi &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\] onde \\(\\theta\\) e \\(\\phi\\) determinam os ângulos entre os projetores e os eixos \\(x\\) e \\(y\\), respectivamente. Se \\(\\mathbf{M}_{orth}\\) é a matriz de projeção ortográfica, então a concatenação \\(\\mathbf{M}_{orth}\\mathbf{H(\\theta,\\phi)}\\) produz uma projeção oblíqua se \\(\\theta\\) ou \\(\\phi\\) forem diferentes de \\(\\pm\\frac{\\pi}{2}\\). Além do conteúdo sobre projeções, este capítulo também apresenta a teoria (seção 8.3) e prática (seção 8.4) da implementação de uma técnica de interação chamada de trackball virtual. Essa técnica permite que o usuário use o mouse para realizar rotações livres de objetos tridimensionais. O capítulo é concluído com a seção 8.5, que apresenta um passo a passo de implementação de um efeito visual que explora a diminuição do tamanho dos objetos projetados como resultado da transformação perspectiva. "],["ortho.html", "8.1 Projeção ortográfica", " 8.1 Projeção ortográfica Por padrão, o pipeline do OpenGL produz uma projeção ortográfica das primitivas contidas no volume de visão, como se as coordenadas \\(z\\) de todos os pontos dentro do volume de visão fossem descartadas para formar figuras no plano de imagem. Lembre-se que o volume de visão é um cubo \\(2 \\times 2 \\times 2\\) centralizado na origem no espaço normalizado do dispositivo (NDC). Logo antes da rasterização, o pipeline converte automaticamente as coordenadas do NDC para o espaço da janela, em pixels. Na configuração padrão de glViewport(x, y, w, h) e glDepthRange(n, f), o canto inferior esquerdo da janela é a origem do espaço da janela (\\(x=y=0\\)), e o canto superior direito é a coordenada \\((w,h)\\) onde \\(w\\) e \\(h\\) correspondem respectivamente à largura e altura da janela em pixels29. Assim, o seguinte mapeamento é feito internamente pelo pipeline: \\(x_{\\textrm{ndc}} \\in [-1,1]\\) em NDC torna-se \\(x_w \\in [x,w]\\) no espaço da janela. \\(y_{\\textrm{ndc}} \\in [-1,1]\\) em NDC torna-se \\(y_w \\in [y,h]\\) no espaço da janela. \\(z_{\\textrm{ndc}} \\in [-1,1]\\) em NDC torna-se \\(z_w \\in [n,f]\\) no espaço da janela. O mapeamento pode ser representado pela seguinte matriz de viewport: \\[ \\begin{bmatrix} x_w\\\\[0.5em] y_w \\\\[0.5em] z_w \\end{bmatrix} = \\begin{bmatrix} \\frac{w}{2} &amp; 0 &amp; 0 &amp; x+\\frac{w}{2}\\\\ 0 &amp; \\frac{h}{2} &amp; 0 &amp; y+\\frac{h}{2}\\\\ 0 &amp; 0 &amp; \\frac{f-n}{2} &amp; \\frac{f+n}{2} \\end{bmatrix} \\begin{bmatrix} x_{\\textrm{ndc}}\\\\ y_{\\textrm{ndc}}\\\\ z_{\\textrm{ndc}}\\\\ 1 \\end{bmatrix}. \\] Por padrão, \\(n=0\\) e \\(f=1\\) (configuração padrão de glDepthRange). Após o mapeamento para o espaço da janela, as primitivas são rasterizadas. Se mais de um fragmento for mapeado para o mesmo pixel no framebuffer, o teste de profundidade pode ser utilizado para manter apenas o fragmento de menor valor \\(z_w\\). A figura 8.5 mostra um exemplo no qual o espaço NDC contém 8 cubos posicionados em \\((\\pm0.5, \\pm0.5, \\pm0.5)\\), alinhados aos eixos principais. Após a rasterização com o teste de profundidade habilitado na configuração padrão, o conteúdo rasterizado no espaço da janela exibirá apenas a face da frente dos 4 cubos de menor valor \\(z\\), como ocorreria numa projeção ortográfica sobre o plano \\(z=-1\\) em NDC. A figura também mostra que a origem do espaço NDC (\\(O_{\\textrm{ndc}}\\)) é mapeada para o centro da janela. Figura 8.5: Objetos em NDC e conteúdo correspondente no espaço da janela. Uma vez que o pipeline do OpenGL usa a projeção ortográfica como padrão, podemos supor inicialmente que nossa matriz de projeção é a matriz identidade, isto é, nenhuma transformação adicional precisa ser feita para produzir a projeção ortográfica. Assim, no vertex shader, as coordenadas serão modificadas apenas pelas matrizes de modelo e visão (\\(\\mathbf{M}_{\\textrm{view}}\\mathbf{M}_{\\textrm{model}}\\)), gerando pontos no espaço da câmera. Relembre que, no espaço da câmera, a posição da câmera é o ponto de referência do frame, e a direção de visão é a direção do eixo \\(z\\) negativo. A matriz de projeção é responsável por converter coordenadas do espaço da câmera para o espaço de recorte. Entretanto, como estamos supondo que a matriz de projeção é a matriz identidade, o espaço de recorte é, neste caso, idêntico ao espaço da câmera. Então, os pontos no espaço da câmera podem ser enviados diretamente à variável embutida gl_Position. Internamente, o pipeline supõe que, no espaço de recorte, todas as primitivas com coordenadas menores que \\(-w\\) e maiores que \\(+w\\) devem ser recortadas. Uma vez que \\(w=1\\) para todos os pontos, são recortadas todas as primitivas que estiverem fora do cubo que vai de \\((-1,-1,-1)\\) até \\((1,1,1)\\). Após o recorte, as coordenadas são divididas por \\(w\\) para converter coordenadas do espaço de recorte para coordenadas normalizadas do dispositivo. Mas, como \\(w=1\\), as coordenadas continuam com o mesmo valor. Então, neste caso, o espaço NDC é idêntico ao espaço de recorte projetado, que por sua vez é idêntico ao espaço da câmera. Há um problema em usar a matriz identidade como matriz de projeção: consideramos até agora que os modelos geométricos são representados em um sistema que segue a regra da mão direita. Entretanto, as coordenadas normalizadas do dispositivo seguem a regra da mão esquerda. Isso faz com que orientação dos triângulos fique invertida (CW vira CCW e vice-versa). Felizmente, é fácil construir uma transformação que converte as coordenadas para a regra da mão direita: basta negarmos a coordenada \\(z\\) de cada ponto. Essa transformação pode ser feita por uma matriz de projeção ligeiramente diferente de uma matriz identidade: \\[ \\mathbf{M}_{\\textrm{orth}}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Agora, no vertex shader, gl_Position receberá os pontos transformados por \\(\\mathbf{M}_{\\textrm{proj}}\\mathbf{M}_{\\textrm{view}}\\mathbf{M}_{\\textrm{view}}\\), onde \\(\\mathbf{M}_{\\textrm{proj}}=\\mathbf{M}_{\\textrm{orth}}\\). Com a matriz \\(\\mathbf{M}_{\\textrm{orth}}\\) conseguimos consertar a inversão de orientação das primitivas. No entanto, há ainda outro problema: como a câmera está na origem em NDC, só conseguimos enxergar na tela a geometria que estiver contida no cubo de tamanho 2 em torno da câmera (isto é, a geometria após o recorte). Esse tamanho é muito limitante para a maioria das cenas. Além disso, a posição da câmera no centro do cubo não parece ser algo muito intuitivo. Na câmera LookAt, a direção de visão é a direção de \\(z\\) negativo. Logo, não deveríamos ser capazes de enxergar algo que está com \\(z\\) positivo (isto é, atrás da câmera, ainda que dentro do cubo de tamanho de 2). Para resolver isso, vamos criar uma nova matriz de projeção que supõe que o volume de visão está sempre situado em algum lugar do espaço da câmera com \\(z&lt;0\\), como ilustra a figura 8.6. Figura 8.6: Volume de visão genérico para projeção ortográfica. Nessa figura, o lado mais perto do volume de visão está a uma distância \\(n\\) da câmera, medida ao longo de sua linha de visão (eixo \\(z\\) negativo). Esse lado mais próximo em relação à posição da câmera é chamado de plano de recorte próximo ou near clipping plane. O lado mais distante do volume de visão está a uma distância \\(f\\) da câmera, e é chamado de plano de recorte distante ou far clipping plane. Na definição desse novo volume de visão, usaremos parâmetros \\(l\\) (left), \\(r\\) (right), \\(b\\) (bottom), \\(t\\) (top) para especificar a posição dos lados esquerdo e direito, de baixo e de cima do volume. Desse modo, o volume não precisará ser mais um cubo de tamanho 2 em cada direção. Podemos obter essa configuração através da modificação da matriz de projeção. É interessante notar que a matriz de projeção ortográfica é simplesmente uma matriz que transforma o volume de visão, do espaço da câmera, para o volume de visão em NDC, como mostra a figura 8.7. Figura 8.7: A matriz de projeção ortográfica representa a transformação do volume de visão do espaço da câmera para o volume de visão em NDC. Esse mapeamento da transformação de projeção consiste em fazer com que os pontos \\((l,b,-n)\\) e \\((r,t,-f)\\) no espaço da câmera tornem-se respectivamente os pontos \\((-1,-1,-1)\\) e \\((1,1,1)\\) em NDC. Tal processo é chamado de normalização do volume de visão. Podemos fazer a normalização em três etapas: Translação do volume de visão de modo a centralizá-lo na origem. Escala do volume de visão de modo a deixá-lo com tamanho 2 em cada direção. Reflexão para inverter a coordenada \\(z\\). Como a reflexão é uma escala com inversão de sinal, as etapas 2 e 3 podem ser feitas em conjunto, como veremos a seguir. Translação O centroide \\(C=(c_x,c_y,c_z)\\) do volume de visão no espaço da câmera é \\[ c_x = \\frac{r+l}{2},\\qquad c_y = \\frac{t+b}{2},\\qquad c_z = -\\frac{f+n}{2}. \\] Logo, a matriz de translação que desloca o volume de visão para a origem é a matriz de translação por \\(-C\\): \\[ \\mathbf{T}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -\\frac{r+l}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{t+b}{2}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{f+n}{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Escala e reflexão Os fatores de escala \\(S=(s_x, s_y, s_z)\\) para redimensionar o volume de visão em um cubo com tamanho 2 em cada direção são: \\[ s_x = \\frac{2}{r-l},\\qquad s_y = \\frac{2}{t-b},\\qquad s_z = \\frac{2}{f-n}. \\] Entretanto, precisamos refletir o cubo na direção \\(z\\) para a conversão da regra da mão direita para mão esquerda. Assim, precisamos inverter o sinal de \\(s_z\\): \\[ s_z = -\\frac{2}{f-n}. \\] A matriz de escala ficará como a seguir: \\[ \\mathbf{S}= \\begin{bmatrix} \\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{2}{t-b} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -\\frac{2}{f-n} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Matriz de projeção Concatenando as transformações de translação, escala e reflexão, obtemos a nova matriz de projeção ortográfica: \\[ \\begin{align} \\mathbf{M}_{\\textrm{orth}}=\\mathbf{S}\\mathbf{T}&amp;= \\begin{bmatrix} \\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{2}{t-b} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -\\frac{2}{f-n} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -\\frac{r+l}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{t+b}{2}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{f+n}{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix},\\\\ \\mathbf{M}_{\\textrm{orth}}&amp;= \\begin{bmatrix} \\frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\\frac{r+l}{r-l}\\\\ 0 &amp; \\frac{2}{t-b} &amp; 0 &amp; -\\frac{t+b}{t-b}\\\\ 0 &amp; 0 &amp; -\\frac{2}{f-n} &amp; -\\frac{f+n}{f-n}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\end{align} \\] Na biblioteca GLM, tal matriz pode ser criada com a função glm::ortho, definida em glm/gtc/matrix_transform.hpp: glm::mat4 glm::ortho(float left, float right, float bottom, float top, float zNear, float zFar); glm::dmat4 glm::ortho(double left, double right, double bottom, double top, double zNear, double zFar); onde left, right, bottom, top, zNear e zFar correspondem respectivamente aos valores \\(l\\), \\(r\\), \\(b\\), \\(t\\), \\(n\\) e \\(f\\). Na maioria das aplicações, trabalhamos com volumes simétricos nas direções \\(x\\) e \\(y\\). Nesse caso, os pontos \\((0,0,z)\\) em NDC são projetados no centro do viewport. Em um volume simétrico, \\[ r = -l,\\\\ t = -b. \\] Com isso, os termos da matriz anterior podem ser simplificados como segue: \\[ \\begin{align} r+l&amp;=0,\\\\ r-l&amp;=2r,\\\\ t+b&amp;=0,\\\\ t-b&amp;=2t,\\\\ \\end{align} \\] e a matriz assume o fomato \\[ \\mathbf{M}_{\\textrm{orth}}= \\begin{bmatrix} \\frac{1}{r} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{1}{t} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -\\frac{2}{f-n} &amp; -\\frac{f+n}{f-n}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] A configuração padrão é a configuração utilizada em todos os projetos da ABCg feitos até agora. "],["perspective.html", "8.2 Projeção perspectiva", " 8.2 Projeção perspectiva Na projeção perspectiva, quanto mais distantes os objetos estiverem do centro de projeção, menor ficarão quando projetados. Isso produz o efeito de diminuição de tamanho de objetos distantes, que é o que percebemos no mundo real. A figura 8.8 mostra esse efeito em uma fotografia. Note como os elementos da cena parecem convergir em um ponto distante. Esse ponto de convergência é chamado de ponto de fuga. Figura 8.8: Diminuição de tamanho na projeção perspectiva (fonte). O número de pontos de fuga é determinado pela orientação da câmera em relação a um objeto cuboide referencial no espaço do mundo (figura 8.9). Figura 8.9: Pontos de fuga na projeção perspectiva. Se o cubo tiver arestas paralelas aos eixos \\(x\\) e \\(y\\) da câmera, a projeção terá 1 ponto de fuga. Se o cubo tiver arestas paralelas apenas em relação a um dos eixos (\\(x\\) ou \\(y\\)), a projeção terá 2 pontos de fuga. Se o cubo não tiver arestas paralelas aos eixos \\(x\\) e \\(y\\), a projeção terá 3 pontos de fuga. Para produzir uma matriz de projeção perspectiva, adotaremos a mesma estratégia de normalizar o volume de visão, isto é, criaremos uma transformação que converte um volume de visão no espaço da câmera para o volume de visão de tamanho \\(2 \\times 2 \\times 2\\) no espaço NDC. Entretanto, dessa vez o volume de visão terá o formato de uma pirâmide truncada (chamada de view frustum), como mostra a figura 8.10. Figura 8.10: Volume de visão genérico para projeção perspectiva. O volume de visão possui um formato piramidal pois todos os pontos do volume estão sobre projetores que convergem em direção à origem do espaço da câmera, que é o centro de projeção. O formato da pirâmide é definido unicamente pelos parâmetros \\(l\\) (left), \\(r\\) (right), \\(b\\) (bottom), \\(t\\) (top), \\(n\\) (near) e \\(f\\) (far). Suponha a cena de um arranjo de 8 cubos conforme mostra a figura 8.11. Figura 8.11: Cena dentro do volume de visão de projeção perspectiva. Após a normalização do volume de visão, todo o seu conteúdo é distorcido proporcionalmente como mostra a figura 8.12. Observe como os objetos mais distantes ficam menores em relação aos objetos mais próximos, e como as arestas laterais dos cubos não são mais paralelas como na cena original. De fato, elas agora convergem para um ponto de fuga. Figura 8.12: Distorção da cena após a normalização do volume de visão. Agora que a geometria da cena está distorcida, podemos seguir com o processamento do pipeline de gráfico. Após a rasterização e o mapeamento ortogonal para o espaço da janela, o resultado será uma imagem que tem a aparência de uma projeção perspectiva (figura 8.13). Figura 8.13: Objetos em NDC e conteúdo correspondente no espaço da janela. Matriz de projeção Para construir a matriz a projeção perspectiva, vamos observar primeiro como um ponto \\((x_e,y_e,z_e)\\) no espaço da câmera (o \\(e\\) subscrito vem de eye space) é projetado para um ponto \\((x_p, y_p, z_p)\\) no plano de recorte próximo (isto é, o plano com \\(z_e=-n\\)). A figura 8.14 mostra a relação entre esses pontos em uma visão de cima do volume de visão. Figura 8.14: Volume de visão visto de cima. Através da razão entre triângulos semelhantes, temos \\[ \\frac{x_p}{-n}=\\frac{x_e}{z_e}. \\] Logo, \\[ x_p=\\frac{-nx_e}{z_e}=\\frac{nx_e}{-z_e}. \\] O mesmo raciocínio pode ser aplicado para determinar \\(y_p\\). A figura 8.15 mostra uma visão lateral do volume de visão. Figura 8.15: Volume de visão visto de lado. Através da razão entre triângulos semelhantes, \\[ \\frac{y_p}{-n}=\\frac{y_e}{z_e}. \\] Logo, \\[ y_p=\\frac{-ny_e}{z_e}=\\frac{ny_e}{-z_e}. \\] O importante a ser notado aqui é que tanto \\(x_p\\) quanto \\(y_p\\) são divididos por \\(-z_e\\). Então, todo ponto no espaço da câmera deverá ser dividido pela sua coordenada \\(z\\) negativa. Podemos incorporar a divisão por \\(-z_e\\) na matriz de projeção. Lembre-se que, no vertex shader, representamos pontos e vetores em coordenadas homogêneas. A matriz de projeção converte coordenadas homogêneas do espaço da câmera (\\(x_e\\), \\(y_e\\), \\(z_e\\), \\(w_e\\)) em coordenadas homogêneas do espaço de recorte (\\(x_c\\), \\(y_c\\), \\(z_c\\), \\(w_c\\)), que são as coordenadas de gl_Position: \\[ \\begin{align} \\begin{bmatrix} x_c\\\\y_c\\\\z_c\\\\w_c \\end{bmatrix} = \\mathbf{M}_{\\textrm{proj}} \\begin{bmatrix} x_e\\\\y_e\\\\z_e\\\\w_e \\end{bmatrix}. \\end{align} \\] Após o recorte, as coordenadas do espaço de recorte são divididas por \\(w_c\\) para produzir coordenadas (\\(x_n\\), \\(y_n\\), \\(z_n\\)) no espaço NDC: \\[ \\begin{align} \\begin{bmatrix} x_n\\\\y_n\\\\z_n \\end{bmatrix} = \\begin{bmatrix} x_c/w_c\\\\y_c/w_c\\\\z_c/w_c \\end{bmatrix}. \\end{align} \\] Aproveitando essa divisão por \\(w\\), podemos obter a divisão por \\(-z_e\\) através da mudança da última linha da matriz de projeção, como a seguir: \\[ \\begin{align} \\begin{bmatrix} x_c\\\\y_c\\\\z_c\\\\w_c \\end{bmatrix} = \\begin{bmatrix} \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x_e\\\\y_e\\\\z_e\\\\w_e \\end{bmatrix}. \\end{align} \\] Observe que \\(w_c=-z_e\\). Portanto, as coordenadas serão divididas por \\(-z_e\\) como desejamos. Da mesma forma como fizemos para normalizar o volume de visão da projeção ortográfica, sabemos que precisamos mapear os intervalos: Em \\(x\\): \\([l, r]\\), no espaço da câmera, para \\([-1, 1]\\) em NDC; Em \\(y\\): \\([b, t]\\), no espaço da câmera, para \\([-1, 1]\\) em NDC; Em \\(z\\): \\([-n, -f]\\), no espaço da câmera, para \\([-1, 1]\\) em NDC. Os fatores de translação e escala em \\(x\\) e em \\(y\\) são os mesmos da projeção ortográfica. Assim, temos a seguinte relação entre coordenadas em NDC \\((x_{\\textrm{ndc}}, y_{\\textrm{ndc}})\\) e coordenadas projetadas \\((x_p, y_p)\\): \\[ x_{\\textrm{ndc}}=a_x x_p + b_x,\\\\ y_{\\textrm{ndc}}=a_y y_p + b_y, \\] onde \\(a\\) e \\(b\\) são, respectivamente, os fatores de escala e translação: \\[ a_x=\\frac{2}{r-l},\\qquad b_x=-\\frac{r+l}{r-l},\\\\[10pt] a_y=\\frac{2}{t-b},\\qquad b_y=-\\frac{t+b}{t-b}. \\] Se substituirmos \\[x_p=\\dfrac{n \\cdot x_e}{-z_e}\\] na expressão \\[x_{\\textrm{ndc}}=a_xx_p + b_x,\\] obtemos a relação final entre a coordenada \\(x_e\\) do espaço da câmera e a coordenada \\(x_{\\textrm{ndc}}\\) no espaço NDC (o mesmo raciocínio pode ser aplicado para a transformação de \\(y_e\\) em \\(y_{\\textrm{ndc}}\\)): \\[ \\begin{align} x_{\\textrm{ndc}}&amp;=a_x x_p + b_x\\\\[10pt] &amp;=\\dfrac{2x_p}{r-l}-\\dfrac{r+l}{r-l}\\\\[10pt] &amp;=\\dfrac{2 \\cdot \\dfrac{n \\cdot x_e}{-z_e}}{r-l}-\\dfrac{r+l}{r-l}\\\\[10pt] &amp;=\\dfrac{2n \\cdot x_e}{-z_e(r-l)}-\\frac{r+l}{r-l}\\\\[10pt] &amp;=\\dfrac{\\dfrac{2n}{r-l} \\cdot x_e}{-z_e}-\\frac{r+l}{r-l}\\\\[10pt] &amp;=\\dfrac{\\dfrac{2n}{r-l} \\cdot x_e}{-z_e}+\\dfrac{\\dfrac{r+l}{r-l} \\cdot z_e}{-z_e}\\\\[10pt] &amp;=\\dfrac{x_c}{-z_e}, \\end{align} \\] onde \\[ \\begin{align} x_c &amp;= n \\cdot a_x \\cdot x_e - b_x \\cdot z_e\\\\[10pt] &amp;= \\dfrac{2n}{r-l} \\cdot x_e + \\dfrac{r+l}{r-l} \\cdot z_e. \\end{align} \\] De forma semelhante, \\[ y_{\\textrm{ndc}} = \\frac{y_c}{-z_e}, \\] onde \\[ \\begin{align} y_c &amp;= n \\cdot a_y \\cdot y_e - b_y \\cdot z_e\\\\[10pt] &amp;= \\frac{2n}{t-b} \\cdot y_e + \\frac{t+b}{t-b} \\cdot z_e. \\end{align} \\] Atualizando os elementos da matriz de projeção, \\[ \\begin{align} \\begin{bmatrix} x_c\\\\[0.35em]y_c\\\\[0.35em]z_c\\\\[0.35em]w_c \\end{bmatrix} = \\begin{bmatrix} \\frac{2n}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\ 0 &amp; \\frac{2n}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x_e\\\\[0.35em]y_e\\\\[0.35em]z_e\\\\[0.35em]w_e \\end{bmatrix}. \\end{align} \\] Ainda precisamos determinar os elementos da terceira linha da matriz. Esses elementos correspondem à transformação de \\(z_e\\) em \\(z_c\\). O valor de \\(z_c\\) não depende de \\(x_e\\) e \\(y_e\\). Assim, os valores nas duas primeiras colunas da terceira linha devem ser zero. Só precisamos determinar os elementos da terceira e quarta colunas, que chamaremos de \\(\\alpha\\) e \\(\\beta\\): \\[ \\begin{align} \\begin{bmatrix} x_c\\\\[0.35em]y_c\\\\[0.35em]z_c\\\\[0.35em]w_c \\end{bmatrix} = \\begin{bmatrix} \\frac{2n}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\ 0 &amp; \\frac{2n}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\ 0 &amp; 0 &amp; \\alpha &amp; \\beta \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x_e\\\\[0.35em]y_e\\\\[0.35em]z_e\\\\[0.35em]w_e \\end{bmatrix}. \\end{align} \\] Logo, \\[ z_c=\\alpha z_e + \\beta w_e. \\] Após a divisão pelo \\(w\\), \\[ z_n=\\frac{\\alpha z_e + \\beta w_e}{-z_e}. \\] Sabendo que o intervalo \\([-n, -f]\\) deve ser mapeado para o intervalo \\([-1, 1]\\), podemos formar um sistema de equações lineares: \\[ \\begin{array}{l} \\dfrac{-\\alpha n + \\beta}{n}=-1\\\\[10pt] \\dfrac{-\\alpha f + \\beta}{f}=1 \\end{array} \\quad \\rightarrow \\quad \\begin{array}{l} -\\alpha n + \\beta = -n\\\\[10pt] -\\alpha f + \\beta = f \\end{array} \\] Logo, \\[ \\begin{align} \\alpha=-\\frac{f+n}{f-n},\\\\[10pt] \\beta=-\\frac{2fn}{f-n}. \\end{align} \\] Com isso obtemos todos os elementos da matriz de projeção perspectiva: \\[ \\begin{align} \\mathbf{M}_{\\textrm{persp}}= \\begin{bmatrix} \\frac{2n}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\ 0 &amp; \\frac{2n}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\ 0 &amp; 0 &amp; -\\frac{f+n}{f-n} &amp; -\\frac{2fn}{f-n} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix}. \\end{align} \\] Na biblioteca GLM, tal matriz pode ser criada com a função glm::frustum definida em glm/gtc/matrix_transform.hpp: glm::mat4 glm::frustum(float left, float right, float bottom, float top, float zNear, float zFar); glm::dmat4 glm::frustum(double left, double right, double bottom, double top, double zNear, double zFar); onde left, right, bottom, top, zNear e zFar correspondem respectivamente aos valores \\(l\\), \\(r\\), \\(b\\), \\(t\\), \\(n\\) e \\(f\\). Se o volume de visão for simétrico, então \\[ r = -l,\\\\ t = -b. \\] Assim como na projeção ortográfica com volume de visão simétrico, os termos da matriz podem ser simplificados como segue: \\[ \\begin{align} r+l&amp;=0,\\\\ r-l&amp;=2r,\\\\ t+b&amp;=0,\\\\ t-b&amp;=2t,\\\\ \\end{align} \\] e a matriz é simplificada para \\[ \\begin{align} \\mathbf{M}_{\\textrm{persp}}= \\begin{bmatrix} \\frac{n}{r} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{n}{t} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -\\frac{f+n}{f-n} &amp; \\frac{-2fn}{f-n} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix}. \\end{align} \\] Uma forma mais intuitiva de criar um volume de visão simétrico para a projeção perspectiva é através dos seguintes parâmetros: Ângulo \\(\\theta\\) de abertura vertical do campo de visão (field of view ou FOV). Razão de aspecto \\(w/h\\) (largura pela altura) do plano de imagem. Distâncias \\(n\\) e \\(f\\) dos planos de recorte próximo (near) e distante (far). Usando relações trigonométricas, podemos determinar o valor de \\(t\\) (top em glm::frustum) (figura 8.16): \\[ \\begin{align} &amp;\\frac{t}{n} = \\tan \\left( \\frac{\\theta}{2} \\right),\\\\[10pt] &amp;t = n\\tan \\left( \\frac{\\theta}{2} \\right). \\end{align} \\] Por simetria, \\[ \\begin{align} b &amp;= -t.\\\\ \\end{align} \\] Figura 8.16: Ângulo de abertura do campo de visão vertical. Para calcular \\(r\\) (right em glm::frustum), multiplicamos \\(t\\) pela razão de aspecto. \\[ r = t \\frac{w}{h}.\\\\ \\] Assim, em um viewport de tamanho \\(1920 \\times 1080\\), a razão de aspecto será \\(16:9\\) (widescreen). Se \\(t=1080/2=540\\), então \\(r=540\\times \\frac{16}{9}=1920/2=960\\). Por simetria, \\[ \\begin{align} l &amp;= -r.\\\\ \\end{align} \\] Na biblioteca GLM, tal matriz pode ser criada com a função glm::perspective, definida em glm/gtc/matrix_transform.hpp: glm::mat4 perspective(float fovy, float aspect, float zNear, float zFar); glm::dmat4 perspective(double fovy, double aspect, double zNear, double zFar); onde fovy, aspect, zNear e zFar correspondem respectivamente aos valores \\(\\theta\\) (em radianos), \\(w/h\\), \\(n\\) e \\(f\\). "],["vtrackball1.html", "8.3 Trackball virtual", " 8.3 Trackball virtual Em aplicações de visualização interativa de objetos 3D, é desejável que exista um controle de interação de rotação que permita rodar o objeto em torno de si mesmo de modo a visualizá-lo de qualquer direção. Podemos implementar tal interação de rotação através de três widgets do tipo slider, cada um correspondendo a um ângulo de rotação em \\(x\\), \\(y\\) e \\(z\\) que pode ser controlado pelo usuário. Do ponto de vista da implementação, podemos usar esses ângulos para rodar o objeto através de uma matriz de modelo composta pelas seguintes matrizes de rotação \\[ \\mathbf{M}_{\\textrm{model}}=\\mathbf{R}_z\\mathbf{R}_y\\mathbf{R}_x. \\] Nesse tipo de rotação, os ângulos em \\(x\\), \\(y\\) e \\(z\\) são chamados de ângulos de Euler. Dada qualquer orientação no espaço, existe uma sequência específica de ângulos de Euler que levam o objeto para a orientação desejada. As rotações não precisam ser na ordem \\(xyz\\). Qualquer permutação dos eixos pode ser utilizada, mas cada ordem produz ângulos de Euler diferentes para chegar a uma mesma orientação. Embora seja certo que existam ângulos de Euler para chegar a qualquer orientação desejada, o usuário provavelmente terá dificuldades em determinar os valores dos sliders necessários para levar o objeto à orientação-alvo. Existe uma forma mais intuitiva de rodar um objeto 3D em torno de si mesmo. Se tivermos um dispositivo de entrada do tipo trackball, a rotação do objeto pode corresponder diretamente à rotação do dispositivo. A figura 8.17 mostra um trackball típico, com eixos principais sobrepostos na foto. Figura 8.17: Eixos principais sobre um trackball (adaptado do original). Se o usuário rodar o trackball para frente ou para trás, na direção de \\(\\pm y\\), obteremos uma rotação em torno de \\(x\\). Se rodar para os lados, na direção de \\(\\pm x\\), obteremos uma rotação em torno de \\(y\\). Enfim, qualquer rotação do trackball equivale a uma rotação em torno de um eixo (não necessariamente os eixos principais). Podemos simular a rotação livre de um trackball através de um mouse comum, usando a técnica de Virtual Sphere (Chen, Mountford, and Sellen 1988) ou Arcball (Shoemake 1992) (figura 8.18). Figura 8.18: Trackball virtual (adaptado do original). Para simplificar, vamos supor que \\(\\mathbf{M}_{\\textrm{model}}=\\mathbf{I}\\), e que o objeto 3D está centralizado na origem. Primeiramente, considere uma esfera unitária centralizada na origem, definida pela equação \\[x^2+y^2+z^2=1.\\] O hemisfério \\(z&gt;0\\) pode ser mapeado para o espaço da janela como um círculo inscrito no viewport, como se a câmera estivesse olhando na direção de \\(-z\\) no espaço do mundo30. A figura 8.19 ilustra esse mapeamento. Figura 8.19: Trackball virtual no espaço da janela e espaço do mundo. Se o usuário clicar em alguma posição na janela, as coordenadas \\((x_w,y_w)\\) do cursor podem ser convertidas em coordenadas de um ponto \\((x,y,0)\\) no plano \\(z=0\\). Se \\((x,y,0)\\) estiver dentro da esfera unitária, isto é, se \\[\\sqrt{x^2+y^2}\\leq1,\\] a coordenada \\(z\\) da projeção ortogonal do ponto \\((x,y,z)\\) sobre o hemisfério \\(z&gt;0\\) pode ser calculada usando a equação da esfera: \\[ \\begin{align} &amp;x^2+y^2+z^2=1,\\\\ \\\\ &amp;z=\\sqrt{1-x^2-y^2}. \\end{align} \\] Note que, se \\(P=(x,y,z)\\), o vetor \\(\\mathbf{v}=P-O\\) é um vetor unitário. Se o usuário manter o botão do mouse pressionado em uma posição \\(P_1\\) da janela e então arrastar o mouse para outra posição \\(P_2\\), podemos simular o movimento de girar o trackball virtual na direção de \\(P_1\\) a \\(P_2\\) (figura 8.20). Figura 8.20: Produzindo um eixo de rotação através de dois pontos sobre o trackball. Após a projeção de \\(P_1\\) e \\(P_2\\) sobre o hemisfério, obtemos vetores unitários \\(\\mathbf{v}_1\\) e \\(\\mathbf{v}_2\\). O vetor normal ao plano formado entre os dois vetores, \\[ \\mathbf{n}=\\mathbf{v}_1 \\times \\mathbf{v}_2, \\] é o eixo de rotação correspondente. Uma vez que \\(\\mathbf{v}_1\\) e \\(\\mathbf{v}_2\\) são vetores unitários, o ângulo de rotação pode ser calculado a partir da propriedade do produto vetorial: \\[ |\\sin \\theta| = |\\mathbf{v}_1 \\times \\mathbf{v}_2|=|\\mathbf{n}|. \\] Assim, a movimentação do ponto \\(P_1\\) a \\(P_2\\) no espaço da janela representa uma rotação do objeto por um ângulo \\(\\theta\\) em torno de \\(\\mathbf{n}\\). Na prática, os pontos \\(P_1\\) e \\(P_2\\) têm um ângulo muito próximo de zero, pois os pontos equivalem a eventos consecutivos de movimentação do mouse. O ponto \\(P_2\\) é a posição atual do mouse, e \\(P_1\\) é a posição do mouse no último evento de movimentação, ou último evento de pressionamento do botão do mouse. Para ângulos próximos de zero, \\(\\sin \\theta \\approx \\theta\\), e assim não precisamos calcular a função arco seno em \\[\\theta= \\sin^{-1}(|\\mathbf{n}|).\\] Observação Se um ponto \\(P=(x,y,0)\\) estiver fora da esfera unitária, isto é, se \\[\\sqrt{x^2+y^2}&gt;1,\\] podemos considerar como vetor resultante o vetor \\(\\mathbf{v}=P-O\\) normalizado. O resultado é um vetor unitário sobre o plano \\(z=0\\). Na biblioteca GLM, a matriz de rotação por um ângulo \\(\\theta\\) em torno de um eixo arbitrário \\(\\mathbf{n}\\) pode ser obtida com a função glm::rotate, definida em glm/gtc/matrix_transform.hpp: glm::mat4 glm::rotate(glm::mat4 const&amp; m, float angle, glm::vec3 const&amp; axis); glm::dmat4 glm::rotate(glm::dmat4 const&amp; m, double angle, glm::dvec3 const&amp; axis); onde m é a matriz que será multiplicada à esquerda pela matriz de rotação (pode ser a matriz identidade glm::mat4(1.0f) ou glm::mat4()); angle é o ângulo de rotação, em radianos; axis é o eixo de rotação. A seguir, veremos como tal matriz é construída. Observação Na maioria das implementações do trackball virtual, o usuário também pode controlar a escala do objeto através do botão de rolagem (scroll wheel) do mouse. Faremos isso em nossa implementação na seção 8.4. Rotação em torno de um eixo arbitrário Vimos na seção 7.4 como calcular as matrizes de rotação \\(\\mathbf{R}_x\\), \\(\\mathbf{R}_y\\) e \\(\\mathbf{R}_z\\) em torno dos eixos principais \\(x\\), \\(y\\) e \\(z\\). Através da concatenação dessas rotações podemos obter a matriz de rotação em torno de um eixo arbitrário. Suponha que \\(\\hat{\\mathbf{u}}=(u_x, u_y, u_z)\\) é um vetor unitário que define a direção do eixo de rotação por um ângulo \\(\\theta\\). Como o vetor é unitário, \\[ u_x^2+u_y^2+u_z^2=1. \\] A matriz de rotação em torno de \\(\\hat{\\mathbf{u}}\\) pode ser obtida através da seguinte composição de rotações: \\[ \\mathbf{R}=\\mathbf{R}_x(-\\theta_x)\\mathbf{R}_y(-\\theta_y)\\mathbf{R}_z(\\theta)\\mathbf{R}_y(\\theta_y)\\mathbf{R}_x(\\theta_x). \\] Lendo da direita para a esquerda, primeiro aplicamos uma rotação em \\(x\\) (por um ângulo \\(\\theta_x\\)), seguida de uma rotação em \\(y\\) (por um ângulo \\(\\theta_y\\)). Suponha que esses ângulos são tais que alinham o vetor \\(\\hat{\\mathbf{u}}\\) com o eixo \\(z\\). Então, após \\(\\mathbf{R}_y(\\theta_y)\\mathbf{R}_x(\\theta_x)\\), o vetor \\(\\hat{\\mathbf{u}}\\) aponta na direção do eixo \\(z\\). Agora que \\(\\hat{\\mathbf{u}}\\) está alinhado com \\(z\\), a próxima rotação, \\(\\mathbf{R}_z(\\theta)\\), equivale a uma rotação em torno de \\(\\hat{\\mathbf{u}}\\), que é o que desejamos. As transformações restantes desfazem as duas primeiras, em ordem, fazendo com que \\(\\hat{\\mathbf{u}}\\) retorne para sua direção original. Em resumo, a concatenação de transformações pode ser dividida nas seguintes etapas: Alinhamento de \\(\\hat{\\mathbf{u}}\\) com o eixo \\(z\\). Rotação em torno de \\(\\hat{\\mathbf{u}}\\) alinhado com \\(z\\). Retorno de \\(\\hat{\\mathbf{u}}\\) para sua direção original. O resultado final será uma rotação em torno do eixo arbitrário \\(\\hat{\\mathbf{u}}\\). Para determinar \\(\\theta_x\\), observe as relações trigonométricas na figura 8.21. Figura 8.21: Determinando o ângulo de rotação em torno do eixo x. O vetor \\(\\hat{\\mathbf{u}}\\) é composto de componentes \\((u_x,u_y,u_z)\\). Logo, \\(u_z\\) e \\(u_y\\) formam os catetos de um triângulo retângulo. A hipotenusa é \\[ d=\\sqrt{u_y^2+u_z^2} \\] e corresponde ao comprimento da projeção ortogonal de \\(\\hat{\\mathbf{u}}\\) sobre o plano \\(x=0\\). A rotação de \\(\\hat{\\mathbf{u}}\\) pelo ângulo \\(\\theta_x\\) faz com que \\(\\hat{\\mathbf{u}}\\) incida sobre o plano \\(y=0\\). Não é necessário calcular \\(\\theta_x\\) explicitamente. A matriz \\(\\mathbf{R}_x(\\theta_x)\\) usa apenas senos e cossenos de \\(\\theta_x\\). Logo, podemos construir a matriz de rotação apenas observando as relações entre os lados do triângulo retângulo: \\[ \\mathbf{R}_x(\\theta_x)= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; u_z/d &amp; -u_y/d &amp; 0\\\\ 0 &amp; u_y/d &amp; u_z/d &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Após a rotação por \\(\\theta_x\\), podemos determinar \\(\\theta_y\\). Observe a figura 8.22. Figura 8.22: Determinando o ângulo de rotação em torno do eixo y. Novamente, temos um triângulo retângulo com catetos \\(d\\) e \\(u_x\\). A hipotenusa é \\(1\\) pois \\(\\hat{\\mathbf{u}}\\) é unitário. A rotação por \\(\\theta_y\\) faz com que o vetor projetado no plano \\(y=0\\) fique alinhado ao eixo \\(z\\), que é o que desejamos. Também não precisamos calcular \\(\\theta_y\\) explicitamente, pois os elementos da matriz são compostos apenas de senos e cossenos do ângulo. A matriz de rotação em torno de \\(y\\) terá o formato \\[ \\mathbf{R}_y(\\theta_y)= \\begin{bmatrix} d &amp; 0 &amp; -u_x &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0\\\\ u_x &amp; 0 &amp; d &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] Uma vez que já sabemos como calcular \\(\\mathbf{R}_z(\\theta)\\), podemos multiplicar todas as matrizes para obter a rotação \\(\\mathbf{R}\\) final. Referências "],["viewer1.html", "8.4 Visualizador 3D", " 8.4 Visualizador 3D Nesta seção, seguiremos o passo a passo de implementação de um visualizador de modelos geométricos 3D que permite a interação através do trackball virtual. Esta será apenas a primeira de uma série de versões do visualizador 3D. Nos próximos capítulos, faremos aprimoramentos em relação aos shaders e modelos suportados. Por enquanto, nossa primeira versão do visualizador usa um único objeto pré-carregado que, para variar, é o Stanford Bunny. Além disso, só é utilizado um par de shaders (vertex/fragment shader), que é o mesmo já utilizado no projeto lookat (seção 7.7). O resultado ficará como a seguir. Use o mouse para interagir com o objeto. Clique e arraste para rodá-lo. Use o botão de rolagem para aproximar ou distanciar a câmera do objeto. Se a câmera estiver muito próxima, o objeto será recortado pelo plano de recorte próximo (near clipping plane) e será possível ver o interior do objeto. Configuração inicial No arquivo abcg/examples/CMakeLists.txt, inclua a linha: add_subdirectory(viewer1) Crie o subdiretório abcg/examples/viewer1 e o arquivo abcg/examples/viewer1/CMakeLists.txt com o seguinte conteúdo: project(viewer1) add_executable(${PROJECT_NAME} main.cpp model.cpp openglwindow.cpp trackball.cpp) enable_abcg(${PROJECT_NAME}) Crie os seguintes arquivos vazios: main.cpp; model.cpp e model.hpp; openglwindow.cpp e openglwindow.hpp; trackball.cpp e trackball.hpp. Crie o subdiretório abcg/examples/viewer1/assets. Dentro dele, crie os arquivos vazios depth.frag e depth.vert. Além disso, baixe o arquivo bunny.zip e descompacte-o em assets. A estrutura de abcg/examples/viewer1 ficará assim: viewer1/  CMakeLists.txt  main.cpp  model.hpp  model.cpp  openglwindow.hpp  openglwindow.cpp  trackball.hpp  trackball.cpp  assets/  bunny.obj  depth.frag  depth.vert main.cpp O conteúdo é o mesmo do projeto anterior. Só vamos mudar o título da janela: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings({.samples = 4}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Model Viewer (version 1)&quot;}); app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } depth.vert Este também é praticamente o mesmo vertex shader utilizado no projeto lookat: #version 410 layout(location = 0) in vec3 inPosition; uniform vec4 color; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; out vec4 fragColor; void main() { vec4 posEyeSpace = viewMatrix * modelMatrix * vec4(inPosition, 1); float i = 1.0 - (-posEyeSpace.z / 3.0); fragColor = vec4(i, i, i, 1) * color; gl_Position = projMatrix * posEyeSpace; } A única diferença está na linha 15. No cálculo da intensidade i, dividimos posEyeSpace.z por 3.0 e não por 5.0. Lembre-se que o shader faz com que a cor em cada vértice (fragColor) tenha uma intensidade inversamente proporcional à distância do vértice ao longo de \\(z\\) negativo no espaço da câmera. Quanto mais longe o vértice, menor será sua intensidade. Neste caso, a intensidade será zero quando \\(z\\leq-3\\). depth.frag O conteúdo do fragment shader ficará assim: #version 410 in vec4 fragColor; out vec4 outColor; void main() { if (gl_FrontFacing) { outColor = fragColor; } else { outColor = vec4(fragColor.r * 0.5, 0, 0, fragColor.a); } } Se o triângulo estiver orientado de frente para a câmera, a cor final do fragmento será a cor de entrada (fragColor). Caso contrário, a cor será vermelha. model.hpp Neste arquivo definiremos a classe Model, responsável por gerenciar o VBO, EBO e VAO do modelo geométrico lido do arquivo OBJ: #ifndef MODEL_HPP_ #define MODEL_HPP_ #include &lt;vector&gt; #include &quot;abcg.hpp&quot; struct Vertex { glm::vec3 position{}; bool operator==(const Vertex&amp; other) const noexcept { return position == other.position; } }; class Model { public: void loadObj(std::string_view path, bool standardize = true); void render(int numTriangles = -1) const; void setupVAO(GLuint program); void terminateGL(); [[nodiscard]] int getNumTriangles() const { return static_cast&lt;int&gt;(m_indices.size()) / 3; } private: GLuint m_VAO{}; GLuint m_VBO{}; GLuint m_EBO{}; std::vector&lt;Vertex&gt; m_vertices; std::vector&lt;GLuint&gt; m_indices; void createBuffers(); void standardize(); }; #endif Nas linhas 8 a 14 está definida a estrutura Vertex que temos utilizado para descrever os atributos de um vértice. Como nos projetos anteriores, cada vértice só possui um atributo, que é a posição 3D. A classe contém as seguintes funções membro: void loadObj(std::string_view path, bool standardize = true). O conteúdo desta função é o mesmo da função loadModelFromFile utilizada nos projetos anteriores para carregar um arquivo OBJ. Dessa vez, incluímos um parâmetro booleano standardize que indica se o modelo deve ter o tamanho normalizado e centralizado na origem após o carregamento. O comportamento padrão é normalizar o objeto. void render(int numTriangles = -1) const. Esta é a função que deve ser chamada em OpenGLWindow::paintGL para renderizar o objeto. A função aceita um parâmetro numTriangles para indicar quantos triângulos devem ser renderizados. O padrão é -1 e significa que todos os triângulos devem ser processados. void setupVAO(GLuint program). Esta função deve ser chamada para configurar o VAO do modelo de acordo com o programa de shader atualmente utilizado. O identificador do programa de shader deve ser passado no parâmetro program. void terminateGL(). Esta função deve ser chamada em OpenGLWindow::terminateGL para liberar os recursos do OpenGL gerenciados pela classe. int getNumTriangles() const. Esta função retorna o número de triângulos do modelo. Como usamos GL_TRIANGLES com geometria indexada, esse número é o número de índices dividido por 3. Observe que a classe não contém a matriz de modelo. A matriz de modelo será mantida em OpenGLWindow. Neste visualizador, a classe Model representa apenas o VBO, EBO e VAO do objeto. Vimos no projeto lookat que uma cena 3D pode ter diferentes instâncias de um mesmo objeto, e que cada instância precisa ter sua própria matriz de modelo. Então, é uma boa decisão de projeto manter os dados geométricos originais em uma classe, e manter os dados da instância (matriz de modelo) em outra classe. Como nosso visualizador só mostra uma instância do objeto, a escolha de deixar Model sem a matriz de modelo não vai fazer muita diferença. Entretanto, essa classe pode ser reutilizada em outros projetos para compor cenas mais complexas (faremos isso no projeto starfield!). Nesse caso, é recomendável criar uma outra classe ou estrutura só para manter a matriz de modelo e outros dados específicos de cada instância. Se cada instância usar um shader diferente, é recomendável também desacoplar o VAO e deixar apenas o VBO/EBO em Model. model.cpp A definição das funções membro de Model ficará como a seguir: #include &quot;model.hpp&quot; #include &lt;fmt/core.h&gt; #include &lt;tiny_obj_loader.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/hash.hpp&gt; #include &lt;unordered_map&gt; // Explicit specialization of std::hash for Vertex namespace std { template &lt;&gt; struct hash&lt;Vertex&gt; { size_t operator()(Vertex const&amp; vertex) const noexcept { const std::size_t h1{std::hash&lt;glm::vec3&gt;()(vertex.position)}; return h1; } }; } // namespace std void Model::createBuffers() { // Delete previous buffers abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteBuffers(1, &amp;m_VBO); // VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_vertices[0]) * m_vertices.size(), m_vertices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // EBO abcg::glGenBuffers(1, &amp;m_EBO); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(m_indices[0]) * m_indices.size(), m_indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); } void Model::loadObj(std::string_view path, bool standardize) { tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data())) { if (!reader.Error().empty()) { throw abcg::Exception{abcg::Exception::Runtime( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error()))}; } throw abcg::Exception{ abcg::Exception::Runtime(fmt::format(&quot;Failed to load model {}&quot;, path))}; } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } const auto&amp; attrib{reader.GetAttrib()}; const auto&amp; shapes{reader.GetShapes()}; m_vertices.clear(); m_indices.clear(); // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (const auto&amp; shape : shapes) { // Loop over indices for (const auto offset : iter::range(shape.mesh.indices.size())) { // Access to vertex const tinyobj::index_t index{shape.mesh.indices.at(offset)}; // Vertex position const int startIndex{3 * index.vertex_index}; const float vx{attrib.vertices.at(startIndex + 0)}; const float vy{attrib.vertices.at(startIndex + 1)}; const float vz{attrib.vertices.at(startIndex + 2)}; Vertex vertex{}; vertex.position = {vx, vy, vz}; // If hash doesn&#39;t contain this vertex if (hash.count(vertex) == 0) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } if (standardize) { this-&gt;standardize(); } createBuffers(); } void Model::render(int numTriangles) const { abcg::glBindVertexArray(m_VAO); const auto numIndices{(numTriangles &lt; 0) ? m_indices.size() : numTriangles * 3}; abcg::glDrawElements(GL_TRIANGLES, static_cast&lt;GLsizei&gt;(numIndices), GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); } void Model::setupVAO(GLuint program) { // Release previous VAO abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); abcg::glBindVertexArray(m_VAO); // Bind EBO and VBO abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); // Bind vertex attributes const GLint positionAttribute{ abcg::glGetAttribLocation(program, &quot;inPosition&quot;)}; if (positionAttribute &gt;= 0) { abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr); } // End of binding abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindVertexArray(0); } void Model::standardize() { // Center to origin and normalize largest bound to [-1, 1] // Get bounds glm::vec3 max(std::numeric_limits&lt;float&gt;::lowest()); glm::vec3 min(std::numeric_limits&lt;float&gt;::max()); for (const auto&amp; vertex : m_vertices) { max.x = std::max(max.x, vertex.position.x); max.y = std::max(max.y, vertex.position.y); max.z = std::max(max.z, vertex.position.z); min.x = std::min(min.x, vertex.position.x); min.y = std::min(min.y, vertex.position.y); min.z = std::min(min.z, vertex.position.z); } // Center and scale const auto center{(min + max) / 2.0f}; const auto scaling{2.0f / glm::length(max - min)}; for (auto&amp; vertex : m_vertices) { vertex.position = (vertex.position - center) * scaling; } } void Model::terminateGL() { abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Não há nada de realmente novo na definição das funções de Model. O código foi quase todo reaproveitado dos projetos loadmodel e lookat. trackball.hpp Essa é a classe que implementa o trackball virtual: #ifndef TRACKBALL_HPP_ #define TRACKBALL_HPP_ #include &quot;abcg.hpp&quot; class TrackBall { public: void mouseMove(const glm::ivec2&amp; mousePosition); void mousePress(const glm::ivec2&amp; mousePosition); void mouseRelease(const glm::ivec2&amp; mousePosition); void resizeViewport(int width, int height); [[nodiscard]] glm::mat4 getRotation(); private: const float m_maxVelocity{glm::radians(720.0f / 1000.0f)}; glm::vec3 m_axis{1.0f}; float m_velocity{}; glm::mat4 m_rotation{1.0f}; glm::vec3 m_lastPosition{}; abcg::ElapsedTimer m_lastTime{}; bool m_mouseTracking{}; float m_viewportWidth{}; float m_viewportHeight{}; [[nodiscard]] glm::vec3 project(const glm::vec2&amp; mousePosition) const; }; #endif A classe contém as seguintes funções membro: void mouseMove(const glm::ivec2&amp; mousePosition). void mousePress(const glm::ivec2&amp; mousePosition). void mouseRelease(const glm::ivec2&amp; mousePosition). Essas são as funções que devem ser chamadas em OpenGLWindow::handleEvent sempre que ocorrer um evento de movimentação do mouse, pressionamento ou liberação do botão (usaremos o botão esquerdo). A posição do mouse em coordenadas do espaço da janela deve ser passada como parâmetro. void resizeViewport(int width, int height). Esta função deve ser chamada sempre o tamanho do viewport for modificado. O tamanho do viewport é necessário para que possamos fazer a conversão das coordenadas de um ponto no espaço da janela para coordenadas no intervalo \\([-1,1]\\) e assim fazer a projeção sobre o trackball virtual. glm::mat4 getRotation(). Esta é a função que retorna a atual matriz de rotação do trackball. Podemos utilizar a matriz diretamente como matriz de modelo do objeto que está sendo manipulado. glm::vec3 project(const glm::vec2&amp; mousePosition) const. Esta função recebe uma posição do mouse no espaço da janela e retorna a posição 3D correspondente sobre o trackball. É utilizada internamente para atualizar a posição do cursor sobre o hemisfério sempre que o mouse se mover (TrackBall::mouseMove) ou quando um botão for pressionado (TrackBall::mousePress). As variáveis membro da classe são as seguintes: glm::vec3 m_axis: atual eixo de rotação. glm::mat4 m_rotation: atual matriz de rotação. glm::vec3 m_lastPosition: corresponde à posição projetada do ponto \\(P_1\\) visto na seção 8.3. Essa posição é utilizada com a posição \\(P_2\\) do evento mais recente do mouse de modo a calcular os dois vetores necessários para gerar o vetor m_axis. abcg::ElapsedTimer m_lastTime: é um temporizador que mede o tempo entre \\(P_1\\) e \\(P_2\\), isto é, o tempo entre os últimos dois eventos do mouse. float m_velocity: velocidade de rotação, em radianos por segundo. É o ângulo de rotação, mas multiplicado por m_lastTime. Sempre que o usuário soltar o botão do mouse, o objeto continuará sendo rodado por m_velocity, simulando um objeto sem inércia rotacional. A velocidade será zero somente se o usuário soltar o botão com o mouse parado, pois assim \\(P_1=P_2\\) e o ângulo de rotação será zero. Caso contrário, a velocidade será proporcional à velocidade de arrasto no momento da liberação do botão. bool m_mouseTracking: é true se o usuário está segurando o botão do mouse, e false caso contrário. float m_viewportWidth e float m_viewportHeight são as dimensões do viewport informadas em TrackBall::resizeViewport. trackball.cpp A definição das funções membro de TrackBall ficará como a seguir: #include &quot;trackball.hpp&quot; #include &lt;glm/gtc/epsilon.hpp&gt; #include &lt;limits&gt; const auto epsilon{std::numeric_limits&lt;float&gt;::epsilon()}; void TrackBall::mouseMove(const glm::ivec2 &amp;position) { if (!m_mouseTracking) return; const auto msecs{static_cast&lt;float&gt;(m_lastTime.restart()) * 1000.0f}; // Return if mouse cursor hasn&#39;t moved wrt last position const auto currentPosition{project(position)}; if (glm::all(glm::epsilonEqual(m_lastPosition, currentPosition, epsilon))) return; // Rotation axis m_axis = glm::cross(m_lastPosition, currentPosition); // Rotation angle const auto angle{glm::length(m_axis)}; m_axis = glm::normalize(m_axis); // Compute an angle velocity that will be used as a constant rotation angle // when the mouse is not being tracked. m_velocity = angle / (msecs + epsilon); m_velocity = glm::clamp(m_velocity, 0.0f, m_maxVelocity); // Concatenate the rotation: R_old = R_new * R_old m_rotation = glm::rotate(glm::mat4(1.0f), angle, m_axis) * m_rotation; m_lastPosition = currentPosition; } void TrackBall::mousePress(const glm::ivec2 &amp;position) { m_rotation = getRotation(); m_mouseTracking = true; m_lastTime.restart(); m_lastPosition = project(position); m_velocity = 0.0f; } void TrackBall::mouseRelease(const glm::ivec2 &amp;position) { mouseMove(position); m_mouseTracking = false; } void TrackBall::resizeViewport(int width, int height) { m_viewportWidth = static_cast&lt;float&gt;(width); m_viewportHeight = static_cast&lt;float&gt;(height); } glm::mat4 TrackBall::getRotation() { if (m_mouseTracking) return m_rotation; // If not tracking, rotate by velocity. This will simulate // an inertia-free rotation. const auto angle{m_velocity * static_cast&lt;float&gt;(m_lastTime.elapsed()) * 1000.0f}; return glm::rotate(glm::mat4(1.0f), angle, m_axis) * m_rotation; } glm::vec3 TrackBall::project(const glm::vec2 &amp;position) const { // Convert from window coordinates to NDC auto v{glm::vec3(2.0f * position.x / m_viewportWidth - 1.0f, 1.0f - 2.0f * position.y / m_viewportHeight, 0.0f)}; // Project to centered unit hemisphere const auto squaredLength{glm::length2(v)}; if (squaredLength &gt;= 1.0f) { // Outside sphere v = glm::normalize(v); } else { // Inside sphere v.z = std::sqrt(1.0f - squaredLength); } return v; } A implementação segue a abordagem descrita na seção 8.3. É interessante observar como é atualizada a matriz de rotação durante o arrasto do mouse, neste trecho de TrackBall::mouseMove: // Concatenate the rotation: R_old = R_new * R_old m_rotation = glm::rotate(glm::mat4(1.0f), angle, m_axis) * m_rotation; A cada evento de movimentação do mouse, a matriz de rotação (m_rotation) torna-se uma composição da rotação mais recente (glm::rotate) com as rotações anteriores (m_rotation). Assim, m_rotation é uma concatenação \\[ \\mathbf{R}=\\mathbf{R}_k\\dots\\mathbf{R}_3\\mathbf{R}_2\\mathbf{R}_1, \\] onde \\(\\mathbf{R}_1\\) é a matriz que representa a rotação em torno do eixo gerado a partir do ponto \\(P_1\\), quando o usuário pressionou o botão do mouse pela primeira vez, e o ponto \\(P_2\\), do primeiro evento de movimentação do mouse. A matriz \\(\\mathbf{R}_2\\) representa a rotação em torno do eixo gerado a partir do ponto \\(P_2\\) e o ponto \\(P_3\\) do segundo evento de movimentação do mouse. Isso é repetido continuamente, até \\(\\mathbf{R}_k\\), que representa a rotação em torno do eixo gerado pelas duas últimas posições do mouse. Quando o botão do mouse é liberado, m_rotation continua sendo concatenada consigo mesma na forma \\[ \\mathbf{R}=\\mathbf{R}_n\\mathbf{R}, \\] onde \\(\\mathbf{R}_n\\) é a rotação em torno do eixo gerado pelas duas últimas posições do mouse enquanto o botão ainda estava sendo pressionado. openglwindow.hpp A definição da classe OpenGLWindow ficará assim: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &quot;abcg.hpp&quot; #include &quot;model.hpp&quot; #include &quot;trackball.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void handleEvent(SDL_Event&amp; ev) override; void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; Model m_model; int m_trianglesToDraw{}; TrackBall m_trackBall; float m_zoom{}; glm::mat4 m_modelMatrix{1.0f}; glm::mat4 m_viewMatrix{1.0f}; glm::mat4 m_projMatrix{1.0f}; void update(); }; #endif Veja que há uma instância da classe Model (linha 23) e TrackBall (linha 26). Também temos uma variável m_zoom para controlar o tamanho do objeto quando o usuário rolar o botão de rolagem do mouse. Nas linhas 29 a 31 temos as matrizes de modelo (m_modelMatrix), visão (m_viewMatrix) e projeção (m_projMatrix). openglwindow.cpp No início de openglwindow.cpp definimos OpenGLWindow::handleEvent: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;cppitertools/itertools.hpp&gt; void OpenGLWindow::handleEvent(SDL_Event&amp; event) { glm::ivec2 mousePosition; SDL_GetMouseState(&amp;mousePosition.x, &amp;mousePosition.y); if (event.type == SDL_MOUSEMOTION) { m_trackBall.mouseMove(mousePosition); } if (event.type == SDL_MOUSEBUTTONDOWN &amp;&amp; event.button.button == SDL_BUTTON_LEFT) { m_trackBall.mousePress(mousePosition); } if (event.type == SDL_MOUSEBUTTONUP &amp;&amp; event.button.button == SDL_BUTTON_LEFT) { m_trackBall.mouseRelease(mousePosition); } if (event.type == SDL_MOUSEWHEEL) { m_zoom += (event.wheel.y &gt; 0 ? 1.0f : -1.0f) / 5.0f; m_zoom = glm::clamp(m_zoom, -1.5f, 1.0f); } } Veja que as funções de TrackBall são chamadas de acordo com os eventos do mouse, e a variável m_zoom é modificada de acordo com o botão de rolagem. m_zoom é um valor de translação que é utilizado para posicionar a câmera LookAt ao longo do eixo \\(z\\) do espaço do mundo. Na posição inicial, a câmera está em \\(P_{\\textrm{eye}}=(0,0,2)\\), olhando para \\(P_{\\textrm{at}}=(0,0,0)\\). m_zoom é apenas um valor somado à coordenada \\(z\\) de \\(P_{\\textrm{eye}}\\), aproximando ou distanciando a câmera da origem. Vamos agora à definição de OpenGLWindow::initializeOpenGL: void OpenGLWindow::initializeGL() { abcg::glClearColor(0, 0, 0, 1); // Enable depth buffering abcg::glEnable(GL_DEPTH_TEST); // Create program m_program = createProgramFromFile(getAssetsPath() + &quot;depth.vert&quot;, getAssetsPath() + &quot;depth.frag&quot;); // Load model m_model.loadObj(getAssetsPath() + &quot;bunny.obj&quot;); m_model.setupVAO(m_program); m_trianglesToDraw = m_model.getNumTriangles(); } Todo o trabalho de carregamento do modelo foi transferido para a classe Model. Só precisamos chamar Model::loadObj e chamar Model::setupVAO com o identificador do programa de shader. Vamos à definição de OpenGLWindow::paintGL: void OpenGLWindow::paintGL() { update(); // Clear color buffer and depth buffer abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); // Get location of uniform variables (could be precomputed) const GLint viewMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;viewMatrix&quot;)}; const GLint projMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;projMatrix&quot;)}; const GLint modelMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;modelMatrix&quot;)}; const GLint colorLoc{abcg::glGetUniformLocation(m_program, &quot;color&quot;)}; // Set uniform variables used by every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); // Set uniform variables of the current object abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;m_modelMatrix[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 1.0f, 1.0f, 1.0f); // White m_model.render(m_trianglesToDraw); abcg::glUseProgram(0); } O código é semelhante ao utilizado no projeto anterior, mas agora está mais simples pois a chamada a glDrawElements é feita em Model::render. Em OpenGLWindow::paintUI definimos os controles de interface da ImGui: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); // Create window for slider { ImGui::SetNextWindowPos(ImVec2(5, m_viewportHeight - 94)); ImGui::SetNextWindowSize(ImVec2(m_viewportWidth - 10, -1)); ImGui::Begin(&quot;Slider window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); // Create a slider to control the number of rendered triangles { // Slider will fill the space of the window ImGui::PushItemWidth(m_viewportWidth - 25); ImGui::SliderInt(&quot;&quot;, &amp;m_trianglesToDraw, 0, m_model.getNumTriangles(), &quot;%d triangles&quot;); ImGui::PopItemWidth(); } ImGui::End(); } // Create a window for the other widgets { const auto widgetSize{ImVec2(222, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, 5)); ImGui::SetNextWindowSize(widgetSize); ImGui::Begin(&quot;Widget window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); static bool faceCulling{}; ImGui::Checkbox(&quot;Back-face culling&quot;, &amp;faceCulling); if (faceCulling) { abcg::glEnable(GL_CULL_FACE); } else { abcg::glDisable(GL_CULL_FACE); } // CW/CCW combo box { static std::size_t currentIndex{}; const std::vector&lt;std::string&gt; comboItems{&quot;CCW&quot;, &quot;CW&quot;}; ImGui::PushItemWidth(120); if (ImGui::BeginCombo(&quot;Front face&quot;, comboItems.at(currentIndex).c_str())) { for (const auto index : iter::range(comboItems.size())) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); if (currentIndex == 0) { abcg::glFrontFace(GL_CCW); } else { abcg::glFrontFace(GL_CW); } } // Projection combo box { static std::size_t currentIndex{}; std::vector&lt;std::string&gt; comboItems{&quot;Perspective&quot;, &quot;Orthographic&quot;}; ImGui::PushItemWidth(120); if (ImGui::BeginCombo(&quot;Projection&quot;, comboItems.at(currentIndex).c_str())) { for (auto index : iter::range(comboItems.size())) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); if (currentIndex == 0) { const auto aspect{static_cast&lt;float&gt;(m_viewportWidth) / static_cast&lt;float&gt;(m_viewportHeight)}; m_projMatrix = glm::perspective(glm::radians(45.0f), aspect, 0.1f, 5.0f); } else { m_projMatrix = glm::ortho(-1.0f, 1.0f, -1.0f, 1.0f, 0.1f, 5.0f); } } ImGui::End(); } } Observe, na estrutura condicional das linhas 160 a 167, como a matriz de projeção é criada com glm::perspective ou glm::ortho, dependendo da escolha do usuário. O restante de openglwindow.cpp ficará assim: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; m_trackBall.resizeViewport(width, height); } void OpenGLWindow::terminateGL() { m_model.terminateGL(); abcg::glDeleteProgram(m_program); } void OpenGLWindow::update() { m_modelMatrix = m_trackBall.getRotation(); m_viewMatrix = glm::lookAt(glm::vec3(0.0f, 0.0f, 2.0f + m_zoom), glm::vec3(0.0f, 0.0f, 0.0f), glm::vec3(0.0f, 1.0f, 0.0f)); } Em OpenGLWindow::resizeGL, chamamos TrackBall::resizeViewport (linha 178) para atualizar as dimensões da janela ao trackball. Em OpenGLWindow::updateGL, fazemos com que a matriz de modelo seja a própria matriz de rotação do trackball (linha 187). É também nesta função que calculamos a matriz de visão usando a câmera LookAt. Note como m_zoom altera a posição \\(z\\) da câmera. Isso é tudo. Baixe o código completo do projeto a partir deste link. "],["starfield.html", "8.5 Efeito starfield", " 8.5 Efeito starfield Nesta seção aplicaremos a transformação de projeção perspectiva para produzir um interessante efeito de campo estelar (starfield) formado por cubos giratórios. A aplicação permitirá alterar o ângulo de abertura do campo de visão, bem como alternar entre projeção perspectiva e projeção ortográfica. O resultado ficará como a seguir. Veja como o uso de projeção perspectiva é essencial para produzir o efeito desejado (compare com a projeção ortográfica). Configuração inicial No arquivo abcg/examples/CMakeLists.txt, inclua a linha: add_subdirectory(starfield) Crie o subdiretório abcg/examples/starfield e o arquivo abcg/examples/starfield/CMakeLists.txt com o seguinte conteúdo: project(starfield) add_executable(${PROJECT_NAME} main.cpp model.cpp openglwindow.cpp) enable_abcg(${PROJECT_NAME}) Crie os seguintes arquivos vazios: main.cpp; openglwindow.cpp e openglwindow.hpp. trackball.cpp e trackball.hpp. Copie os arquivos model.cpp e model.hpp do projeto da seção anterior (seção 8.4), pois eles serão utilizados sem modificações. Crie o subdiretório abcg/examples/starfield/assets e, dentro dele, crie os arquivos vazios depth.frag e depth.vert. Além disso, copie para este subdiretório o modelo box.obj disponível neste link. A estrutura de abcg/examples/starfield ficará assim: starfield/  CMakeLists.txt  main.cpp  model.hpp  model.cpp  openglwindow.hpp  openglwindow.cpp  assets/  box.obj  depth.frag  depth.vert main.cpp O conteúdo é igual ao do projeto anterior. Só mudamos o título da janela: #include &lt;fmt/core.h&gt; #include &quot;abcg.hpp&quot; #include &quot;openglwindow.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); auto window{std::make_unique&lt;OpenGLWindow&gt;()}; window-&gt;setOpenGLSettings({.samples = 4}); window-&gt;setWindowSettings( {.width = 600, .height = 600, .title = &quot;Starfield Effect&quot;}); app.run(std::move(window)); } catch (const abcg::Exception &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } depth.vert O vertex shader também é igual ao do projeto anterior. Só precisamos alterar -posEyeSpace.z / 3.0 para -posEyeSpace.z / 100.0 para que a intensidade da cor seja zero apenas quando \\(z&lt;-100\\) no espaço da câmera. #version 410 layout(location = 0) in vec3 inPosition; uniform vec4 color; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; out vec4 fragColor; void main() { vec4 posEyeSpace = viewMatrix * modelMatrix * vec4(inPosition, 1); float i = 1.0 - (-posEyeSpace.z / 100.0); fragColor = vec4(i, i, i, 1) * color; gl_Position = projMatrix * posEyeSpace; } depth.frag O conteúdo do fragment shader é mais simples que o depth.frag do projeto anterior. A cor recebida no atributo de entrada (fragColor) é enviada sem modificações para o atributo de saída (outColor): #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } openglwindow.hpp A definição da classe OpenGLWindow ficará assim: #ifndef OPENGLWINDOW_HPP_ #define OPENGLWINDOW_HPP_ #include &lt;random&gt; #include &quot;abcg.hpp&quot; #include &quot;model.hpp&quot; class OpenGLWindow : public abcg::OpenGLWindow { protected: void initializeGL() override; void paintGL() override; void paintUI() override; void resizeGL(int width, int height) override; void terminateGL() override; private: static const int m_numStars{500}; GLuint m_program{}; int m_viewportWidth{}; int m_viewportHeight{}; std::default_random_engine m_randomEngine; Model m_model; std::array&lt;glm::vec3, m_numStars&gt; m_starPositions; std::array&lt;glm::vec3, m_numStars&gt; m_starRotations; float m_angle{}; glm::mat4 m_viewMatrix{1.0f}; glm::mat4 m_projMatrix{1.0f}; float m_FOV{30.0f}; void randomizeStar(glm::vec3 &amp;position, glm::vec3 &amp;rotation); void update(); }; #endif O objeto m_model, definido na linha 27, é utilizado para armazenar o VBO/EBO e VAO que representa a estrela do campo estrelado. No nosso caso, a estrela será um cubo centralizado na origem, definido pelo arquivo box.obj. A cena 3D será formada por 500 instâncias do cubo. Esse número é definido pela constante m_numStars na linha 18. Nas linhas 29 e 30 são definidos arranjos de 500 posições aleatórias (m_starPositions) e 500 eixos aleatórios de rotação (m_starRotations), um para cada cubo da cena. Através desses atributos podemos criar uma matriz de modelo para cada cubo. Cada matriz de modelo será definida como uma concatenação \\[ \\mathbf{M}_{\\textrm{model}}=\\mathbf{T}\\mathbf{S}\\mathbf{R}, \\] onde \\(\\mathbf{R}\\) é uma rotação que faz o cubo centralizado na origem girar em torno de um eixo especificado em m_starRotations, \\(\\mathbf{S}\\) é um fator de escala uniforme (usaremos \\(s=0.2\\) para todos os cubos) e \\(\\mathbf{T}\\) é a translação que faz com que o cubo rotacionado seja posicionado no espaço do mundo na posição indicada em m_starPositions. Na linha 31, m_angle é um ângulo de rotação em radianos que será utilizado para rodar cada cubo por seu respectivo eixo de rotação (os cubos rodam pelo mesmo ângulo, mas cada um tem seu próprio eixo aleatório de rotação). Nas linhas 33 e 34 são definidas as matrizes de visão (m_viewMatrix) e de projeção (m_projMatrix). Aqui elas estão como matrizes identidade, mas serão definidas posteriormente em OpenGLWindow::initializeGL e OpenGLWindow::paintUI. Na linha 35, o atributo m_FOV é o ângulo de abertura vertical do campo de visão da projeção perspectiva. Esse valor poderá ser controlado por um slider da ImGui, variando de \\(5^\\circ\\) a \\(179^\\circ\\). A função OpenGLWindow::randomizeStar (linha 37) usa o gerador de números pseudoaleatórios m_randomEngine (linha 25) para sortear uma posição e eixo de rotação. O resultado é armazenado nos parâmetros position e rotation passados por referência. openglwindow.cpp O arquivo openglwindow.cpp começa com a definição de OpenGLWindow::initializeGL: #include &quot;openglwindow.hpp&quot; #include &lt;imgui.h&gt; #include &lt;cppitertools/itertools.hpp&gt; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; void OpenGLWindow::initializeGL() { abcg::glClearColor(0, 0, 0, 1); // Enable depth buffering abcg::glEnable(GL_DEPTH_TEST); // Create program m_program = createProgramFromFile(getAssetsPath() + &quot;depth.vert&quot;, getAssetsPath() + &quot;depth.frag&quot;); // Load model m_model.loadObj(getAssetsPath() + &quot;box.obj&quot;); m_model.setupVAO(m_program); // Camera at (0,0,0) and looking towards the negative z m_viewMatrix = glm::lookAt(glm::vec3(0.0f, 0.0f, 0.0f), glm::vec3(0.0f, 0.0f, -1.0f), glm::vec3(0.0f, 1.0f, 0.0f)); // Setup stars for (const auto index : iter::range(m_numStars)) { auto &amp;position{m_starPositions.at(index)}; auto &amp;rotation{m_starRotations.at(index)}; randomizeStar(position, rotation); } } Nas linhas 24 a 26 é definida a matriz de visão como uma câmera LookAt que está na origem do espaço do mundo e está olhando na direção do eixo \\(z\\) negativo. Durante a execução, a câmera permanecerá fixa nessa posição e orientação. As estrelas (cubos) é que mudarão de posição para produzir o efeito de animação. O laço das linhas 29 a 34 itera sobre cada elemento do arranjo m_starPositions e m_starRotations, e chama OpenGLWindow::randomizeStar para sortear uma posição inicial e eixo de rotação inicial para cada estrela. A definição de OpenGLWindow::randomizeStar ficará como a seguir: void OpenGLWindow::randomizeStar(glm::vec3 &amp;position, glm::vec3 &amp;rotation) { // Get random position // x and y coordinates in the range [-20, 20] // z coordinates in the range [-100, 0] std::uniform_real_distribution&lt;float&gt; distPosXY(-20.0f, 20.0f); std::uniform_real_distribution&lt;float&gt; distPosZ(-100.0f, 0.0f); position = glm::vec3(distPosXY(m_randomEngine), distPosXY(m_randomEngine), distPosZ(m_randomEngine)); // Get random rotation axis std::uniform_real_distribution&lt;float&gt; distRotAxis(-1.0f, 1.0f); rotation = glm::normalize(glm::vec3(distRotAxis(m_randomEngine), distRotAxis(m_randomEngine), distRotAxis(m_randomEngine))); } Para a escolha da posição aleatória \\((x,y,z)\\): As coordenadas \\(x\\) e \\(y\\) são escolhidas do intervalo \\([-20, 20]\\); A coordenada \\(z\\) é escolhida do intervalo \\([-100, 0]\\). Assim, o campo estrelado é a região cuboide que vai de \\((-20,-20,-100)\\) a \\((20,20,0)\\). Lembre-se que a câmera está em \\(z=0\\), olhando para \\(z\\) negativo. Logo, a câmera pode enxergar potencialmente toda essa região, a depender das configurações de projeção. Para a escolha do eixo de rotação aleatório, criamos um vetor com coordenadas aleatórias do intervalo \\([-1,1]\\) em ponto flutuante e então normalizamos o vetor. Vamos agora à definição de OpenGLWindow::paintGL: void OpenGLWindow::paintGL() { update(); // Clear color buffer and depth buffer abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); abcg::glUseProgram(m_program); // Get location of uniform variables (could be precomputed) const GLint viewMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;viewMatrix&quot;)}; const GLint projMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;projMatrix&quot;)}; const GLint modelMatrixLoc{ abcg::glGetUniformLocation(m_program, &quot;modelMatrix&quot;)}; const GLint colorLoc{abcg::glGetUniformLocation(m_program, &quot;color&quot;)}; // Set uniform variables used by every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); abcg::glUniform4f(colorLoc, 1.0f, 1.0f, 1.0f, 1.0f); // White // Render each star for (const auto index : iter::range(m_numStars)) { const auto &amp;position{m_starPositions.at(index)}; const auto &amp;rotation{m_starRotations.at(index)}; // Compute model matrix of the current star glm::mat4 modelMatrix{1.0f}; modelMatrix = glm::translate(modelMatrix, position); modelMatrix = glm::scale(modelMatrix, glm::vec3(0.2f)); modelMatrix = glm::rotate(modelMatrix, m_angle, rotation); // Set uniform variable abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;modelMatrix[0][0]); m_model.render(); } abcg::glUseProgram(0); } Logo no início, a função OpenGLWindow::update é chamada (linha 56) para atualizar a posição e rotação das estrelas de modo a produzir o efeito de animação. Nas linhas 75 a 77 são definidos os valores das variáveis uniformes compartilhadas por todas as estrelas. Em particular, todas as estrelas usam a mesma matriz de visão e projeção. Além disso, todas usam a mesma cor (branca). No laço da linha 80, cada estrela é renderizada. A matriz de modelo é construída nas linhas 84 a 88 usando as informações da estrela em m_starPositions e m_starRotations. A matriz de modelo é copiada para o vertex shader na linha 91. Em seguida, na linha 93, a função Model::render é chamada para renderizar o objeto que representa a estrela, isto é, o cubo. É importante observar que, embora o mesmo cubo seja renderizado 500 vezes, cada iteração utiliza uma matriz de modelo diferente. Podemos fazer isso pois todas as estrelas usam a mesma malha de triângulos (8 vértices e 12 triângulos). O que muda é a posição e orientação. Atributos como posição, orientação e escala da malha podem ser modificados no vertex shader por matrizes de transformação, que é o que estamos fazendo. A definição de OpenGLWindow::paintUI ficará assim: void OpenGLWindow::paintUI() { abcg::OpenGLWindow::paintUI(); { const auto widgetSize{ImVec2(218, 62)}; ImGui::SetNextWindowPos(ImVec2(m_viewportWidth - widgetSize.x - 5, 5)); ImGui::SetNextWindowSize(widgetSize); ImGui::Begin(&quot;Widget window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); { ImGui::PushItemWidth(120); static std::size_t currentIndex{}; const std::vector&lt;std::string&gt; comboItems{&quot;Perspective&quot;, &quot;Orthographic&quot;}; if (ImGui::BeginCombo(&quot;Projection&quot;, comboItems.at(currentIndex).c_str())) { for (const auto index : iter::range(comboItems.size())) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); ImGui::PushItemWidth(170); const auto aspect{static_cast&lt;float&gt;(m_viewportWidth) / static_cast&lt;float&gt;(m_viewportHeight)}; if (currentIndex == 0) { m_projMatrix = glm::perspective(glm::radians(m_FOV), aspect, 0.01f, 100.0f); ImGui::SliderFloat(&quot;FOV&quot;, &amp;m_FOV, 5.0f, 179.0f, &quot;%.0f degrees&quot;); } else { m_projMatrix = glm::ortho(-20.0f * aspect, 20.0f * aspect, -20.0f, 20.0f, 0.01f, 100.0f); } ImGui::PopItemWidth(); } ImGui::End(); } } Esse é o código que define os widgets da ImGui. Assim como fizemos no projeto anterior, a matriz de projeção (m_projMatrix) é definida como projeção perspectiva ou ortográfica de acordo com a seleção do usuário. O restante de openglwindow.cpp ficará assim: void OpenGLWindow::resizeGL(int width, int height) { m_viewportWidth = width; m_viewportHeight = height; } void OpenGLWindow::terminateGL() { m_model.terminateGL(); abcg::glDeleteProgram(m_program); } void OpenGLWindow::update() { // Animate angle by 90 degrees per second const float deltaTime{static_cast&lt;float&gt;(getDeltaTime())}; m_angle = glm::wrapAngle(m_angle + glm::radians(90.0f) * deltaTime); // Update stars for (const auto index : iter::range(m_numStars)) { auto &amp;position{m_starPositions.at(index)}; auto &amp;rotation{m_starRotations.at(index)}; // Z coordinate increases by 10 units per second position.z += deltaTime * 10.0f; // If this star is behind the camera, select a new random position and // orientation, and move it back to -100 if (position.z &gt; 0.1f) { randomizeStar(position, rotation); position.z = -100.0f; // Back to -100 } } } Em OpenGLWindow::update, incrementamos m_angle a uma taxa de \\(90^\\circ\\) por segundo (linha 157). Além disso, iteramos sobre todas as estrelas no laço da linha 160. A coordenada \\(z\\) da posição de cada estrela é incrementada a uma taxa de 10 unidades por segundo (linha 165). Assim, uma estrela que começa em \\(z=-100\\) (distância máxima em relação à câmera) chega até \\(z=0\\) (onde está a câmera) em 10 segundos. A condicional da linha 169 verifica se a estrela já passou para trás da câmera. Verificamos se a coordenada \\(z\\) da posição é maior que \\(0.1\\) pois a estrela é um cubo unitário que foi reduzido em tamanho por um fator de escala \\(0.2\\). Logo, quando o cubo está em \\(z=0\\) (isto é, na posição \\(z\\) da câmera), ainda está com metade da malha triangular (metade do cubo) no espaço \\(z&lt;0\\). Isso significa que o cubo ainda pode ser visto pela câmera, pois a câmera está olhando na direção de \\(z\\) negativo e a distância do plano de recorte próximo foi definida como \\(0.01\\) (veja a chamada a glm::perspective na linha 130). Se \\(z&gt;0.1\\), então com certeza o cubo está totalmente atrás da câmera. Quando isso acontece, OpenGLWindow::randomizeStar é chamada novamente para sortear uma nova posição e eixo de rotação para a estrela. Além disso, a estrela é deslocada para \\(z=-100\\) para começar um novo percurso em direção ao plano \\(z=0\\) da câmera. Isso conclui o projeto starfield! Baixe o código completo usando este link. "],["lighting.html", "9 Iluminação", " 9 Iluminação Em todos os projetos que fizemos até agora, utilizamos estratégias simples para determinar a cor dos pixels, tais como: Usar um atributo de cor para cada vértice e deixar o rasterizador interpolar linearmente essas cores para gerar um degradê, como no projeto coloredtriangles (seção 4.4). Desenhar todos os fragmentos de um objeto com uma mesma cor pré-definida pela aplicação, e enviar essa cor ao shader como uma variável uniforme, como no projeto asteroids (seção 5.2). Calcular uma intensidade de cor com base na coordenada \\(z\\) do vértice ou fragmento. No projeto loadmodel (seção 6.4), fizemos isso no fragment shader usando gl_FragCoord.z no espaço da janela. No projeto lookat (seção 7.6), fizemos isso no vertex shader usando a coordenada \\(z\\) no espaço da câmera. A figura 9.1 mostra alguns desses resultados. Figura 9.1: Uma amostra dos resultados dos projetos anteriores, cada um como uma estratégia diferente para determina as cores dos pixels. A partir deste capítulo, veremos como renderizar cenas iluminadas por uma ou mais fontes de luz. O uso de iluminação (lighting) e sombreamento (shading) de superfícies desempenha um papel importante na percepção da forma e volume da geometria 3D projetada. Compare na figura 9.2 a renderização do Stanford Bunny usando uma cor sólida (esquerda), o shader que utilizamos no projeto lookat (centro), e um dos shaders de sombreamento que abordaremos neste capítulo (direita). Muitos dos detalhes de variação da curvatura da superfície só são percebidos na superfície iluminada. Figura 9.2: Percepção da forma e volume com e sem sombreamento. Esquerda: cor sólida. Centro: intensidade de cor de acordo com profundidade do pixel. Direita: sombreamento de superfície iluminada por uma fonte de luz. A interação da luz com os objetos tridimensionais de uma cena virtual pode ser modelada através de um modelo de iluminação. O modelo de iluminação descreve a relação entre a luz incidente em cada ponto de uma superfície e a luz refletida em uma dada direção. Tal relação depende das propriedades do material da superfície, das fontes de luz existentes na cena, e da luz refletida por outras superfícies. O capítulo está organizado da seguinte forma: Na seção 9.1 é apresentada a forma geral da equação que modela a interação entre a luz que incide sobre um ponto de uma superfície do cenário virtual, e então é refletida em alguma direção, que pode ser a direção até um pixel do plano de imagem. Na seção 9.2 é descrito o modelo de reflexão de Phong, que é um modelo de iluminação que considera apenas a iluminação direta entre uma fonte de luz e o ponto de uma superfície. O modelo é simples o suficiente para ser utilizado em renderização em tempo real. Uma variação do modelo de Phong  o modelo de BlinnPhong  é apresentada na seção 9.3. A seção 9.4 apresenta as diferentes abordagens de avaliação do modelo de iluminação sobre superfícies suaves aproximadas por malhas poligonais: avaliação nas faces da malha poligonal (sombreamento flat), nos vértices (sombreamento de Gouraud) e nos fragmentos (sombreamento de Phong). As seções 9.5 e 9.6 contêm o passo a passo de implementação do cálculo de vetores normais em triângulos e vértices (projeto viewer2), e a implementação do modelo de Phong e Blinn-Phong no vertex shader e fragment shader (projeto viewer3). "],["renderingequation.html", "9.1 Equação de renderização", " 9.1 Equação de renderização Para produzir a imagem de uma cena iluminada por fontes de luz virtuais, precisamos determinar a quantidade de luz que incide sobre cada pixel do plano de imagem. Isso depende da forma como a luz é refletida entre as diferentes superfícies da cena até finalmente atingir o pixel no plano de imagem. Essa interação da luz pode ser descrita matematicamente através de uma equação de renderização (Kajiya 1986). A equação de renderização descreve a intensidade luminosa total que sai de um ponto \\(\\mathbf{x}\\) de uma superfície em uma dada direção \\(\\omega_o=(\\theta_o, \\phi_o)\\), levando em conta a luz que incide sobre \\(\\mathbf{x}\\) vindo de todas as direções \\(\\omega_i=(\\theta_i, \\phi_i)\\) de um hemisfério \\(\\Omega\\) de possíveis direções. A figura 9.3 ilustra a geometria da equação de renderização, supondo que \\(\\omega_o\\) é a direção até a câmera, e \\(\\omega_i\\) é a direção até uma fonte de luz. Tanto \\(\\omega_i\\) quanto \\(\\omega_o\\) estão em coordenadas polares em relação a um espaço local no qual o vetor normal \\(\\mathbf{n}\\) em \\(\\mathbf{x}\\) corresponde ao eixo \\(z\\). Assim, \\(\\phi_i\\) e \\(\\phi_o\\) são ângulos em torno do eixo \\(z\\), e \\(\\theta_i\\) e \\(\\theta_o\\) são ângulos em torno do eixo \\(x\\) do plano que passa pelo ponto \\(\\mathbf{x}\\). Figura 9.3: Geometria da equação de renderização. A equação de renderização pode ser escrita como \\[ I(\\mathbf x, \\omega_{\\text{o}}) = I_{\\text{e}}(\\mathbf x, \\omega_{\\text{o}}) \\ + \\int_{\\Omega} f_{\\text{r}}(\\omega_{\\text{i}}, \\omega_{\\text{o}}) I(\\mathbf x, \\omega_{\\text{i}}) \\cos \\theta_i \\;\\mathrm{d} \\omega_{\\text{i}}, \\] onde \\(I(\\mathbf x, \\omega_{\\text{o}})\\) é a luz refletida de \\(\\mathbf{x}\\) na direção \\(\\omega_o\\). \\(I_e(\\mathbf x, \\omega_{\\text{o}})\\) é a luz emitida por \\(\\mathbf{x}\\) na direção \\(\\omega_o\\). Esse fator so é maior que zero se \\(\\mathbf{x}\\) for uma fonte de luz. \\(f_{\\text{r}}(\\omega_{\\text{i}}, \\omega_{\\text{o}})\\) é chamada de BRDF (bidirectional reflectance distribution function) ou função de distribuição de reflectância bidirecional, e corresponde à razão entre a luz refletida na direção \\(\\omega_o\\) e a luz incidente na direção \\(\\omega_i\\). Cada tipo de material possui sua própria BRDF. A BRDF pode ser modelada matematicamente ou adquirida de materiais do mundo físico através de instrumentos de radiometria. Para que os resultados sejam fisicamente plausíveis, a BRDF deve respeitar a lei de conservação de energia, isto é, o total de luz refletida e absorvida deve corresponder ao total de luz incidente. \\(I(\\mathbf x, \\omega_{\\text{i}})\\) é a luz incidente em \\(\\mathbf{x}\\), na direção \\(\\omega_i\\). \\(\\cos \\theta_i\\) é o cosseno do ângulo formado pelo vetor normal em \\(\\mathbf{x}\\) e o vetor na direção \\(\\omega_i\\), e está relacionado à lei do cosseno de Lambert que veremos mais a seguir. \\(\\int_{\\Omega}\\dots \\mathrm{d} \\omega_{\\text{i}}\\) é a integral sobre o hemisfério de direções sobre \\(\\mathbf{x}\\). A equação de renderização possui uma dependência espectral. Se quisermos gerar uma imagem colorida, precisamos resolver a equação três vezes, uma para cada componente de cor RGB. Note que a equação de renderização tem natureza recursiva: a luz que incide sobre \\(\\mathbf{x}\\) e vem de cada direção de \\(\\Omega\\) é, ela mesma, a luz refletida de algum outro lugar, que pode ser uma outra superfície ou fonte de luz. Cada superfície reflete parte da luz incidente e absorve outra parte. A iluminação é, portanto, o resultado de um equilíbrio energético das interreflexões de luz entre os diferentes objetos de uma cena. O objetivo dos métodos de iluminação global é resolver a equação de renderização. Isso pode ser feito, por exemplo, traçando os diferentes caminhos da luz (como nas técnicas de traçado de raios baseadas no método de Monte Carlo), ou resolvendo a equação usando métodos de elementos finitos (como na técnica de radiosidade). A BRDF também pode assumir diferentes formas. Por exemplo: A SVBRDF (spatially varying BRDF) é uma BRDF \\(f_{\\text{r}}(\\mathbf{x}, \\omega_{\\text{i}}, \\omega_{\\text{o}})\\) que varia de acordo com a posição \\(\\mathbf{x}\\) sobre a superfície. A BSSRDF (bidirectional scattering-surface RDF) é uma BRDF \\(f_{\\text{r}}(\\mathbf{x}_i, \\omega_{\\text{i}}, \\mathbf{x}_o, \\omega_{\\text{o}})\\) que considera os casos em que a luz que incide em um ponto \\(\\mathbf{x}_i\\) da superfície pode ser espalhada no interior do objeto e sair em outra posição \\(\\mathbf{x}_o\\). Dessa forma é possível simular o fenômeno de espalhamento de subsuperfície (figura 9.4). Figura 9.4: Espalhamento de subsuperfície no modelo Suzanne, renderizado no Blender (fonte). Solucionar a equação de renderização pode ser um processo muito custoso. Para conseguir simular em tempo real a iluminação de superfícies, precisamos adotar modelos de iluminação que aproximem e simplifiquem a equação. Uma possível abordagem de simplificação da equação de renderização consiste em ignorar as interreflexões de luz entre os objetos da cena e supor que a luz incidente em um ponto de uma superfície é determinada unicamente pela luz que vem de uma ou mais fontes de luz. Em outras palavras, ignora-se a iluminação indireta e considera-se apenas a iluminação direta entre cada ponto da superfície e a(s) fonte(s) de luz. Modelos de iluminação que seguem essa suposição são chamados de modelos de iluminação local. Descreveremos dois modelos de iluminação local frequentemente utilizados em renderização em tempo real: Modelo de reflexão de Phong (seção 9.2); Modelo de reflexão de BlinnPhong (seção 9.3). Esses modelos são simples o suficiente para serem implementados no vertex shader ou fragment shader, como veremos nas seções 9.5 e 9.6. Referências "],["phongmodel.html", "9.2 Modelo de reflexão de Phong", " 9.2 Modelo de reflexão de Phong O modelo de reflexão de Phong (Phong 1973) é um modelo de iluminação local que modela de forma empírica a quantidade de luz refletida de um ponto \\(P\\) de uma superfície em uma direção \\(\\hat{\\mathbf{v}}\\) até a câmera. O modelo não é fisicamente correto e, por exemplo, não respeita a lei de conservação de energia. Entretanto, produz resultantes suficientemente adequados para produzir a percepção de objetos iluminados. Além disso, é muito eficiente. A figura 9.5 ilustra a geometria do modelo de reflexão de Phong considerando apenas uma fonte de luz. Figura 9.5: Geometria do modelo de reflexão de Phong. Nessa figura, \\(P\\) é o ponto de uma superfície, e \\(\\hat{\\mathbf{n}}\\) é o vetor normal unitário correspondente. A luz que incide sobre \\(P\\) vem de uma direção \\(-\\hat{\\mathbf{l}}\\), onde \\[\\hat{\\mathbf{l}}=\\frac{L-P}{|L-P|}\\] é o vetor que vai de \\(P\\) até a fonte de luz situada em um ponto \\(L\\), mas normalizado para se transformar em um vetor unitário. Parte da luz incidente em \\(P\\) é refletida na direção do vetor \\[\\hat{\\mathbf{v}}=\\frac{E-P}{|E-P|},\\] que é o vetor normalizado de \\(P\\) até a câmera posicionada em um ponto \\(E\\). A intensidade da luz refletida na direção \\(\\hat{\\mathbf{v}}\\) é calculada como a soma de três componentes de reflexão: ambiente (\\(\\mathbf{I}_a\\)), difusa (\\(\\mathbf{I}_d\\)) e especular (\\(\\mathbf{I}_s\\)): \\[ \\mathbf{I}=\\mathbf{I}_a + \\mathbf{I}_d + \\mathbf{I}_s. \\] A figura 9.6 mostra um exemplo da contribuição de cada componente para a formação da renderização do modelo Stanford Bunny. Figura 9.6: Soma das componentes de reflexão ambiente, difusa e especular. Nessa imagem, o coelho está centralizado na origem do espaço do mundo. A câmera está em \\(E=(0,0,2)\\) olhando da direção de \\(z\\) negativo, e a fonte de luz está localizada em \\(L=(100,100,100)\\) (acima, à direita e à frente do coelho). O aspecto de material feito de plástico é característico de superfícies iluminadas com o modelo de reflexão de Phong. Em geral, uma cena possui mais de uma fonte de luz. Nesse caso, as componentes \\(\\mathbf{I}_d\\) e \\(\\mathbf{I}_s\\) devem ser calculadas para cada fonte de luz e então somadas. Se a cena tiver \\(m\\) fontes de luz (figura 9.7), então a reflexão deverá ser calculada como \\[ \\mathbf{I}=\\mathbf{I}_a + \\sum_{i=1}^m \\left( \\mathbf{I}_{d_i} + \\mathbf{I}_{s_i} \\right). \\] Figura 9.7: Geometria do modelo de reflexão de Phong para a interação com várias fontes de luz. A seguir, detalharemos as componentes de reflexão ambiente, difusa e especular. Reflexão ambiente A componente de reflexão ambiente é uma constante que procura aproximar a iluminação indireta resultante das interreflexões de luz entre as superfícies. A componente não depende da posição de \\(P\\), da posição da câmera \\(E\\) ou das fontes de luz \\(L_1, L_2, \\dots, L_m\\), e é computada simplesmente como \\[\\mathbf{I}_a=\\kappa_a \\iota_a,\\] onde \\(\\kappa_a\\) é a constante de reflexão ambiente que determina o quanto o material reflete a luz ambiente. \\(\\iota_a\\) é a intensidade de luz ambiente incidente em \\(P\\). Uma vez que \\(\\mathbf{I}_a\\) é constante para todos os pontos de um objeto, esse valor pode ser pré-calculado e reutilizado em todos os pontos. Na maioria das aplicações, \\(\\mathbf{I}_a\\) é um valor bastante baixo. A figura 9.8 mostra o resultado da renderização usando apenas a componente de reflexão ambiente, com \\(\\kappa_a=0.1\\) e \\(\\iota_a=1.0\\). O objeto é desenhado com um tom de cinza, que neste caso é a cor RGB \\((0.1, 0.1, 0.1)\\), isto é, cada componente de cor é o próprio valor \\(\\mathbf{I}_a\\). Figura 9.8: Renderização usando apenas a componente ambiente. Mais adiante descreveremos como estender o cálculo de reflexão para gerar cores em vez de tons de cinza. Reflexão difusa A componente de reflexão difusa \\(\\mathbf{I}_d\\) representa a luz que é refletida supondo que \\(P\\) faz parte de uma superfície difusa ideal, também chamada de superfície lambertiana em homenagem ao matemático e físico suíço Johann Heinrich Lambert (17281777), que introduziu tal conceito. Não há superfícies idealmente difusas no mundo físico. Entretanto, aproximações incluem, por exemplo, paredes de gesso e argamassa, e superfícies em geral que possuem aspecto fosco. Superfícies difusas são aquelas que não possuem brilho especular. Em uma superfície lambertiana, a luz que incide sobre \\(P\\) é refletida igualmente em todas as direções, como ilustra a figura 9.9. Figura 9.9: Reflexão difusa ideal. A intensidade da luz refletida é proporcional ao cosseno do ângulo entre \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{n}}\\) (relação chamada de lei do cosseno de Lambert), que é o mesmo que o produto escalar entre os vetores unitários: \\[\\cos \\theta = \\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}}.\\] Observe que esse valor é também a projeção escalar de \\(\\hat{\\mathbf{l}}\\) sobre \\(\\hat{\\mathbf{n}}\\). Quanto maior o ângulo entre \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{n}}\\), menor a intensidade da reflexão difusa. A intensidade é mínima quando os vetores são perpendiculares, pois \\(\\cos(\\pi/2) = 0\\), e máxima quando paralelos, pois \\(\\cos(0)=1\\). Isso ocorre porque a energia luminosa que incide sobre uma área da superfície é mais concentrada quanto mais perpendicular a luz estiver em relação à superfície (figura 9.10). Figura 9.10: A concentração da luz em uma superfície depende da orientação da luz em relação à superfície. Esse fator de atenuação da reflexão da luz está presente na equação de renderização, através do termo \\(\\cos \\theta_i\\) da integral: \\[ I(\\mathbf x, \\omega_{\\text{o}}) = I_{\\text{e}}(\\mathbf x, \\omega_{\\text{o}}) \\ + \\int_{\\Omega} f_{\\text{r}}(\\omega_{\\text{i}}, \\omega_{\\text{o}}) I(\\mathbf x, \\omega_{\\text{i}}) \\cos \\theta_i \\;\\mathrm{d} \\omega_{\\text{i}}. \\] Em uma superfície idealmente difusa, a BRDF é uma constante, isto é, \\(f_r(\\omega_i,\\omega_o)=\\kappa_d\\). Logo, a equação de renderização torna-se \\[ \\begin{align} I(\\mathbf x, \\omega_{\\text{o}}) &amp;= I_{\\text{e}}(\\mathbf x, \\omega_{\\text{o}}) \\ + \\int_{\\Omega} \\kappa_d I(\\mathbf x, \\omega_{\\text{i}}) \\cos \\theta_i \\;\\mathrm{d} \\omega_{\\text{i}}\\\\ &amp;= I_{\\text{e}}(\\mathbf x, \\omega_{\\text{o}}) \\ + \\kappa_d \\int_{\\Omega} I(\\mathbf x, \\omega_{\\text{i}}) \\cos \\theta_i \\;\\mathrm{d} \\omega_{\\text{i}}. \\end{align} \\] O modelo de reflexão de Phong aproxima isso calculando a componente difusa como \\[ \\mathbf{I}_d = \\kappa_d \\iota_{d} (\\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}}), \\] onde \\(\\kappa_d\\) é a constante de reflexão difusa que determina o quanto o material reflete a luz difusa. \\(\\iota_d\\) é a intensidade de luz difusa incidente em \\(P\\). \\(\\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}}\\) é o fator de atenuação relacionado à lei de cosseno de Lambert. São válidos apenas os valores no intervalo \\([0,1]\\). A figura 9.11 mostra o resultado da renderização usando apenas a componente de reflexão difusa, com \\(\\kappa_d=0.7\\) e \\(\\iota_d=1.0\\). Figura 9.11: Renderização usando apenas a componente difusa. Se tivermos \\(m&gt;1\\) fontes de luz, cada fonte de luz terá seu próprio vetor \\(\\hat{\\mathbf{l}}\\) e sua própria intensidade \\(\\iota_{d}\\). A componente \\(\\mathbf{I}_d\\) deverá levar em conta a soma de todas essas intensidades: \\[ \\mathbf{I}_d = \\kappa_d \\sum_{i=1}^m \\iota_{d_i} (\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}). \\] Observação Note a semelhança entre \\(\\mathbf{I}_d\\) e a integral da equação de renderização com BRDF constante: \\[\\kappa_d \\sum_{i=1}^m \\iota_{d_i} (\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}) \\approx \\kappa_d \\int_{\\Omega} I(\\mathbf x, \\omega_{\\text{i}}) \\cos \\theta_i \\;\\mathrm{d} \\omega_{\\text{i}}.\\] A integral é substituída pelo somatório, pois as fontes de luz podem ser consideradas como pontos discretos no espaço; \\(\\iota_{d_i}\\) é uma aproximação de \\(I(\\mathbf x, \\omega_{\\text{i}})\\) para a incidência da luz da \\(i\\)-ésima fonte de luz; \\(\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}\\) é \\(\\cos \\theta_i\\). Note também que \\(\\mathbf{I}_d\\) não usa o vetor \\(\\hat{\\mathbf{v}}\\), que é o vetor de direção até a câmera. Isso significa que a componente de reflexão difusa não depende da posição da câmera. Se a cena é estática, isto é, se os objetos e as fontes de luz não se movem, a componente de reflexão difusa de cada ponto pode ser pré-calculada. Essa característica é explorada na técnica de radiosidade (Greenberg, Cohen, and Torrance 1986). A técnica considera que todas as superfícies da cena são lambertianas. Desse modo, a solução da equação de renderização pode ser pré-computada e a intensidade de reflexão difusa pode ser gravada como a cor de cada vértice. A cena pode então ser visualizada por uma câmera LookAt em tempo real. Reflexão especular Uma superfície idealmente especular é um espelho ideal. A luz que incide em \\(P\\) é refletida apenas na direção reflexa \\(\\hat{\\mathbf{r}}\\) do vetor \\(\\hat{\\mathbf{l}}\\) em torno de \\(\\hat{\\mathbf{n}}\\), como mostra a figura 9.12. Figura 9.12: Reflexão especular ideal. O vetor \\(\\hat{\\mathbf{r}}\\) de reflexão ideal é calculado como \\[ \\hat{\\mathbf{r}} = 2(\\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}})\\hat{\\mathbf{n}} - \\hat{\\mathbf{l}}. \\] A figura 9.13 mostra uma interpretação geométrica desssa expressão. Figura 9.13: Calculando o vetor de reflexão especular. No modelo de reflexão de Phong, a reflexão especular \\(\\mathbf{I}_s\\) é um valor que varia de acordo com o ângulo formado entre \\(\\hat{\\mathbf{r}}\\) e \\(\\hat{\\mathbf{v}}\\): \\[ \\mathbf{I}_s= \\kappa_s \\iota_{s} (\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}})^\\alpha, \\] onde \\(\\kappa_s\\) é a constante de reflexão especular que determina o quanto o material reflete a luz especular. \\(\\iota_s\\) é a intensidade de luz especular incidente em \\(P\\). \\(\\alpha \\geq 0\\) é uma constante que determina o espalhamento do brilho especular. É uma propriedade do material. Quanto maior é o valor de \\(\\alpha\\), mais concentrado será o brilho especular. Desse modo, \\(\\alpha\\) define o quão lustro é o material. Se \\(\\alpha=\\infty\\), o resultado é um superfície especular ideal (espelho ideal). \\(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}}\\) é o cosseno do ângulo entre \\(\\hat{\\mathbf{r}}\\) e \\(\\hat{\\mathbf{v}}\\). São válidos apenas os valores no intervalo \\([0,1]\\). O brilho especular é máximo (\\(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}}=1\\)) se \\(\\hat{\\mathbf{r}}=\\hat{\\mathbf{v}}\\). O brilho especular é mínimo (\\(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}}=0\\)) se \\(\\hat{\\mathbf{r}}\\) e \\(\\hat{\\mathbf{v}}\\) são perpendiculares. A figura 9.14 mostra o resultado da variação do brilho especular para diferentes valores de \\(\\alpha\\) e ilustra a variação correspondente de \\(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}}\\). As renderizações consideram os valores de intensidade ambiente e difusa mostrados nas figuras 9.8 e 9.11. Figura 9.14: Renderização usando diferentes valores de brilho especular. A figura 9.15 mostra o resultado da renderização usando apenas a componente de reflexão especular, com \\(\\kappa_s=0.7\\) e \\(\\iota_s=1.0\\) e \\(\\alpha=50\\). Figura 9.15: Renderização usando apenas a componente especular. Se tivermos \\(m&gt;1\\) fontes de luz, cada fonte de luz terá seu próprio vetor \\(\\hat{\\mathbf{r}}\\) e sua própria intensidade \\(\\iota_{s}\\). A componente \\(\\mathbf{I}_s\\) deverá levar em conta a soma de todas essas intensidades: \\[ \\mathbf{I}_s = \\kappa_s \\sum_{i=1}^m \\iota_{s_i} (\\hat{\\mathbf{r}}_i \\cdot \\hat{\\mathbf{v}})^\\alpha. \\] Modelo completo Combinando as componentes ambiente, difusa e especular, temos a equação completa \\[ \\mathbf{I}=\\kappa_a \\iota_a + \\sum_{i=1}^m \\left(\\kappa_d \\iota_{d_i} (\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}) + \\kappa_s \\iota_{s_i} (\\hat{\\mathbf{r}}_i \\cdot \\hat{\\mathbf{v}})^\\alpha\\right). \\] A equação pode ser avaliada em um vertex shader ou fragment shader, pois podemos armazenar as seguintes informações como variáveis uniformes (uniform): \\(\\alpha\\) (constante de espalhamento de brilho especular). \\(m\\) (número de fontes de luz). \\(\\kappa_a\\), \\(\\kappa_d\\), \\(\\kappa_s\\) (coeficientes de reflexão do material). \\(\\iota_a\\), \\(\\iota_d\\), \\(\\iota_s\\) (intensidades de luz, em um arranjo de \\(m\\) elementos, um para cada fonte de luz). Além disso, \\(\\hat{\\mathbf{n}}\\) pode ser pré-calculado como um atributo do vértice, isto é, como um VBO de vetores normais de vértices. Podemos calcular \\(\\hat{\\mathbf{l}}\\) como \\[\\hat{\\mathbf{l}}=\\frac{L-P}{|L-P|},\\] onde \\(L\\) é a posição da fonte de luz, que também pode ser armazenada como uma variável uniforme, e \\(P\\) é o atributo de posição do vértice atual (caso a equação seja avaliada no vertex shader) ou fragmento atual (caso a equação seja avaliada no fragment shader). Podemos calcular \\(\\hat{\\mathbf{v}}\\) como \\[\\hat{\\mathbf{v}}=\\frac{E-P}{|E-P|},\\] onde \\(E\\) é a posição da câmera. Se considerarmos que \\(P\\) está no espaço da câmera, então a posição da câmera é a origem: \\[E=(0,0,0)\\] e assim não é necessário enviar \\(E\\) ao shader como uma variável uniforme. Como já vimos, \\(\\hat{\\mathbf{r}}\\) pode ser calculado como \\[\\hat{\\mathbf{r}} = 2(\\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}})\\hat{\\mathbf{n}} - \\hat{\\mathbf{l}}.\\] Enfim, temos tudo o que é preciso para implementar o modelo de reflexão de Phong no pipeline gráfico. Faremos isso em um passo a passo de implementação nas seções 9.5 e 9.6. Iluminação colorida Até agora só consideramos fontes de luz e materiais monocromáticos. Se quisermos representar fontes de luz coloridas ou materiais coloridos, devemos calcular \\(\\mathbf{I}\\) para cada uma das componentes RGB. Assim, as constantes de material (\\(\\kappa_a\\), \\(\\kappa_d\\), \\(\\kappa_s\\)) e as intensidades (\\(\\iota_a\\), \\(\\iota_d\\), \\(\\iota_s\\)) de cada fonte de luz deverão ser tuplas de três elementos: \\[ \\kappa_a=(\\kappa_{a,r}, \\kappa_{a,g}, \\kappa_{a,b}),\\\\ \\kappa_d=(\\kappa_{d,r}, \\kappa_{d,g}, \\kappa_{d,b}),\\\\ \\kappa_s=(\\kappa_{s,r}, \\kappa_{s,g}, \\kappa_{s,b}), \\] e \\[ \\iota_a=(\\iota_{a,r}, \\iota_{a,g}, \\iota_{a,b}),\\\\ \\iota_d=(\\iota_{d,r}, \\iota_{d,g}, \\iota_{d,b}),\\\\ \\iota_s=(\\iota_{s,r}, \\iota_{s,g}, \\iota_{s,b}). \\] Os elementos dessas tuplas correspondem a cores RGB. Por exemplo, um objeto de cor vermelha poderá ser especificado com um material com constante de reflexão difusa \\[ \\kappa_d=(1,0,0). \\] Isso significa que o material reflete toda luz vermelha (\\(\\kappa_{d,r}=1\\)) e absorve completamente as outras cores (\\(\\kappa_{d,g}=\\kappa_{d,b}=0\\)) das fontes de luz. Alguns exemplos de materiais são mostrados na figura 9.16. Todos esses materiais estão sendo iluminados por uma fonte de luz branca, isto é, \\(\\iota_a=\\iota_d=\\iota_s=(1,1,1)\\). Figura 9.16: Diferentes materiais no modelo de Phong. Como regra geral, se o material não é um metal, \\(\\kappa_s\\) deve ter o mesmo valor para cada componente RGB, pois a cor do brilho especular deve ser a cor da fonte de luz. Se o material é um metal, \\(\\kappa_s\\) deve ser a cor do material. Por exemplo, em um objeto feito de ouro, a cor especular deve ser amarelada. A cor de uma fonte de luz pode ser especificada através da intensidade de luz difusa. Por exemplo, uma luz verde é definida com \\[ \\iota_d=(0,1,0). \\] Suponha que a luz verde ilumine um material vermelho com \\(\\kappa_d=(1,0,0)\\). O resultado de \\(\\kappa_d\\iota_d\\) (multiplicação elemento a elemento) será a cor preta \\[ \\kappa_d\\iota_d = (0,0,0). \\] Isso faz sentido pois, de fato, um objeto vermelho têm a aparência de um objeto preto quanto iluminado por uma luz verde. Fontes de luz Até agora, consideramos que cada fonte de luz do modelo de reflexão de Phong é um ponto \\(L\\) no espaço. Essa é a definição de uma fonte de luz pontual. Além da luz pontual, também é comum o uso de luz direcional. A seguir, definiremos a luz direcional e revisitaremos a definição de luz pontual com a introdução do conceito adicional de atenuação espacial. Luz pontual Uma fonte de luz pontual é definida por um ponto que emite luz em igual intensidade em todas as direções, como uma lâmpada tradicional de bulbo. Se \\(L\\) é a posição da luz, então o vetor \\(\\hat{\\mathbf{l}}\\) do modelo de reflexão é definido como \\[\\hat{\\mathbf{l}}=\\frac{L-P}{|L-P|}.\\] Podemos simular um efeito de atenuação da luz, isto é, diminuição da intensidade da fonte de luz de acordo com a distância do ponto \\(L\\) em relação ao ponto \\(P\\). As intensidades que devem ser atenuadas são as contantes \\(\\iota_d\\) (intensidade difusa) e \\(\\iota_s\\) (intensidade especular). Opcionalmente, podemos atenuar \\(\\iota_a\\) (intensidade ambiente) caso consideremos que a fonte de luz em questão contribui para a intensidade ambiente. No mundo real, a intensidade luminosa é proporcional ao inverso da distância ao quadrado. Entretanto, podemos obter um maior controle artístico se considerarmos que o fator de atenuação é definido de forma mais geral como: \\[ F_{\\textrm{att}}=\\frac{1}{k_c+k_ld+k_qd^2}. \\] onde \\(d\\) é a distância, isto é, \\(|L-P|\\). \\(k_c\\), \\(k_l\\) e \\(k_q\\) são, respectivamente, os termos constante, linear e quadrático da atenuação. Esses valores devem estar no intervalo \\([0,1]\\). Em geral, \\(k_c=1\\) para evitar que o valor de atenuação seja maior que \\(1\\). Geralmente, \\(k_l\\leq 1\\) e \\(k_q \\leq 1\\), mas a escolha dos valores dependerá do efeito desejado. Um bom ponto de partida é começar com \\(k_l=k_q=0.2\\). Isso faz com que a fonte de luz ilumine objetos até aproximadamente \\(d=20\\). Quanto menor o valor de \\(k_l\\) e \\(k_q\\), maior a distância coberta pela fonte de luz. Se considerarmos que todas as fontes de luz são pontuais e atenuadas, podemos reformular o modelo de reflexão como a seguir: \\[ \\mathbf{I}=\\sum_{i=1}^m F_{\\textrm{att}_i}\\left(\\frac{\\kappa_a \\iota_a}{m} + \\kappa_d \\iota_{d_i} (\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}) + \\kappa_s \\iota_{s_i} (\\hat{\\mathbf{r}}_i \\cdot \\hat{\\mathbf{v}})^\\alpha\\right), \\] onde \\(F_{\\textrm{att}_i}\\) é o fator de atenuação da \\(i\\)-ésima fonte de luz. Note que o termo \\(\\kappa_a \\iota_a\\) é atenuado for \\(F_{\\textrm{att}_i}/m\\). Isso supõe que todas as fontes de luz contribuem igualmente para a intensidade \\(\\iota_a\\). Luz direcional A fonte de luz direcional simula o comportamento de uma fonte de luz pontual infinitamente distante, de tal modo que os raios de luz que chegam à superfície são paralelos entre si. No mundo físico, uma aproximação de fonte de luz direcional é a luz do sol. A luz direcional é definida simplesmente por um vetor \\(\\mathbf{u}\\) de direção da luz. Não há posição do espaço, logo não há como calcular a atenuação. Dada uma direção \\(\\mathbf{u}\\) de luz direcional, o vetor \\(\\hat{\\mathbf{l}}\\) do modelo de reflexão de Phong é definido como \\[\\hat{\\mathbf{l}}=-\\frac{\\mathbf{u}}{|\\mathbf{u}|}.\\] Referências "],["blinnphongmodel.html", "9.3 Modelo de BlinnPhong", " 9.3 Modelo de BlinnPhong O modelo de BlinnPhong (Blinn 1977) é uma modificação do modelo de reflexão de Phong. A reflexão especular \\(\\mathbf{I}_s\\) para uma fonte de luz é calculada como \\[ \\mathbf{I}_s=\\kappa_s \\iota_{s}(\\hat{\\mathbf{n}} \\cdot \\hat{\\mathbf{h}})^\\alpha, \\] onde \\[ \\hat{\\mathbf{h}}=\\frac{\\hat{\\mathbf{l}} + \\hat{\\mathbf{v}}}{|\\hat{\\mathbf{l}} + \\hat{\\mathbf{v}}|}. \\] O vetor \\(\\hat{\\mathbf{h}}\\) é chamado de halfway vector e está na metade do caminho entre os vetores \\(\\hat{\\mathbf{l}}\\) e \\(\\hat{\\mathbf{v}}\\) (figura 9.17). Figura 9.17: Geometria do halfway vector. Ao usar o cosseno do ângulo entre \\(\\hat{\\mathbf{n}}\\) e \\(\\hat{\\mathbf{h}}\\) no lugar do cosseno do ângulo entre \\(\\hat{\\mathbf{r}}\\) e \\(\\hat{\\mathbf{v}}\\), o modelo de Blinn-Phong consegue reproduzir melhor o comportamento da reflexão especular. No modelo de Phong, o brilho especular é sempre redondo em uma superfície plana. No modelo de BlinnPhong, o brilho especular é redondo quando a superfície é vista de frente, e alongado verticalmente quando a direção de visão e a direção à fonte de luz estão rentes à superfície. A figura 9.18 compara o formato do brilho especular sobre o lado plano de um objeto visto nessa configuração. No mundo real, o alongamento do brilho especular pode ser observado, por exemplo, no brilho da luz do sol sobre o mar quando o sol está próximo do horizonte, ou no reflexo da luz sobre um chão molhado, quando a luz é gerada por um poste distante ou pelos faróis de um carro. Figura 9.18: Diferença entre o formato do brilho especular nos modelos de Phong e BlinnPhong. O modelo de BlinnPhong também pode ser mais eficiente que o modelo de Phong. Se a projeção utilizada for ortográfica e a fonte de luz estiver infinitamente longe do objeto iluminado (como na fonte de luz direcional), então \\(\\hat{\\mathbf{h}}\\) só precisa ser calculado uma vez e pode ser reutilizado para todos os pontos do objeto. O vetor só precisa ser atualizado se a direção de visão ou a direção da fonte de luz for modificada. A possibilidade de otimização do modelo de BlinnPhong é pouco explorada atualmente, uma vez que a maior parte das aplicações usa projeção perspectiva ou usa câmeras ou fontes de luz dinâmicas. Entretanto, esse aspecto foi uma vantagem importante nos limitados sistemas gráficos da década de 1980 e início da década de 1990. A equação completa do modelo de BlinnPhong é a seguinte: \\[ \\mathbf{I}=\\kappa_a \\iota_a + \\sum_{i=1}^m \\left(\\kappa_d \\iota_{d_i} (\\hat{\\mathbf{l}}_i \\cdot \\hat{\\mathbf{n}}) + \\kappa_s \\iota_{s_i} (\\hat{\\mathbf{n}} \\cdot \\hat{\\mathbf{h}})^\\alpha\\right), \\] As constantes utilizadas são as mesmas do modelo de Phong. Entretanto, um mesmo valor da constante \\(\\alpha\\) produzirá um brilho especular maior no modelo de BlinnPhong, como mostra a figura 9.19. Essa alteração do brilho pode ser compensada aumentando o valor da constante. Figura 9.19: Tamanho do brilho especular nos modelos de Phong e BlinnPhong usando o mesmo expoente de brilho. Pela sua simplicidade e melhor fidelidade de reprodução de brilhos especulares, o modelo de BlinnPhong é mais utilizado que o modelo de Phong em síntese de imagens em tempo real. Referências "],["shading.html", "9.4 Sombreamento", " 9.4 Sombreamento Sombreamento ou tonalização (do inglês shading) é o processo de modificar a intensidade das cores de uma imagem através de tons claros e escuros, de modo a produzir a percepção de volume e profundidade de um objeto tridimensional. Em computação gráfica, as intensidades de cor calculadas através do modelo de iluminação são utilizadas para fazer o sombreamento da geometria projetada. Há três modelos de sombreamento comumente utilizados: Sombreamento flat (flat shading); Sombreamento de Gouraud (Gouraud shading); Sombreamento de Phong (Phong shading). Flat O sombreamento flat consiste em avaliar a equação do modelo de iluminação uma vez para cada face da malha poligonal. Assim, o vetor \\(\\hat{\\mathbf{n}}\\) da equação deve ser o vetor normal do plano que contém a face. A cor resultante é utilizada para preencher todos os pixels da face. A aplicação abaixo apresenta um exemplo de objeto renderizado com sombreamento flat. Use o botão esquerdo do mouse para rodar o objeto, e o botão direito para mudar a orientação da fonte de luz. Uma vantagem do sombreamento flat é a eficiência. O número de vezes que a equação do modelo de ilumninação é avaliada depende apenas do número de faces que, em geral, é menor que o número de pixels. No sombreamento flat, as descontinuidades da iluminação sobre a superfície ficam bem evidentes e o resultado é um aspecto facetado da malha poligonal. Isso pode ser desejável caso o objeto visualizado seja um poliedro. Entretanto, se a malha geométrica for a aproximação de uma superfície suave, o resultado só será satisfatório se a malha for subdividida em faces menores. No exemplo acima, a malha triangular é uma aproximação de uma esfera. Experimente aumentar o número de subdivisões e observe como as descontinuidades de luz entre as faces são atenuadas. Atualmente, não há melhoramento de eficiência de renderização com a implementação do sombreamento flat na GPU, pois o pipeline gráfico atual é otimizado para o processamento de atributos de vértices, e não atributos de triângulos. Gouraud Como forma de melhorar o sombreamento de malhas que aproximam superfícies suaves, o cientista da computação Henri Gouraud desenvolveu a técnica que ficou conhecida como sombreamento de Gouraud (Gouraud 1971), em substituição ao sombreamento flat. A ideia do sombreamento de Gouraud é avaliar a equação do modelo de iluminação para cada vértice da malha. Assim, cada vértice tem uma cor calculada. No rasterizador, as cores dos vértices são interpoladas linearmente para gerar a cor final de cada pixel. A aplicação abaixo implementa o sombreamento de Gouraud. O sombreamento de Gouraud suaviza as descontinuidades de iluminação entre as faces. O resultado é mais convincente que o sombreamento flat para o mesmo número de faces. Entretanto, a intensidade sobre as arestas pode parecer ligeiramente maior que a intensidade dentro da primitiva. Esse defeito visual, chamado de bandas de Mach, se deve ao fato do sistema visual humano exagerar naturalmente o contraste nas extremidades de variações lineares de gradientes de cor. Outro problema do sombreamento de Gouraud é a perda do brilho especular quando nenhum vértice da malha encontra-se dentro da região do brilho. Na aplicação acima, observe como o brilho especular surge e desaparece de forma intermitente quando o número de subdivisões é baixo (de 0 a 2). Isso pode ser melhorado aumentando o refinamento da malha, mas sob o custo de aumentar o tempo de processamento. Ao usar sombreamento de Gouraud, o vetor \\(\\hat{\\mathbf{n}}\\) da equação do modelo de iluminação precisa ser o vetor normal de vértice, que pode ser calculado como a média dos vetores normais das faces adjacentes (seção 6.3). Phong O sombreamento de Phong é o mais custoso dos sombreamentos, e consiste em avaliar a equação do modelo de iluminação para cada fragmento. O resultado é muito superior ao sombreamento de Gouraud para um mesmo número de faces. No sombreamento de Phong, as bandas de Mach são praticamente imperceptíveis. Além disso, o brilho especular é mantido independentemente do refinamento da malha. Ao usar sombreamento de Phong, precisamos calcular um vetor \\(\\hat{\\mathbf{n}}\\) para cada fragmento. Isso pode feito através da interpolação linear das coordenadas \\((x,y,z)\\) dos vetores normais de vértice, da mesma forma como as componentes RGB das cores dos vértices são interpoladas para criar um degradê de cores. Entretanto, neste caso, o vetor de coordenadas interpoladas é re-normalizado no fragment shader para ser utilizado como vetor \\(\\hat{\\mathbf{n}}\\) da equação. Atualmente, o sombreamento de Phong é o sombreamento mais utilizado em síntese de imagens em tempo real. Observação Note que modelo de reflexão de Phong e sombreamento de Phong são conceitos diferentes: O modelo de reflexão de Phong é o modelo empírico de iluminação local descrito através da equação de reflexão vista na seção 9.2. O sombreamento de Phong é a estratégia de avaliar, para cada fragmento, a equação de um modelo de iluminação. O modelo de iluminação avaliado não precisa ser o modelo de reflexão de Phong. Referências "],["viewer2.html", "9.5 Normais como cores", " 9.5 Normais como cores Nesta seção, implementaremos a segunda versão do visualizador de modelos 3D apresentado originalmente na seção 8.4. As seguintes funcionalidade serão incorporadas: Cálculo de normais de vértices, usando o procedimento descrito no final da seção 6.3; Um novo shader que visualiza os vetores normais através de cores; Uma caixa de combinação (combo box) para selecionar entre o shader do projeto anterior e o novo shader. Um botão Load 3D Model para carregar arquivos OBJ durante a execução. O resultado ficará como a seguir: Configuração inicial Faça uma cópia do projeto viewer1 da seção 8.4 e renomeie-o para viewer2. Dentro de abcg/examples/viewer2/assets, crie os arquivos vazios normal.frag e normal.vert. Esses serão os arquivos do novo shader de visualização de vetores normais como cores. Baixe o arquivo imfilebrowser.h do repositório AirGuanZ/imgui-filebrowser e salve-o em abcg/examples/viewer2. Esse arquivo contém a implementação do controle imgui-filebrowser, que é uma caixa de diálogo de seleção de arquivos usando a interface da ImGui. Como os demais arquivos utilizados são os mesmos do projeto anterior, vamos nos concentrar apenas nas partes que serão modificadas. model.hpp Modifique a estrutura Vertex para que cada vértice tenha um atributo adicional de vetor normal glm::vec3 normal: struct Vertex { glm::vec3 position{}; glm::vec3 normal{}; bool operator==(const Vertex&amp; other) const noexcept { static const auto epsilon{std::numeric_limits&lt;float&gt;::epsilon()}; return glm::all(glm::epsilonEqual(position, other.position, epsilon)) &amp;&amp; glm::all(glm::epsilonEqual(normal, other.normal, epsilon)); } }; Observação Na implementação do operador de igualdade de Vertex, estamos agora utilizando um método mais preciso de determinar se dois vetores são iguais. Em vez de fazer position == other.position e normal == other.normal, verificamos se cada elemento de cada tupla está a uma distância que corresponde a um epsilon de um float (diferença entre 1.0f e o próximo valor representado por um float). Essa é uma forma mais robusta de comparar dois vértices, pois vértices equivalentes podem ter coordenadas ligeiramente diferentes por conta de erros de conversão de ponto flutuante durante a leitura do arquivo OBJ. Dentro da classe Model, incluiremos as seguintes definições: bool m_hasNormals{false}; void computeNormals(); Durante a leitura do arquivo OBJ, se o modelo já vier com vetores normais calculados, m_hasNormals será true. Caso contrário, será false e então chamaremos Model::computeNormals para calcular as normais. model.cpp Como modificamos a estrutura Vertex em model.hpp, precisamos modificar também a especialização de std::hash para gerar um valor de hashing que leve em conta tanto a posição do vértice quanto o vetor normal. Afinal, dois vértices na mesma posição espacial são vértices diferentes caso tenham vetores normais diferentes: // Explicit specialization of std::hash for Vertex namespace std { template &lt;&gt; struct hash&lt;Vertex&gt; { size_t operator()(Vertex const&amp; vertex) const noexcept { const std::size_t h1{std::hash&lt;glm::vec3&gt;()(vertex.position)}; const std::size_t h2{std::hash&lt;glm::vec3&gt;()(vertex.normal)}; return h1 ^ h2; } }; } // namespace std O valor de hashing é calculado como h1 ^ h2, onde ^ é o operador ou exclusivo bit a bit. Essa é uma forma simples de misturar dois valores para obter um valor de hashing. Calculando as normais de vértices O cálculo dos vetores normais dos vértices é feito em Model::computeNormals: void Model::computeNormals() { // Clear previous vertex normals for (auto&amp; vertex : m_vertices) { vertex.normal = glm::zero&lt;glm::vec3&gt;(); } // Compute face normals for (const auto offset : iter::range&lt;int&gt;(0, m_indices.size(), 3)) { // Get face vertices Vertex&amp; a{m_vertices.at(m_indices.at(offset + 0))}; Vertex&amp; b{m_vertices.at(m_indices.at(offset + 1))}; Vertex&amp; c{m_vertices.at(m_indices.at(offset + 2))}; // Compute normal const auto edge1{b.position - a.position}; const auto edge2{c.position - b.position}; const glm::vec3 normal{glm::cross(edge1, edge2)}; // Accumulate on vertices a.normal += normal; b.normal += normal; c.normal += normal; } // Normalize for (auto&amp; vertex : m_vertices) { vertex.normal = glm::normalize(vertex.normal); } m_hasNormals = true; } Esta função é chamada logo após o carregamento do modelo, isto é, quando m_vertices e m_indices já contêm a geometria indexada do modelo, mas antes da criação do VBO/EBO. Antes de calcular os vetores normais, todos os vetores normais em m_vertices são definidos como \\((0,0,0)\\): // Clear previous vertex normals for (auto&amp; vertex : m_vertices) { vertex.normal = glm::zero&lt;glm::vec3&gt;(); } Para cada triângulo \\(\\triangle ABC\\) da malha, o vetor normal é calculado como \\[\\mathbf{n}=(B-A) \\times (C-B),\\] // Compute normal const auto edge1{b.position - a.position}; const auto edge2{c.position - b.position}; const glm::vec3 normal{glm::cross(edge1, edge2)}; O resultado é acumulado nos vértices: // Accumulate on vertices a.normal += normal; b.normal += normal; c.normal += normal; Lembre-se que, como estamos usando geometria indexada, um mesmo vértice pode ser compartilhado por vários triângulos. Então, ao final do laço que itera todos os triângulos, o atributo normal de cada vértice será a soma dos vetores normais dos triângulos que usam tal vértice. Por exemplo, se um vértice é compartilhado por 5 triângulos, então seu atributo normal será a soma dos vetores normais desses 5 triângulos. Para finalizar, basta normalizar o atributo normal de cada vértice. O resultado será um vetor unitário que corresponde à média dos vetores normais dos triângulos adjacentes: // Normalize for (auto&amp; vertex : m_vertices) { vertex.normal = glm::normalize(vertex.normal); } Leitura do arquivo OBJ A função Model::loadObj é modificada para ler vetores normais caso estejam presentes (linhas 116 a 126 do código abaixo). Se os vetores normais não são encontrados, chamamos Model::computeNormals (linhas 148 a 150): void Model::loadObj(std::string_view path, bool standardize) { tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data())) { if (!reader.Error().empty()) { throw abcg::Exception{abcg::Exception::Runtime( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error()))}; } throw abcg::Exception{ abcg::Exception::Runtime(fmt::format(&quot;Failed to load model {}&quot;, path))}; } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } const auto&amp; attrib{reader.GetAttrib()}; const auto&amp; shapes{reader.GetShapes()}; m_vertices.clear(); m_indices.clear(); m_hasNormals = false; // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (const auto&amp; shape : shapes) { // Loop over indices for (const auto offset : iter::range(shape.mesh.indices.size())) { // Access to vertex const tinyobj::index_t index{shape.mesh.indices.at(offset)}; // Vertex position const int startIndex{3 * index.vertex_index}; const float vx{attrib.vertices.at(startIndex + 0)}; const float vy{attrib.vertices.at(startIndex + 1)}; const float vz{attrib.vertices.at(startIndex + 2)}; // Vertex normal float nx{}; float ny{}; float nz{}; if (index.normal_index &gt;= 0) { m_hasNormals = true; const int normalStartIndex{3 * index.normal_index}; nx = attrib.normals.at(normalStartIndex + 0); ny = attrib.normals.at(normalStartIndex + 1); nz = attrib.normals.at(normalStartIndex + 2); } Vertex vertex{}; vertex.position = {vx, vy, vz}; vertex.normal = {nx, ny, nz}; // If hash doesn&#39;t contain this vertex if (hash.count(vertex) == 0) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } if (standardize) { this-&gt;standardize(); } if (!m_hasNormals) { computeNormals(); } createBuffers(); } Mapeamento do VBO com as normais Uma vez que cada vértice tem agora dois atributos (posição e vetor normal), precisamos configurar como o VBO será mapeado para os atributos de entrada do vertex shader que chamaremos de inPosition e inNormal. Isso é feito em Model::setupVAO: void Model::setupVAO(GLuint program) { // Release previous VAO abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); abcg::glBindVertexArray(m_VAO); // Bind EBO and VBO abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); // Bind vertex attributes const GLint positionAttribute{ abcg::glGetAttribLocation(program, &quot;inPosition&quot;)}; if (positionAttribute &gt;= 0) { abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr); } const GLint normalAttribute{abcg::glGetAttribLocation(program, &quot;inNormal&quot;)}; if (normalAttribute &gt;= 0) { abcg::glEnableVertexAttribArray(normalAttribute); GLsizei offset{sizeof(glm::vec3)}; abcg::glVertexAttribPointer(normalAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), reinterpret_cast&lt;void*&gt;(offset)); } // End of binding abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindVertexArray(0); } Nosso VBO usa dados intercalados no formato \\[\\left[ [x\\;\\; y\\;\\; z]_1\\;\\; [n_x\\;\\; n_y\\;\\; n_z]_1\\;\\; [x\\;\\; y\\;\\; z]_2\\;\\; [n_x\\;\\; n_y\\;\\; n_z]_2\\;\\; \\cdots\\;\\; [x\\;\\; y\\;\\; z]_m\\;\\; [n_x\\;\\; n_y\\;\\; n_z]_m \\right],\\] onde \\([x\\;\\; y\\;\\; z]_i\\) e \\([n_x\\;\\; n_y\\;\\; n_z]_i\\) são a posição e vetor normal do \\(i\\)-ésimo vértice do arranjo. Logo, o mapeamento para inNormal precisa usar um deslocamento (offset) de sizeof(glm::vec3), que é o que fazemos nas linhas 191 a 194. openglwindow.hpp Na versão anterior deste visualizador (projeto viewer1) só era possível usar um único programa de shader, identificado por m_program. Em particular, esse programa de shader correspondia ao par de shaders depth.vert e depth.frag. Nesta aplicação, o usuário poderá escolher entre dois programas de shaders. Para permitir isso, a variável m_program definida na classe OpenGLWindow será substituída por um conjunto de variáveis: std::vector&lt;const char*&gt; m_shaderNames{&quot;normal&quot;, &quot;depth&quot;}; std::vector&lt;GLuint&gt; m_programs; int m_currentProgramIndex{-1}; onde m_shaderNames é um arranjo de nomes dos pares de shaders contidos no subdiretório assets. Neste projeto usaremos os shaders normal e depth. Vamos supor que cada nome corresponde a dois arquivos, um com extensão .vert (vertex shader) e outro com extensão .frag (fragment shader). m_programs é um arranjo de identificadores dos programas de shader compilados, um para cada elemento de m_shaderNames; m_currentProgramIndex é um índice para m_programs que indica qual é o programa atualmente selecionado pelo usuário. Sempre que um novo programa for selecionado usando a caixa de combinação da ImGui, Model::SetupVAO será chamada para o novo programa, pois o VAO é modificado de acordo com os shaders. Por padrão, o valor de m_currentProgramIndex é -1. Na primeira chamada a OpenGLWindow::paintUI, esse valor é modificado para 0, que é o índice padrão da caixa de combinação de seleção de shaders. Como isso equivale a uma mudança de programa, a função Model::SetupVAO é chamada (poderíamos chamar Model::SetupVAO em OpenGLWindow::initializeGL, mas assim evitamos duplicação de código). openglwindow.cpp No início do arquivo precisamos incluir alguns cabeçalhos a mais: #include &lt;glm/gtc/matrix_inverse.hpp&gt; #include &quot;imfilebrowser.h&quot; initializeGL Em OpenGLWindow::initializeGL, compilamos e ligamos todos os shaders mencionados em m_shaderNames, supondo que o arquivo .vert tem o mesmo nome do arquivo .frag: void OpenGLWindow::initializeGL() { abcg::glClearColor(0, 0, 0, 1); abcg::glEnable(GL_DEPTH_TEST); // Create programs for (const auto&amp; name : m_shaderNames) { const auto program{createProgramFromFile(getAssetsPath() + name + &quot;.vert&quot;, getAssetsPath() + name + &quot;.frag&quot;)}; m_programs.push_back(program); } // Load model m_model.loadObj(getAssetsPath() + &quot;bunny.obj&quot;); m_trianglesToDraw = m_model.getNumTriangles(); } Observe que continuamos carregando bunny.obj como modelo 3D inicial. A função Model::loadObj será chamada novamente sempre que o usuário selecionar um novo arquivo usando o botão Load 3D Model que definiremos mais adiante em OpenGLWindow::paintUI. paintGL A definição de OpenGLWindow::paintGL ficará assim: void OpenGLWindow::paintGL() { update(); abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportWidth, m_viewportHeight); // Use currently selected program const auto program{m_programs.at(m_currentProgramIndex)}; abcg::glUseProgram(program); // Get location of uniform variables const GLint viewMatrixLoc{abcg::glGetUniformLocation(program, &quot;viewMatrix&quot;)}; const GLint projMatrixLoc{abcg::glGetUniformLocation(program, &quot;projMatrix&quot;)}; const GLint modelMatrixLoc{ abcg::glGetUniformLocation(program, &quot;modelMatrix&quot;)}; const GLint normalMatrixLoc{ abcg::glGetUniformLocation(program, &quot;normalMatrix&quot;)}; // Set uniform variables used by every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); // Set uniform variables of the current object abcg::glUniformMatrix4fv(modelMatrixLoc, 1, GL_FALSE, &amp;m_modelMatrix[0][0]); const auto modelViewMatrix{glm::mat3(m_viewMatrix * m_modelMatrix)}; const glm::mat3 normalMatrix{glm::inverseTranspose(modelViewMatrix)}; abcg::glUniformMatrix3fv(normalMatrixLoc, 1, GL_FALSE, &amp;normalMatrix[0][0]); m_model.render(m_trianglesToDraw); abcg::glUseProgram(0); } Observe que, além de enviar para o shader as matrizes \\(4 \\times 4\\) de visão (viewMatrix), projeção (projMatrix) e modelo (modelMatrix), também enviamos uma matriz \\(3 \\times 3\\) chamada de normalMatrix. Nas linhas 73 a 75, a matriz normalMatrix é calculada como a transposta da inversa de \\(M_{\\textrm{view}}M_{\\textrm{model}}\\), isto é: \\[ M_{\\textrm{normal}}=\\left((M_{\\textrm{view}}M_{\\textrm{model}})^{-1}\\right)^{T}. \\] \\(M_{\\textrm{normal}}\\) é a matriz que transforma um vetor normal do espaço do mundo para um vetor normal do espaço da câmera. Existe um motivo especial para usar essa matriz no lugar de \\(M_{\\textrm{view}}M_{\\textrm{model}}\\) para transformar vetores normais. Isso será explicado logo mais no final desta seção. paintUI No início de OpenGLWindow::paintUI, inicializamos o objeto que define a caixa de diálogo do elemento de interface imgui-filebrowser: static ImGui::FileBrowser fileDialog; fileDialog.SetTitle(&quot;Load 3D Model&quot;); fileDialog.SetTypeFilters({&quot;.obj&quot;}); fileDialog.SetWindowSize(m_viewportWidth * 0.8f, m_viewportHeight * 0.8f); fileDialog.SetPwd(getAssetsPath()); Com essa configuração, o navegador de arquivos mostrará arquivos com extensão .obj no subdiretório assets, e a caixa de diálogo ocupará 80% do tamanho do viewport. A caixa de seleção de shaders é implementada com o seguinte trecho de código: // Shader combo box { static std::size_t currentIndex{}; ImGui::PushItemWidth(120); if (ImGui::BeginCombo(&quot;Shader&quot;, m_shaderNames.at(currentIndex))) { for (const auto index : iter::range(m_shaderNames.size())) { const bool isSelected{currentIndex == index}; if (ImGui::Selectable(m_shaderNames.at(index), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); // Set up VAO if shader program has changed if (static_cast&lt;int&gt;(currentIndex) != m_currentProgramIndex) { m_currentProgramIndex = static_cast&lt;int&gt;(currentIndex); m_model.setupVAO(m_programs.at(m_currentProgramIndex)); } } Veja que usamos os nomes de m_shaderNames como elementos da caixa de combinação. Observe também que a função Model::setupVAO é chamada sempre que um novo shader é selecionado (linhas 196 a 200). O botão Load 3D Model é criado com o código a seguir: if (ImGui::Button(&quot;Load 3D Model...&quot;, ImVec2(-1, -1))) { fileDialog.Open(); } Quando o botão é pressionado, chamamos fileDialog.Open para abrir a caixa de diálogo de seleção de arquivos OBJ. No fim de OpenGLWindow::paintUI, colocamos o código responsável pela renderização da caixa de diálogo e pela leitura do novo modelo 3D. fileDialog.Display(); if (fileDialog.HasSelected()) { // Load model m_model.loadObj(fileDialog.GetSelected().string()); m_model.setupVAO(m_programs.at(m_currentProgramIndex)); m_trianglesToDraw = m_model.getNumTriangles(); fileDialog.ClearSelected(); } Se algum arquivo foi selecionado na caixa de diálogo (linha 212), chamamos Model::loadObj para carregar o arquivo, e então Model::setupVAO para configurar o VAO. Por fim, atualizamos a variável m_trianglesToDraw, utilizada para controlar o número de triângulos processados por glDrawElements. Isso é tudo em relação às mudanças do código em C++. Vamos agora à definição dos shaders normal.vert e normal.frag usando GLSL. normal.frag Vamos começar pelo conteúdo de normal.frag, que é bem simples: #version 410 in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } A cor de entrada é simplesmente copiada para a cor de saída, como já fizemos em vários outros projetos. Assim, se cada vértice do triângulo tiver uma cor diferente, fragColor será uma cor interpolada linearmente a partir dos vértices. O resultado será um gradiente de cor. normal.vert Este shader converte as coordenadas do vetor normal de vértice em uma cor RGB. Em muitos casos, é mais fácil visualizar a direção de vetores normais através de cores do que através do desenho de setas que saem dos vértices. Se o modelo tiver muitos vértices, as setas cobrirão todo o objeto e não conseguiremos distinguir um vetor de outro. Isso é ainda mais importante se quisermos observar os vetores normais calculados para cada fragmento. As coordenadas \\((x, y, z)\\) de um vetor unitário estão no intervalo \\([-1,1]\\). Uma cor RGB tem componentes \\((r, g, b)\\) no intervalo \\([0,1]\\). Logo, a conversão das coordenadas em cores é um simples mapeamento linear de \\([-1,1]\\) para \\([0,1]\\): \\[ r = \\frac{x+1}{2}, \\qquad g = \\frac{y+1}{2}, \\qquad b = \\frac{z+1}{2}. \\] Assim, se o vetor normal tiver coordenadas \\((1,0,0)\\) (direção do eixo \\(x\\) positivo), o resultado será um tom próximo ao vermelho \\((1, 0.5, 0.5)\\). Se o vetor normal tiver coordenadas \\((0,1,0)\\) (direção de \\(y\\) positivo), o resultado será um tom próximo ao verde \\((0.5, 1, 0.5)\\). Se tiver coordenadas \\((0,0,1)\\) (direção de \\(z\\) positivo), terá um tom próximo ao azul \\((0.5, 0.5, 1)\\). Essa convenção de cores é a mesma que temos utilizado nas ilustrações dos eixos principais em todas as figuras. A figura 9.20 mostra as cores correspondentes para as direções \\(\\pm x\\), \\(\\pm y\\) e \\(\\pm z\\). Figura 9.20: Cores correspondentes para as direções positivas e negativas dos eixos principais. O código ficará como a seguir: #version 410 layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; out vec4 fragColor; void main() { mat4 MVP = projMatrix * viewMatrix * modelMatrix; gl_Position = MVP * vec4(inPosition, 1.0); vec3 N = inNormal; // Object space // vec3 N = normalMatrix * inNormal; // Eye space // Convert from [-1,1] to [0,1] fragColor = vec4((N + 1.0) / 2.0, 1.0); } Temos dois atributos de entrada: inPosition (linha 3) e inNormal (linha 4), que correspondem à posição do vértice e seu vetor normal unitário. Vamos supor que ambos estão no espaço do objeto. Temos apenas um atributo de saída (linha 11), que é a cor que iremos calcular com base no vetor normal. Na linha 14, criamos uma matriz MVP que é a composição das matrizes de modelo, visão e projeção. Na linha 16, multiplicamos MVP pela posição do vértice, de modo a converter a posição em coordenadas do espaço do objeto para coordenadas do espaço de recorte. O resultado é atribuído a gl_Position. Na linha 18, criamos um vetor N que é uma cópia de inNormal. A conversão de XYZ para RGBA é feita na linha 22 (a componente A é sempre 1). Dica Da forma como está, fragColor é a cor que representa um vetor normal unitário no espaço do objeto. Experimente comentar a linha 18 e, no lugar, usar o código que está comentado na linha 19. Isso fará com que fragColor represente um vetor normal unitário no espaço da câmera. Tente identificar visualmente a diferença entre vetores normais no espaço do objeto e no espaço da câmera. Há alguma cor que aparece para N em um espaço e não aparece para N em outro espaço? Por quê? Convertendo normais para o espaço da câmera Se usarmos a linha 19 no lugar da linha 18, N será transformado por normalMatrix para converter o vetor normal do espaço do objeto para o espaço da câmera. Em muitos casos, isso é o mesmo que fazer vec4 N = viewMatrix * modelMatrix * vec4(inNormal, 0); O problema é que transformar um vetor normal pela matriz de modelo e visão nem sempre resulta em um vetor normal à superfície. Esse é o caso quando a matriz de modelo (ou de visão) contém uma escala não uniforme. Veja, na figura 9.21, como uma escala não uniforme faz com que a maioria dos vetores normais não sejam mais perpendiculares às faces (que nesse caso são lados) do objeto. Figura 9.21: A escala não uniforme pode alterar o ângulo entre o vetor normal e um vetor tangente à superfície. Suponha que os vetores \\(\\mathbf{n}\\) e \\(\\mathbf{t}\\) da figura 9.21 sejam matrizes coluna \\[\\mathbf{n}=\\begin{bmatrix}n_x\\\\n_y\\\\n_z\\end{bmatrix},\\qquad \\mathbf{t}=\\begin{bmatrix}t_x\\\\t_y\\\\t_z\\end{bmatrix}.\\] Os vetores são perpendiculares. Logo, \\[\\mathbf{n} \\cdot \\mathbf{t} = 0.\\] Também podemos escrever na notação de multiplicação entre matrizes: \\[ \\mathbf{n}^T\\mathbf{t} = 0. \\] Seja \\(\\mathbf{M}\\) a matriz modelo-visão: \\[ \\mathbf{M}=\\mathbf{M}_{\\textrm{view}}\\mathbf{M}_{\\textrm{model}}. \\] Já sabemos que nem sempre \\(\\mathbf{M}\\mathbf{n} \\cdot \\mathbf{M}\\mathbf{t}=0\\). Acabamos de ver um contraexemplo na figura 9.21. Entretanto, suponha que existe uma matriz \\(\\mathbf{W}\\) tal que \\[(\\mathbf{W}\\mathbf{n}) \\cdot (\\mathbf{M}\\mathbf{t}) = 0.\\] Podemos reescrever a expressão como \\[ \\begin{align} (\\mathbf{W}\\mathbf{n})^T(\\mathbf{M}\\mathbf{t}) = 0,\\\\ (\\mathbf{n}^T\\mathbf{W}^T)(\\mathbf{M}\\mathbf{t}) = 0,\\\\ \\mathbf{n}^T(\\mathbf{W}^T\\mathbf{M})\\mathbf{t} = 0.\\\\ \\end{align} \\] Nesta última expressão, observe que, se o termo entre parênteses resultar em uma matriz identidade, isto é, se \\[(\\mathbf{W}^T\\mathbf{M})=\\mathbf{I},\\] então \\[\\mathbf{n}^T\\mathbf{t} = 0,\\] que é o que declaramos no início (os vetores são perpendiculares). Podemos isolar \\(\\mathbf{W}\\) para obter a forma final da matriz que devemos usar para transformar o vetor normal: \\[ \\begin{align} \\mathbf{W}^T\\mathbf{M}&amp;=\\mathbf{I},\\\\ \\mathbf{W}^T&amp;=\\mathbf{M}^{-1},\\\\ \\mathbf{W}&amp;=(\\mathbf{M}^{-1})^T.\\\\ \\end{align} \\] Isso mostra que a matriz que devemos utilizar para converter um vetor normal do espaço do objeto para o espaço da câmera é a transposta da inversa da matriz modelo-visão: \\[ \\mathbf{M}_{\\textrm{normal}}=(\\mathbf{M_{\\textrm{modelview}}}^{-1})^T. \\] Baixe o código completo do projeto usando este link. "],["viewer3.html", "9.6 Iluminação na prática", " 9.6 Iluminação na prática Nesta seção, veremos mais um aprimoramento do visualizador de modelos 3D, como uma continuação do projeto viewer2 da seção anterior (seção 9.6). Esta será a versão 3 do visualizador (viewer3), e terá shaders extras que implementam os seguintes modelos de reflexão e sombreamento: O modelo de Phong, usando sombreamento de Gouraud (shaders gouraud.vert e gouraud.frag); O modelo de Phong, usando sombreamento de Phong (shaders phong.vert e phong.frag); O modelo de BlinnPhong, usando sombreamento de Phong (shaders blinnphong.vert e blinnphong.frag); Há também algumas funcionalidades complementares, como a possibilidade de modificar em tempo real os parâmetros \\(\\kappa\\), \\(\\iota\\) e \\(\\alpha\\) através de sliders do tipo ImGui::ColorEdit3, e a possibilidade de mudar a orientação da fonte de luz (uma fonte de luz direcional) usando o botão direito do mouse com o trackball virtual. O resultado ficará como a seguir. O código C++ é semelhante ao do projeto anterior. As poucas modificações feitas são relacionadas a conceitos que já vimos anteriormente: modificação da interface ImGui e uso do trackball virtual. A seguir, vamos nos concentrar nas modificações mais relevantes que são os novos shaders de iluminação. Baixe o código completo deste link. Phong com sombreamento de Gouraud Iniciaremos com o sombreamento de Gouraud, que consiste em avaliar a equação do modelo de iluminação para cada vértice. Desse modo, praticamente todo o trabalho será feito no vertex shader. Como o fragment shader é mais simples, começaremos por ele. gouraud.frag #version 410 in vec4 fragColor; out vec4 outColor; void main() { if (gl_FrontFacing) { outColor = fragColor; } else { float i = (fragColor.r + fragColor.g + fragColor.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } Este fragment shader é parecido com o que utilizamos nos últimos projetos. O shader recebe uma cor interpolada do rasterizador (fragColor) e copia essa cor para o atributo de saída (outColor). Na linha 7, verificamos se o fragmento pertence a um triângulo visto de frente. Se sim, a cor de saída é a própria cor de entrada. Se não, calculamos a média entre as componentes RGB (linha 10), e fazemos com que a cor de saída seja um tom de vermelho usando esse valor médio (linha 11). Tal cálculo não faz parte do modelo de Phong ou sombreamento de Gouraud. É só uma forma de distinguirmos visualmente o que é o lado da frente e o lado de trás de um triângulo. Vamos ao que interessa, que é o shader gouraud.vert. gouraud.vert Para simplificar, implementaremos o modelo de reflexão de Phong para apenas uma fonte de luz direcional. Assim, a equação terá o formato mais simples \\[ \\mathbf{I}=\\kappa_a \\iota_a + \\kappa_d \\iota_{d} (\\hat{\\mathbf{l}} \\cdot \\hat{\\mathbf{n}}) + \\kappa_s \\iota_{s} (\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}})^\\alpha. \\] O conteúdo completo do shader é listado a seguir. Vamos comentá-lo parte por parte. #version 410 layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; // Light properties uniform vec4 lightDirWorldSpace; uniform vec4 Ia, Id, Is; uniform float shininess; // Material properties uniform vec4 Ka, Kd, Ks; out vec4 fragColor; vec4 Phong(vec3 N, vec3 L, vec3 V) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { // vec3 R = normalize(2.0 * dot(N, L) * N - L); vec3 R = reflect(-L, N); V = normalize(V); float angle = max(dot(R, V), 0.0); specular = pow(angle, shininess); } vec4 diffuseColor = Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = Ka * Ia; return ambientColor + diffuseColor + specularColor; } void main() { vec3 P = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 N = normalMatrix * inNormal; vec3 L = -(viewMatrix * lightDirWorldSpace).xyz; vec3 V = -P; fragColor = Phong(N, L, V); gl_Position = projMatrix * vec4(P, 1.0); } A entrada é um atributo de posição e um atributo de vetor normal unitário. Ambos estão em coordenadas do espaço do objeto (coordenadas do VBO): layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; A saída é um atributo de cor: out vec4 fragColor; O trecho a seguir contém a definição das variáveis uniformes: uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; // Light properties uniform vec4 lightDirWorldSpace; uniform vec4 Ia, Id, Is; // Material properties uniform vec4 Ka, Kd, Ks; uniform float shininess; Entre as variáveis uniformes, temos as matrizes de transformação (modelMatrix até normalMatrix), as constantes de intensidade da fonte de luz (Ia,Id,Is), as constantes de reflexão do material (Ka,Kd,Ks) e o expoente de brilho especular (shininess) que é a constante \\(\\alpha\\) do termo especular da equação. Como a fonte de luz é direcional, a direção da luz é dada pelo vetor lightDirWorldSpace, que está em coordenadas do espaço do mundo (isto é, não precisamos multiplicar pela matriz modelMatrix). Vamos analisar inicialmente o código de main: void main() { vec3 P = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 N = normalMatrix * inNormal; vec3 L = -(viewMatrix * lightDirWorldSpace).xyz; vec3 V = -P; fragColor = Phong(N, L, V); gl_Position = projMatrix * vec4(P, 1.0); } Na linha 46, calculamos P como inPosition transformado para o espaço da câmera. O sufixo .xyz significa que queremos apenas as coordenadas \\(x\\), \\(y\\) e \\(z\\) do vetor (no cálculo da iluminação, não utilizamos a coordenada homogênea). Esse P corresponde ao ponto \\(P\\) do modelo de Phong apresentado na seção 9.2. Na linha 47, N é a normal de vértice inNormal convertida para o espaço da câmera. N corresponde ao vetor \\(\\mathbf{n}\\) do modelo de Phong, mas sem estar normalizado. Na linha 48, L é a direção oposta da direção da luz, convertida para o espaço da câmera. O vetor resultante é o vetor \\(\\mathbf{l}\\) do modelo de Phong. Este vetor também não está normalizado (ainda). Na linha 49, V é o vetor de direção até a câmera, e corresponde ao vetor \\[ \\mathbf{v}=E-P \\] do modelo de Phong, onde \\(E\\) é a posição da câmera, e \\(P\\) é a posição do ponto. Como \\(P\\) (P) está no espaço da câmera, então \\(E=(0,0,0)\\). Isso é assim porque, no espaço da câmera, a posição da câmera é a própria origem. Logo, \\[ \\begin{align} \\mathbf{v}&amp;=\\mathbf{0}-P\\\\ &amp;=-P. \\end{align} \\] Perceba que agora temos os vetores principais (\\(\\mathbf{n}\\), \\(\\mathbf{l}\\) e \\(\\mathbf{v}\\)) necessários para avaliar a equação do modelo de reflexão de Phong. Só ficou faltando o vetor \\(\\mathbf{r}\\) (vetor de reflexão ideal), mas este pode ser obtido a partir de \\(\\mathbf{n}\\) e \\(\\mathbf{l}\\). Importante Os vetores N, L, V, e também o ponto P, estão em um mesmo espaço, que neste caso é o espaço da câmera. Poderíamos ter representado os pontos e vetores em outro espaço, como o espaço do objeto ou o espaço do mundo. Para o modelo de reflexão de Phong, isso não faz diferença. Entretanto, na avaliação da equação, todos os vetores da equação devem estar em um mesmo espaço. Nossa escolha em usar o espaço da câmera é simplesmnte uma conveniência. No espaço da câmera, \\(E=(0,0,0)\\) e assim não precisamos enviar a posição da câmera ao vertex shader como mais uma variável uniforme. Após a definição dos vetores, chamamos uma função Phong que recebe N, L e V, e avalia a equação do modelo de reflexão O resultado é a cor do vértice que será enviada ao rasterizador: fragColor = Phong(N, L, V); Na linha 53, gl_Position é o ponto P transformado do espaço da câmera para o espaço de recorte através da matriz de projeção: gl_Position = projMatrix * vec4(P, 1.0); Vamos agora à definição da função Phong: vec4 Phong(vec3 N, vec3 L, vec3 V) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { // vec3 R = normalize(2.0 * dot(N, L) * N - L); vec3 R = reflect(-L, N); V = normalize(V); float angle = max(dot(R, V), 0.0); specular = pow(angle, shininess); } vec4 diffuseColor = Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = Ka * Ia; return ambientColor + diffuseColor + specularColor; } A função começa com a normalização de N e L para obter \\(\\hat{\\mathbf{n}}\\) e \\(\\hat{\\mathbf{l}}\\): N = normalize(N); L = normalize(L); Em seguida é calculado o cosseno do ângulo entre \\(\\hat{\\mathbf{n}}\\) e \\(\\hat{\\mathbf{l}}\\) (\\(\\hat{\\mathbf{n}} \\cdot \\hat{\\mathbf{l}}\\)), que aqui chamamos de termo lambertiano: // Compute lambertian term float lambertian = max(dot(N, L), 0.0); Só consideramos valores no intervalo \\([0,1]\\). O mínimo é fixado em \\(0\\) através da função max(..., 0.0); o máximo é \\(1\\) porque os vetores são unitários. Após o cálculo do termo lambertiano, temos o cálculo do termo especular: // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { // vec3 R = normalize(2.0 * dot(N, L) * N - L); vec3 R = reflect(-L, N); V = normalize(V); float angle = max(dot(R, V), 0.0); specular = pow(angle, shininess); } Primeiro, note que só calculamos o termo especular se o termo lambertiano for positivo (linha 30). Fazemos isso pois, na equação de renderização, não existe brilho especular caso a luz não incida sobre a superfície. Em outras palavras, não existe brilho especular na sombra. O vetor \\(\\hat{\\mathbf{r}}\\) é calculado na linha 32 usando a função reflect do GLSL. Essa função faz o mesmo que está comentado na linha 31. O vetor \\(\\hat{\\mathbf{v}}\\) é obtido na linha 33 através da normalização de \\(\\mathbf{v}\\). Na linha 34 é calculado o cosseno do ângulo entre \\(\\hat{\\mathbf{r}}\\) e \\(\\hat{\\mathbf{v}}\\) (isto é, \\(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}}\\)), e novamente só são considerados valores no intervalo \\([0,1]\\). Na linha 35, o valor é elevado à constante shininess para obter o valor final \\((\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{v}})^\\alpha\\). No restante do código, as reflexões difusa, especular e ambiente são calculadas usando as intensidades da fonte de luz e termos de reflexão do material. O resultado é somado para obter a cor final: vec4 diffuseColor = Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = Ka * Ia; return ambientColor + diffuseColor + specularColor; Isso conclui o modelo de reflexão de Phong com sombreamento de Gouraud. Phong com sombreamento de Phong phong.vert Quando usamos sombreamento de Phong, precisamos calcular vetores \\(\\mathbf{v}\\), \\(\\mathbf{n}\\) e \\(\\mathbf{l}\\) para cada fragmento, pois a função Phong que utilizamos em gouraud.vert agora será chamada no fragment shader. No sombreamento de Phong, o vertex shader é responsável por calcular os vetores V, L e N e enviá-los ao fragment shader através de atributos de saída fragV, fragL e fragN. Assim, o atributo de saída não será mais uma cor fragColor. O código completo é mostrado a seguir: #version 410 layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; uniform vec4 lightDirWorldSpace; out vec3 fragV; out vec3 fragL; out vec3 fragN; void main() { vec3 P = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 N = normalMatrix * inNormal; vec3 L = -(viewMatrix * lightDirWorldSpace).xyz; fragL = L; fragV = -P; fragN = N; gl_Position = projMatrix * vec4(P, 1.0); } phong.frag O código completo do fragment shader é listado abaixo. Vamos analisá-lo na sequência. #version 410 in vec3 fragN; in vec3 fragL; in vec3 fragV; // Light properties uniform vec4 Ia, Id, Is; // Material properties uniform vec4 Ka, Kd, Ks; uniform float shininess; out vec4 outColor; vec4 Phong(vec3 N, vec3 L, vec3 V) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { // vec3 R = normalize(2.0 * dot(N, L) * N - L); vec3 R = reflect(-L, N); V = normalize(V); float angle = max(dot(R, V), 0.0); specular = pow(angle, shininess); } vec4 diffuseColor = Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = Ka * Ia; return ambientColor + diffuseColor + specularColor; } void main() { vec4 color = Phong(fragN, fragL, fragV); if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } Observe que os atributos de entrada do fragment shader são os atributos de saída do vertex shader: in vec3 fragN; in vec3 fragL; in vec3 fragV; As constantes utilizadas na equação são definidas como variáveis uniformes: // Light properties uniform vec4 Ia, Id, Is; // Material properties uniform vec4 Ka, Kd, Ks; uniform float shininess; A função Phong é exatamente a mesma que utilizamos em gouraud.vert. O resto do código é o código de main: void main() { vec4 color = Phong(fragN, fragL, fragV); if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } Esta função main é praticamente a mesma de gouraud.frag. A diferença é que a cor do fragmento é calculada por Phong. Blinn-Phong com sombreamento de Phong O vertex shader do modelo de BlinnPhong com sombreamento de Phong é exatamente o mesmo de phong.vert, pois os vetores da equação de iluminação são os mesmos. O fragment shader também é praticamente idêntico. A única diferença é que utilizamos uma função BlinnPhong no lugar de Phong. A função é definida a seguir: vec4 BlinnPhong(vec3 N, vec3 L, vec3 V) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { V = normalize(V); vec3 H = normalize(L + V); float angle = max(dot(H, N), 0.0); specular = pow(angle, shininess); } vec4 diffuseColor = Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = Ka * Ia; return ambientColor + diffuseColor + specularColor; } O código também é muito parecido com o da função Phong. A única mudança está na forma de calcular o termo especular. Na linha 27, o vetor halfway \\(\\hat{\\mathbf{h}}\\) (H) é computado como a normalização da soma de \\(\\hat{\\mathbf{l}}\\) (L) e \\(\\hat{\\mathbf{v}}\\) (V). Na linha 28, calcula-se o cosseno do ângulo entre \\(\\hat{\\mathbf{h}}\\) e \\(\\hat{\\mathbf{n}}\\) (isto é, \\(\\hat{\\mathbf{h}} \\cdot \\hat{\\mathbf{n}}\\)), e novamente consideramos apenas os valores no intervalo \\([0,1]\\). O restante do código é o mesmo de Phong. "],["texturing.html", "10 Texturização", " 10 Texturização Texturização, ou mapeamento de textura, é uma técnica de síntese de imagens que consiste em mapear coordenadas de pontos de uma superfície 3D para pontos de um mapa de textura, que geralmente é uma imagem 2D. Um mapa de textura é também chamado simplesmente de textura. Os valores de uma textura podem ser utilizados para modificar as propriedades locais do material em cada ponto da superfície. Com isso, pode-se aumentar o nível de detalhes do objeto renderizado sem precisar aumentar o número de vértices da geometria. A figura 10.1 mostra um exemplo de mapeamento de textura retangular sobre uma esfera unitária que representa a Terra. A esfera é renderizada com o modelo de reflexão de Blinn-Phong e sombreamento de Phong. As cores RGB da textura definem os valores de reflexão difusa do material (\\(\\kappa_d\\)). Uma textura que, como esta, modifica os valores de reflexão difusa, é chamada de textura difusa. Figura 10.1: Mapeamento de textura sobre uma esfera representando a Terra. No mapeamento da figura 10.1, cada ponto \\((x,y,z) \\in \\mathbb{R}^3\\) de uma esfera unitária centralizada na origem e com os polos atravessando o eixo \\(y\\) corresponde a um ponto \\((u,v) \\in \\mathbb{R}^2\\) do espaço de textura. O mapeamento realizado é um mapeamento esférico, definido pelas equações \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(x, z\\right)}{2\\pi}+0.5,\\\\ v&amp;=\\frac{\\arcsin(y)}{\\pi}+0.5, \\end{align} \\] onde \\(\\textrm{arctan2}(x, z)\\) é uma função que corresponde ao arco tangente de \\(x/z\\), mas que retorna um ângulo no intervalo \\([-\\pi, \\pi]\\) e usa os sinais dos argumentos para determinar o quadrante correto no plano \\(zx\\). O mapeamento esférico é detalhado na seção 10.1 junto com outras formas comuns de mapeamento. A operação de determinar o valor da textura em um ponto \\((u,v)\\) é chamada de amostragem de textura. No OpenGL é possível amostrar texturas nos shaders e configurar o comportamento do amostrador para produzir diferentes efeitos. A menor unidade de uma textura é chamada de texel (texture element). Um texel corresponde a um pixel do mapa da textura. Entretanto, o termo texel é usado no lugar de pixel neste contexto porque um pixel da textura (isto é, um texel) nem sempre corresponde exatamente a um pixel da tela. Durante a amostragem de textura, é comum que um texel seja mapeado para vários pixels do framebuffer, ou que muitos texels sejam mapeados para apenas um pixel. Podemos definir critérios de filtragem de textura para definir o que deve ser feito em cada caso. Os modos de filtragem de textura utilizados no OpenGL são descritos na seção 10.3. As coordenadas \\((u,v)\\) do espaço de textura são chamadas de coordenadas de textura. No OpenGL, a origem das coordenadas do espaço de textura é o canto inferior esquerdo da textura. O canto superior direito tem coordenadas \\((1,1)\\). É possível amostrar uma textura em coordenadas fora do intervalo \\([0,1]\\), mas nesse caso o comportamento será definido de acordo com o modo de empacotamento de textura da API. Os diferentes comportamentos possíveis no OpenGL são descritos na seção 10.2. Nosso foco será no uso de texturas 2D (GL_TEXTURE_2D). Entretanto, o OpenGL também fornece suporte a texturas 1D (GL_TEXTURE_1D) e 3D (GL_TEXTURE_3D). Uma textura 1D só possui uma coordenada \\(u\\) e é composta por uma sequência linear de texels. De forma semelhante, uma textura 3D possui coordenadas \\((u,v,w)\\) e é composta por um volume de texels. "],["texmapping.html", "10.1 Mapeamento", " 10.1 Mapeamento Qualquer função \\(f : \\mathbb{R}^3 \\mapsto \\mathbb{R}^2\\) que mapeia pontos \\((x,y,z)\\) do espaço 3D para pontos \\((u,v)\\) do espaço 2D é uma função de mapeamento de textura. Nesta seção, veremos algumas das funções de mapeamento mais utilizadas: o mapeamento planar, o mapeamento cilíndrico e o mapeamento esférico. Além desses, abordaremos também o mapeamento UV unwrap, também chamado de desdobramento UV, muito utilizado em programas de modelagem 3D e jogos. Para simplificar, vamos considerar nesta seção que as coordenadas de textura estão restritas ao intervalo \\([0,1]\\). Entretanto, isso não é uma limitação. Coordenadas fora desse intervalo podem ser tratadas de acordo com as abordagens descritas na seção 10.2. Mapeamento planar O mapeamento planar consiste em uma projeção linear e paralela dos pontos do espaço 3D para o plano do espaço de textura 2D. Geralmente a projeção é feita na direção de algum eixo principal do espaço 3D. Por exemplo, um mapeamento planar na direção do eixo \\(x\\) pode ser definido como: \\[ \\begin{align} u&amp;=1-z,\\\\ v&amp;=y. \\end{align} \\] Considere o mapa de textura difusa mostrado na figura 10.2. Usando mapeamento planar na direção de \\(x\\), um cubo unitário de \\((0,0,0)\\) a \\((1,1,1)\\) mapeado com essa textura terá a aparência mostrada na figura 10.3. Figura 10.2: Textura difusa com padrão de teste. Figura 10.3: Mapeamento planar na direção x sobre um cubo unitário. Observe como o lado de cima e lado esquerdo do cubo repetem, respectivamente, a cor dos texels com \\(v=1\\) e \\(u=0\\) (a mudança de tom é resultado da iluminação). Isso acontece porque todos os pontos ao longo de uma reta na direção \\(x\\) são mapeados para um mesmo texel. Um mapeamento planar na direção do eixo \\(y\\) pode ser definido como: \\[ \\begin{align} u&amp;=x,\\\\ v&amp;=1-z. \\end{align} \\] A figura 10.4 mostra o resultado desse mapeamento sobre o cubo unitário. Figura 10.4: Mapeamento planar na direção y sobre um cubo unitário. Um mapeamento planar na direção do eixo \\(z\\) pode ser definido como: \\[ \\begin{align} u&amp;=x,\\\\ v&amp;=y. \\end{align} \\] A figura 10.5 mostra o resultado desse mapeamento sobre o cubo unitário. Figura 10.5: Mapeamento planar na direção z sobre um cubo unitário. Os mapeamentos planares nas direções \\(x\\), \\(y\\) e \\(z\\) podem ser combinados para formar um mapeamento triplanar. A ideia consiste em calcular três pares de coordenadas \\((u,v)\\), um para cada mapeamento planar, \\[ (u_x,v_x),\\qquad(u_y,v_y),\\qquad(u_z,v_z), \\] e então combinar o valor dos texels amostrados com essas coordenadas de acordo com um critério. Um critério simples é calcular uma média ponderada dos texels. Os pesos da média podem ser as coordenadas \\((n_x, n_y, n_z)\\) do vetor normal normalizado, em valor absoluto. Por exemplo, se \\(T(u,v)\\) é o texel amostrado na posição \\((u,v)\\) do espaço de textura, a combinação do mapeamento triplanar pode ser calculada como \\[ T(u_x, v_x)|n_x| + T(u_y, v_y)|n_y| + T(u_z, v_z)|n_z|. \\] Na renderização do cubo unitário, o resultado será semelhante ao exibido na figura 10.6. Figura 10.6: Mapeamento triplanar sobre um cubo unitário. Mapeamento cilíndrico No mapeamento cilíndrico, a textura é mapeada de tal forma que, se o objeto renderizado é um cilindro unitário alinhado com o eixo \\(y\\) e com base em \\(y=0\\), o resultado será equivalente a envolver a área lateral do cilindro com a textura. A figura 10.7 ilustra um exemplo com a visão de frente e de trás do cilindro. Observe como os lados esquerdo (\\(u=0\\)) e direito (\\(u=1\\)) da textura se unem na parte de trás do cilindro. Figura 10.7: Mapeamento cilíndrico em um cilindro unitário. Seja \\(\\mathbf{p}=\\begin{bmatrix}p_x &amp; p_y &amp; p_z\\end{bmatrix}^T\\) um ponto do espaço euclidiano. O mapeamento cilíndrico é definido a partir do ângulo \\(\\theta\\) que \\(\\mathbf{p}\\) forma em torno do cilindro alinhado ao eixo \\(y\\), e a elevação \\(p_y\\) em relação à altura do cilindro, como mostra a figura 10.8. Figura 10.8: Geometria do mapeamento cilíndrico. O ângulo \\(\\theta \\in [-\\pi, \\pi]\\) é mapeado para \\(u \\in [0,1]\\). A altura \\(y\\) é mapeada diretamente para \\(v\\), isto é, \\(v=p_y\\). Observe que \\[ \\tan{\\theta}=\\frac{p_x}{p_z}. \\] Logo, \\[ \\theta=\\arctan\\left({\\frac{p_x}{p_z}}\\right). \\] O ângulo é calculado corretamente para \\(p_z&gt;0\\). Entretanto, a imagem da função arco tangente está restrita ao intervalo \\(\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)\\). Para que \\(\\theta\\) seja um ângulo em um intervalo de 360 graus, precisamos ajustar o intervalo da função arco tangente de acordo com o sinal de \\(p_x\\) e \\(p_z\\). Isso pode ser feito através da definição de uma função \\(\\textrm{arctan2}(p_x, p_z)\\), que retorna um ângulo no intervalo \\((-\\pi, \\pi]\\): \\[ \\operatorname{atan2}(p_x, p_z) = \\begin{cases} \\arctan\\left(\\dfrac{p_x}{p_z}\\right) &amp;\\text{se } p_z &gt; 0, \\\\ \\arctan\\left(\\dfrac{p_x}{p_z}\\right) + \\pi &amp;\\text{se } p_z &lt; 0 \\text{ e } p_x \\ge 0, \\\\ \\arctan\\left(\\dfrac{p_x}{p_z}\\right) - \\pi &amp;\\text{se } p_z &lt; 0 \\text{ e } p_x &lt; 0, \\\\ +\\dfrac{\\pi}{2} &amp;\\text{se } p_z = 0 \\text{ e } p_x &gt; 0, \\\\ -\\dfrac{\\pi}{2} &amp;\\text{se } p_z = 0 \\text{ e } p_x &lt; 0, \\\\ \\text{indefinido} &amp;\\text{se } p_z = 0 \\text{ e } p_x = 0. \\end{cases} \\] A função \\(\\operatorname{atan2}\\) está presente nas bibliotecas matemáticas das principais linguagens de programação. Por exemplo, na biblioteca padrão do C++, a função é implementada por std::atan2. Em GLSL, a função é atan e recebe dois parâmetros (a função com um parâmetro é a função arco tangente convencional). A conversão de \\(\\theta \\in (-\\pi, \\pi]\\) para \\(u \\in (0,1]\\) é obtida com o mapeamento linear: \\[ u = \\frac{\\theta}{2\\pi}+0.5. \\] Logo, o mapeamento cilíndrico é definido como \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5,\\\\ v&amp;=p_y, \\end{align} \\] Mapeamento esférico No mapeamento esférico, a textura é mapeada de tal forma que, se o objeto renderizado é uma esfera centralizada na origem, o resultado será equivalente a envolver a esfera fazendo com que \\(u\\) e \\(v\\) sejam, respectivamente, a longitude e a latitude, como em uma projeção cilíndrica equidistante usada em cartografia: Os texels da linha \\(v=0\\) e \\(v=1\\) serão mapeados, respectivamente, para o polo sul e polo norte da esfera no eixo \\(y\\); Os texels com \\(v=0.5\\) serão mapeados para o equador da esfera no plano \\(y=0\\); Os texels com \\(u=0\\) e \\(u=1\\) serão mapeados para o meridiano central no plano \\(yz\\) com \\(z&lt;0\\). A figura 10.9 mostra um exemplo de esfera texturizada com mapeamento esférico, visto de frente e de cima (polo norte). Figura 10.9: Mapeamento esférico em uma esfera unitária. Seja \\(\\mathbf{p}=\\begin{bmatrix}p_x &amp; p_y &amp; p_z\\end{bmatrix}^T\\) um ponto do espaço euclidiano. O mapeamento cilíndrico é definido a partir do ângulo \\(\\theta\\) (longitude) e \\(\\phi\\) (latitude) da esfera que contém \\(\\mathbf{p}\\), como mostra a geometria da figura 10.10. Figura 10.10: Geometria do mapeamento esférico. A coordenada \\(u\\) é calculada como no mapeamento cilíndrico: \\[ u=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5. \\] Para determinar \\(v\\), observe, na figura 10.10, que \\[ \\begin{align} \\sin \\phi = \\frac{p_y}{|\\mathbf{p}|}.\\\\ \\end{align} \\] Logo, \\[ \\phi = \\arcsin \\left( \\frac{p_y}{|\\mathbf{p}|} \\right), \\] onde \\[ |\\mathbf{p}|=\\sqrt{x^2+y^2+z^2}. \\] A conversão de \\(\\phi \\in \\left[-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right]\\) para \\(v \\in (0,1]\\) é obtida com o mapeamento linear: \\[ v = \\frac{\\phi}{\\pi}+0.5. \\] Logo, o mapeamento esférico é definido como \\[ \\begin{align} u&amp;=\\frac{\\textrm{arctan2}\\left(p_x, p_z\\right)}{2\\pi}+0.5,\\\\ v&amp;=\\frac{\\arcsin\\left(\\dfrac{p_y}{|\\mathbf{p}|}\\right)}{\\pi}+0.5, \\end{align} \\] Mapeamento UV unwrap No lugar de usar as funções anteriores de mapeamento de textura, podemos definir diretamente quais são as coordenadas \\((u,v)\\) de cada vértice da malha geométrica. Isso pode ser feito através da inclusão de um atributo adicional de vértice, de forma semelhante como fizemos para a definição de cores nos vértices ou normais de vértices. Durante a rasterização, as coordenadas de textura definidas nos vértices são interpoladas para cada fragmento da primitiva. Assim, cada fragmento terá coordenadas de textura interpoladas e a textura pode então ser amostrada no fragment shader. Para determinar as coordenadas \\((u,v)\\) de cada vértice de uma malha poligonal, técnicas de modelagem geométrica podem ser utilizadas para desdobrar a malha sobre o plano da textura, em um processo chamado de UV unwrap ou desdobramento UV. A figura 10.11 mostra um exemplo de objeto 3D renderizado com mapeamento UV unwrap. O objeto é uma lâmpada à óleo romana do acervo do Museu Britânico. O modelo original tem 500 mil triângulos e foi adquirido por um scanner 3D. Aqui, o modelo exibido tem apenas 10 mil triângulos para facilitar a visualização do mapeamento no sistema de coordenadas de textura. Observe, no mapa UV, como a malha é recortada em diferentes pedaços para ser desdobrada sem sobreposições sobre o plano. O mapeamento procura manter a proporção entre a área dos polígonos originais de modo que a densidade dos texels amostrados na superfície seja a mais uniforme possível. Figura 10.11: Mapeamento UV. Como referência, a figura 10.12 mostra o mapa de textura utilizado e o objeto sem texturização. Figura 10.12: Mapa de textura da lâmpada romana, e objeto sem textura. O desdobramento UV é a técnica mais utilizada de mapeamento de texturas em programas de modelagem e renderização, como o Blender. O formato OBJ suporta modelos com coordenadas de textura por vértice definidas através de desdobramento UV. "],["texwrapping.html", "10.2 Empacotamento", " 10.2 Empacotamento No OpenGL, o modo de empacotamento da textura (em inglês, texture wrapping) define o comportamento do amostrador de textura quando as coordenadas \\((u,v)\\) estão fora do intervalo \\([0, 1]\\). Há três comportamentos principais: GL_REPEAT: repete a textura fora do intervalo. Esse é o modo padrão de empacotamento. GL_MIRRORED_REPEAT: igual ao anterior, mas a textura é espelhada em \\(u\\) e/ou \\(v\\) quando a parte inteira da coordenada é um número ímpar. GL_CLAMP_TO_EDGE: Fixa as coordenadas no intervalo \\([0, 1]\\). O resultado é a repetição dos valores das primeiras e últimas linhas/colunas da textura. A figura 10.13 mostra o resultado dos diferentes modos de empacotamento no intervalo de \\((-1,-1)\\) a \\((2,2)\\) no espaço de textura. Figura 10.13: Modos de empacotamento do OpenGL. O modo de repetição (GL_REPEAT) é frequentemente utilizado para produzir padrões formados por texturas ininterruptas (seamless textures). Essas texturas, quando dispostas lado a lado ou como um ladrilho, formam um padrão contínuo. Um exemplo é mostrado na figura 10.14. Figura 10.14: Textura ininterrupta com empacotamento GL_REPEAT (adaptado do original). O modo de empacotamento é configurado com a função glTexParameteri. É possível configurar um comportamento diferente para a direção \\(u\\) (com GL_TEXTURE_WRAP_S) e direção \\(v\\) (com GL_TEXTURE_WRAP_T). Por exemplo, o código a seguir habilita o modo de repetição em \\(u\\) e o modo de repetição espelhada em \\(v\\): glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_MIRRORED_REPEAT); Em texturas 3D, é possível usar ainda a direção \\(w\\) (com GL_TEXTURE_WRAP_R). Outros modos de empacotamento estão disponíveis além dos modos GL_REPEAT, GL_MIRRORED_REPEAT e GL_CLAMP_TO_EDGE, mas não são suportados em todas as especificações OpenGL. Por exemplo, o modo GL_CLAMP_TO_BORDER, que usa uma cor sólida para texels fora do intervalo \\([0,1]\\), não é suportado no OpenGL ES utilizado no WebGL. "],["texfiltering.html", "10.3 Filtragem", " 10.3 Filtragem Na amostragem de texturas, raramente temos um mapeamento \\(1:1\\) entre texels e pixels. Geralmente, cada pixel corresponde a vários texels, ou um mesmo texel pode ser mapeado a vários pixels. Tanto o problema de minificação (mais de um texel mapeado para um único pixel) quanto de magnificação (mais de um pixel mapeado para um mesmo texel) exigem o uso de filtros de interpolação para calcular a cor correspondente a uma posição \\((u,v)\\) no espaço da textura. No OpenGL, o funcionamento desses filtros pode ser configurado de forma independente usando os identificadores GL_TEXTURE_MAG_FILTER e GL_TEXTURE_MIN_FILTER com a função glTexParameteri. Magnificação Os filtros de magnificação (GL_TEXTURE_MAG_FILTER) são dois: Interpolação por vizinho mais próximo (GL_NEAREST): consiste em usar o valor do texel que está mais próximo da posição \\((u,v)\\) de amostragem, considerando a menor distância Manhattan entre os quatro texels mais próximos. Assim, os texels da textura aumentada têm um aspecto pixelado. Esse modo pode ser habilitado com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); Interpolação bilinear (GL_LINEAR): consiste em realizar uma média entre os quatro texels mais próximos da posição \\((u, v)\\) de amostragem. Isso é feito calculando a interpolação linear entre dois pares de texels em uma direção (por exemplo, direção \\(u\\)), e calculando em seguida a interpolação linear dos dois valores resultantes na outra direção (por exemplo, direção \\(v\\)). Essa é a filtragem padrão do OpenGL. A interpolação bilinear é ativada com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); A figura 10.15 mostra a comparação entre os dois filtros em um mesmo detalhe ampliado de textura. Figura 10.15: Exemplo de uso de filtros de magnificação do OpenGL. Minificação Os filtros de minificação do OpenGL (GL_TEXTURE_MIN_FILTER) também incluem os filtros de interpolação por vizinho mais próximo (GL_NEAREST) e interpolação bilinear (GL_LINEAR), e podem ser ativados respectivamente com glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); e glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); Entretanto, na minificação, é possível que mais do que quatro texels sejam mapeados para um mesmo pixel. Em tais casos, a filtragem bilinear usando somente os quatro vizinhos mais próximos não é suficiente, e erros de aliasing podem ser introduzidos. Uma forma eficiente de reduzir esse problema é através da técnica de mipmapping. Mipmapping Um mipmap é uma sequência ordenada de texturas repetidas, cada uma com metade da resolução da textura anterior. O nível base ou nível 0 do mipmap é a textura em sua resolução original. A textura do nível seguinte contém metade da resolução da textura do nível anterior. No último nível do mipmap, a textura tem tamanho \\(1 \\times 1\\). A figura 10.16 mostra todos os níveis de um mipmap para uma textura de resolução \\(256 \\times 256\\). Figura 10.16: Níveis de mipmap. Se a textura é quadrada e a resolução é uma potência de dois, um texel de um nível do mipmap é definido de tal forma que seu valor é exatamente a média entre quatro texels do nível anterior. Isso pode ser visto na figura 10.17, que mostra o detalhe ampliado dos três últimos níveis do mipmap da figura 10.16. Observe, no nível 8, que a cor do único texel resultante é cinza. Essa cor cinza é a média de todos os texels do nível anterior (nível 7), que é o mesmo que a média de todos os texels da textura. De forma semelhante, no nível 7, o texel mais escuro (canto inferior esquerdo) é a média dos texels de \\((0,0)\\) até \\((0.5, 0.5)\\) dos níveis anteriores. Figura 10.17: Detalhe ampliado dos três últimos níveis de um mipmap. A figura 10.18 mostra uma comparação da qualidade de renderização de um padrão de xadrez sobre um plano, sem e com mipmapping. A renderização sem mapping introduz artefatos de aliasing espacial resultantes da subamostragem. Figura 10.18: Comparação de renderização sem mipmapping (esquerda) e com mipmapping (direita) (fonte). No OpenGL, um mipmap pode ser gerado facilmente com a função glGenerateMipmap. Quando a função é chamada, o mipmap é gerado para a textura atualmente ligada com glBindTexture. Por exemplo, glBindTexture(GL_TEXTURE_2D, textureID); glGenerateMipmap(GL_TEXTURE_2D); gera o mipmap para a textura com identificador textureID. Mipmapping é a técnica de filtrar texels usando mipmaps. Quando uma textura com mipmap é amostrada, o amostrador primeiro determina qual nível do mipmap será utilizado. Isso é calculado com base na estimativa do número de texels mapeados para o pixel. Por exemplo, se a área do texel projetado no espaço da janela é tal que todos os texels da textura cabem em um único pixel, então a textura amostrada é a textura do último nível (textura \\(1 \\times 1\\)), pois o texel dessa textura contém a média de todos os texels. No OpenGL, há diferentes modos de filtragem usando mipmapping (válido apenas para GL_TEXTURE_MIN_FILTER): GL_NEAREST_MIPMAP_NEAREST: escolhe o nível de mipmap que mais se aproxima do tamanho do pixel que está sendo texturizado, e então amostra tal nível usando a interpolação por vizinho mais próximo. GL_LINEAR_MIPMAP_NEAREST: escolhe o nível de mipmap que mais se aproxima do tamanho do pixel que está sendo texturizado, e então amostra tal nível usando a interpolação bilinear (média dos quatro texels mais próximos). GL_NEAREST_MIPMAP_LINEAR: escolhe os dois níveis de mipmap que mais se aproximam do tamanho do pixel que está sendo texturizado, e então amostra os dois níveis usando a interpolação por vizinho mais próximo. O valor final é então calculado como a média entre os dois valores amostrados, ponderada de acordo com a proximidade dos níveis de mipmap ao tamanho do pixel. Esse é o modo padrão de filtro de minificação. GL_LINEAR_MIPMAP_LINEAR: escolhe os dois níveis de mipmap que mais se aproximam do tamanho do pixel que está sendo texturizado, e então amostra os dois níveis usando interpolação bilinear. O valor final é então calculado como a média entre os dois valores amostrados, ponderada de acordo com a proximidade dos níveis de mipmap ao tamanho do pixel. O modo GL_LINEAR_MIPMAP_LINEAR também é chamado de interpolação trilinear, pois faz uma interpolação linear sobre dois valores obtidos com interpolação bilinear. Esse é o modo de filtragem que produz resultados com menos artefatos de aliasing, mas também é o mais custoso. "],["viewer4.html", "10.4 Texturização na prática", " 10.4 Texturização na prática Nesta seção, daremos continuidade ao projeto do visualizador de modelos 3D apresentado na seção 9.6. Esta será a versão 4 do visualizador (viewer4) e terá um shader que usa uma textura para modificar as propriedades de reflexão difusa (\\(\\kappa_d\\)) e ambiente (\\(\\kappa_a\\)) do material utilizado no modelo de reflexão de BlinnPhong. Se o objeto lido do arquivo OBJ já vier com coordenadas de textura definidas em seus vértices (mapeamento UV unwrap), o visualizador usará essas coordenadas para amostrar a textura. Entretanto, também poderemos selecionar, através da interface da ImGui, um mapeamento pré-definido: triplanar, cilíndrico ou esférico. Nesta nova versão, o botão Load 3D Model será transformado em um item do menu File. Há também uma opção de menu para carregar uma textura como um arquivo de imagem no formato PNG ou JPEG. O resultado ficará como a seguir: Como o código dessa versão contém apenas mudanças incrementais em relação ao anterior, nosso foco será apenas nessas mudanças. Baixe o código completo deste link. Carregando texturas Para carregar uma textura no OpenGL, primeiro devemos chamar a função glGenTextures para criar um recurso de textura. Por exemplo, para criar apenas uma textura, podemos fazer glGenTextures(1, &amp;textureID); onde textureID é uma variável do tipo GLuint que será preenchida com o identificador do recurso de textura criado pelo OpenGL. Em seguida, devemos ligar o recurso de textura a um alvo de textura, que é GL_TEXTURE_2D para texturas 2D: glBindTexture(GL_TEXTURE_2D, textureID); Neste momento, a textura ainda está vazia. Para definir seu conteúdo, devemos ter um mapa de bits que descreve o conteúdo de uma imagem. Podemos carregar o conteúdo do mapa de bits através da função glTexImage2D. Por exemplo, considere o código a seguir: glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, 800, 600, 0, GL_RGBA, GL_UNSIGNED_BYTE, pixels); glTexImage2D copia o mapa de bits contido no ponteiro pixels, supondo que o mapa é um arranjo de \\(800 \\times 600\\) pixels. A função considera que cada pixel é uma tupla de valores RGBA (GL_RGBA), e que cada componente de cor é um byte sem sinal (GL_UNSIGNED_BYTE). Não é necessário entrarmos em detalhes sobre os diferentes parâmetros de glTexImage2D, pois usaremos a função abcg::opengl::loadTexture da ABCg que se encarrega de fazer todo o procedimento de criação de textura a partir de um arquivo de imagem. Essa função é definida em abcg/abcg_image.cpp e tem a seguinte assinatura: [[nodiscard]] GLuint loadTexture(std::string_view path, bool generateMipmaps = true); Internamente, essa função usa funções da SDL para converter os dados do arquivo de imagem em um mapa de bits, e então chama as funções do OpenGL para criar o identificador de textura. Assim, para criar uma textura a partir de um arquivo imagem.png, podemos fazer simplesmente: textureID = abcg::opengl::loadTexture(&quot;imagem.png&quot;); Por padrão, abcg::opengl::loadTexture cria também o mipmap da textura e usa o filtro de minificação GL_LINEAR_MIPMAP_LINEAR. Se quisermos que o mipmap não seja gerado, basta usarmos false como segundo argumento: textureID = abcg::opengl::loadTexture(&quot;imagem.png&quot;, false); Para destruir a textura e liberar seus recursos, devemos chamar manualmente glDeleteTextures. Por exemplo, o seguinte código libera a textura textureID criada com abcg::opengl::loadTexture. glDeleteTextures(1, &amp;textureID); Nessa nova versão do visualizador, a classe Model implementa a função membro Model::loadDiffuseTexture, que carrega um arquivo de imagem e gera um identificador de recurso de textura difusa na variável m_diffuseTexture. A função é definida como a seguir (em model.cpp): void Model::loadDiffuseTexture(std::string_view path) { if (!std::filesystem::exists(path)) return; abcg::glDeleteTextures(1, &amp;m_diffuseTexture); m_diffuseTexture = abcg::opengl::loadTexture(path); } Essa função recebe em path o caminho contendo o nome do arquivo de imagem PNG ou JPEG. Se o arquivo não existir, a função retorna sem fazer nada (linha 78). Caso contrário, o recurso de textura anterior é liberado e abcg::opengl::loadTexture é chamada para criar a nova textura. Carregando modelos com textura Um arquivo OBJ pode vir acompanhado de um arquivo .mtl opcional que contém a descrição das propriedades dos materiais de cada objeto31. Por exemplo, o arquivo roman_lamp.obj (lâmpada romana da figura 10.11) vem acompanhado do arquivo roman_lamp.mtl que tem o seguinte conteúdo: newmtl roman_lamp Ns 25.0000 Ni 1.5000 Tr 0.0000 Tf 1.0000 1.0000 1.0000 illum 2 Ka 0.2000 0.2000 0.2000 Kd 1.0000 1.0000 1.0000 Ks 0.6000 0.6000 0.6000 Ke 0.0000 0.0000 0.0000 map_Ka maps/roman_lamp_diffuse.jpg map_Kd maps/roman_lamp_diffuse.jpg map_bump maps/roman_lamp_normal.jpg bump maps/roman_lamp_normal.jpg Entre outras coisas, o arquivo especifica o parâmetro de brilho especular (Ns) do material e as propriedades de reflexão ambiente (Ka), difusa (Kd) e especular (Ks). Além disso, o arquivo contém o nome do mapa de textura que deve ser utilizado para modificar a reflexão ambiente (map_Ka) e reflexão difusa (map_Kd). Há outras propriedades, mas elas não serão utilizadas no momento. Nossa implementação de Model::loadObj, que utiliza funções da TinyObjLoader, carrega automaticamente a textura difusa, se houver. A definição completa dessa versão atualizada de Model::loadObj é mostrada seguir (arquivo model.cpp). void Model::loadObj(std::string_view path, bool standardize) { auto basePath{std::filesystem::path{path}.parent_path().string() + &quot;/&quot;}; tinyobj::ObjReaderConfig readerConfig; readerConfig.mtl_search_path = basePath; // Path to material files tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data(), readerConfig)) { if (!reader.Error().empty()) { throw abcg::Exception{abcg::Exception::Runtime( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error()))}; } throw abcg::Exception{ abcg::Exception::Runtime(fmt::format(&quot;Failed to load model {}&quot;, path))}; } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } const auto&amp; attrib{reader.GetAttrib()}; const auto&amp; shapes{reader.GetShapes()}; const auto&amp; materials{reader.GetMaterials()}; m_vertices.clear(); m_indices.clear(); m_hasNormals = false; m_hasTexCoords = false; // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (const auto&amp; shape : shapes) { // Loop over indices for (const auto offset : iter::range(shape.mesh.indices.size())) { // Access to vertex const tinyobj::index_t index{shape.mesh.indices.at(offset)}; // Vertex position const int startIndex{3 * index.vertex_index}; const float vx{attrib.vertices.at(startIndex + 0)}; const float vy{attrib.vertices.at(startIndex + 1)}; const float vz{attrib.vertices.at(startIndex + 2)}; // Vertex normal float nx{}; float ny{}; float nz{}; if (index.normal_index &gt;= 0) { m_hasNormals = true; const int normalStartIndex{3 * index.normal_index}; nx = attrib.normals.at(normalStartIndex + 0); ny = attrib.normals.at(normalStartIndex + 1); nz = attrib.normals.at(normalStartIndex + 2); } // Vertex texture coordinates float tu{}; float tv{}; if (index.texcoord_index &gt;= 0) { m_hasTexCoords = true; const int texCoordsStartIndex{2 * index.texcoord_index}; tu = attrib.texcoords.at(texCoordsStartIndex + 0); tv = attrib.texcoords.at(texCoordsStartIndex + 1); } Vertex vertex{}; vertex.position = {vx, vy, vz}; vertex.normal = {nx, ny, nz}; vertex.texCoord = {tu, tv}; // If hash doesn&#39;t contain this vertex if (hash.count(vertex) == 0) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } // Use properties of first material, if available if (!materials.empty()) { const auto&amp; mat{materials.at(0)}; // First material m_Ka = glm::vec4(mat.ambient[0], mat.ambient[1], mat.ambient[2], 1); m_Kd = glm::vec4(mat.diffuse[0], mat.diffuse[1], mat.diffuse[2], 1); m_Ks = glm::vec4(mat.specular[0], mat.specular[1], mat.specular[2], 1); m_shininess = mat.shininess; if (!mat.diffuse_texname.empty()) loadDiffuseTexture(basePath + mat.diffuse_texname); } else { // Default values m_Ka = {0.1f, 0.1f, 0.1f, 1.0f}; m_Kd = {0.7f, 0.7f, 0.7f, 1.0f}; m_Ks = {1.0f, 1.0f, 1.0f, 1.0f}; m_shininess = 25.0f; } if (standardize) { this-&gt;standardize(); } if (!m_hasNormals) { computeNormals(); } createBuffers(); } Note que, logo no início da função, definimos um objeto readerConfig (linha 87) que é utilizado como argumento de ObjReader::ParseFromFile da TinyObjLoader (linha 92) para informar o diretório onde estão os arquivos de materiais. Por padrão, esse caminho é o mesmo diretório onde está o arquivo OBJ: void Model::loadObj(std::string_view path, bool standardize) { auto basePath{std::filesystem::path{path}.parent_path().string() + &quot;/&quot;}; tinyobj::ObjReaderConfig readerConfig; readerConfig.mtl_search_path = basePath; // Path to material files tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data(), readerConfig)) { if (!reader.Error().empty()) { throw abcg::Exception{abcg::Exception::Runtime( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error()))}; } throw abcg::Exception{ abcg::Exception::Runtime(fmt::format(&quot;Failed to load model {}&quot;, path))}; } Nas linhas 170 a 186, as propriedades do primeiro material são utilizadas. Se não houver nenhum material, valores padrão são utilizados: // Use properties of first material, if available if (!materials.empty()) { const auto&amp; mat{materials.at(0)}; // First material m_Ka = glm::vec4(mat.ambient[0], mat.ambient[1], mat.ambient[2], 1); m_Kd = glm::vec4(mat.diffuse[0], mat.diffuse[1], mat.diffuse[2], 1); m_Ks = glm::vec4(mat.specular[0], mat.specular[1], mat.specular[2], 1); m_shininess = mat.shininess; if (!mat.diffuse_texname.empty()) loadDiffuseTexture(basePath + mat.diffuse_texname); } else { // Default values m_Ka = {0.1f, 0.1f, 0.1f, 1.0f}; m_Kd = {0.7f, 0.7f, 0.7f, 1.0f}; m_Ks = {1.0f, 1.0f, 1.0f, 1.0f}; m_shininess = 25.0f; } Durante a leitura dos atributos dos vértices, Model::loadObj também verifica se a malha contém coordenadas de textura. Isso é feito nas linhas 143 a 151: // Vertex texture coordinates float tu{}; float tv{}; if (index.texcoord_index &gt;= 0) { m_hasTexCoords = true; const int texCoordsStartIndex{2 * index.texcoord_index}; tu = attrib.texcoords.at(texCoordsStartIndex + 0); tv = attrib.texcoords.at(texCoordsStartIndex + 1); } Se o vértice contém coordenadas de textura (linha 146), o flag m_hasTexCoords é definido como true e as coordenadas são carregadas em tu e tv. Se o arquivo OBJ não tiver coordenadas de textura, tu e tv serão zero para todos os vértices. A estrutura Vertex é criada com o novo atributo de coordenadas de textura: Vertex vertex{}; vertex.position = {vx, vy, vz}; vertex.normal = {nx, ny, nz}; vertex.texCoord = {tu, tv}; texCoord é um novo atributo de Vertex (um glm::vec2), criado de forma semelhante ao modo como criamos o atributo normal no projeto viewer2 (seção 9.5), isto é, usamos o atributo como chave de hashing e carregamos seus dados no formato de um VBO de dados intercalados. Renderizando Em Model::render (chamado em OpenGLWindow::paintGL), incluímos a ativação da textura no pipeline. A definição completa ficará como a seguir: void Model::render(int numTriangles) const { abcg::glBindVertexArray(m_VAO); abcg::glActiveTexture(GL_TEXTURE0); abcg::glBindTexture(GL_TEXTURE_2D, m_diffuseTexture); // Set minification and magnification parameters abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); // Set texture wrapping parameters abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); abcg::glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); const auto numIndices{(numTriangles &lt; 0) ? m_indices.size() : numTriangles * 3}; abcg::glDrawElements(GL_TRIANGLES, static_cast&lt;GLsizei&gt;(numIndices), GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); } Na linha 202, a função glActiveTexture é chamada para ativar a primeira unidade de textura (GL_TEXTURE0). O número de unidades ativas corresponde ao número de texturas diferentes que poderão ser utilizadas ao mesmo tempo na execução do programa de shader. Podemos ativar pelo menos 80 unidades de textura no OpenGL para desktop, e 32 no OpenGL ES (usado pelo WebGL), mas o número pode ser maior dependendo da implementação do driver. Como queremos manter as coisas simples, nosso visualizador por enquanto só utilizará uma unidade de textura. Nas próximas versões veremos como usar mais unidades. Após a ativação da unidade de textura, a função glBindTexture é chamada para ligar o identificar de textura à unidade recém ativada. Nas linha 205 a 211, a função glTexParameteri é chamada para configurar os filtros de textura e modos de empacotamento. Os valores aqui utilizados são os valores padrão do OpenGL. Isso é tudo para configurar a texturização. O restante agora é feito nos shaders. O projeto viewer4 define dois novos shaders com suporte a texturas: texture.vert e texture.frag. Esses shaders são bem similares aos shaders do modelo de BlinnPhong. Observação Para o conteúdo de assets ficar mais organizado, a partir desta versão do visualizador, as texturas ficarão armazenadas em assets/maps, e os shaders ficarão armazenados em assets/shaders. Os arquivos .obj continuam na pasta assets, agora junto também com os arquivos .mtl. texture.vert O código completo do shader é mostrado a seguir: #version 410 layout(location = 0) in vec3 inPosition; layout(location = 1) in vec3 inNormal; layout(location = 2) in vec2 inTexCoord; uniform mat4 modelMatrix; uniform mat4 viewMatrix; uniform mat4 projMatrix; uniform mat3 normalMatrix; uniform vec4 lightDirWorldSpace; out vec3 fragV; out vec3 fragL; out vec3 fragN; out vec2 fragTexCoord; out vec3 fragPObj; out vec3 fragNObj; void main() { vec3 P = (viewMatrix * modelMatrix * vec4(inPosition, 1.0)).xyz; vec3 N = normalMatrix * inNormal; vec3 L = -(viewMatrix * lightDirWorldSpace).xyz; fragL = L; fragV = -P; fragN = N; fragTexCoord = inTexCoord; fragPObj = inPosition; fragNObj = inNormal; gl_Position = projMatrix * vec4(P, 1.0); } Esse código contém algumas poucas modificações em relação ao conteúdo de blinnphong.vert. Observe como agora temos o atributo de entrada inTexCoord (linha 5) contendo as coordenadas de textura lidas do arquivo OBJ. O vertex shader não faz nenhum processamento com as coordenadas de textura e simplesmente passa-as adiante para o fragment shader através do atributo de saída fragTexCoord (linha 17). O vertex shader também possui dois outros atributos adicionais de saída: fragPObj (linha 18) é a posição do vértice no espaço do objeto (isto é, uma cópia de inPosition); fragNObj (linha 19) é o vetor normal no espaço do objeto (isto é, uma cópia de inNormal). Esses atributos são utilizados para calcular, no fragment shader, as coordenadas de textura do mapeamento triplanar, cilíndrico ou esférico (o vetor normal só é utilizado no mapeamento triplanar). Se o mapeamento utilizado é aquele fornecido pelo arquivo OBJ, isto é, o mapeamento determinado pelas coordenada de textura de inTexCoord, então os atributos fragPObj e fragNObj não são utilizados. texture.frag O código completo do shader é mostrado a seguir. Vamos analisá-lo parte por parte. #version 410 in vec3 fragN; in vec3 fragL; in vec3 fragV; in vec2 fragTexCoord; in vec3 fragPObj; in vec3 fragNObj; // Light properties uniform vec4 Ia, Id, Is; // Material properties uniform vec4 Ka, Kd, Ks; uniform float shininess; // Diffuse texture sampler uniform sampler2D diffuseTex; // Mapping mode // 0: triplanar; 1: cylindrical; 2: spherical; 3: from mesh uniform int mappingMode; out vec4 outColor; // Blinn-Phong reflection model vec4 BlinnPhong(vec3 N, vec3 L, vec3 V, vec2 texCoord) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { V = normalize(V); vec3 H = normalize(L + V); float angle = max(dot(H, N), 0.0); specular = pow(angle, shininess); } vec4 map_Kd = texture(diffuseTex, texCoord); vec4 map_Ka = map_Kd; vec4 diffuseColor = map_Kd * Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = map_Ka * Ka * Ia; return ambientColor + diffuseColor + specularColor; } // Planar mapping vec2 PlanarMappingX(vec3 P) { return vec2(1.0 - P.z, P.y); } vec2 PlanarMappingY(vec3 P) { return vec2(P.x, 1.0 - P.z); } vec2 PlanarMappingZ(vec3 P) { return P.xy; } #define PI 3.14159265358979323846 // Cylindrical mapping vec2 CylindricalMapping(vec3 P) { float longitude = atan(P.x, P.z); float height = P.y; float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = height - 0.5; // Base at y = -0.5 return vec2(u, v); } // Spherical mapping vec2 SphericalMapping(vec3 P) { float longitude = atan(P.x, P.z); float latitude = asin(P.y / length(P)); float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = latitude / PI + 0.5; // From [-pi/2, pi/2] to [0, 1] return vec2(u, v); } void main() { vec4 color; if (mappingMode == 0) { // Triplanar mapping // Sample with x planar mapping vec2 texCoord1 = PlanarMappingX(fragPObj); vec4 color1 = BlinnPhong(fragN, fragL, fragV, texCoord1); // Sample with y planar mapping vec2 texCoord2 = PlanarMappingY(fragPObj); vec4 color2 = BlinnPhong(fragN, fragL, fragV, texCoord2); // Sample with z planar mapping vec2 texCoord3 = PlanarMappingZ(fragPObj); vec4 color3 = BlinnPhong(fragN, fragL, fragV, texCoord3); // Compute average based on normal vec3 weight = abs(normalize(fragNObj)); color = color1 * weight.x + color2 * weight.y + color3 * weight.z; } else { vec2 texCoord; if (mappingMode == 1) { // Cylindrical mapping texCoord = CylindricalMapping(fragPObj); } else if (mappingMode == 2) { // Spherical mapping texCoord = SphericalMapping(fragPObj); } else if (mappingMode == 3) { // From mesh texCoord = fragTexCoord; } color = BlinnPhong(fragN, fragL, fragV, texCoord); } if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } A primeira mudança em relação ao shader blinnphong.frag é o número de atributos de entrada, que agora inclui os atributos fragTexCoord, fragPObj e fragNObj da saída do vertex shader: in vec3 fragN; in vec3 fragL; in vec3 fragV; in vec2 fragTexCoord; in vec3 fragPObj; in vec3 fragNObj; Há também duas novas variáveis uniformes: // Diffuse texture sampler uniform sampler2D diffuseTex; // Mapping mode // 0: triplanar; 1: cylindrical; 2: spherical; 3: from mesh uniform int mappingMode; diffuseTex é um amostrador de textura 2D (sampler2D) que acessa a primeira unidade de textura (GL_TEXTURE0) ativada com glActiveTexture em Model::render. É através desse amostrador que acessamos os texels da textura difusa. mappingMode é um inteiro que identifica o modo de mapeamento escolhido pelo usuário. Esse valor é determinado pelo índice da caixa de combinação UV mapping da ImGui. O padrão é 3 para arquivos OBJ que possuem coordenadas de textura, e 0 quando as coordenadas de textura não foram encontradas. Importante O pipeline associa diffuseTex à unidade de textura GL_TEXTURE0 pois o valor dessa variável uniforme é definido como 0 no código em C++. Isso é feito na linha 108 de OpenGLWindow::paintGL junto com a definição das outras variáveis uniformes: const GLint diffuseTexLoc{abcg::glGetUniformLocation(program, &quot;diffuseTex&quot;)}; const GLint mappingModeLoc{ abcg::glGetUniformLocation(program, &quot;mappingMode&quot;)}; // Set uniform variables used by every scene object abcg::glUniformMatrix4fv(viewMatrixLoc, 1, GL_FALSE, &amp;m_viewMatrix[0][0]); abcg::glUniformMatrix4fv(projMatrixLoc, 1, GL_FALSE, &amp;m_projMatrix[0][0]); abcg::glUniform1i(diffuseTexLoc, 0); abcg::glUniform1i(mappingModeLoc, m_mappingMode); A definição da função BlinnPhong é ligeiramente diferente daquela do shader blinnphong.frag: // Blinn-Phong reflection model vec4 BlinnPhong(vec3 N, vec3 L, vec3 V, vec2 texCoord) { N = normalize(N); L = normalize(L); // Compute lambertian term float lambertian = max(dot(N, L), 0.0); // Compute specular term float specular = 0.0; if (lambertian &gt; 0.0) { V = normalize(V); vec3 H = normalize(L + V); float angle = max(dot(H, N), 0.0); specular = pow(angle, shininess); } vec4 map_Kd = texture(diffuseTex, texCoord); vec4 map_Ka = map_Kd; vec4 diffuseColor = map_Kd * Kd * Id * lambertian; vec4 specularColor = Ks * Is * specular; vec4 ambientColor = map_Ka * Ka * Ia; return ambientColor + diffuseColor + specularColor; } Agora, a função tem o parâmetro adicional de coordenadas de textura texCoord (linha 27). Até a linha 41, o código é igual ao anterior. A linha 43 contém o código utilizado para amostrar a textura difusa. A função texture recebe como argumentos o amostrador de textura (diffuseTex) e as coordenadas de textura (texCoord). O resultado é a cor RGBA amostrada na posição dada, usando o modo de filtragem e modo de empacotamento definidos pelo código C++ antes da renderização. Como estamos amostrando uma textura difusa, a cor da textura (map_Kd) é multiplicada por Kd * Id * lambertian para compor a componente difusa final (linha 46). Também criamos uma cor ambiente map_Ka (linha 44) que multiplica as componentes de reflexão ambiente (linha 48). Nesse caso, consideramos que map_Ka é igual a map_Kd, pois geralmente é esse o caso (a textura difusa é também a textura ambiente). Entretanto, é possível que um material defina uma textura diferente para a componente ambiente. Nesse caso teríamos de mudar o shader para incluir um outro amostrador específico para map_Ka. Além da função BlinnPhong, o shader também define as funções de geração de coordenadas de textura usando mapeamento planar (PlanarMappingX, PlanarMappingY, PlanarMappingZ), cilíndrico (CylindricalMapping) e esférico (SphericalMapping): // Planar mapping vec2 PlanarMappingX(vec3 P) { return vec2(1.0 - P.z, P.y); } vec2 PlanarMappingY(vec3 P) { return vec2(P.x, 1.0 - P.z); } vec2 PlanarMappingZ(vec3 P) { return P.xy; } #define PI 3.14159265358979323846 // Cylindrical mapping vec2 CylindricalMapping(vec3 P) { float longitude = atan(P.x, P.z); float height = P.y; float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = height - 0.5; // Base at y = -0.5 return vec2(u, v); } // Spherical mapping vec2 SphericalMapping(vec3 P) { float longitude = atan(P.x, P.z); float latitude = asin(P.y / length(P)); float u = longitude / (2.0 * PI) + 0.5; // From [-pi, pi] to [0, 1] float v = latitude / PI + 0.5; // From [-pi/2, pi/2] to [0, 1] return vec2(u, v); } Todas as funções recebem como parâmetro a posição do ponto (P) e retornam as coordenadas de textura correspondentes ao mapeamento. O código reproduz as equações descritas na seção 10.1. A única exceção é o cálculo da componente \\(v\\) do mapeamento cilíndrico (linha 66), que aqui é deslocada para fazer com que o cilindro tenha base em \\(-0.5\\) em vez de \\(0\\). Assim, a textura fica centralizada verticalmente em objetos de raio unitário centralizados na origem, que é o nosso caso pois usamos a função Model::standardize após a leitura do arquivo OBJ. As funções de geração de coordenadas de textura são chamadas em main de acordo com o valor de mappingMode: void main() { vec4 color; if (mappingMode == 0) { // Triplanar mapping // Sample with x planar mapping vec2 texCoord1 = PlanarMappingX(fragPObj); vec4 color1 = BlinnPhong(fragN, fragL, fragV, texCoord1); // Sample with y planar mapping vec2 texCoord2 = PlanarMappingY(fragPObj); vec4 color2 = BlinnPhong(fragN, fragL, fragV, texCoord2); // Sample with z planar mapping vec2 texCoord3 = PlanarMappingZ(fragPObj); vec4 color3 = BlinnPhong(fragN, fragL, fragV, texCoord3); // Compute average based on normal vec3 weight = abs(normalize(fragNObj)); color = color1 * weight.x + color2 * weight.y + color3 * weight.z; } else { vec2 texCoord; if (mappingMode == 1) { // Cylindrical mapping texCoord = CylindricalMapping(fragPObj); } else if (mappingMode == 2) { // Spherical mapping texCoord = SphericalMapping(fragPObj); } else if (mappingMode == 3) { // From mesh texCoord = fragTexCoord; } color = BlinnPhong(fragN, fragL, fragV, texCoord); } if (gl_FrontFacing) { outColor = color; } else { float i = (color.r + color.g + color.b) / 3.0; outColor = vec4(i, 0, 0, 1.0); } } Observe, no mapeamento triplanar (linhas 86102), como a textura é amostrada três vezes (linhas 8898) e então combinada em uma média ponderada pelo valor absoluto das componentes do vetor normal (linhas 101102). Observação No mapeamento triplanar, não seria necessário chamar BlinnPhong três vezes, pois a iluminação sem a textura é a mesma nas três chamadas. O código ficaria mais eficiente se BlinnPhong retornasse uma estrutura contendo as componentes ambiente, difusa e especular separadas. Poderíamos então criar uma outra função só para fazer a amostragem da textura difusa e compor a cor final. Se mappingMode é 3, então nenhuma função de mapeamento é chamada e as coordenadas de textura utilizadas em BlinnPhong são aquelas contidas em fragTexCoord, pois essas são as coordenadas de textura interpoladas a partir das coordenadas definidas nos vértices. Isso resume as modificações necessárias para habilitar a texturização. O restante do código contém modificações complementares relacionadas a conceitos que já foram abordados em projetos anteriores, como a mudança da interface da ImGui e a determinação de um ângulo e eixo de rotação inicial para o trackball virtual. Nosso visualizador suporta apenas um objeto por arquivo OBJ e, portanto, suporta apenas um material. Por isso, todos os arquivos OBJ que utilizaremos devem ter apenas um objeto e um material. "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
