[["index.html", "MCTA008-17 Computação Gráfica Apresentação", " MCTA008-17 Computação Gráfica Harlen Batagelo (harlen.batagelo@ufabc.edu.br) Universidade Federal do ABC 3º quadrimestre de 2023 Apresentação Caro/a estudante, Este site contém as notas de aula da disciplina MCTA008-17 Computação Gráfica para o 3º quadrimestre de 2023. O conteúdo foi elaborado originalmente para o ensino remoto dos Quadrimestres Suplementares seguindo o formato de um livro interativo voltado ao estudo autodirigido. Neste quadrimestre ele será usado como material de apoio às aulas presenciais. Os capítulos estão divididos em tópicos teóricos sobre o processo de geração de imagens no computador, e partes práticas de desenvolvimento de aplicações gráficas interativas usando C++ com a API gráfica OpenGL. Para informações sobre o cronograma das atividades e critérios de avaliação, consulte o plano de ensino disponível no Moodle. Bons estudos! — Harlen "],["pré-requisitos.html", "Pré-requisitos", " Pré-requisitos Para acompanhar a disciplina é recomendável ter conhecimento prévio do conteúdo abordado nas disciplinas de Algoritmos e Estruturas de Dados I e Geometria Analítica. As atividades práticas avaliativas serão desenvolvidas na linguagem C++. Embora não seja necessário ter fluência em C++, é recomendável ter proficiência em programação em C e familiaridade com conceitos básicos de programação orientada a objetos. Também é recomendável ter familiaridade com o Git e com o uso de alguma plataforma de hospedagem de repositórios Git tal como o GitHub. Você deverá ser capaz de gerenciar seus próprios repositórios ao longo do quadrimestre. Atividades práticas Para realizar as atividades práticas é necessário ter um computador com sistema operacional 64 bits (Windows 10 ou superior, Linux ou macOS) e placa de vídeo compatível com OpenGL 3.3 ou superior. O OpenGL 3.3 é suportado em placas gráficas da família Nvidia GeForce 400 (2010) ou mais recentes, AMD Radeon HD 5000 (2009) em diante e Intel HD Graphics a partir dos processadores Intel de 7ª geração (2012). Caso a sua placa de vídeo seja de uma geração a partir de 2012, provavelmente ela deve suportar OpenGL 3.3. Se não suportar, há a possibilidade de simular o processamento gráfico em software através do driver Gallium llvmpipe da biblioteca Mesa. Visualizando este site Parte do conteúdo deste site requer um navegador com suporte a WebGL 2.0. Para informações detalhadas sobre o suporte do seu navegador a WebGL 2.0, consulte o WebGL Report. Dica Para garantir a visualização correta do conteúdo WebGL 2.0, utilize a versão mais recente do Google Chrome ou Mozilla Firefox. Além disso, use o navegador em um computador desktop ou laptop. Embora o site funcione em tablets e smartphones, pode ser difícil interagir com o conteúdo WebGL nesses dispositivos. Dependendo das configurações de DPI utilizadas no sistema de janelas, podem ocorrer problemas de redimensionamento dos elementos de interface no Chrome e em navegadores baseados no Chromium, como o Microsoft Edge ou Brave. Por exemplo, o cubo exibido acima pode ser redimensionado e as arestas podem apresentar distorções, parecendo mais serrilhadas que o normal: No Chrome, isso pode ser resolvido iniciando o navegador com a opção /high-dpi-support=1 /force-device-scale-factor=1 na linha de comando, ou então incluindo essas opções no atalho, ou simplesmente ajustando o zoom. "],["config.html", "1 Configuração do ambiente", " 1 Configuração do ambiente Neste capítulo veremos como configurar o ambiente de desenvolvimento para realizar as atividades práticas no computador. Qualquer que seja a plataforma – Linux, macOS ou Windows – será necessário instalar as seguintes ferramentas e bibliotecas conforme as instruções que veremos nas próximas seções: CMake, para automatizar a geração de scripts de compilação e ligação de forma independente de plataforma. Emscripten, para compilar código C++ e gerar binário em WebAssembly de modo a executar as aplicações no navegador. Git, para clonar do GitHub o repositório do SDK do Emscripten e da biblioteca de desenvolvimento que usaremos na disciplina. Também é recomendável usar o Git para o controle de versão das atividades que serão feitas ao longo do quadrimestre. GLEW, para carregamento das funções da API gráfica OpenGL. Simple DirectMedia Layer (SDL) 2.0, para gerenciamento de dispositivos de vídeo, dispositivos de entrada, áudio, entre outros componentes de hardware. SDL_image 2.0, para leitura de arquivos de imagem. Precisaremos também de um compilador recente com suporte a C++20, como o GCC 11, Clang 13, ou MSVC 19. Acompanhe nas seções a seguir o passo a passo da instalação desses recursos de acordo com o sistema operacional utilizado: Seção 1.1 para instalação no Linux; Seção 1.2 para instalação no macOS; Seção 1.3 para instalar no Windows. Não é necessário usar um IDE ou editor específico de código-fonte para o desenvolvimento das atividades. A compilação pode ser disparada através de scripts de linha de comando. Entretanto, como um exemplo, veremos na seção 1.4 como fazer a configuração básica do Visual Studio Code para o desenvolvimento de aplicações C++ com CMake. Na seção 1.5 veremos como instalar o framework (ABCg) criado especialmente para esta disciplina. O framework será utilizado em todas as atividades do curso para facilitar o desenvolvimento das aplicações gráficas. Dica Preferencialmente, configure o ambiente de desenvolvimento no sistema operacional nativo. Entretanto, caso o seu computador tenha recursos de processamento e memória suficientes, é possível configurar todo o ambiente em uma máquina virtual. O VMware Workstation Player (Windows e Linux) e VMWare Fusion Player (macOS) possuem suporte a aceleração gráfica 3D e são adequados para desenvolver as atividades da disciplina. Tanto o VMWare Workstation Player quanto o Fusion Player podem ser utilizados gratuitamente através de uma licença de uso pessoal. O Windows Subsystem for Linux 2 (WSL 2) também suporta aceleração gráfica através do Direct3D 12. É possível configurar o ambiente de desenvolvimento em um container Docker. Entretanto, o suporte a gráficos com aceleração de hardware exige o uso do Linux e do NVIDIA Container Toolkit, que só funciona com GPUs da NVIDIA. "],["linux.html", "1.1 Linux", " 1.1 Linux As ferramentas e bibliotecas necessárias para o desenvolvimento das atividades da disciplina estão disponíveis nos repositórios de pacotes das principais distribuições Linux. A seguir veremos como instalar os pacotes na versão para desktop do Ubuntu 22.04 LTS. Há pacotes equivalentes em outras distribuições. Em um terminal, execute os passos a seguir. Atualize o sistema: sudo apt update &amp;&amp; sudo apt upgrade Instale o pacote build-essential (GCC, GDB, Make, etc): sudo apt install build-essential Instale o CMake e Git: sudo apt install cmake git Instale as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0: sudo apt install libglew-dev libsdl2-dev libsdl2-image-dev Dica É recomendável instalar também as ferramentas Clang-Tidy e ClangFormat. Elas podem ser utilizadas em linha de comando ou no seu editor de código de preferência para fazer a formatação automática e linting de código C++. Instale o Clang-Tidy e ClangFormat com o seguinte comando: sudo apt install clang-tidy clang-format Outra ferramenta recomendada é o Ccache. Ela serve para acelerar a recompilação das atividades através da manutenção de um cache das compilações anteriores: Instale o pacote ccache: sudo apt install ccache Atualize os links simbólicos dos compiladores instalados: sudo /usr/sbin/update-ccache-symlinks Execute o comando a seguir para fazer com que o caminho do Ccache seja prefixado ao PATH sempre que um novo terminal for aberto: echo &#39;export PATH=&quot;/usr/lib/ccache:$PATH&quot;&#39; &gt;&gt; ~/.bashrc Reabra o terminal ou execute source ~/.bashrc. Para testar se o Ccache está ativado, execute o comando which g++. A saída deverá incluir o caminho /usr/lib/ccache/, como a seguir: /usr/lib/ccache/g++ Verificando o OpenGL O suporte ao OpenGL vem integrado no kernel do Linux através dos drivers de código aberto da biblioteca Mesa (drivers Intel/AMD/Nouveau). Para as placas da NVIDIA e AMD há a possibilidade de instalar os drivers proprietários do repositório nonfree (repositório restricted no Ubuntu), ou diretamente dos sites dos fabricantes: AMD ou NVIDIA. Os drivers proprietários, especialmente os da NVIDIA, geralmente têm desempenho superior aos de código aberto. Para verificar a versão do OpenGL suportada pelos drivers instalados, instale primeiro o pacote mesa-utils: sudo apt install mesa-utils Execute o comando: glxinfo | grep version O resultado deverá ser parecido com o seguinte: server glx version string: 1.4 client glx version string: 1.4 GLX version: 1.4 Max core profile version: 4.1 Max compat profile version: 4.1 Max GLES1 profile version: 1.1 Max GLES[23] profile version: 2.0 OpenGL core profile version string: 4.1 (Core Profile) Mesa 21.0.3 OpenGL core profile shading language version string: 4.10 OpenGL version string: 4.1 (Compatibility Profile) Mesa 21.0.3 OpenGL shading language version string: 4.10 OpenGL ES profile version string: OpenGL ES 2.0 Mesa 21.0.3 OpenGL ES profile shading language version string: OpenGL ES GLSL ES 1.0.16 Importante A versão em OpenGL version string ou OpenGL core profile version string deve ser 3.3 ou superior. Caso não seja, instale os drivers proprietários e certifique-se de que sua placa de vídeo tem os requisitos necessários para o OpenGL 3.3. Atualizando o GCC As atividades da disciplina farão uso de um framework de desenvolvimento que exige um compilador com suporte a C++20. Esse requisito é atendido se instalarmos uma versão recente do GCC, como o GCC 11 ou posterior. A versão padrão do GCC no Ubuntu 22.04 LTS é compatível com C++20. Então, se você seguiu as instruções de instalação até aqui usando o Ubuntu 22.04 ou uma versão mais recente, não é necessário atualizar o GCC. Caso sua versão do Ubuntu seja mais antiga que a 22.04, verifique no terminal a saída de g++ --version. Por exemplo, no Ubuntu 20.04 o GCC padrão é o 9.3, e a saída será assim: g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Neste caso é necessário instalar uma versão mais recente. Para isso, adicione primeiro o PPA ubuntu-toolchain-r/test: sudo apt install software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test Agora podemos instalar o GCC 11: sudo apt install gcc-11 g++-11 A instalação do GCC 11 não substitui a versão mais antiga já instalada. Entretanto, é necessário criar links simbólicos de gcc e g++ para a versão mais recente. Uma forma simples de fazer isso é através do comando update-alternatives. Primeiro, execute o comando a seguir para definir um valor de prioridade (neste caso, 100) para o GCC 11: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11 Use o comando a seguir para definir um valor de prioridade mais baixo (por exemplo, 90) para a versão anterior do GCC, que neste exemplo é a versão 9: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 Agora, execute o comando a seguir para escolher qual versão do GCC instalada será utilizada por padrão: sudo update-alternatives --config gcc Na lista de versões instaladas, selecione o GCC 11 caso ainda não esteja selecionado. Isso criará os links simbólicos. Para testar se a versão correta do GCC está sendo utilizada, execute g++ --version novamente. A saída deverá ser parecida com a seguinte: g++ (Ubuntu 11.2.0-1ubuntu1~20.04) 11.2.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Instalando o Emscripten Vá a um diretório onde queira instalar o SDK do Emscripten. Por exemplo, o diretório home: cd Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.45 (ef3e4e3b044de98e1811546e0bc605c65d3412f4) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante O passo 6 precisa ser refeito sempre que um terminal for aberto. Para automatizar o processo, execute o seguinte comando: echo &#39;source &quot;./emsdk/emsdk_env.sh&quot; &gt; /dev/null 2&gt;&amp;1&#39; &gt;&gt; ~/.bashrc Isso adiciona source \"./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1\" ao final de ~/.bashrc, fazendo com que o script emsdk_env.sh seja executado automaticamente toda vez que um novo terminal for aberto (ajuste o caminho caso o Emscripten não esteja em ~/emsdk). "],["macos.html", "1.2 macOS", " 1.2 macOS Em um terminal, execute os passos a seguir: Execute o comando gcc. Se o GCC não estiver instalado, aparecerá uma caixa de diálogo solicitando a instalação das ferramentas de desenvolvimento de linha de comando. Clique em “Install”. Esse procedimento também instalará outras ferramentas, como o Make e Git. Para verificar se o GCC foi instalado, execute gcc --version. A saída deverá ser parecida com a seguinte (note que o GCC é apenas um atalho para o Apple Clang): Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1 Apple clang version 12.0.0 (clang-1200.0.32.28) Target: x86_64-apple-darwin19.6.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin Se o procedimento acima não funcionar (as instruções acima foram testadas no macOS Catalina), baixe o Command Line Tools for Xcode usando sua conta de desenvolvedor do Apple Developer, ou execute xcode-select --version no terminal. Em versões mais antigas do macOS pode ser necessário instalar o Xcode. Para instalar os demais pacotes de bibliotecas e ferramentas, instale o Homebrew com o seguinte comando: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Instale o CMake: brew install cmake Instale o GLEW, SDL 2.0 e SDL_image 2.0: brew install glew brew install sdl2 brew install sdl2_image Está pronto! Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o Ccache usando o Homebrew: brew install ccache Anote a saída de echo $(brew --prefix) (por exemplo, /usr/local). Abra o modo de edição do PATH: sudo nano /etc/paths Insira como primeira linha o caminho $(brew --prefix)/opt/ccache/libexec, onde $(brew --prefix) é a saída do passo 2. Por exemplo, /usr/local/opt/ccache/libexec. Salve (Ctrl+X e Y) e reinicie o terminal. Para testar, digite which gcc. A saída deverá ser um caminho que inclui o Ccache, como a seguir: /usr/local/opt/ccache/libexec/gcc Instalando o Emscripten Vá ao diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.45 (ef3e4e3b044de98e1811546e0bc605c65d3412f4) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha de ~/.zshrc (se estiver usando o shell Zsh no macOS Catalina ou posterior) ou ~/.bashrc (se estiver usando o shell Bash em versões anteriores) para que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e o erro padrão (stderr). "],["windows.html", "1.3 Windows", " 1.3 Windows Para a instalação das ferramentas e bibliotecas de desenvolvimento no Windows, utilizaremos o MSYS2. MSYS2 é um ambiente de terminal tipo Unix com acesso a um repositório de ferramentas e bibliotecas de desenvolvimento de aplicações nativas em Windows através do gerenciador de pacotes pacman. Essas ferramentas e bibliotecas incluem o CMake, Git, GLEW, SDL 2.0 e SDL_image 2.0, entre outras que vamos utilizar na disciplina. O MSYS2 também permite instalar o MinGW-W64. Com isso conseguiremos usar o compilador GCC para gerar binários nativos para o Windows 64 bits e poderemos também usar o GDB para depurar nossos programas. Siga os passos a seguir para instalar o MSYS2 e as ferramentas/bibliotecas de desenvolvimento: Baixe o instalador de https://www.msys2.org, execute-o e siga as instruções de instalação. Abra o shell do MSYS2 (aplicativo “MSYS2 MSYS” no menu Iniciar) e execute o seguinte comando: pacman -S git mingw-w64-x86_64-ccache mingw-w64-x86_64-cmake mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb mingw-w64-x86_64-ninja mingw-w64-x86_64-glew mingw-w64-x86_64-SDL2 mingw-w64-x86_64-SDL2_image Isso instalará as ferramentas Git, Ccache, CMake, Ninja (Ninja substitui o GNU Make no Windows), GCC e GDB (do MinGW-W64), e as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0. Opcionalmente, instale o pacote mingw-w64-x86_64-glslang com o comando a seguir. Isso instalará a ferramenta glslangValidator que poderá ser usada como linter de código na linguagem GLSL (linguagem de shader que será utilizada na disciplina): pacman -S mingw-w64-x86_64-glslang Ao terminar a instalação, feche o shell do MSYS2. Baixe o instalador do Python 3 para Windows 64 bits e execute-o. No restante do texto usaremos como exemplo a versão 3.10, mas qualquer versão a partir da 3.6 é suportada. Durante a instalação, certifique-se de ativar a opção “Add Python 3.10 to PATH”. Após a instalação, abra o Prompt de Comando (cmd.exe) e execute o comando where python para exibir os diferentes caminhos em que o python é alcançado pela variável de ambiente Path. O resultado deverá ser parecido com o seguinte: C:\\&gt;where python C:\\Users\\ufabc\\AppData\\Local\\Programs\\Python\\Python310\\python.exe C:\\Users\\ufabc\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe O primeiro caminho exibido deve ser o caminho do executável do Python que acabou de ser instalado. Neste caso está correto, pois o Python foi instalado no local padrão do instalador que é %LocalAppData%\\Programs\\Python\\Python310 (neste exemplo, o nome do usuário é ufabc). No menu inicial, procure por “Editar variáveis de ambiente para a sua conta”. Edite a variável Path do usuário atual e inclua os caminhos para mingw64\\bin e usr\\bin do MSYS2. Por exemplo, se o MSYS2 foi instalado em C:\\msys64, inclua os seguintes caminhos no Path1: C:\\msys64\\usr\\bin C:\\msys64\\mingw64\\bin Edite a ordem dos caminhos de tal forma que os caminhos do Python sejam listados antes dos caminhos do MSYS2, como mostra a figura a seguir. Essa ordem é importante pois o MSYS2 também instala o Python 3, mas queremos usar o Python do Windows: Para testar se o MSYS2 foi instalado corretamente, abra o Prompt de Comando e execute o comando g++ --version. A saída deverá ser parecida com a seguinte: g++ (Rev7, Built by MSYS2 project) 13.1.0 Copyright (C) 2023 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Verificando o OpenGL O suporte ao OpenGL vem integrado ao Windows. Apenas certifique-se de instalar os drivers mais recentes da sua placa de vídeo. Instalando o Emscripten Abra o Prompt de Comando em algum caminho onde queira instalar a pasta do SDK do Emscripten (por exemplo, C:\\). Note the o terminal deve ser o Prompt de Comando e não o PowerShell. Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre na pasta recém-criada: cd emsdk Baixe e instale o SDK atualizado (latest): emsdk install latest Ative o SDK latest para o usuário atual: emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: emsdk_env.bat Para testar se a instalação foi bem-sucedida, execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.45 (ef3e4e3b044de98e1811546e0bc605c65d3412f4) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal ou execute o comando a seguir para registrar permanentemente as variáveis de ambiente no registro do sistema: emsdk_env.bat --permanent Caso não queira modificar o registro do sistema, crie um atalho para cmd.exe e use a opção /k para executar o arquivo emsdk_env.bat sempre que o Prompt de Comando for aberto. Por exemplo: cmd.exe /k &quot;C:\\emsdk\\emsdk_env.bat&quot; Mude esse caminho de acordo com o local onde emsdk_env.bat foi instalado. A variável Path existe tanto nas variáveis do usuário quanto nas variáveis do sistema. A modificação do Path do usuário geralmente é suficiente para uma instalação correta. Se ocorrer algum erro ao seguir as instruções de construção do “Hello, World!” na seção 1.5 (erro de construção ou de DLL não encontrada), experimente alterar o Path do sistema ao invés do usuário.↩︎ "],["vscode.html", "1.4 Visual Studio Code", " 1.4 Visual Studio Code Para desenvolver as atividades não é necessário usar um IDE ou editor em particular. Os códigos podem ser escritos em qualquer editor de texto não formatado e a compilação pode ser feita em linha de comando. Entretanto, é recomendável utilizar um editor/IDE como o CLion, Visual Studio Code, ou outro semelhante que seja capaz de oferecer funcionalidades de preenchimento automático de código, detecção de erros, ajuda sensível ao contexto e integração de construção com o CMake. A seguir veremos como configurar o Visual Studio Code (VS Code) para deixá-lo pronto para o desenvolvimento das atividades. O procedimento é bem simples e é o mesmo no Linux, macOS e Windows: Instale o VS Code através do instalador disponível em https://code.visualstudio.com/ ou através do gerenciador de pacotes de seu sistema operacional. Abra o editor e, na janela de extensões (Ctrl+Shift+X), instale a extensão C/C++ Extension Pack. Isso já é o suficiente para começarmos a trabalhar. Ainda precisaremos de algumas configurações extras para habilitar a depuração de código, mas veremos isso na seção a seguir (1.5). Caso você queira usar outro editor ou IDE, consulte a documentação específica de seu editor sobre como fazer a integração com o CMake e sobre como usar o GDB/LLDB para depurar código. Importante Qualquer que seja o IDE/editor utilizado, certifique-se de que o CMake e GCC estejam instalados e visíveis no PATH de acordo com as instruções mostradas nas seções anteriores. Dicas Em sistemas que possuem as ferramentas extras do Clang para linting e formatação de código como o Clang-Tidy e ClangFormat, é possível instalar extensões para fazer análise estática em tempo real e formatar o código automaticamente sempre que um arquivo é salvo. Uma dessas extensões é o clangd, baseado no servidor de mesmo nome do LLVM. Como vimos na seção 1.1, o Clang-Tidy e ClangFormat podem ser instalados no Ubuntu com os seguintes comandos: sudo apt install clang-tidy clang-format No Windows com MSYS2, abra o shell do MSYS2 e execute o seguinte comando: pacman -S mingw-w64-x86_64-clang-tools-extra Para ativar a formatação automática de código sempre que o arquivo for salvo, abra o arquivo JSON contendo as configurações do usuário do VS Code (opção “Preferences: Open User Settings (JSON)” da paleta de comandos) e adicione a chave \"editor.formatOnSave\": true. Para a análise estática em tempo real de código GLSL, instale a extensão GLSL Lint. Isso ajudará a evitar erros de sintaxe comuns na programação dos shaders em GLSL. Essa extensão instala também o Shader languages support for VS Code para habilitar o syntax highlighting e autocomplete do código GLSL. "],["abcg.html", "1.5 ABCg", " 1.5 ABCg Para facilitar o desenvolvimento das atividades práticas utilizaremos a biblioteca ABCg desenvolvida especialmente para esta disciplina. ABCg permite a prototipagem rápida de aplicações gráficas interativas 3D em C++ capazes de rodar tanto no desktop com binário nativo, quanto no navegador com binário WebAssembly. Internamente a ABCg utiliza a biblioteca SDL para gerenciar o acesso a dispositivos de entrada (mouse/teclado/gamepad) e saída (vídeo e áudio) de forma independente de plataforma, e a biblioteca GLEW para acesso às funções da API gráfica OpenGL. Além disso, a API do Emscripten é utilizada sempre que a aplicação é compilada para WebAssembly. ABCg é mais propriamente um framework do que uma biblioteca, pois ela gerencia o fluxo de trabalho da aplicação. Nosso código deverá ser construído em torno da estrutura fornecida pela ABCg. Do ponto de vista do desenvolvedor, essa estrutura é apenas uma leve camada de abstração das APIs utilizadas. Por exemplo, é possível acessar diretamente as funções da API gráfica OpenGL. De fato, faremos isso na maior parte das vezes. Outras bibliotecas também utilizadas e que podem ser acessadas diretamente são: CPPIterTools: para o suporte a laços range-based em C++ usando funções do tipo range, enumerate e zip similares às da linguagem Python; Dear ImGui: para gerenciamento de widgets de interface gráfica do usuário, tais como janelas, botões e caixas de edição; {fmt}: como alternativa mais eficiente ao stdio da linguagem C (printf, scanf, etc) e iostreams do C++ (std::cout, std::cin, etc), e para formatação de strings com uma sintaxe similar às f-strings do Python; Guidelines Support Library (GSL): para uso de funções e tipos de dados recomendados pelo C++ Core Guidelines; OpenGL Mathematics (GLM): para suporte a operações de transformação geométrica com vetores e matrizes; tinyobjloader: para a leitura de modelos 3D no formato Wavefront OBJ. A seguir veremos como instalar e compilar a ABCg junto com um exemplo de uso. Instalação Em um terminal, clone o repositório do GitHub: git clone https://github.com/hbatagelo/abcg.git Observação A release mais recente da ABCg também pode ser baixada como um arquivo compactado de https://github.com/hbatagelo/abcg/releases/latest. Atenção No Windows, certifique-se de clonar/descompactar o repositório em um diretório cujo nome não contenha espaços ou caracteres especiais. Por exemplo, clone/descompacte em C:\\cg em vez de C:\\computação gráfica. O repositório tem a estrutura mostrada a seguir. Para simplificar, os arquivos e subdiretórios .git* foram omitidos: abcg │ .clang-format │ .clang-tidy │ build.bat │ build.sh | build-vs.bat │ build-wasm.bat │ build-wasm.sh | CHANGELOG.md │ CMakeLists.txt | DockerFile │ LICENSE │ README.md │ runweb.bat │ runweb.sh │ └───abcg │ │ ... │ └───cmake │ │ ... │ └───examples │ │ ... │ └───public │ ... Os arquivos .clang-format e .clang-tidy são arquivos de configuração utilizados pelas ferramentas ClangFormat e Clang-Tidy caso estejam instaladas. Os arquivos build.* são scripts de compilação em linha de comando. Note que há scripts correspondentes com extensão .bat para usar no Prompt de Comando do Windows 2: build.sh/build.bat: para compilar a ABCg e os exemplos em binários nativos usando o compilador padrão (no nosso caso, o GCC); build-wasm.sh/build-wasm.bat: similar ao build.sh, mas para gerar binário em WebAssembly dentro do subdiretório public; build-vs.bat: similar ao build.bat, mas usando o compilador do Visual Studio 2022 ao invés do GCC do MSYS2. O arquivo CMakeLists.txt é o script de compilação do CMake. Os arquivos runweb.sh e runweb.bat podem ser usados para criar um servidor web local para servir o conteúdo de public. Os subdiretórios são os seguintes: abcg contém o código-fonte da ABCg e suas dependências. cmake contém scripts auxiliares de configuração do CMake. examples contém um exemplo de uso da ABCg: o “Hello, World!”; public contém os códigos HTML para exibir o “Hello, World!” no navegador. Observação O “Hello, World!” pode usar tanto a API gráfica OpenGL (código-fonte em examples/opengl) quanto a API gráfica Vulkan (código-fonte em examples/vulkan). Nesta disciplina usaremos o OpenGL em todas as atividades. Em particular, usaremos o OpenGL 3.3. Assim poderemos possível construir aplicações para desktop e web usando o mesmo código-fonte. Por este motivo, os scripts CMake da ABCg estão configurados para usar o OpenGL por padrão. Compilando na linha de comando Execute o script build.sh (Linux/macOS) ou build.bat (Windows) para iniciar o processo de configuração e construção da versão OpenGL do “Hello, World!”. A saída será similar a esta (o exemplo a seguir mostra a saída no Ubuntu): -- The C compiler identification is GNU 11.2.0 -- The CXX compiler identification is GNU 11.2.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/lib/ccache/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/lib/ccache/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done Using ccache -- Found OpenGL: /usr/lib/x86_64-linux-gnu/libOpenGL.so -- Found GLEW: /usr/include (found version &quot;2.2.0&quot;) -- Looking for pthread.h -- Looking for pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success -- Found Threads: TRUE -- Found SDL2: /usr/lib/x86_64-linux-gnu/libSDL2main.a;/usr/lib/x86_64-linux-gnu/libSDL2.so -- Found SDL2_image: /usr/lib/x86_64-linux-gnu/libSDL2_image.so -- Configuring done -- Generating done -- Build files have been written to: /home/ufabc/abcg/build ... [100%] Linking CXX executable ../../bin/helloworld [100%] Built target helloworld Ao final, os binários estarão disponíveis no subdiretório build. A biblioteca estática estará em build/abcg/libabcg.a e o executável do exemplo “Hello, World!” estará em build/bin/helloworld. Para testar, execute o helloworld. No Linux/macOS: ./build/bin/helloworld/helloworld No Windows: .\\build\\bin\\helloworld\\helloworld.exe | cat Importante No Windows, a saída deve sempre ser redirecionada para cat ou tee. Se isso não for feito, nenhuma saída de texto será exibida no terminal. Isso se deve a um bug do MSYS2. Observação Observe o conteúdo de build.sh (build.bat contém instruções equivalentes): #!/bin/bash set -euo pipefail BUILD_TYPE=${1:-Release} CMAKE_EXTRA_ARGS=${2:-&quot;&quot;} # Reset build directory rm -rf build mkdir -p build &amp;&amp; cd build # Configure cmake -DCMAKE_BUILD_TYPE=&quot;$BUILD_TYPE&quot; &quot;$CMAKE_EXTRA_ARGS&quot; .. # Build if [[ &quot;$OSTYPE&quot; == &quot;darwin&quot;* ]]; then # macOS NUM_PROCESSORS=&quot;$(sysctl -n hw.ncpu)&quot; else NUM_PROCESSORS=&quot;$(nproc)&quot; fi cmake --build . --config &quot;$BUILD_TYPE&quot; -- -j &quot;$NUM_PROCESSORS&quot; A variável BUILD_TYPE é Release por padrão, mas pode ser modificada passando a string Debug, MinSizeRel ou RelWithDebInfo como primeiro argumento de build.sh. A opção Debug ou RelWithDebInfo é utilizada quando queremos gerar símbolos de depuração. O modo Release (padrão) ou MinSizeRel devem ser usados quando queremos gerar um binário otimizado e sem arquivos de símbolos de depuração. Em particular, Release otimiza em favor do código mais rápido, enquanto MinSizeRel otimiza em favor do binário de menor tamanho. build.sh/build.bat também aceita um segundo argumento que corresponde a uma string a ser passada como argumento do CMake durante a configuração do projeto. Por exemplo, para compilar no modo Release e com a API gráfica OpenGL podemos executar build.sh Release -DGRAPHICS_API=OpenGL (neste caso basta executar build.sh, pois essas já são as configurações padrão). Observe que o script apaga o subdiretório build antes de criá-lo novamente. Portanto, não salve arquivos dentro de build pois eles serão removidos na próxima compilação! A geração dos binários usando o CMake é composta de duas etapas: a configuração (cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE ..) e a construção (cmake --build . --config $BUILD_TYPE). A configuração gera os scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja). A construção dispara a compilação e ligação usando tais scripts. Todos os arquivos gerados na configuração e construção ficam armazenados no subdiretório build. Compilando no VS Code Primeiramente, apague o subdiretório build caso você já tenha compilado a ABCg via linha de comando na seção anterior. No VS Code, selecione o menu “File &gt; Open Folder…” e abra a pasta abcg. Na seguinte janela, selecione “Yes, I trust the authors”: No canto inferior direito da janela aparecerá uma notificação perguntando se você quer configurar o projeto. Selecione “Yes”. Ao fazer isso, será feita uma varredura no sistema para identificar os compiladores e toolchains disponíveis. Uma lista dos “kits” de compilação encontrados aparecerá na parte superior da janela. O exemplo a seguir é o resultado exibido no Ubuntu: Selecione um kit compatível, como o GCC 11.2 ou mais recente. Ao fazer isso, o CMake iniciará o processo de configuração do projeto. Esse processo gera, dentro de um subdiretório build, os arquivos que serão utilizados pelo sistema de construção nativo. Caso queira invocar manualmente a configuração do CMake, acesse a paleta de comandos (Ctrl+Shift+P) e digite a opção “CMake: Configure”. Se aparecer uma notificação pedindo para configurar o projeto sempre que ele for aberto, selecione “Yes”: Após o término da configuração, é possível que apareça uma outra notificação solicitando permissão para configurar o Intellisense. Selecione “Allow”. Além disso, pode aparecer também uma notificação sobre o uso do arquivo compile_commands.json, como mostrado a seguir. Selecione “Yes” novamente: O arquivo compile_commands.json é gerado automaticamente pelo CMake. Ele contém os comandos de compilação e o caminho de cada unidade de tradução utilizada no projeto. O IntelliSense (ou clangd) utiliza as informações desse arquivo para habilitar as referências cruzadas. Importante A construção dos projetos usando o CMake é feita em duas etapas: Configuração: consiste na geração dos scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja); Construção: consiste no disparo da compilação e ligação usando os scripts gerados na configuração, além da execução de etapas de pré e pós-construção definidas nos scripts dos arquivos CMakeLists.txt. Tanto os arquivos da configuração quanto os da construção (binários) são gravados no subdiretório build. Geralmente a configuração só precisa ser feita uma vez e depois refeita caso o subdiretório build tenha sido apagado, ou após a alteração do kit de compilação, ou ainda após a alteração do build type (por exemplo, de Debug para Release). As informações de configuração ficam armazenadas em um arquivo CMakeCache.txt dentro de build. Como indicado na figura abaixo, na barra de status há botões para selecionar o build type/configurar, selecionar o kit de compilação, e construir a aplicação. A opção de construir já se encarrega de configurar o projeto caso os arquivos de configuração ainda não tenham sido gerados. Essas opções também estão disponíveis na paleta de comandos. Os comandos são: “CMake: Select Variant”: para selecionar um build type; “CMake: Select a Kit”: para selecionar um kit de compilação; “CMake: Configure”: para configurar o projeto usando o kit e o build type atual; “CMake: Build”: para construir o projeto. “CMake: Clean Rebuild”: para apagar as configurações anteriores do CMake, reconfigurar e reconstruir o projeto. Observação Os build types permitidos no CMake são: Debug para gerar binários não otimizados e com arquivos de símbolos de depuração. Esse é o build type padrão (ao contrário de build.sh e build.bat, que usam o modo Release por padrão); RelWithDebInfo para gerar arquivos de símbolos de depuração com binários otimizados; Release para gerar binários otimizados e favorecer código mais rápido. Essa opção não gera os arquivos de símbolos de depuração; MinSizeRel, semelhante ao Release, mas a otimização tenta gerar binário de menor tamanho. Para compilar e gerar os binários, tecle F7 ou clique em “Build” na barra de status. O progresso será exibido na janela “Output” do CMake/Build. Se a construção terminar com sucesso, a última linha de texto da janela Output será: [build] Build finished with exit code 0 Os arquivos gerados na construção ficam armazenados no subdiretório build, da mesma forma como ocorre na compilação via linha de comando. Para testar, abra um terminal e execute ./build/bin/helloworld/helloworld (Linux/macOS) ou .\\build\\bin\\helloworld\\helloworld.exe (Windows). Atenção A configuração do CMake gerada a partir do VS Code não é necessariamente a mesma gerada usando os scripts de linha de comando: o compilador pode ser diferente, ou o build type pode ser diferente. Se em algum momento você construir o projeto via linha de comando usando os scripts .sh ou .bat e depois quiser construir pelo editor, apague o subdiretório build antes de retornar ao VS Code. Isso forçará uma nova configuração do CMake e evitará erros de incompatibilidade entre as configurações. Na primeira vez que um arquivo com extensão .cpp for aberto no editor, algumas notificações poderão aparecer. Experimente abrir, por exemplo, examples\\helloworld\\main.cpp. Se a extensão clangd foi instalada como sugerido na seção 1.4, a caixa de mensagem a seguir será exibida. Selecione “Install”. A seguinte mensagem também poderá aparecer. Neste caso, selecione “Disable IntelliSense”: Ao final dessas configurações, reinicie o editor. Isso pode ser feito rapidamente selecionando a opção “Developer: Reload Window” da paleta de comandos. Depurando no VS Code Podemos depurar facilmente nossas aplicações com GDB ou LLDB usando a interface do VS Code. Após construir um projeto com build type Debug ou RelWithDebInfo, devemos abrir um de seus arquivos com extensão .cpp. Isso é necessário para fazer com que o VS Code identifique que queremos configurar a depuração de um projeto em linguagem C++. Por exemplo, para configurar a depuração do projeto “Hello, World!”, abra o arquivo examples\\helloworld\\main.cpp. Selecione a opção “Run and Debug” na barra de atividades (Ctrl+Shift+D). Em seguida, clique na opção “create a launch.json file”: Se aparecer um drop-down list de seleção do ambiente de depuração, selecione “C++ (GDB/LLDB)”. Isso criará o arquivo launch.json. O arquivo também será aberto no editor. Copie e cole em launch.json o conteúdo exibido a seguir. Este é uma exemplo para depurar o “Hello, World!” no Linux ou macOS usando o GDB. Um exemplo de configuração para o Windows é mostrado mais adiante: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${fileDirname}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;Set Disassembly Flavor to Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Observe que o valor da chave program aponta para o executável do projeto: ${workspaceFolder}/build/bin/helloworld/helloworld. Observação ${workspaceFolder} é uma variável pré-definida do VS Code que contém o caminho da pasta do projeto. Consulte a documentação para informações sobre outras variáveis disponíveis. A listagem a seguir mostra um exemplo de launch.json para depurar o “Hello, World!” no Windows usando o GDB do MSYS2: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld.exe&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;miDebuggerPath&quot;: &quot;C:\\\\msys64\\\\mingw64\\\\bin\\\\gdb.exe&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;Set Disassembly Flavor to Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Veja que o valor da chave miDebuggerPath contém o caminho completo do GDB, que é C:\\msys64\\mingw64\\bin\\gdb.exe supondo que o MSYS2 tenha sido instalado em C:\\msys64. O valor da chave externalConsole pode ser modificado para true caso você prefira que um novo terminal seja aberto durante a depuração. Consulte a documentação sobre depuração para informações sobre outras opções e informações gerais sobre como depurar código no editor. Após modificar o arquivo launch.json, selecione novamente a opção “Run” na barra de atividades ou aperte F5 para iniciar o programa no modo de depuração. Reedite o arquivo launch.json sempre que mudar o nome do executável que você queira depurar. Observação No VS Code para Windows, configure o terminal padrão para “Command Prompt” no lugar de “PowerShell”, uma vez que nossos scripts são compatíveis apenas com o Prompt de Comando. Para fazer isso, abra a paleta de comandos (Ctrl+Shift+P), acesse o comando “Terminal: Select Default Profile” e então selecione “Command Prompt”. Compilando para WASM Podemos compilar as aplicações ABCg para WebAssembly (WASM) de modo a executá-las diretamente no navegador. A construção é feita via linha de comando usando o toolchain Emscripten. Acompanhe a seguir como construir o “Hello, World!” para WASM e como testá-lo no navegador: Em um terminal (shell ou Prompt de Comando), ative as variáveis de ambiente do Emscripten (script emsdk_env.sh/emsdk_env.bat do SDK). Após isso, o compilador emcc deverá estar visível no PATH; No diretório abcg, execute build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Isso fará com que o CMake inicie a configuração do projeto e a construção dos binários. Os arquivos resultantes serão gravados em abcg/public. Em nosso caso, esses arquivos são helloworld.data (arquivo de dados/assets), helloworld.js (arquivo JavaScript) e helloworld.wasm (binário WebAssembly); Execute o script runweb.sh (Linux/macOS) ou runweb.bat (Windows) para rodar um servidor web local. O conteúdo de public ficará disponível em http://localhost:8080/; Abra a página http://localhost:8080/helloworld.html que chama o script helloworld.js recém-criado. A página HTML não faz parte do processo de construção e foi criada previamente. O resultado será semelhante ao exibido a seguir: uma aplicação mostrando um triângulo colorido e uma caixa de diálogo com alguns controles de interface. A pequena janela de texto abaixo da janela da aplicação é uma área de texto em HTML que mostra o conteúdo do terminal. Aqui, são exibidas algumas informações sobre o OpenGL (versão utilizada, fornecedor do driver, etc). Observação O subdiretório public contém, além do helloworld.html: full_window.html: para exibir o “Hello, World!” ocupando a janela inteira do navegador; full_window_console.html: idêntico ao anterior, mas com a sobreposição das mensagens do console na tela. Nos próximos capítulos veremos como construir novas aplicações usando a ABCg. Revisão de C++ Se você está mais habituado com programação em C ou C++ anterior ao C++11, o código das atividades da disciplina usando a ABCg poderá parecer pouco familiar. Revisaremos nesta seção alguns dos conceitos de C++ que podem gerar dúvidas em programadores que vieram da linguagem C, como a conversão explícita de tipos usando named casts, inicialização uniforme com {} e uso da palavra-chave auto. A aplicação desses conceitos segue as boas práticas de programação indicadas no C++ Core Guidelines e são seguidas em todos os códigos de atividades da disciplina. Dica Aproveite as primeiras semanas de aula para se familiarizar com os conceitos do chamado “C++ moderno” (C++11 em diante). Isso facilitará o entendimento do código da ABCg nos próximos capítulos. Uma referência rápida (cheatsheet) ao C++ moderno está disponível em https://github.com/AnthonyCalandra/modern-cpp-features. Uma excelente introdução ao C++ é o A Tour of C++, de Bjarne Stroustrup. Há também recursos gratuitos como os sites learncpp.com e tutorialspoint.com. A documentação da Microsoft sobre C++ é uma opção em português. Há uma referência sobre a linguagem C++ e sobre a biblioteca C++ padrão. Consulte também o C++ Core Guidelines para ficar a par das boas práticas de programação. Uma referência mais completa e aprofundada da linguagem está disponível em cppreference.com. Algumas partes estão traduzidas para o português. Named casts Observe os seguintes exemplos de conversões explícitas de tipos (casts) usando a sintaxe tradicional oriunda da linguagem C: int a = (int)7.9; // double para int float b = (float)1 / 3; // int para float unsigned *pa = (unsigned *)&amp;a; // int* para unsigned* void foo(Base *b) { Derived *pd = (Derived *)b; // Base* para Derived* // ... } Estes casts “estilo C” têm uma sintaxe concisa, mas nem sempre traduzem de forma clara a intenção do programador. Além disso, a conversão é feita por conta e risco do programador. Não há verificação da validade da conversão, por exemplo, na conversão de Base* (ponteiro de uma classe base) para Derived* (ponteiro para uma classe derivada de Base). C++ procura tornar as conversões explícitas mais expressivas e seguras através de quatro tipos de casts chamados de named casts: static_cast para conversões entre tipos básicos cuja validade pode ser verificada em tempo de compilação (por exemplo, float para int e vice-versa); const_cast para conversões entre tipos de diferentes qualificações (por exemplo, de int const para int); reinterpret_cast para conversões entre tipos representados por diferentes padrões de bits na memória (por exemplo, int* para char*); dynamic_cast para conversões de ponteiros e referências a objetos com polimorfismo dinâmico. Os exemplos anteriores podem ser escritos assim com named casts: int a = static_cast&lt;int&gt;(7.9); // double para int float b = static_cast&lt;float&gt;(1) / 3; // int para float unsigned *pa = reinterpret_cast&lt;unsigned *&gt;(&amp;a); // int* para unsigned* void foo(Base *b) { // Não compila se Base não for uma classe polimórfica Derived *pd = dynamic_cast&lt;Derived *&gt;(b); // Base* para Derived* // ... } A sintaxe é mais verbosa, mas a intenção do programador fica mais clara e o código torna-se mais seguro. Por exemplo, dynamic_cast retorna nulo se b (ponteiro para Base) não puder ser convertido para Derived* (por exemplo, se o objeto apontado por b é de uma classe incompatível com Derived). Esse erro passaria despercebido com o cast estilo C. O ideal é usarmos a menor quantidade possível de casts. Em geral só precisaremos usar static_cast, e reinterpret_cast nas atividades do curso. Entretanto, alguns trechos dos códigos das atividades vão um pouco mais além e seguem a diretriz Pro.safety do C++ Core Guidelines. Essa diretriz recomenda o uso de gsl::narrow_cast e gsl::narrow no lugar de static_cast para conversões com estreitamento: gsl::narrow_cast é o mesmo que static_cast. gsl::narrow é o mesmo que static_cast quando não há perda de informação na conversão. Se houver perda de informação, ocorre um erro de compilação. Conversões com estreitamento são aquelas em que existe a possiblidade de perda de informação (por exemplo, float para int, ou double para float). Considere o seguinte exemplo de substituição de static_cast por gsl::narrow em uma conversão com estreitamento: // float x = static_cast&lt;float&gt;(7.9); // Antes: OK float x = gsl::narrow&lt;float&gt;(7.9); // Depois: intenção mais clara Poderíamos usar static_cast, mas o uso de gsl::narrow deixa nossa intenção mais clara: esperamos que a conversão de 7.9 (um double) para float não perca informação. Afinal, float tem precisão suficiente para representar uma casa decimal. Se por algum motivo houver perda de informação, gsl::narrow gerará um erro de compilação e então saberemos que algo está errado3. Agora considere o seguinte exemplo de substituição de static_cast por gsl::narrow_cast: // int x = static_cast&lt;int&gt;(7.9f); // Antes: OK int x = gsl::narrow_cast&lt;int&gt;(7.9f); // Depois: intenção mais clara Nesse caso também poderíamos usar static_cast, mas gsl::narrow_cast deixa nossa intenção mais clara: esperamos que a conversão de 7.9f (um float) para um int perca informação. Afinal, a parte fracionária será perdida. gsl::narrow_cast indica que estamos ciente dessa perda e que isso não é um problema para nós. Em resumo, em conversões com estreitamento, isto é, com possível perda de informação, o uso de gsl::narrow e gsl::narrow_cast mostra melhor a intenção do programador. Ademais, de nossa parte, faz com que tenhamos que pensar melhor se estamos usando o static_cast corretamente. Inicialização uniforme O código das atividades do curso usam inicialização uniforme, que consiste no uso de {} no lugar de = ou () para inicializar objetos. Segundo a diretriz ES.23 do C++ Core Guidelines, “as regras para inicialização com {} são mais simples, mais gerais, menos ambíguas, e mais seguras do que outras formas de inicialização”. Observe os exemplos abaixo sem inicialização com {}, isto é, usando apenas = ou (): int a = 0; // Inicializa a com 0 int b(42); // Inicializa b com 42 double c(); // Declaração de uma função (ambíguo) int d = 7.9; // Inicializa implicitamente com 7 (inseguro) double pi = 3.1415f; // Inicializa implicitamente com 3.1415 std::vector&lt;int&gt; v1(3); // Inicializa v1 com 3 elementos de valor 0 (ambíguo) std::vector&lt;int&gt; v2(3, 2); // Inicializa v2 com 3 elementos de valor 2 (ambíguo) Compare com os exemplos abaixo com inicialização com {}: int a{}; // Inicializa a com valor default (0) int b{42}; // Inicializa b com 42 double c{}; // Inicializa c com valor default (0.0) // int d{7.9}; // ERRO: conversão com estreitamento int d{gsl::narrow_cast&lt;int&gt;(7.9)}; // OK: conversão explícita de double para int double pi{3.1415f}; // OK: conversão implícita sem estreitamento std::vector v1{3}; // Inicializa v1 com um único elemento 3 std::vector v2{3, 2}; // Inicializa v2 com os elementos 3 e 2 Observe que a inicialização com {} proíbe conversões implícitas com estreitamento. A inicialização uniforme nos obriga a usar conversões explícitas sempre que houver a possibilidade de perda de informação. Por um lado, isso deixa o código mais verboso, como no uso do gsl::narrow_cast no exemplo anterior. Por outro lado, deixa evidente a intenção do programador e evita bugs decorrentes de conversões implícitas. Palavra-chave auto A palavra-chave auto permite a dedução automática do tipo de uma variável a partir de sua inicialização. O código das atividades prioriza o uso de auto para evitar repetições redundantes de tipos de dados. Essa é uma recomendação da diretiva ES.11 do C++ Core Guidelines. Alguns exemplos de uso de auto são mostrados a seguir: auto a{42}; // int auto b{7.9f}; // float auto c{1.0 / 3}; // double // Os exemplos a seguir funcionam com qualquer tipo de dado em v std::vector v{3, 2}; auto e{v[0]}; // Cópia de v[0] auto &amp;f{v[0]}; // Referência a v[0]) auto const &amp;g{v[0]}; // Referência apenas de leitura auto *h{&amp;v[0]}; // Ponteiro para v[0] auto it{v.begin()}; // std::vector&lt;int&gt;::iterator // Ponteiro inteligente auto p{std::make_unique&lt;int[]&gt;(1024)}; // std::unique_ptr&lt;int[]&gt; // Expressão lambda auto addTen{[](auto x) { return x + 10; }}; auto x{0U}; // unsigned auto y{addTen(x)}; // unsigned o PowerShell não é suportado! Use apenas o Prompt de Comando.↩︎ Por exemplo, gsl::narrow&lt;T&gt;(3.1415926535) deve funcionar em uma plataforma em que T é um ponto de flutuante de 48 bits (exótico, mas possível), mas falha em um plataforma em que T tem 32 bits.↩︎ "],["intro.html", "2 Introdução", " 2 Introdução Computação gráfica (CG) é o conjunto de métodos e técnicas para a construção, manipulação, armazenagem e exibição de imagens por meio de um computador (ISO 2015). A computação gráfica tem suas origens no desenvolvimento dos primeiros computadores eletrônicos equipados com dispositivos de exibição. A partir do final da década de 1940, o computador experimental Whirlwind I do MIT (Instituto de Tecnologia de Massachusetts) e o sistema SAGE (Semi-Automatic Ground Environment) da Força Aérea dos Estados Unidos, foram os primeiros a utilizar dispositivos de exibição do tipo CRT (cathod-ray tube, ou tubo de raios catódicos) para exibir gráficos vetoriais compostos de linhas e pontos. O uso do termo “computação gráfica” é um pouco mais recente. William Fetter, projetista gráfico da Boeing, utilizou o termo pela primeira vez em 1960 por sugestão de seu supervisor Verne L. Hudson, para descrever seu trabalho. Fetter utilizava gráficos tridimensionais no computador para criar um modelo estilizado de corpo humano que ficou conhecido como “Boeing Man” (figura 2.1). O modelo, composto de curvas e segmentos, era utilizado em simulações de ergonomia do piloto na cabine do avião. Figura 2.1: “Boeing Man” desenhado por William Fetter em um IBM 7094 (fonte). O acrônimo CGI (Computer-Generated Imagery) é frequentemente utilizado para se referir à geração de imagens e efeitos visuais em computador com aplicações em arte, entretenimento, simulação e visualização científica. Uma forma de CGI bastante conhecida e que chama a atenção por suas imagens bonitas é a síntese de imagens fotorrealistas. Um exemplo de imagem fotorrealista gerada em computador é mostrado na figura 2.2. A imagem é uma cena de arquitetura interior produzida com o Cycles Render Engine. Figura 2.2: Scandinavian Interior, por Arnaud Imobersteg (fonte). Atualmente, uma imagem como a da figura 2.2 pode ser gerada em qualquer computador pessoal que tenha sido fabricado nos últimos 10 anos. A imagem foi gerada com o software Blender, gratuito e de código aberto, e que tem o Cycles Render Engine como um de seus renderizadores. Entretanto, existe uma máxima em computação gráfica – a chamada 1ª lei de Peddie – que diz: “Em computação gráfica, demais nunca é o suficiente.” — Jon Peddie De fato, mesmo com toda a evolução do hardware gráfico e das técnicas de CG, ainda não é possível gerar em tempo real imagens com o nível de qualidade obtido por renderizadores como o Cycles. Eventualmente, esse dia chegará. Porém, quando isso acontecer, o nível de exigência dos usuários e desenvolvedores será igualmente maior. Desejaremos imagens ainda mais detalhadas, com maior resolução, mais realistas, mais interativas, com mais efeitos cinemáticos, etc. Enfim, demais nunca é o suficiente. Em computação gráfica, é necessário um cuidadoso compromisso entre a qualidade das imagens geradas e a eficiência com que essas imagens podem ser sintetizadas em um dado sistema computacional. Isso é particularmente importante quando as imagens precisam ser geradas em tempo real. Em jogos digitais, é comum que imagens de alta resolução tenham de ser geradas a uma taxa de, no mínimo, 30 quadros por segundo4. Mesmo os jogos que não visam o fotorrealismo exigem imagens em um nível de qualidade que só pode ser alcançado com o uso de técnicas avançadas de sombreamento e iluminação. Essas técnicas visam produzir resultados que, ainda que não sejam necessariamente acurados do ponto de vista físico, sejam suficientemente convincentes para um público cada vez mais exigente. No decorrer do quadrimestre veremos que a evolução das técnicas e ferramentas de computação gráfica em tempo real é impulsionada pela busca da melhor qualidade de imagem que pode ser obtida de forma eficiente no hardware gráfico disponível no momento. Como resultado, é comum que os métodos adotem simplificações inusitadas, mas ao mesmo tempo muito efetivas, e explorem diferentes aspectos da percepção visual humana para criar uma ilusão de realismo que seja suficiente para chegar ao resultado desejado. Referências "],["áreas-correlatas.html", "2.1 Áreas correlatas", " 2.1 Áreas correlatas A computação gráfica se relaciona, e em certa medida se sobrepõe, a diferentes campos de atuação da ciência da computação. Uma breve introdução às principais áreas correlatas é dada a seguir: Síntese de imagem: é o que geralmente se entende por computação gráfica. Compreende o processo de rendering (imageamento ou renderização) que consiste em converter especificações de geometria, cor, textura, iluminação, entre outras especificações de características de uma cena, em uma imagem exibida em um display gráfico. A figura 2.3 mostra o resultado da síntese de imagem de uma cena fotorrealista usando técnicas combinadas de traçado de raios e radiosidade. Figura 2.3: Imagem gerada no renderizador POV-Ray, por Gilles Tran (fonte). Visão computacional: compreende o processo de adquirir, processar e interpretar dados visuais para gerar as especificações de uma cena. A partir de uma imagem digital, técnicas de visão computacional podem ser utilizadas para tarefas como a reconstrução dos modelos geométricos vistos na imagem, o particionamento dos pixels em segmentos correspondentes aos diferentes objetos da cena, reconhecimento de texturas, identificação dos atributos da câmera e da iluminação, e extração de outras informações semânticas. A visão computacional com frequência se relaciona com a visão de máquina, que compreende as técnicas e ferramentas voltadas a aplicações de visão em inspeção automática, controle de processos industriais e orientação de robôs. A figura 2.4 mostra um exemplo de aplicação de visão computacional: uma técnica de segmentação semântica utilizando aprendizagem profunda para identificar objetos em uma imagem. Figura 2.4: Segmentação semântica usando o sistema YOLO (fonte). Processamento de imagem: compreende o processo de aplicar filtros e operações sobre uma imagem digital que resultam em uma nova imagem digital. Técnicas de processamento digital de imagem podem ser utilizadas para enfatizar características de uma imagem (por exemplo, ajustar brilho, contraste, nitidez), restaurar imagens que sofreram algum tipo de degradação por ruído, mudar cores e tons, comprimir e quantizar, entre diversas outras operações. O escopo do processamento de imagens frequentemente se intersecta com aquele das técnicas de visão computacional. A figura 2.5 mostra um exemplo de processamento de imagem: a aplicação de filtros de remoção de ruído em uma imagem renderizada pelo método de traçado de raios estocástico. O ruído é inerente ao método de Monte Carlo utilizado nesse tipo de renderização. Figura 2.5: Uso dos filtros de processamento de imagem do Intel Open Image Denoise para remoção de ruído de uma imagem de traçado de raios (fonte). Modelagem geométrica: está relacionada com a criação e processamento de representações matemáticas de formas. Técnicas de modelagem geométrica podem ser utilizadas para criar modelos compostos de curvas e superfícies a partir de aquisição de dados (por exemplo, a partir de uma nuvem de pontos de uma aquisição por scanner 3D), construir e manipular modelos sintéticos através da combinação de primitivas geométricas, converter uma representação geométrica em outra, e realizar operações geométricas e topológicas diversas. A figura 2.6 mostra um exemplo de reconstrução de malha geométrica usando o software MeshLab. O modelo à esquerda é o modelo original. Na reconstrução (à direita), os buracos foram preenchidos e o resultado é uma única malha de triângulos. Figura 2.6: Reconstrução de malha geométrica usando o MeshLab (fonte). Neste curso teremos como foco a síntese de imagens. Em particular, a síntese de imagens em tempo real. Como parte disso, veremos como representar e processar cenários virtuais compostos de objetos tridimensionais animados. Veremos como implementar modelos de iluminação capazes de simular de forma eficiente a iluminação de superfícies, e como gerar imagens digitais do ponto de vista de uma câmera virtual. Faremos isso usando a API gráfica OpenGL de modo a explorar o pipeline de processamento gráfico programável das placas de vídeo atuais. Com isso conseguiremos obter o nível de eficiência necessário para produzir animações e permitir a sensação de interatividade. Na UFABC, os tópicos de visão computacional e processamento de imagens são abordados nas disciplinas “ESZA019-17 Visão Computacional” e “MCZA018-17 Processamento Digital de Imagens”. "],["linha-do-tempo.html", "2.2 Linha do tempo", " 2.2 Linha do tempo Nesta seção acompanharemos um resumo da evolução histórica da computação gráfica. Iniciaremos na década de 1950, com os primeiros computadores eletrônicos de uso geral e o surgimento das primeiras aplicações de computação gráfica, e seguiremos até a década atual com os desenvolvimentos mais recentes das atuais GPUs (Graphics Processing Units). Embora a computação gráfica seja recente, assim como a própria ciência da computação, o desenvolvimento de seus fundamentos é anterior ao século XX e só foi possível devido às contribuições artísticas e matemáticas de diversos pioneiros. Para citar apenas alguns: Euclides de Alexandria (300 a.C.), com sua contribuição no desenvolvimento da geometria; Filippo Brunelleschi (1377–1446), com seus estudos sobre perspectiva; René Descartes (1596–1650), com o desenvolvimento da geometria analítica e os sistemas de coordenadas; Christiaan Huygens (1629–1695) e Isaac Newton (1643–1727) por suas investigações sobre os fenômenos da luz; Leonhard Euler (1707–1783), por sua contribuição na trigonometria e em topologia; James Joseph Sylvester (1814–1897), por suas contribuições na teoria das matrizes e invenção da notação matricial. O uso de gráficos no computador também não teria sido possível sem os esforços que contribuíram para o surgimento dos computadores eletrônicos, e também dos primeiros dispositivos de exibição, como o tubo de raios catódicos no final do século XIX. 1950 Os primeiros computadores eletrônicos com dispositivos de exibição surgem neste período. O computador Whirlwind I, do MIT, originalmente projetado para ser parte de um simulador de vôo, foi um dos primeiros computadores digitais de uso geral com processamento em tempo real. O Whirlwind I era equipado com um CRT vetorial capaz de desenhar linhas e pontos. Charles W. Adams e John T. Gilmore, programadores da equipe de desenvolvimento do Whirlwind, implementaram um programa de avaliação de equações diferenciais para produzir a animação da trajetória de uma bola quicando. Essa simulação pode ser considerada a primeira aplicação de computação gráfica interativa e um precursor do jogo de computador, pois o operador podia controlar, através de um botão, a frequência do quicar na tentativa de fazer a bola acertar uma lacuna na tela simulando um buraco no chão. O sistema de defesa aérea SAGE evoluiu a partir do Whirlwind ao longo da década de 1950. As estações do SAGE contavam com telas CRT que exibiam dados de diferentes radares combinados com informações de referência geográfica. Cada estação era também equipada com uma caneta óptica. Através da caneta óptica, o operador podia apontar e selecionar elementos gráficos diretamente na tela (figura 2.7). Figura 2.7: Operador do SAGE usando uma caneta óptica em um CRT vetorial (fonte). 1960 Nesse período a computação gráfica se desenvolve nos laboratórios de pesquisa de universidades e surgem as primeiras aplicações de CAD (Computer-Aided Design) nas indústrias automotiva e aeroespacial. Na década de 1960 ocorrem importantes desenvolvimentos na área de modelagem geométrica, como o uso de curvas de Bézier e NURBS (Non-Uniform Rational Basis Spline). Em 1960, a Digital Equipment Corporation (DEC) começa a produzir em escala comercial o computador PDP-1, equipado com CRT e caneta óptica. Em 1961, o cientista da computação Steve Russell (MIT) cria o “Spacewar!” (figura 2.8). O jogo ganha popularidade dentro e fora da universidade e vira referência no desenvolvimento de jogos digitais5. Figura 2.8: Estudantes do MIT jogando Spacewar! no DEC PDP-1 (fonte). Em 1963, Ivan Sutherland desenvolve o SketchPad, um sistema de projeto gráfico interativo que permite ao usuário manipular primitivas gráficas vetoriais através de uma caneta óptica e um CRT (Sutherland 1963). A figura 2.9 mostra Sutherland operando o SketchPad no computador TX-2 do MIT. O SketchPad é um marco no uso da interface gráfica do usuário (GUI, acrônimo de Graphical User Interface) e um precursor das aplicações de projeto assistido por computador (CAD). Figura 2.9: Ivan Sutherland operando o SketchPad em 1962 (fonte). Na década de 1960 surgem também os primeiros seminários e grupos de interesse em pesquisa sobre gráficos em computador. Na ACM (Association for Computing Machinery), tradicional sociedade científica e educacional dedicada à computação, é fundado o grupo SICGRAPH (Special Interest Committe on Computer Graphics) para promover seminários de computação gráfica. No final da década, o SICGRAPH muda de nome para SIGGRAPH (Special Interest Group on Computer Graphics and Interactive Techniques). A conferência SIGGRAPH é realizada anualmente e é hoje uma das principais conferências de computação gráfica no mundo. 1970 Durante a década de 1970 são desenvolvidas muitas das técnicas de síntese de imagens em tempo real utilizadas atualmente. Em 1971, o então aluno de doutorado Henri Gouraud, trabalhando com Dave Evans e Ivan Sutherland na Universidade de Utah, desenvolve uma técnica eficiente de melhoramento da percepção visual do sombreamento (shading) de superfícies suaves aproximadas por malhas poligonais (Gouraud 1971). Tal técnica, conhecida como Gouraud shading, consiste em interpolar linearmente os valores de intensidade de luz refletida dos vértices da malha poligonal. O resultado é a suavização da variação da reflexão de luz sem a necessidade de aumentar a resolução da malha geométrica (Figura 2.10). Figura 2.10: Visualização de uma esfera aproximada por triângulos, exibindo o aspecto facetado (esquerda) e suavizado com Gouraud shading (direita). Em 1973, Bui Phong, também na Universidade de Utah, desenvolve o Phong shading como um melhoramento de Gouraud shading para reproduzir com mais fidelidade as reflexões especulares em aproximações de superfícies curvas (Phong 1973). Na figura 2.11 é possível comparar Gouraud shading e Phong shading lado a lado. Phong shading reproduz de forma mais acurada o brilho especular da esfera sem precisar usar uma malha poligonal mais refinada. Figura 2.11: Visualização de uma esfera aproximada por triângulos, com Gouraud shading (esquerda) e Phong shading (direita). Phong também propôs um modelo empírico de iluminação local de pontos sobre superfícies conhecido como modelo de reflexão de Phong. Em 1977, Jim Blinn, aluno da mesma universidade, propôs uma alteração do modelo de reflexão de Phong – o modelo de Blinn–Phong – mais acurado fisicamente e mais eficiente sob certas condições de visualização e iluminação (Blinn 1977). Nas décadas seguintes, o modelo de Blinn–Phong tornaria-se o padrão de indústria para síntese de imagens em tempo real, e ainda é muito utilizado atualmente. Em 1974, Wolfgang Straßer, na Universidade Técnica de Berlim, e Ed Catmull, na Universidade de Utah, desenvolvem ao mesmo tempo, mas de forma independente, uma técnica que viria a ser conhecida como Z-buffering. Tal técnica permite identificar, de forma conceituamente simples e favorável à implementação em hardware, quais partes da geometria 3D estão visíveis de um determinado ponto de vista. Atualmente, essa técnica é largamente utilizada em síntese de imagens e é suportada em todo hardware gráfico. Além de ter contribuído com a técnica de Z-buffering, Catmull também trouxe diversos avanços na área de modelagem geométrica, especialmente em subdivisão de superfícies e representação paramétrica de superfícies bicúbicas (Catmull 1974). Outra importante contribuição de Catmull foi o desenvolvimento da técnica de mapeamento de textura, ubíqua nas aplicações gráficas atuais e que permite aumentar a percepção de detalhes de superfícies sem aumentar a complexidade da geometria (figura 2.12). Figura 2.12: Animação do mapeamento de uma textura 2D sobre um modelo poligonal 3D (fonte). Em 1975, o matemático Benoît Mandelbrot, na IBM, desenvolve o conceito de geometria de dimensão fracionária e cria o termo fractal (Albers and Alexanderson 2008). Desde então, fractais começam a ser explorados em síntese de imagens e modelagem geométrica para representar os mais diversos padrões e fenômenos naturais tais como contornos de mapas, relevo de terrenos, nuvens, texturas e plantas. Vol Libre “Vol Libre”, de Loren Carpenter, foi o primeiro filme criado com fractais. O vídeo, de apenas dois minutos, foi apresentado pela primeira vez na conferência SIGGRAPH ’80 após uma palestra técnica de Carpenter sobre a renderização de curvas e superfícies fractais: De acordo com o livro “Droidmaker: George Lucas And the Digital Revolution” (Rubin 2005), ao final da exibição do vídeo, Ed Catmull e Alvy Smith, da Lucasfilm, abordaram Carpenter e ofereceram a ele um emprego na divisão de computação da empresa. Carpenter aceitou imediatamente. Após a carreira na Lucasfilm, Carpenter ainda seria co-fundador da Pixar (junto com Catmull, Smith e outros) e cientista-chefe do estúdio de animação. Em 1976, Steve Jobs, Steve Wozniak e Ronald Wayne fundam a Apple Computer (atualmente Apple Inc.). Em 1979, Steve Jobs entra em contato com as pesquisas de desenvolvimento de interface gráfica na Xerox PARC (atualmente PARC), divisão de pesquisa da Xerox em Palo Alto, Califórnia. Na PARC, Jobs conhece o Xerox Alto, o primeiro computador com uma interface gráfica baseada na metáfora do desktop e no uso do mouse (figura 2.13). Figura 2.13: Xerox Alto (fonte). O Xerox Alto foi o resultado de desenvolvimentos iniciados por Douglas Engelbart e Dustin Lindberg no Stanford Research Institute, atual SRI International, por sua vez inspirados no SketchPad de Sutherland. Alguns anos depois, a Apple implementaria os conceitos do Xerox Alto nos computadores Apple Lisa e Macintosh, iniciando uma revolução no uso da interface gráfica nos computadores pessoais (PCs). Em 1977, surge a primeira tentativa de padronização de especificação de comandos em sistemas gráficos: o Core Graphics System (ou simplesmente Core), proposto pelo Graphic Standards Planning Committee (GSPC) da ACM SIGGRAPH (Chappell and Bono 1978). Em 1978, Jim Blinn desenvolve uma técnica de mapeamento de textura para simulação de vincos e rugosidades em superfícies: o bump mapping (Blinn 1978). Uma forma de bump mapping muito utilizada atualmente é o normal mapping. A técnica pode ser muito efetiva para manter a ilusão de uma superfície detalhada, mesmo quando a geometria utilizada é muito simples. A figura 2.14 mostra um exemplo dessa simplificação. Ao longo do quadrimestre implementaremos esta e outras técnicas de texturização. Figura 2.14: Uso de normal mapping para simular a renderização de um modelo de quatro milhões de triângulos usando apenas dois triângulos (fonte). No final da década, J. Turner Whitted desenvolve a técnica de traçado de raios (Whitted 1979). O traçado de raios consegue simular com mais precisão, e de forma conceitualmente simples, efeitos ópticos de reflexão, refração, espalhamento e dispersão da luz. Como resultado, consegue gerar imagens mais fotorrealistas, ainda que sob um custo computacional muito elevado quando comparado com a renderização baseada na rasterização, que consiste na varredura e preenchimento de primitivas geométricas projetadas. Figura 2.15: Esferas e tabuleiro de xadrez: uma das primeiras imagens geradas com traçado de raios, por Turner Whitted. 1980 Essa é a década em que a computação gráfica marca sua presença definitiva na indústria de cinema. O uso de cenas de computação gráfica é popularizado a partir de filmes como “Star Trek II: The Wrath of Khan” (1982), “Tron” (1982) e “Young Sherlock Holmes (1985), como resultado dos avanços das técnicas de síntese de imagem e modelagem geométrica da década anterior, combinado com o avanço da capacidade de processamento dos computadores. Durante essa década ocorrem também importantes avanços nas técnicas de síntese de imagens. Em 1984, Robert Cook, Thomas Porter e Loren Carpenter desenvolvem o traçado de raios distribuído (distributed ray tracing), o qual permite reproduzir efeitos de sombras suaves, entre outros efeitos não contemplados pelo método original de Whitted (Cook, Porter, and Carpenter 1984). A figura 2.16 mostra um exemplo de renderização da cena de teste “Cornell box” usando essa técnica. A imagem tende a ser granulada como resultado da natureza estocástica do algoritmo. Figura 2.16: Imagem gerada com traçado de raios distribuído/estocástico. Ainda em 1984, Donald Greenberg, Michael Cohen e Kenneth Torrance propõem a técnica de radiosidade (Greenberg, Cohen, and Torrance 1986) baseada no uso do método de elementos finitos para simular interreflexões de luz entre superfícies idealmente difusas. A solução da radiosidade de uma cena pode ser pré-processada e não depende da posição da câmera. Isso permite a visualização da cena em tempo real, desde que a posição dos objetos e fontes de luz mantenha-se estática. A figura 2.17 mostra um exemplo de cena renderizada com radiosidade usando o software RRV (Radiosity Renderer and Visualizer). O método de radiosidade pode ser combinado com traçado de raios para gerar imagens com melhor fidelidade de simulação de reflexão difusa e especular. Figura 2.17: Imagem gerada com radiosidade (fonte). Em 1985, o GKS (Graphical Kernel System), desenvolvido como um melhoramento da API Core, torna-se a API padrão ISO para gráficos independentes do dispositivo (ISO 1985). Através do GKS, o código de descrição de comandos para manipulação de gráficos 2D permite a portabilidade entre diferentes linguagens de programação, sistemas operacionais e hardware gráfico compatível. Entretanto, gráficos 3D ainda não são contemplados nesta API. Em 1986, Steve Jobs adquire a divisão de computação gráfica da Lucasfilm e funda a Pixar junto com Ed Catmull, Alvy Smith e outros. Nessa época, Catmull, Loren Carpenter e Robert Cook desenvolvem o sistema de renderização RenderMan, muito utilizado na produção de efeitos visuais em filmes e animações. Após 14 anos, Catmull, Carpenter e Cook receberiam da Academia de Artes e Ciências Cinematográficas a estatueta do Oscar na categoria “Academy Scientific and Technical Awards” pelas contribuições à indústria do cinema representadas pelo desenvolvimento do RenderMan. O sucesso do RenderMan deve-se em parte à sua elegante API – a RenderMan Interface (RISpec) – inspirada na linguagem PostScript. A API permite a descrição completa de cenas 3D com todos os componentes necessários à renderização. Isso garante resultados consistentes, independentes do software de modelagem utilizado. O conceito de shaders, amplamente utilizado em hardware gráfico atual, surge do RenderMan shading language, desenvolvido na década de 1990 e incorporado no RISpec em 2005 como uma linguagem – dessa vez inspirada na linguagem C – de especificação de propriedades de superfícies, fontes de luz e efeitos atmosféricos de cena. Em 1988 é organizado o 1º Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens (SIBGRAPI), em Petrópolis, RJ. O evento, organizado anualmente pela CEGRAPI/SBC, internacionalizou-se e atualmente é chamado de Conference on Graphics, Patterns and Images. 1990 1990 é a década das APIs gráficas 3D e da popularização do hardware gráfico nos PCs. Empresas como a Sun Microsystems (adquirida pela Oracle em 2010), IBM, HP (Hewlett-Packard), e as agora extintas NeXT, SGI (Silicon Graphics, Inc.) e DEC, desenvolvem estações gráficas de alto desempenho equipadas com hardware capaz de acelerar operações de renderização baseadas em rasterização com suporte a Z-buffer, mapeamento de texturas, iluminação e sombreamento de superfícies (figura 2.18). Figura 2.18: Workstation SGI IRIS Indigo (fonte). Neste período surgem as primeiras APIs para gráficos 3D como tentativa de padronizar a interface de programação entre as diferentes arquiteturas de hardware. Uma dessas APIs, desenvolvida ao longo da década de 1980 e que se estabelece como padrão da indústria na década de 1990, é o PHIGS (Programmer’s Hierarchical Interactive Graphics System) (Shuey 1987). PHIGS utiliza o conceito de grafo de cena: uma estrutura de dados hierárquica que representa as relações entre os modelos geométricos e outras entidades de uma cena. A API trabalha com malhas poligonais e síntese de imagens baseada na rasterização (em oposição ao traçado de raios), prevê o suporte a Gouraud e Phong shading, mas não oferece suporte a mapeamento de texturas. Em oposição ao PHIGS, a SGI utiliza em suas estações gráficas IRIS a API proprietária IRIS GL (Integrated Raster Imaging System Graphics Library) com características semelhantes ao PHIGS, porém com suporte a mapeamento de texturas (McLendon 1992). Diferentemente do PHIGS, o IRIS GL não adota o conceito de grafo de cena. As primitivas gráficas são enviadas imediatamente ao hardware gráfico em um pipeline de transformação geométrica e visualização. Esse modo de enviar os dados, conhecido como immediate mode, acaba por revelar-se mais apropriado para implemetação em hardware do que o retained mode do PHIGS com seu grafo de cena. Em 1991, Mark Segal e Kurt Akeley, da SGI, iniciam o desenvolvimento de uma versão aberta do IRIS GL como tentativa de criar um novo padrão de indústria. Para isso, removem o código proprietário e modificam a API de modo a torná-la independente do sistema de janelas e de dispositivos de entrada. Deste desenvolvimento surge, em 1992, o OpenGL (Open Graphics Library) (Woo et al. 1999), que rapidamente ocupa o lugar do PHIGS como API padrão para gráficos 3D. Desde então, revisões periódicas do OpenGL são feitas de modo a suportar os aprimoramentos mais recentes do hardware gráfico. O aspecto minimalista e de facilidade de uso do IRIS GL continuam presentes no OpenGL. Essas características fizeram – e ainda fazem – do OpenGL uma das APIs gráficas 3D mais populares em aplicações multiplataforma. InfiniteReality No início da década de 1990, as estações gráficas de alto desempenho suportavam apenas um número reduzido de características do OpenGL, sendo o restante simulado em software. O sistema RealityEngine (Akeley 1993), lançado em 1992 pela SGI, foi o primeiro hardware gráfico capaz de oferecer suporte para todas as etapas de transformação e iluminação da versão 1.0 do OpenGL, incluindo o mapeamento de texturas 2D com mipmapping (uma técnica de pré-filtragem de texturas) e antialiasing (suavização de serrilhado). A arquitetura foi sucedida em 1996 pelo InfiniteReality (Montrym et al. 1997), desenvolvido especificamente para o OpenGL. Dependendo da configuração final, o custo de uma estação gráfica baseada no InfiniteReality poderia ser superior a 1 milhão de dólares. Uma demonstração da SGI sobre as capacidades de renderização em tempo real do InfiniteReality em 1996 pode ser vista no vídeo de YouTube “Silicon Graphics - Onyx Infinite Reallity 50FPS”. O sistema InfiniteReality evoluiu até o início da década de 2000. A figura 2.19 mostra um supercomputador equipado com o InfiniteReality 4. Figura 2.19: SGI Onyx 300 com InfinityReality 4, de 2002 (fonte). A partir de 1995, surgem nos PCs as primeiras placas de vídeo com aceleração de processamento gráfico 3D, também chamadas de aceleradoras gráficas 3D. As primeiras aceleradoras gráficas eram capazes de realizar apenas a varredura de linhas não texturizadas e, em alguns casos, tinham desempenho similar ao código de máquina otimizado na CPU. Por outro lado, logo essas limitações foram vencidas e surgiram placas eficientes e com suporte a mapeamento de textura, impulsionadas pelo emergente mercado de jogos de computador. Enquanto as primeiras estações gráficas da SGI implementavam um pipeline completo de transformação de vértices, ainda que sem suporte à texturização, as aceleradoras gráficas para PCs, produzidas por empresas como Diamond Multimedia, S3 Graphics (extinta em 2003), Trident Microsystems (extinta em 2012), Matrox Graphics e NVIDIA, ofereciam suporte ao mapeamento de texturas, porém sem transformação de geometria ou processamento de iluminação. A 3Dfx Interactive (adquirida em 2000 pela NVIDIA), com a sua série de aceleradoras Voodoo Graphics lançadas a partir de 1996, ampliou enormemente o uso do hardware gráfico em jogos de computador. As placas Voodoo eram capazes de exibir triângulos texturizados com mipmapping e filtragem bilinear (figura 2.20). Entretanto, o hardware ainda dependia da CPU para preparar os triângulos para a rasterização. Os triângulos só poderiam ser processados pelo hardware gráfico se fossem previamente convertidos em trapézios degenerados, alinhados em coordenadas da tela. Figura 2.20: Jogo “Carmageddon II: Carpocalypse Now” (Stainless Games) em uma placa gráfica 3Dfx Voodoo, de 1998 (fonte). Outra limitação das aceleradoras gráficas nesse período era a falta de suporte adequado a uma API padrão de indústria. A arquitetura de tais placas era incompatível com aquela especificada no OpenGL e fazia com que os desenvolvedores precisassem recorrer a APIs proprietárias, como a API Glide da 3Dfx (3Dfx 1997). As placas da 3Dfx foram populares até o final da década quando então o OpenGL e a API Direct3D, da Microsoft, começaram a ser suportados de maneira eficiente pelas placas de concorrentes como a ATI Technologies (adquirida em 2006 pela AMD), Matrox e NVIDIA. Na segunda metade da década, o desenvolvimento das placas gráficas para PCs acompanhou a evolução da API Direct3D. Em 1995, a Microsoft lança o Windows 95 Games SDK, um conjunto de APIs de baixo nível para o desenvolvimento de jogos e aplicações multimídia de alto desempenho no Windows. Em 1996, o Windows 95 Games SDK muda de nome para DirectX e sua segunda e terceira versões são disponibilizadas em junho e setembro desse mesmo ano. Entre as APIs contidas no DirectX, o Direct3D é concebido como uma API para hardware gráfico compatível com o pipeline de processamento do OpenGL. Embora no início o Direct3D fosse criticado por sua arquitetura demasiadamente confusa e mal documentada em comparação com o OpenGL (como relatado por John Carmack, da id Software, em sua carta sobre o OpenGL), eventualmente torna-se a API mais utilizada em jogos uma vez que novas versões começam a ser distribuídas em intervalos menores que aqueles do OpenGL. A revisão do OpenGL dependia do ARB (Architecture Review Board): um consórcio independente formado por representantes de diversas empresas de hardware e software que se reuniam periodicamente para propor e aprovar mudanças na API. O Direct3D, por ser proprietário, respondia melhor ao rápido desenvolvimento das placas gráficas naquele momento e passou a ditar a especificação das futuras aceleradoras gráficas voltadas ao mercado de jogos. Em 1997 é anunciado o DirectX 5 (o DirectX 4 nunca chegou a ser lançado), acompanhando as primeiras placas capazes de renderizar triângulos, tais como a ATI Rage Pro e NVIDIA Riva 128 (figura 2.21). A Riva 128 não alcançava a mesma qualidade de imagem produzida pelas placas da 3Dfx, mas ultrapassava as placas Voodoo em várias medições de desempenho. Ainda assim, a aceleração de processamento de geometria era inexistente e a CPU era responsável por calcular as transformações geométricas e interpolações de atributos de vértices ao longo das arestas para cada triângulo transformado. Figura 2.21: Placa gráfica Diamond com o chip NVIDIA Riva 128, de 1997 (fonte). Em 1998 é lançado o DirectX 6 e surgem as primeiras aceleradoras gráficas capazes de interpolar atributos ao longo de arestas. Nessa geração de hardware gráfico, a CPU ainda era responsável pela transformação e iluminação de cada vértice, mas agora bastava enviar à placa gráfica os atributos de cada vértice em vez de atributos interpolados para cada aresta de cada triângulo. Um ano depois, o DirectX 7 é lançado com suporte para aceleração em hardware de transformação e iluminação (figura 2.22). As primeiras placas compatíveis com DirectX 7 surgiriam no ano seguinte. Figura 2.22: Demonstração do benchmark 3DMark2000 (UL) usando DirectX 7 com transformação de geometria e cálculo de iluminação em hardware. 2000 A década de 2000 presencia o que pode ser considerado uma revolução no uso do hardware gráfico: surgem os primeiros processadores gráficos programáveis (programmable GPUs) capazes de alterar o comportamento do pipeline de renderização sem depender da CPU. Isso torna possível a implementação de diversos novos modelos de reflexão para além do tradicional modelo de Blinn–Phong disponível no pipeline de função fixa (pipeline não programável). Além disso, a capacidade de programar processadores gráficos possibilita a implementação de um incontável número de novos efeitos visuais. As GPUs programáveis tornam-se muito populares em PCs, impulsionadas pelas exigentes demandas do mercado de jogos. Ao mesmo tempo, tornam-se muito flexíveis e poderosas não só para jogos, mas também para processamento de propósito geral. O hardware gráfico programável surge no início de 2001 com o lançamento da GPU NVIDIA GeForce 3 (figura 2.23), inicialmente para o computador Apple Macintosh (Lindholm, Kilgard, and Moreton 2001). Figura 2.23: GPU NVIDIA GeForce (fonte). No início de 2001, durante o evento MacWorld Expo Tokyo, é exibido o curta metragem “Luxo Jr.”, produzido pela Pixar em 1986. Entretanto, desta vez o filme é renderizado em tempo real em um computador equipado com uma GeForce 3. Steve Jobs, então CEO da Apple, observou: “Há 15 anos, o que levava 75 horas para produzir cada segundo de vídeo, está agora sendo renderizado em tempo real na GeForce 3.” — Steve Jobs (Morris 2001) Mais tarde, as potencialidades de uma GPU similar seriam exibidas durante uma demonstração de tecnologia na conferência SIGGRAPH 2001: uma versão interativa do filme “Final Fantasy: The Spirits Within”, de Hironobu Sakaguchi, renderizada em tempo real em uma GPU NVIDIA Quadro DCC (Sakaguchi and Aida 2001). Neste evento, a NVIDIA destacou que o desempenho em operações em ponto flutuante utilizadas para desenhar apenas um quadro do filme era superior ao poder computacional total de um supercomputador Cray (tradicional fabricante de supercomputadores, adquirida em 2019 pela Hewlett Packard Enterprise) naquele momento. Ao longo da década, as GPUs de baixo custo (na faixa de 100 a 250 dólares), produzidas por empresas como NVIDIA e ATI, desbancam as estações gráficas de alto desempenho ainda baseadas em tecnologias da década anterior. As placas gráficas para computadores pessoais ultrapassam rapidamente as capacidades computacionais de sistemas como o RealityEngine da SGI, mas ao mesmo tempo com uma redução de custo superior a 90% em comparação com esses sistemas. De acordo com a Lei de Moore, e observando a diminuição do custo das CPUs nesse período, tais placas deveriam custar muito mais, em torno de 15 mil dólares. Esse avanço expressivo das GPUs é implacável com as fabricantes de estações gráficas. Em 2009, a SGI decreta falência. As APIs Direct3D (em 2006) e OpenGL (em 2009) anunciam a descontinuidade do suporte ao pipeline de função fixa. Com isso, as aplicações migram definitivamente ao uso dos shaders: programas que modificam o comportamento das etapas programáveis do pipeline, como o processamento de geometria e fragmentos (amostras de primitivas rasterizadas). Com o aumento do conjunto de instruções suportadas nas GPUs, percebe-se que é possível usar o hardware gráfico para processamento de propósito geral em tarefas como simulação de dinâmica de fluidos, operações em bancos de dados, modelagem de dinâmica molecular, criptoanálise, entre muitas outras tarefas capazes de se beneficiar de processamento paralelo. O termo GPGPU (General-Purpose Computation on GPUs) é utilizado para se referir a esse uso. Uma das tecnologias pioneiras de GPGPU foi o BrookGPU, desenvolvido na Universidade Stanford em 2004, composta de um compilador e um módulo de tempo de execução compatível com Direct3D e OpenGL (Buck et al. 2004). Até então, o processamento de propósito geral usando GPUs exigia do desenvolvedor conhecimento de APIs gráficas como Direct3D ou OpenGL para a criação de shaders customizados de vértices e pixels em linguagens como HLSL (da Microsoft), Cg (da NVIDIA), ou até mesmo em shader assembly. BrookGPU possibilitou simplificar esse fluxo de trabalho ao oferecer uma extensão de ANSI C – a linguagem Brook – voltada especificamente ao processamento paralelo de fluxos de dados. Em 2007, a NVIDIA lança a plataforma CUDA (Compute Unified Device Architecture), composta por um conjunto de ferramentas/bibliotecas e API de GPGPU para GPUs da NVIDIA. A plataforma é muito popular atualmente, impulsionada pelo crescimento das aplicações em ciência de dados e aprendizado de máquina. Influenciada pelo CUDA, surgem em 2009 outras plataformas como o DirectCompute, da Microsoft (como parte do Direct3D 11), e a especificação aberta OpenCL do Khronos Group, mesmo consórcio de indústrias que mantém o OpenGL. As primeiras oficinas e conferências sobre GPGPU, como a ACM GPGPU e a GPU Technology Conference (GTC), da NVIDIA, surgem neste período. A figura 2.24 mostra um exemplo atual de aplicação de GPGPU para a modelagem de DNA. Figura 2.24: Ligante de sulco menor do DNA, modelado através de GPGPU com o software Abalone. (fonte). 2010 A partir da década de 2010, a aceleração de gráficos 3D se expande e se populariza nos dispositivos móveis. O uso de multitexturização (uso de vários estágios de texturização) e de técnicas como normal mapping, cube mapping (para simulação de superfícies reflexivas) e shadow mapping (para simulação de sombras) torna-se comum em aplicações gráficas interativas. Em 2011, o Khronos Group anuncia o padrão WebGL, ampliando a possibilidade de uso de aceleração de gráficos 3D nos navegadores. Na segunda metade da década, a renderização baseada em física, do inglês Physically Based Rendering (PBR), começa a ser empregada em jogos de computador e em consoles. O jogo Alien: Isolation (Creative Assembly), de 2014, é um dos primeiros a explorar essa tecnologia (figura 2.25). Figura 2.25: Uso de renderização baseada em física no jogo “Alien: Isolation” (Creative Assembly) (fonte). A renderização baseada em física procura simular de forma fisicamente correta a interação da luz com os diferentes materiais de uma cena (Pharr, Jakob, and Humphreys 2016). Até então os algoritmos de iluminação e sombreamento em tempo real eram baseados em modelos empíricos, simplificados e pouco realistas, desenvolvidos para o hardware mais limitado da década anterior. Na renderização baseada em física, os materiais são descritos por informações de detalhes de microsuperfície obtidos por fotogrametria. A figura 2.26 mostra o modelo de uma arma renderizada com PBR e o conjunto de texturas utilizado. Figura 2.26: Arma renderizada com PBR usando o toolkit de renderização Marmoset, e conjunto de texturas utilizadas (fonte). Em 2014, Ian Goodfellow e seus colegas da Universidade de Montreal anunciam as Redes Adversárias Generativas (GANs) (Goodfellow et al. 2014). GANs são arquiteturas de redes neurais que permitem a geração de dados originais a partir do treinamento simultâneo de duas redes que competem entre si: uma rede geradora (por exemplo, treinada para gerar imagens de rostos de pessoas) e uma rede discriminadora (por exemplo, treinada para diferenciar rostos reais de rostos falsos). A rede geradora é otimizada a partir da discriminadora, como em um jogo minimax em que o discriminador tenta maximizar a chance de diferenciar corretamente os dados gerados dos dados reais de treinamento, e o gerador tenta minimizar a chance do discriminador classificar que os dados gerados são falsos. Em 2019, a NVIDIA Research desenvolve o StyleGAN (Karras, Laine, and Aila 2019), uma arquitetura de GAN que combina técnicas de aprendizado profundo e transferência de estilo neural (Gatys, Ecker, and Bethge 2016) para gerar rostos indistinguíveis de imagens reais. A técnica é popularizada com o site This Person Does Not Exist (Essa Pessoa Não Existe) que usa o modelo StyleGan2 para gerar um novo rosto a cada vez que a página é atualizada (figura 2.27). Figura 2.27: Rosto gerado pelo site https://thispersondoesnotexist.com/ usando o modelo StyleGAN2 da NVIDIA. Em 2016 são lançadas novas gerações de headsets de realidade virtual como o Oculus Rift e HTC Vive, que elevam as exigências de hardware gráfico para jogos que utilizam essa tecnologia. Também em 2016, o Khronos Group lança a API Vulkan como uma API de baixo nível ideal para explorar os recursos gráficos e de computação das novas gerações de GPUs. Vulkan dá ao desenvolvedor maior controle para gerenciar tarefas que anteriormente eram feitas exclusivamente pelo driver de vídeo, como a alocação, sincronização e transferência de recursos para a GPU. Vulkan também permite um melhor aproveitamento do processamento concorrente entre a CPU e a GPU. Em 2018 é incorporado ao Direct3D 12 o DirectX Raytracing (DXR), que introduz um novo pipeline gráfico destinado ao traçado de raios em tempo real. Ainda em 2018, as GPUs NVIDIA RTX série 20 são as primeiras a suportar essa tecnologia. Em 2019 a NVIDIA anuncia o Deep Learning Super Sampling (DLSS): um conjunto de tecnologias baseadas em redes neurais de aprendizagem profunda capazes de aumentar em tempo real a resolução dos quadros de exibição de jogos em computadores com GPUs RTX. A partir de uma imagem de baixa resolução, o modelo treinado consegue inferir uma imagem de alta resolução de forma mais eficiente e com mesmo nível de detalhes do que o jogo conseguiria obter caso renderizasse diretamente a imagem em alta resolução. O vídeo de divulgação a a seguir mostra o ganho de desempenho obtido com o uso de DLSS em diversos jogos: 2020 Em 2020, as GPUs NVIDIA RTX série 20 são sucedidas pela série 30, ampliando ainda mais a possibilidade de uso de traçado de raios em tempo real. A AMD lança as GPUs da série Radeon RX 6000, também com suporte a traçado de raios. Além disso, uma API de traçado de raios é incorporada ao Vulkan como o conjunto de extensões Vulkan Ray Tracing. O vídeo a seguir mostra exemplos de renderização com traçado de raios nas GPUs RTX: Ainda estamos no início da década, mas o aumento da capacidade de processamento e largura de banda de memória do hardware gráfico deve continuar a empurrar os limites do que é possível renderizar em tempo real. Efeitos atmosféricos, texturas de altíssima resolução e iluminação global6 em tempo real devem se popularizar nos próximos anos. Um exemplo do estado-da-arte em técnicas de renderização em tempo real usando iluminação global pode ser visto neste vídeo de apresentação do motor de jogo Unreal Engine 5: As tecnologias baseadas em aprendizagem profunda também devem continuar trazendos aprimoramentos na qualidade e eficiência em síntese de imagens em tempo real. A NVIDIA tem expandido tecnologias como o NVIDIA Real-Time Denoiser (NRD) para remover o ruído de imagens renderizadas com traçado de raios, o Deep Learning Anti-Aliasing (DLAA) para suavização de serrilhados (anti-aliasing) em resolução nativa, e a tecnologia Deep Learning Dynamic Super Resolution (DLDSR), que renderiza imagens de alta resolução e então reduz para a resolução da tela, obtendo com isso uma imagem de qualidade superior com mesma eficiência da renderização na resolução nativa. Esta também deve ser a década da criação de conteúdo através de modelos generativos. Em 2021, a OpenAI anunciou o DALL-E: um conjunto de modelos neurais baseados em métodos de difusão capazes de gerar imagens a partir de textos descritivos em linguagem natural. A tecnologia vem se desenvolvendo rapidamente. O DALL-E 2, anunciado em abril de 2022, permite criar imagens originais de alta qualidade, variações de imagens existentes em diferentes estilos, além de permitir a extensão de imagens para além de suas bordas originais (outpainting). Também em 2022, o Google Brain anunciou tecnologias semelhantes como o Imagen (Saharia et al. 2022) e o Parti (Yu et al. 2022). Além desses, o laboratório de pesquisa independente Midjourney anunciou uma ferramenta similar com versão beta disponível ao público através de um bot no Discord. Recentemente, uma imagem produzida no Midjourney (figura 2.28) ganhou a competição de arte digital na 2022 Colorado State Fair, gerando controvérsia entre os artistas. Figura 2.28: “Théâtre d’Opéra Spatial”, assinada por “Jason T. Allen via Midjourney”, vencedora da competição de arte digital da 2002 Colorado State Fair (fonte). O vídeo a seguir destaca os principais recursos do DALL-E 2: DALL-E, Imagen, Parti e Midjourney são tecnologias proprietárias que não disponibilizam o código fonte ou os pesos dos modelos. Entretanto, recentemente começaram a surgir modelos abertos, entre eles o Crayon, também conhecido como Dall-E Mini da Crayon LLC, o Latent Diffusion da Universidade de Heidelberg (Rombach et al. 2022), e o Stable Diffusion da StabilityAI. Demonstrações estão disponíveis online em https://www.craiyon.com/ e na plataforma Hugging Face (Latent Diffusion e Stable Diffusion). O vídeo a seguir, do canal de YouTube Two Minute Papers, apresenta os recursos do Stable Diffusion: Por fim, a década de 2020 tem presenciado avanços significativos na área de renderização neural. Renderização neural consiste no uso de tecnologias baseadas em redes neurais para reconstruir modelos 3D a partir de uma coleção de imagens 2D. Uma dessas tecnologias é o Neural Radiance Field, ou NeRF (Mildenhall et al. 2020). NeRFs são modelos de redes neurais completamente conectadas que representam a forma como a luz se propaga dentro de uma cena a partir de uma posição e orientação de visão. NeRFs podem ser usadas para gerar representações implícitas das superfícies da cena, mas também para renderizar imagens a partir de qualquer ponto de vista, com sombreamento e iluminação. Uma técnica estado-da-arte de renderização neural é o Instant NeRF, da NVIDIA Research. Um vídeo de demonstração é exibido a seguir: Referências "],["firstapp.html", "2.3 Primeiro programa", " 2.3 Primeiro programa Nesta seção seguiremos um passo a passo de construção de um primeiro programa com a biblioteca ABCg. Será o nosso “Hello, World!”, similar ao exemplo da ABCg mostrado na seção 1.5, mas sem o triângulo colorido renderizado com OpenGL. Configuração inicial Faça uma cópia (ou fork) do conteúdo de https://github.com/hbatagelo/abcg.git. Desse modo você poderá modificar livremente a biblioteca e armazená-la em seu repositório pessoal. Como a ABCg já tem um projeto de exemplo chamado helloworld, vamos chamar o nosso de firstapp. Em abcg/examples, crie o subdiretório abcg/examples/firstapp. A escolha de deixar o projeto como um subdiretório de abcg/examples é conveniente pois bastará construir a ABCg para que o nosso projeto seja automaticamente construído como um exemplo adicional da biblioteca. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(firstapp). O conteúdo ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) Dessa forma o CMake incluirá o subdiretório firstapp na busca de um script CMakeLists.txt contendo a configuração do projeto. Crie o arquivo abcg/examples/firstapp/CMakeLists.txt. Edite-o com o seguinte conteúdo: project(firstapp) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) O comando project na primeira linha define o nome do projeto. Em seguida, add_executable define que o executável terá o mesmo nome definido em project e será gerado a partir dos fontes main.cpp e window.cpp (não é necessário colocar os arquivos .h ou .hpp). Por fim, a função enable_abcg() configura o projeto para usar a ABCg. Essa função é definida em abcg/cmake/ABCg.cmake, que é um script CMake chamado a partir do CMakeLists.txt do diretório raiz. Em abcg/examples/firstapp, crie os arquivos main.cpp, window.cpp e window.hpp. Vamos editá-los a seguir. main.cpp Em main.cpp definimos o ponto de entrada da aplicação: #include &quot;window.hpp&quot; int main(int argc, char **argv) { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window Window window; window.setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(window); return 0; } Na primeira linha incluímos o arquivo de cabeçalho que terá a definição de uma classe customizada Window responsável pelo comportamento da janela da aplicação. Faremos com que essa classe seja derivada de abcg::OpenGLWindow para que possamos usar as funções da ABCg de gerenciamento de janelas compatíveis com OpenGL; Na linha 5 criamos um objeto app da classe abcg::Application, responsável pelo gerenciamento da aplicação; Na linha 8 criamos o objeto window que presenta nossa janela customizada; Na linha 9 definimos o título da janela. setWindowSettings é uma função membro de abcg::OpenGLWindow, classe base de Window. A função recebe uma estrutura abcg::WindowSettings contendo as configurações da janela; Na linha 12, a função abcg::Application::run é chamada para inicializar os subsistemas da SDL, inicializar a janela recém-criada e entrar no laço principal da aplicação. A função retornará somente quando a janela de aplicação for fechada. Observação Todas as classes e funções da ABCg fazem parte do namespace abcg. Como vimos no código anterior, abcg::OpenGLWindow é uma classe da ABCg responsável pelo gerenciamento de janelas compatíveis com OpenGL. De modo semelhante, abcg::Application é uma classe da ABCg responsável pelo gerenciamento da aplicação. Em todos os programas que faremos durante a disciplina, começaremos definindo uma classe derivada de abcg::OpenGLWindow, como a classe Window de nossa primeira aplicação. Tal classe derivada será customizada com comandos do OpenGL para que possamos desenhar o conteúdo da janela. Internamente a ABCg usa tratamento de exceções. As exceções são lançadas como objetos da classe abcg::Exception, derivada de std::exception. Vamos alterar um pouco o código anterior para capturar as exceções que possam ocorrer e imprimir no console a mensagem de erro correspondente. O código final de main.cpp ficará assim: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window Window window; window.setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } O código anterior foi colocado dentro do escopo try de um bloco try...catch. No escopo catch, a função fmt::print imprime no erro padrão (stderr) a mensagem de erro associada com a exceção capturada. fmt::print faz parte da biblioteca {fmt}. Ela permite a formatação e impressão de strings usando uma sintaxe parecida com as f-strings da linguagem Python7. window.hpp No arquivo window.hpp definiremos nossa classe Window que será responsável pelo gerenciamento do conteúdo da janela da aplicação: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow {}; #endif Observe novamente que nossa classe Window é derivada de abcg::OpenGLWindow, que faz parte da ABCg. abcg::OpenGLWindow gerencia uma janela capaz de renderizar gráficos com a API OpenGL. A classe possui um conjunto de funções virtuais que podem ser substituídas pela classe derivada de modo a alterar o comportamento da janela. O comportamento padrão consiste em desenhar a janela com fundo preto, com um contador de FPS (Frames Per Second, ou quadros por segundo) sobreposto no canto superior esquerdo da janela, e um botão no canto inferior esquerdo para alternar entre tela cheia e modo janela (com atalho pela tecla F11). O contador e o botão são gerenciados pela biblioteca Dear ImGui (no restante do texto vamos chamá-la apenas de ImGui). Por enquanto nossa classe não faz nada de especial. Ela só deriva de abcg::OpenGLWindow e não define nenhuma função ou variável membro. Mesmo assim, já podemos construir a aplicação. Experimente fazer isso. Na linha de comando, use o script build.sh (Linux/macOS) ou build.bat (Windows). Se você estiver no Visual Studio Code, abra a pasta abcg pelo editor, use a opção de configuração do CMake e então construa o projeto (F7). O executável será gerado em abcg/build/bin/firstapp. Da forma como está, a aplicação mostrará uma janela com fundo preto e os dois controles de GUI (widgets) mencionados anteriomente. Isso acontece porque Window não substitui nenhuma das funções virtuais de abcg::OpenGLWindow. Todo o comportamento está sendo definido pela classe base: Vamos alterar o conteúdo e o comportamento da nossa janela Window. Imitaremos o comportamento do projeto helloworld que cria uma pequena subjanela da ImGui. Modifique window.hpp para o código a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; private: std::array&lt;float, 4&gt; m_clearColor{0.906f, 0.910f, 0.918f, 1.0f}; }; #endif onCreate, onPaint e onPaintUI substituem funções virtuais de abcg::OpenGLWindow. A palavra-chave override é opcional, mas é recomendável pois deixa explícito que as funções são substituições das funções virtuais da classe base: onCreate é onde colocaremos os comandos de inicialização do estado da janela e do OpenGL. Internamente a ABCg chama essa função apenas uma vez no início do programa, após ter inicializado os subsistemas da SDL e o OpenGL. onPaint é onde colocaremos todas as funções de desenho do OpenGL. Internamente a ABCg chama essa função continuamente no laço principal da aplicação, uma vez para cada quadro (frame) de exibição. Por exemplo, na imagem acima, onPaint estava sendo chamada a uma média de 3988,7 vezes por segundo; onPaintUI é onde colocaremos todas as funções de desenho de widgets da ImGui (botões, menus, caixas de seleção, etc). Internamente, onPaintUI é chamado logo depois que onPaint é chamado; m_clearColor é um arranjo de quatro valores float entre 0 e 1. Esses valores definem a cor RGBA de fundo da janela, que neste caso é um cinza claro. Observação Poderíamos ter definido m_clearColor da seguinte forma, mais familiar aos programadores em C: float m_clearColor[4] = {0.906f, 0.910f, 0.918f, 1.0f}; Entretanto, em C++ o std::array é a forma recomendada e mais segura de trabalhar com arranjos. window.cpp Em window.cpp, definiremos as funções virtuais substituídas: #include &quot;window.hpp&quot; void Window::onCreate() { auto const &amp;windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } void Window::onPaint() { // Set the clear color abcg::glClearColor(m_clearColor.at(0), m_clearColor.at(1), m_clearColor.at(2), m_clearColor.at(3)); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onPaintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::onPaintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto const &amp;windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Vejamos com mais atenção o trecho com a definição de Window::onCreate: void Window::onCreate() { auto const &amp;windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } Na linha 4, windowSettings é uma estrutura abcg::WindowSettings retornada por abcg::OpenGLWindow::getWindowSettings() com as configurações da janela. Na linha 5, fmt::print imprime no console o tamanho da janela8. Observe agora o trecho com a definição de Window::onPaint: void Window::onPaint() { // Set the clear color abcg::glClearColor(m_clearColor.at(0), m_clearColor.at(1), m_clearColor.at(2), m_clearColor.at(3)); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } Aqui são chamadas duas funções do OpenGL: glClearColor e glClear. glClearColor é utilizada para determinar a cor que será usada para limpar a janela9. A função recebe quatro parâmetros do tipo float (red, green, blue, alpha), que correspondem às componentes de cor RGB e um valor adicional de opacidade (alpha). Esse formato de cor é chamado de RGBA. Os valores são fixados (clamped) no intervalo \\([0,1]\\) em ponto flutuante. glClear, usando como argumento a constante GL_COLOR_BUFFER_BIT, limpa a janela com a cor especificada na última chamada de glClearColor. Em resumo, nosso onPaint limpa a tela com a cor RGBA especificada em m_clearColor. Importante As funções do OpenGL são prefixadas com as letras gl; As constantes do OpenGL são prefixadas com GL_. Neste curso usaremos as funções do OpenGL que são comuns ao OpenGL ES 3.0 de modo a manter compatibilidade com o WebGL 2.0. Assim conseguiremos fazer aplicações que podem ser executadas tanto no desktop quanto no navegador usando o mesmo código fonte. A versão mais recente do OpenGL é a 4.6. A documentação de cada versão está disponível em https://registry.khronos.org/OpenGL/index_gl.php. Observação Na ABCg, podemos prefixar as funções gl com o namespace abcg de modo a rastrear erros do OpenGL com o sistema de tratamento de exceções da ABCg. Por exemplo, ao escrevermos abcg::glClear no lugar de glClear, estamos na verdade chamando uma função wrapper que verifica automaticamente se a chamada da função OpenGL é válida. Se algum erro ocorrer, uma exceção será lançada e capturada pelo catch que implementamos na função main. A mensagem de erro (retornada por exception.what() no escopo do catch) inclui a descrição do erro, o nome do arquivo, o nome da função e o número da linha do código onde o erro foi detectado. Isso pode ser bastante útil para a depuração de erros do OpenGL. Sempre que possível, prefixe as funções do OpenGL com abcg::. A verificação automática de erros do OpenGL é habilitada somente quando a aplicação é compilada no modo Debug. Não há sobrecarga nas chamadas das funções do OpenGL com o namespace abcg quando a aplicação é compilada em modo Release. Agora vamos à definição de Window::onPaintUI, responsável pelo desenho da interface usando a ImGui: void Window::onPaintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::onPaintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto const &amp;windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Na linha 19 é chamada a função membro onPaintUI da classe base, que mostra o medidor de FPS e o botão para alternar entre o modo janela e tela cheia. Na linha 24 é criada uma janela da ImGui com o título “Hello, First App!”. A partir desta linha, até a linha 43, todas as chamadas a funções da ImGui criam widgets dentro dessa janela recém-criada. Apenas para isso ficar mais explícito, todo o código que corresponde a esta janela está dentro do escopo delimitado pelas chaves nas linhas 22 e 44. Na linha 27 é criado um texto estático que mostra o tamanho atual da janela. Na linha 32 é criado um slider horizontal que pode variar de 0 a 1 em ponto flutuante. O valor do slider é armazenado em f. A variável f é declarada como static para que seu estado seja retido entre as chamadas de onPaintUI (outra opção é declarar a variável como membro da classe). Na linha 36 é criado um widget de edição de cor para alterar os valores de m_clearColor. Na linha 39 é criado mais um texto estático com informações de FPS extraídas de ImGui::GetIO().Framerate. Esse código é praticamente o mesmo do “Hello, World!”. Construa a aplicação para ver o resultado: Exercício A seguir temos alguns exemplos de uso de outros widgets da ImGui. Experimente incluir esses trechos de código em onPaintUI: Botões: // 100x50 button if (ImGui::Button(&quot;Press me!&quot;, ImVec2(100, 50))) { fmt::print(&quot;1st button pressed.\\n&quot;); } // Nx50 button, where N is the remaining width available ImGui::Button(&quot;Press me!&quot;, ImVec2(-1, 50)); // See also IsItemHovered, IsItemActive, etc if (ImGui::IsItemClicked()) { fmt::print(&quot;2nd Button pressed.\\n&quot;); } Checkbox: static bool enabled{true}; ImGui::Checkbox(&quot;Some option&quot;, &amp;enabled); fmt::print(&quot;The checkbox is {}\\n&quot;, enabled ? &quot;enabled&quot; : &quot;disabled&quot;); Combo box: static std::size_t currentIndex{}; std::vector comboItems{&quot;AAA&quot;, &quot;BBB&quot;, &quot;CCC&quot;}; if (ImGui::BeginCombo(&quot;Combo box&quot;, comboItems.at(currentIndex))) { for (auto index{0U}; index &lt; comboItems.size(); ++index) { bool const isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index), isSelected)) currentIndex = index; // Set the initial focus when opening the combo (scrolling + keyboard // navigation focus) if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } fmt::print(&quot;Selected combo box item: {}\\n&quot;, comboItems.at(currentIndex)); Menu (em uma janela de tamanho fixo e com o flag adicional ImGuiWindowFlags_MenuBar para permitir o uso da barra de menu): ImGui::SetNextWindowSize(ImVec2(300, 100)); auto flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize}; ImGui::Begin(&quot;Window with menu&quot;, nullptr, flags); { bool save{}; static bool showCompliment{}; // Hold state // Menu Bar if (ImGui::BeginMenuBar()) { // File menu if (ImGui::BeginMenu(&quot;File&quot;)) { ImGui::MenuItem(&quot;Save&quot;, nullptr, &amp;save); ImGui::EndMenu(); } // View menu if (ImGui::BeginMenu(&quot;View&quot;)) { ImGui::MenuItem(&quot;Show Compliment&quot;, nullptr, &amp;showCompliment); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (save) { // Save file... } if (showCompliment) { ImGui::Text(&quot;You&#39;re a beautiful person.&quot;); } } ImGui::End(); Mais sliders: static std::array pos2d{0.0f, 0.0f}; ImGui::SliderFloat2(&quot;2D position&quot;, pos2d.data(), 0.0, 50.0); static std::array pos3d{0.0f, 0.0f, 0.0f}; ImGui::SliderFloat3(&quot;3D position&quot;, pos3d.data(), -1.0, 1.0); Dica A ImGui não tem um manual com exemplos de uso de todos os widgets suportados. A melhor referência atualmente é o código da função ImGui::ShowDemoWindow em abcg/external/imgui/imgui_demo.cpp. Essa função cria uma janela de demonstração contendo uma grande variedade de exemplos de uso de widgets e recursos da ImGui. No exemplo “Hello, World!”, tal janela é exibida quando a caixa de seleção “Show demo window” está ativada. Por exemplo, caso você queira implementar um “List Box” como exibido na janela de demonstração abaixo, procure pela string “List boxes” ou “listbox 1” em abcg/external/imgui/imgui_demo.cpp, copie o código correspondente e adapte-o em sua aplicação. O código correspondente em imgui_demo.cpp é o seguinte: const char* items[] = { &quot;AAAA&quot;, &quot;BBBB&quot;, &quot;CCCC&quot;, &quot;DDDD&quot;, &quot;EEEE&quot;, &quot;FFFF&quot;, &quot;GGGG&quot;, &quot;HHHH&quot;, &quot;IIII&quot;, &quot;JJJJ&quot;, &quot;KKKK&quot;, &quot;LLLLLLL&quot;, &quot;MMMM&quot;, &quot;OOOOOOO&quot; }; static int item_current_idx = 0; // Here we store our selection data as an index. if (ImGui::BeginListBox(&quot;listbox 1&quot;)) { for (int n = 0; n &lt; IM_ARRAYSIZE(items); n++) { const bool is_selected = (item_current_idx == n); if (ImGui::Selectable(items[n], is_selected)) item_current_idx = n; // Set the initial focus when opening the combo (scrolling + keyboard navigation focus) if (is_selected) ImGui::SetItemDefaultFocus(); } ImGui::EndListBox(); } A documentação das funções, constantes e enumerações está no formato de comentários nos arquivos abcg/external/imgui/imgui.h e abcg/external/imgui/imgui.cpp. Observação A ImGui é uma biblioteca de GUI que trabalha em modo imediato (o “Im” de ImGui vem de immediate mode), isto é, os controles de UI não retém estado entre os quadros de exibição. Sempre que a função onPaintUI é chamada, a GUI é redesenhada por completo. O gerenciamento de estado deve ser feito pelo usuário, por exemplo, através de variáveis estáticas (como a variável f da linha 32 de window.cpp para guardar o valor do slider) ou variáveis membros da classe (como a variável m_clearColor da classe Window)10. Compilando para WebAssembly Para compilar nossa aplicação para WebAssembly basta usar o script build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Apenas certifique-se de habilitar antes as variáveis de ambiente do SDK do Emscripten como fizemos na seção 1.5. Após a construção do projeto, os arquivos resultantes (firstapp.js e firstapp.wasm) serão gravados no subdiretório public. Para usá-los vamos precisar de um arquivo HTML. Faça uma cópia de um dos arquivos HTML já existentes em public (helloworld.html, full_window.html ou full_window_console.html). Mude o nome do arquivo para firstapp.html. No final do arquivo, mude a string src=\"helloworld.js\" para src=\"firstapp.js\", assim: &lt;script async type=&quot;text/javascript&quot; src=&quot;firstapp.js&quot;&gt;&lt;/script&gt; Para testar, monte o servidor local com runweb.sh ou runweb.bat e abra o arquivo HTML em http://localhost:8080/. Dica Disponibilize o conteúdo web de seus projetos no GitHub Pages para formar um portfólio de atividades feitas no curso: Na sua conta do GitHub, crie um repositório com visibilidade pública. Pode ser seu próprio fork da ABCg. No diretório raiz, crie um subdiretório firstapp com os arquivos firstapp.*, mas renomeie firstapp.html para index.html; Nas configurações do repositório no GitHub, habilite o GitHub Pages informando o branch que será utilizado (por exemplo, main). O conteúdo estará disponível em https://username.github.io/reponame/firstapp/ onde username e reponame são respectivamente seu nome de usuário e o nome do repositório. Ao longo do quadrimestre, suba seus projetos nesse repositório. No diretório raiz você pode criar um index.html com a descrição do portfólio e o link para cada página de projeto. Um subconjunto da {fmt} foi incorporado à biblioteca de formatação de texto no C++20. O suporte equivalente ao fmt::print (impressão formatada com saída padrão) está disponível no C++23 através de std::print, mas ainda não é suportado nas principais implementações da biblioteca padrão.↩︎ O tamanho padrão para uma aplicação desktop é 800x600. Na versão para web, a janela pode ser redimensionada de acordo com a regra CSS do elemento canvas do HTML5.↩︎ Mais precisamente, glClearColor define a cor que será utilizada para limpar os buffers de cor do framebuffer. Veremos mais sobre o conceito de framebuffer nos próximos capítulos.↩︎ A ImGui pode reter algum estado dos controles de UI entre os frames. Por exemplo, em uma janela com vários botões, a ImGui guarda internamente qual botão está com o foco atual. Em um widget do tipo árvore, ela guarda a informação de quais nós estão expandidos e quais estão colapsados.↩︎ "],["tictactoe.html", "2.4 Jogo da Velha", " 2.4 Jogo da Velha Usando o projeto firstapp como base, faremos nesta seção um “Jogo da Velha” com interface composta apenas de widgets da ImGui. Com isso ficaremos mais familiarizados com a ImGui e entraremos em contato com novas funções da biblioteca, tais como: ImGui::BeginTable e ImGui::EndTable para fazer tabelas; ImGui::Spacing para adicionar espaçamentos verticais; ImGui::PushFont e ImGui::PopFont para usar novas fontes. A ideia principal é simular o tabuleiro do jogo com um arranjo de 3x3 botões: O jogo começará com os botões vazios, sem texto. Cada vez que um botão for pressionado, seu texto será substituído por um X ou O de acordo com o turno do jogador. Para simplificar não jogaremos contra o computador: o jogo só funcionará no modo “humano versus humano”; Internamente manteremos um arranjo contendo o estado do jogo para determinar se houve um vencedor ou se “deu velha” (empate); Usaremos um widget de texto estático para mostrar o turno atual e o resultado do jogo; Incluiremos também um botão e uma opção de menu para reiniciar o jogo. O resultado ficará como a seguir: Configuração inicial Nosso projeto será chamado tictactoe. Em abcg/examples, crie o subdiretório abcg/examples/tictactoe. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(tictactoe). Para evitar que os projetos anteriores continuem sendo compilados, comente as linhas anteriores. O resultado ficará assim: # add_subdirectory(helloworld) # add_subdirectory(firstapp) add_subdirectory(tictactoe) Crie o arquivo abcg/examples/tictactoe/CMakeLists.txt. O conteúdo é o mesmo do projeto anterior. A única mudança é o nome do projeto: project(tictactoe) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Em abcg/examples/tictactoe, crie o subdiretório assets. Nas aplicações usando a ABCg, o subdiretório assets é utilizado para armazenar arquivos de recursos utilizados em tempo de execução (arquivos de fontes, imagens, sons, etc). No nosso caso, colocaremos em assets o arquivo de fonte TrueType Inconsolata-Medium.ttf que será utilizado para o texto dos Xs e Os. O arquivo pode ser baixado, ou simplesmente copiado de abcg/abcg/assets (essa também é a fonte padrão da ABCg). Importante Sempre que um projeto da ABCg é configurado pelo CMake, o diretório assets é copiado automaticamente para o diretório do executável (build/bin/proj, onde proj é o nome do projeto). Toda vez que um arquivo de assets for modificado, é necessário limpar o diretório build para forçar a cópia de assets para build/bin/proj na próxima compilação. Isso pode ser feito de diferentes maneiras: Removendo o diretório build antes de compilar novamente; No VS Code, usando o comando “CMake: Clean Rebuild” da paleta de comandos (Ctrl+Shift+P); Construindo o projeto através da linha de comando com build.sh/build.bat. Em abcg/examples/tictactoe, crie os arquivos main.cpp, window.cpp e window.hpp. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é praticamente o mesmo de nossa primeira aplicação. A única diferença é o título da janela e seu tamanho inicial, que agora será 600x600. #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setWindowSettings( {.width = 600, .height = 600, .title = &quot;Tic-Tac-Toe&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } window.hpp Aqui definiremos nossa classe Window, responsável pelo gerenciamento da janela da aplicação e também da lógica do jogo. O conteúdo ficará como a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaintUI() override; private: static int const m_N{3}; // Board size is m_N x m_N enum class GameState { Play, Draw, WinX, WinO }; GameState m_gameState; bool m_XsTurn{true}; std::array&lt;char, m_N * m_N&gt; m_board{}; // &#39;\\0&#39;, &#39;X&#39; or &#39;O&#39; ImFont *m_font{}; void checkEndCondition(); void restartGame(); }; #endif Em comparação com o projeto firstapp, desta vez não substituímos o método onPaint. Podemos fazer isso pois todo o conteúdo da janela será composto por controles de UI desenhados em onPaintUI. Nossa aplicação precisa de algumas variáveis para armazenar o estado do jogo. Na linha 12, m_N é o tamanho dos lados do tabuleiro. O Jogo da Velha é jogado em um tabuleiro 3x3, mas podemos mudar esse valor para jogar com um tabuleiro 4x4, 5x5, etc. O código é genérico o suficiente para permitir isso. Na linha 14 definimos GameState como uma enumeração de todos os possíveis estados do jogo. Os estados serão interpretados da seguinte maneira: GameState::Play é quando a partida está sendo jogada. Nesse estado o jogador do turno atual poderá clicar em algum lugar do tabuleiro para colocar um X ou O; GameState::Draw é quando o jogo acabou e “deu velha”; GameState::WinX é quando o jogo acabou e X ganhou; GameState::WinO é quando o jogo acabou e O ganhou. O estado atual será indicado por m_gameState na linha 15. Na linha 17, m_XsTurn é uma variável que indica se o turno atual é do X. Na linha 18, m_board é o estado do tabuleiro, definido como um arranjo de 3x3=9 caracteres (arranjo 3x3 orientado a linhas). Cada caractere pode ser \\0 (caractere nulo) para indicar que a posição está vazia, ou a letra X, ou a letra O. Na linha 20, o ponteiro m_font será usado para representar a fonte dos Xs e Os. A classe tem duas funções: checkEndCondition, que será usada no final de cada turno para verificar se m_board está em alguma condição de vitória ou empate; restateGame, para limpar o tabuleiro e iniciar um novo jogo. window.cpp Aqui definiremos as funções membro da classe Window. Começaremos definindo Window::onCreate. Como Window::onCreate é chamada apenas uma vez quando a janela é criada, ela é ideal para fazermos as configurações iniciais da aplicação, como o carregamento da nova fonte para os Xs e Os, que tem tamanho maior que a fonte padrão. O resultado ficará assim: #include &quot;window.hpp&quot; void Window::onCreate() { // Load font with bigger size for the X&#39;s and O&#39;s auto const filename{abcg::Application::getAssetsPath() + &quot;Inconsolata-Medium.ttf&quot;}; m_font = ImGui::GetIO().Fonts-&gt;AddFontFromFileTTF(filename.c_str(), 72.0f); if (m_font == nullptr) { throw abcg::RuntimeError{&quot;Cannot load font file&quot;}; } restartGame(); } Na linha 5 criamos uma string com o caminho completo do arquivo Inconsolata-Medium.ttf. Para isso usamos a função abcg::Application::getAssetsPath() que retorna o caminho atual do subdiretório assets. Na linha 7 chamamos as funções da ImGui para carregar a fonte com tamanho 72. Se ocorrer algum erro no carregamento, o ponteiro m_Font será nulo. Neste caso lançamos uma exceção (linha 9) que será capturada no bloco try...catch de main. Na linha 12 chamamos a função restartGame para deixar o jogo pronto para uma nova partida. A propósito, vejamos como fica a implementação de restartGame: void Window::restartGame() { m_board.fill(&#39;\\0&#39;); m_gameState = GameState::Play; } A função simplesmente preenche o tabuleiro com caracteres nulos e define o estado do jogo como GameState::Play. Vamos agora definir Window::onPaintUI. É nessa função que a interface será desenhada e a lógica de interação com os controles de UI implementada. Começaremos definindo a janela da ImGui: void Window::onPaintUI() { // Get size of application&#39;s window auto const appWindowWidth{gsl::narrow&lt;float&gt;(getWindowSettings().width)}; auto const appWindowHeight{gsl::narrow&lt;float&gt;(getWindowSettings().height)}; // &quot;Tic-Tac-Toe&quot; window { ImGui::SetNextWindowSize(ImVec2(appWindowWidth, appWindowHeight)); ImGui::SetNextWindowPos(ImVec2(0, 0)); auto const flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse}; ImGui::Begin(&quot;Tic-Tac-Toe&quot;, nullptr, flags); // TODO: Add Menu // TODO: Add static text showing current turn or win/draw messages // TODO: Add game board // TODO: Add &quot;Restart game&quot; button ImGui::End(); } } Nas linhas 17 e 18 pegamos o tamanho atual da janela da aplicação através da função abcg::getWindowSettings. Essa função retorna uma referência a um objeto do tipo abcg::WindowSettings contendo as configurações da janela, incluindo sua largura (width) e altura (height). Os valores são inteiros e precisam ser convertidos para float para serem utilizados nas funções da ImGui. O conteúdo da janela da ImGui é definido no escopo das linhas 21 a 35. Em particular, na linha 22 chamamos ImGui::SetNextWindowSize para informar que a janela que estamos prestes a criar terá o tamanho da janela da aplicação. De forma parecida, na linha 23 chamamos ImGui::SetNextWindowPos para informar que tal janela deverá ser posicionada na coordenada (0,0) da janela da aplicação (canto superior esquerdo). Na linha 25 definimos uma máscara de bits com as propriedades da janela que será criada. A janela terá uma barra de menu (ImGuiWindowFlags_MenuBar), não poderá ser redimensionada (ImGuiWindowFlags_NoResize), e não poderá ser colapsada ao clicar na barra de título (ImGuiWindowFlags_NoCollapse). Na linha 27 criamos de fato o controle de UI da janela. Ela é criada com as configurações definidas anteriormente. Todos os widgets criados entre essa linha de ImGui::Begin até a linha 34 de ImGui::End() serão colocados dentro dessa janela. Por enquanto a janela está vazia e só deixamos alguns comentários de tarefas a fazer: // TODO: Add Menu // TODO: Add static text showing current turn or win/draw messages // TODO: Add game board // TODO: Add &quot;Restart game&quot; button Vamos fazer essas tarefas a seguir. Adicionando o menu Substitua a linha de comentário TODO: Add Menu pelo seguinte trecho de código: // Menu { bool restartSelected{}; if (ImGui::BeginMenuBar()) { if (ImGui::BeginMenu(&quot;Game&quot;)) { ImGui::MenuItem(&quot;Restart&quot;, nullptr, &amp;restartSelected); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (restartSelected) { restartGame(); } } Este código cria uma barra de menu com uma opção “Game”. Dentro desta opção há apenas um item de menu chamado “Restart”. Observe que o estado de “Restart” é armazenado na variável booleana restartSelected, inicializada com false na linha 31. Se o item de menu é selecionado, ImGui::MenuItem muda restartSelected para true e assim chamamos restartGame na linha 40 para reiniciar o estado do jogo. Adicionando o texto do turno atual, de vitória e empate Continuando com as tarefas por fazer, substitua a linha de comentário TODO: Add static text showing current turn or win/draw messages pelo seguinte código: // Static text showing current turn or win/draw messages { std::string text; switch (m_gameState) { case GameState::Play: text = fmt::format(&quot;{}&#39;s turn&quot;, m_XsTurn ? &#39;X&#39; : &#39;O&#39;); break; case GameState::Draw: text = &quot;Draw!&quot;; break; case GameState::WinX: text = &quot;X&#39;s player wins!&quot;; break; case GameState::WinO: text = &quot;O&#39;s player wins!&quot;; break; } // Center text ImGui::SetCursorPosX( (appWindowWidth - ImGui::CalcTextSize(text.c_str()).x) / 2); ImGui::Text(&quot;%s&quot;, text.c_str()); ImGui::Spacing(); } ImGui::Spacing(); Na linha 46 definimos uma string que recebe um texto diferente dependendo do estado atual de m_gameState. Se o jogo está no modo Play, o texto será X's turn ou O's turn. Se o jogo está no modo Draw, WinX ou WinO, mensagens correspondentes de empate e vitória serão utilizadas. Na linha 62, ImGui::SetCursorPosX define a posição horizontal da janela em que o texto começará a ser exibido, da esquerda para a direita. Para que o texto fique centralizado horizontalmente, sua posição inicial deve ser a metade da largura da janela (appWindowWidth / 2) menos a metade da largura do texto (calculada com ImGui::CalcTextSize). O widget de texto é criado na linha 64. Em seguida, adicionamos um espaçamento vertical na linha 65, e outro na linha 68 para deixar um bom espaço entre o texto e o tabuleiro que será desenhado na próxima etapa. Implementando o tabuleiro O tabuleiro será mostrado como uma grade de 3x3 botões (no caso de m_N ser 3), sendo que cada botão terá como texto o caractere correspondente em m_board. Substitua a linha de comentário TODO: Add game board pelo seguinte código: // Game board { auto const gridHeight{appWindowHeight - 22 - 60 - (m_N * 10) - 60}; auto const buttonHeight{gridHeight / m_N}; // Use custom font ImGui::PushFont(m_font); if (ImGui::BeginTable(&quot;Game board&quot;, m_N)) { for (auto i : iter::range(m_N)) { ImGui::TableNextRow(); for (auto j : iter::range(m_N)) { ImGui::TableSetColumnIndex(j); auto const offset{i * m_N + j}; // Get current character auto ch{m_board.at(offset)}; // Replace null character with whitespace because the button label // cannot be an empty string if (ch == 0) { ch = &#39; &#39;; } // Button text is ch followed by an ID in the format ##ij auto buttonText{fmt::format(&quot;{}##{}{}&quot;, ch, i, j)}; if (ImGui::Button(buttonText.c_str(), ImVec2(-1, buttonHeight))) { if (m_gameState == GameState::Play &amp;&amp; ch == &#39; &#39;) { m_board.at(offset) = m_XsTurn ? &#39;X&#39; : &#39;O&#39;; checkEndCondition(); m_XsTurn = !m_XsTurn; } } } ImGui::Spacing(); } ImGui::EndTable(); } ImGui::PopFont(); } ImGui::Spacing(); Neste código, primeiro começamos calculando dois valores de altura (linhas 72 e 73). gridHeight é a altura da área útil do tabuleiro. Ela é calculada a partir da altura da janela, subtraída da altura aproximada dos outros controles de UI (22 da barra de menu, 60+60 para a área acima e abaixo do tabuleiro) e do espaçamento entre os botões do tabuleiro (10). buttonHeight é a altura de cada botão. Na linha 76, ImGui::PushFont faz com que a fonte m_font seja ativada no lugar da fonte padrão. Todo controle de UI definido entre essa linha até ImGui::PopFont (linha 107) usará essa fonte. Na linha 77 criamos uma tabela “Game board”, composta de m_N colunas. O nome “Game board” não aparece na tela. Ele serve apenas como um identificador deste controle de UI. Os laços das linha 78 e 79 iteram sobre as linhas e colunas do tabuleiro, respectivamente. Para cada nova linha chamamos ImGui::TableNextRow (linha 79), e para cada coluna chamamos ImGui::TableSetColumnIndex (linha 81) com o índice da coluna. Na linha 85 lemos o caractere atual de m_board para a linha e coluna atual. Este caractere é a letra (X, O, ou vazio) que deve ser exibida como texto do botão. Entretanto, se for um caractere nulo, mudamos para um espaço (linhas 89 a 91) pois o comando ImGui::Button não aceita strings vazias. Na linha 95 criamos o botão atual. Seu tamanho é ImVec2(-1, buttonHeight), o que significa que a largura será a máxima possível (definida por -1 ou outro valor negativo) e a altura será buttonHeight. O texto do botão (buttonText) é definido de forma um pouco mais complicada. O texto não é só o caractere ch. Se o caractere na posição (0,2) for um X, buttonText será X##02. Esse ##02 não é mostrado no botão. Ele é utilizado para indicar à ImGui que o identificador do botão é a string 02. Cada botão precisa ter um identificador único. A ImGui usa esses identificadores para definir quem está com o foco atual do teclado. Geralmente a ImGui usa o próprio texto do botão como identificador, mas como temos possivelmente vários botões com o mesmo texto (X, O ou espaço), precisamos recorrer à sintaxe ##id para definir identificadores únicos. Se o botão é pressionado, ImGui::Button retorna true. Nesse caso, precisamos verificar se a posição correspondente de m_board pode ser de fato modificada. Para isso o jogo deve estar no modo Play e ch não pode ser X ou O (linha 96), isto é, cada posição do tabuleiro só pode ser preenchida uma vez. Na linha 97 modificamos m_board para X ou O dependendo de quem está jogando o turno atual. Em seguida chamamos checkEndCondition para verificar se houve vitória ou empate (linha 98), e então alternamos o turno do jogador (linha 99). Adicionando o botão de reinício Abaixo do tabuleiro colocaremos um botão para reiniciar o jogo. Substitua a linha de comentário TODO: Add \"Restart game\" button pelo seguinte código: // &quot;Restart game&quot; button { if (ImGui::Button(&quot;Restart game&quot;, ImVec2(-1, 50))) { restartGame(); } } O botão terá largura máxima (-1) e altura 50. Note que, sempre que definimos um novo controle de UI, a ImGui cria o controle em uma nova linha (com exceção dos botões da tabela, criados entre ImGui::BeginTable e ImGui::EndTable). Fora de uma tabela, se quisermos que os controles não fiquem empilhados, podemos chamar ImGui::SameLine antes de criar o próximo controle. Assim ele será criado do lado direito do anterior. Verificando a condição de vitória e empate Para concluir a implementação de nosso Jogo da Velha, precisamos definir a função checkEndCondition. Essa função é chamada após cada jogada para verificar se m_board está em alguma condição de vitória ou empate: Se alguma linha, coluna ou diagonal de m_board tiver somente X, então o jogador do X ganhou (devemos mudar o estado do jogo para GameState::WinX). Se alguma linha, coluna ou diagonal de m_board tiver somente O, então o jogador do O ganhou (devemos ir ao estado GameState::WinO). Se o tabuleiro não tiver mais nenhum caractere nulo, e nem o X nem o O ganharam, então “deu velha” (devemos ir ao estado GameState::Draw). Vamos implementar a função aos poucos, começando com o código a seguir. Note que há vários comentários com tarefas por fazer (TODO): void Window::checkEndCondition() { if (m_gameState != GameState::Play) { return; } // Lambda expression that checks if a string contains only Xs or Os. If so, it // changes the game state to WinX or WinO accordingly and returns true. // Otherwise, returns false. auto allXsOrOs{[&amp;](std::string_view str) { if (str == std::string(m_N, &#39;X&#39;)) { m_gameState = GameState::WinX; return true; } if (str == std::string(m_N, &#39;O&#39;)) { m_gameState = GameState::WinO; return true; } return false; }}; // TODO: Check rows // TODO: Check columns // TODO: Check main diagonal // TODO: Check inverse diagonal // TODO: Check draw } O tabuleiro só precisa ser verificado quando o jogo está no estado GameState::Play. A condição da linha 124 verifica isso. Na linha 131 definimos uma expressão lambda que será usada várias vezes posteriormente para verificar as linhas, colunas e diagonais de m_board. Mais especificamente, a expressão lambda verifica se uma string str passada como parâmetro é composta apenas por 3 caracteres X ou 3 caracteres O (supondo que m_N é 3). Se sim, ela retorna true e seta o estado do jogo para WinX ou WinO de forma correspondente. Se não, ela só retorna false. Vamos agora implementar o código que corresponde ao comentário TODO: Check rows, isto é, o código que verifica cada linha do tabuleiro: // Check rows for (auto const i : iter::range(m_N)) { std::string concatenation; for (auto const j : iter::range(m_N)) { concatenation += m_board.at(i * m_N + j); } if (allXsOrOs(concatenation)) { return; } } Os laços aninhados iteram as linhas (i) e colunas (j) do tabuleiro. Para cada linha, uma string concatenation é preenchida com os caracteres daquela linha. allXsOrOs é então chamada para verificar se a string contém 3 X ou 3 O. Se sim, checkEndCondition retorna na linha 150 pois a condição final já foi encontrada. Caso contrário, a verificação deve continuar. O código dos outros TODOs é similar: // Check columns for (auto const j : iter::range(m_N)) { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + j); } if (allXsOrOs(concatenation)) { return; } } // Check main diagonal { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + i); } if (allXsOrOs(concatenation)) { return; } } // Check inverse diagonal { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + (m_N - i - 1)); } if (allXsOrOs(concatenation)) { return; } } // Check draw if (std::find(m_board.begin(), m_board.end(), &#39;\\0&#39;) == m_board.end()) { m_gameState = GameState::Draw; } A verificação do empate é feita na condicional da linha 188. Observe que ela só será executada se as condições de vitória anteriores não tiverem sido satisfeitas. Então, se nesse momento não tiver nenhum caractere nulo em m_board, significa que o tabuleiro está todo preenchido com X e O mas ninguém ganhou, isto é, “deu velha”. Laços baseados em intervalos Observe que usamos laços for baseados em intervalos (range-based for loops) juntos com a função iter::range da biblioteca CPPItertools. Use laços baseados em intervalos sempre que possível. Eles são mais fáceis de ler e mais seguros pois evitam bugs comuns como trocar &lt; por outro comparador (&gt;, ou &lt;=) ou incrementar a variável errada. Por exemplo, para iterar com um índice i de 0 a 9, ao invés de usar o for tradicional for (int i = 0; i &lt; 10; ++i) { // i = 0, 1, ..., 9 } prefira fazer assim: for (auto i : iter::range(10)) { // i = 0, 1, ..., 9 } iter::range funciona da mesma forma que a função range do Python. De forma semelhante, para iterar um arranjo a e imprimir seu conteúdo, prefira fazer assim std::array a{&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;}; for (auto const &amp;str : a) { fmt::print(&quot;{}&quot;, str); } ao invés de: std::array a{&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;}; for (std::size_t i = 0; i &lt; a.size(); ++i) { fmt::print(&quot;{}&quot;, a[i]); } O projeto completo do Jogo da Velha pode ser baixado deste link. "],["graphicssystem.html", "3 Sistemas gráficos", " 3 Sistemas gráficos Um sistema gráfico é um sistema computacional com capacidade de processar dados para gerar imagens em um dispositivo de exibição. Em sistemas interativos, a interação com os modelos de dados gráficos se dá através de um ou mais dispositivos de entrada. Assim, de maneira geral, um sistema gráfico é composto pelos seguintes componentes: Dispositivos de entrada: teclado e dispositivos apontadores como mouse, touch pad, touch screen, graphics tablet, trackball, joystick, gamepad, entre outros. Processadores: unidade de processamento central (CPU, central processing unit), unidade de processamento gráfico (GPU, graphics processing unit) e seus subsistemas (controladores, memórias e barramentos) necessários ao processamento dos modelos de dados e conversão em representações visuais; Dispositivos de saída: monitores e telas de LCD (liquid-crystal display), OLED (organic light-emitting diodes), CRT (cathode-ray tube) ou plasma, entre outros dispositivos de exibição. A definição de um sistema gráfico com esses componentes é bastante ampla e pode incluir tanto as estações gráficas de alto desempenho equipadas com várias GPUs, quanto os computadores pessoais sem processador a gráficos. Pode incluir também os consoles de videogames, smartphones, smartwatches, smart TVs, GPSs, entre outros dispositivos com poder computacional suficiente para produzir saída em um dispositivo de exibição (figura 3.1). Figura 3.1: Exemplos de sistemas gráficos. Em sistemas gráficos atuais, o papel principal do processador gráfico é realizar a conversão de primitivas geométricas em uma imagem digital. Nos sistemas mais simples, sem aceleração de gráficos em hardware, a CPU é responsável por todo o processamento gráfico e não há distinção entre a memória da CPU e a memória de processamento gráfico. Em sistemas com processador dedicado a gráficos (GPU), a GPU pode estar integrada com o chip da CPU (como o Intel HD Graphics ou AMD HD Graphics), ou situada em uma placa de vídeo discreta com memória dedicada. GPUs discretas podem coexistir em um mesmo sistema com GPUs integradas, em diferentes configurações. Em sistemas de alto desempenho, várias GPUs em placas discretas podem ser combinadas para dividir a carga de processamento usando tecnologias como a SLI e NVLink da NVIDIA, e MGPU da AMD. A saída de um sistema gráfico é um mapa de bits armazenado em uma área da memória RAM chamada de framebuffer. Esse mapa de bits corresponde a uma imagem digital pronta para ser exibida no dispositivo de exibição. O hardware gráfico atualiza continuamente o dispositivo de exibição com o conteúdo o framebuffer. A imagem mostrada em cada atualização é chamada de quadro de exibição. A figura 3.2 mostra uma visão geral de um sistema gráfico atual com GPU dedicada em uma configuração comum em desktops. Os dados gráficos são enviados da CPU à GPU através de um barramento de alta velocidade. A GPU opera de forma assíncrona à CPU e gera a saída no framebuffer situado na memória RAM da placa gráfica. Figura 3.2: Arquitetura de um sistema gráfico atual com placa gráfica dedicada. Assim como as CPUs atualmente são compostas por vários núcleos programáveis de processamento, assim também são as GPUs atualmente, mas em uma escala muito maior. Enquanto as CPUs topo-de-linha para desktop possuem no máximo 18 núcleos, como o processador Intel Core i9-10980XE, uma GPU como a NVIDIA RTX 3080 Ti contém 10240 núcleos de processamento (chamados de CUDA cores) divididos entre 80 multiprocessadores. Por outro lado, enquanto os núcleos de uma CPU são destinados a processamento de propósito geral, os núcleos de uma GPU suportam um conjunto mais limitado de instruções, dedicado ao processamento de fluxos de dados. GPUs também podem ter núcleos para processamento ainda mais específico. Por exemplo, a mesma NVIDIA RTX 3080 Ti conta também com 320 núcleos dedicados a operações com tensores (tensor cores). Um tensor core é especializado em uma única operação: multiplicar duas matrizes 4x4 em ponto flutuante de 16 bits e acumular o resultado em uma matriz 4x4 em ponto flutuante de 16 ou 32 bits. Além disso, a RTX 3080 Ti também possui 80 núcleos dedicados a processamento de traçado de raios (RT cores). "],["lightcolor.html", "3.1 Luz e cor", " 3.1 Luz e cor Luz é a radiação eletromagnética visível ao olho humano, situada em uma faixa de frequências entre a radiação infravermelha e a ultravioleta (figura 3.3). Em particular, a luz corresponde à radiação nas frequências eletromagnéticas na faixa de 420 e 750 THz, mas é mais frequentemente caracterizada pelo comprimento de onda em vez da frequência. Cada comprimento de onda corresponde a uma cor monocromática ou cor espectral, de 400 nm, que corresponde ao limite entre o ultravioleta e o violeta, até 700 nm, que corresponde ao limite entre o vermelho e o infravermelho. Figura 3.3: Espectro eletromagnético e espectro visível (fonte). A luz de um laser é um exemplo de luz monocromática. Entretanto, em geral, a luz é uma combinação de um contínuo de diferentes comprimentos de onda em diferentes intensidades. Por exemplo, a cor branca não é uma cor monocromática, mas a composição de um espectro de luz visível que corresponde aproximadamente à luz do sol. É possível representar a distribuição de energia emitida, transmitida ou refletida de um objeto através de uma função de distribuição espectral de potência radiante (SPD, ou spectral power distribution). SPDs podem mensuradas a partir de objetos reais através de um espectrorradiômetro. A figura 3.4 mostra algumas dessas distribuições: para uma lâmpada incandescente (lâmpada halógena de tungstênio) e para a luz do dia (luz solar filtrada pela atmosfera). O eixo horizontal representa o comprimento de onda e o eixo vertical representa uma potência relativa (normalizada). Figura 3.4: Gráficos de distribuição de energia espectral (SPD) para uma lâmpada halógena e para a luz do dia. Cor é uma sensação visual; é o que percebemos quando uma composição espectral de luz estimula as células fotorreceptoras de nossos olhos, produzindo impulsos nervosos que são interpretados pelo cérebro. A sensação da cor, e a visão de uma forma geral, depende de fatores físicos (por exemplo, a distribuição da energia espectral da luz), fatores biológicos (por exemplo, a distribuição das células fotorreceptoras no fundo do olho) e psicológicos (por exemplo, como o cérebro interpreta os impulsos nervosos recebidos pelo nervo óptico). Visão tricromática A estrutura básica do olho humano é mostrada na figura 3.5. A parte da frente é análoga a um sistema de lentes de uma câmera. A íris controla a entrada da luz pela pupila. A córnea e o cristalino transmitem e focalizam a luz no fundo do olho. A luz incide sobre a retina, onde encontram-se as células fotorreceptoras. O ponto cego é uma área da retina da qual parte o nervo óptico responsável por transmitir os impulsos nervosos até o cérebro (Dale Purves 2018). Figura 3.5: Estrutura do olho humano. A retina é composta por dois tipos de células fotorreceptoras: os cones e os bastonetes. A distribuição dessas células varia de acordo com o ângulo relativo à fóvea (figura 3.6). Figura 3.6: Concentração de cones e bastonetes na retina em relação ao ângulo a partir da fóvea. Os bastonetes estão presentes em maior número (cerca de 90 milhões) e são responsáveis pela visão periférica. Bastonetes não são capazes de distinguir cores e possuem baixa acuidade visual. Por outro lado, são mais sensíveis ao brilho em condições de baixa luminosidade. Em ambientes escuros, como por exemplo em uma noite de lua nova, usamos apenas os bastonetes. A visão nessas condições é chamada de visão escotópica. Os cones estão presentes em menor número (cerca de 4,5 milhões), concentrados em torno de uma pequena região central da retina chamada de fóvea. Os cones são responsáveis pela visão em condições normais de luminosidade, com máxima acuidade e distinção de cores. A visão com os cones é chamada de visão fotópica. Há três tipos de cones: Cones S cobrem comprimentos de onda mais curtos, com sensibilidade máxima em torno de 420 a 440 nm; Cones M cobrem comprimentos de onda médios, com sensibilidade máxima em torno de 530 a 545 nm; Cones L cobrem comprimentos de onda mais longos, com sensibilidade máxima entre 560 a 580 nm. A figura 3.7 mostra a sensibilidade média dos cones para cada comprimento de onda, em uma escala normalizada. Figura 3.7: Sensibilidade dos cones S, M e L aos diferentes comprimentos de onda do espectro visível (fonte). A sobreposição da sensibilidade dos cones S, M e L fundamenta a teoria da visão tricromática desenvolvida no século XIX por Thomas Young e Herman von Helmholtz. Em 1802, Young levantou a hipótese da existência de três tipos de células fotorreceptoras com sensibilidades sobrepostas a diferentes comprimentos de onda (Young 1802). Na década de 1850, Helmholtz sugeriu que os cones possuem maior sensibilidade a três cores primárias que correspondem aproximadamente ao vermelho, verde e azul. James Clerk Maxwell também contribuiu com a teoria da visão tricromática através de experimentos de correspondência de cores e demonstração matemática de sua viabilidade (Maxwell 1857). O triângulo de Maxwell (figura 3.8) mostra como as cores verde, vermelho e azul podem ser combinadas para formar uma gama de cores. As arestas contêm cores compostas por combinações de duas primárias, enquanto que o interior contém cores obtidas através da mistura das três primárias. A teoria de Young-Helmholtz foi eventualmente validada no século XX por experimentos de observação da luz refletida da retina, experimentos subjetivos de correspondência de cores, e técnicas de microespectrofotometria. Figura 3.8: Triângulo de Maxwell. Um aspecto importante que decorre do espalhamento das sensibilidades dos cones é a possibilidade de diferentes SPDs produzirem a mesma sensação de cor. Por exemplo, a resposta do estímulo de um cone S em ~445 nm (pico de sensibilidade) pode ser a mesma do estímulo em ~500 nm (baixa sensibilidade) com intensidade mais alta. Isso significa que objetos que refletem luz com diferentes SPDs podem ser percebidos como uma mesma cor. Esse fenômeno é chamado de metamerismo, e uma mesma cor produzida com diferentes SPDs é chamada de cor metamérica. É por causa do metamerismo que, por exemplo, a sensação da cor espectral 510 nm (ciano) ou 558 nm (amarelo) pode ser reproduzida em um monitor de LED composto apenas por LEDs vermelhos, verdes e azuis. A figura 3.9 mostra um gráfico de eficiência luminosa que mostra a sensibilidade média da visão humana para cada comprimento de onda do espectro visível na visão fotópica e escotópica. Os dados que compõem este gráfico foram obtidos através de avaliações subjetivas com diferentes indivíduos, e hoje servem como uma linha de base da sensibilidade teórica da visão humana. O gráfico da visão fotópica, denotado por \\(y(\\lambda)\\) ou \\(V(\\lambda)\\), foi padronizado em 1931 pela comissão internacional de iluminação (CIE, commission internationale de l’éclairage) (CIE 1932). Figura 3.9: Funções de eficiência luminosa para a visão fotópica e escotópica. Em condições de luminosidade intermediária (por exemplo, à luz da lua), usamos tanto os bastonetes quanto os cones, na chamada visão mesópica. Na visão mesópica, a eficácia relativa da visão é maior na região mais próxima ao azul. Isso faz com que objetos vermelhos pareçam mais escuros que objetos de outra cor, e a luz em geral tende a parecer mais fria (efeito Purkinje). Modelos de cor Um modelo de cor é um modelo matemático utilizado para representar cores através de tuplas de números. Os modelos mais utilizados são o modelo RGB (e suas variantes, como HSL e HSV), e o modelo CMY e CMYK. RGB, HSL e HSV O modelo RGB é o modelo de cor utilizado por dispositivos de exibição baseados em emissão de luz. Uma cor no formato RGB é representada por três números que correspondem a uma combinação de intensidades de luz de cor vermelha (R, red), verde (G, green) e azul (B, blue). Em geral, cada componente de cor é representada por valores em ponto flutuante entre 0 (intensidade mínima) e 1 (intensidade máxima), ou como valores inteiros entre 0 e 255 de modo que a cor possa ser representada com três bytes. Esse último formato é o formato padrão das cores na web, comumente escritos em notação hexadecimal. As cores primárias no modelo RGB podem ser combinadas para formar cores secundárias: ciano (verde + azul), magenta (azul + vermelho) e amarelo (vermelho + verde). A combinação das cores primárias em igual proporção produz tons de cinza, do preto (0,0,0) ao branco (1,1,1). O conjunto de todas possíveis combinações pode ser visualizado como um cubo de cores como mostra a figura 3.10. Figura 3.10: Modelo RGB (fonte). Embora o modelo RGB seja o modelo padrão de representação de cores em imagens digitais e em APIs gráficas, a composição de cores através da mistura de cores primárias pode ser pouco intuitiva. Em aplicativos de edição de imagens é comum o uso de modelos alternativos, como o HSL (hue, saturation, lightness) e HSV (hue, saturation, value). Esses modelos permitem que as cores sejam combinadas através de valores de matiz (entre 0º e 360º), saturação (entre 0 e 1) e luminosidade/intensidade (entre 0 e 1). A figura 3.11 ilustra esses modelos. Figura 3.11: Modelo HSL e HSV (fonte). CMY e CMYK O modelo RGB é um modelo aditivo de cor pois as cores são obtidas através da adição de cores primárias. O oposto do modelo aditivo é o modelo subtrativo, utilizado em tintas e pigmentos. Quando uma luz incide sobre um pigmento, este absorve determinados comprimentos de onda e reflete outros, de modo que a cor percebida depende da SPD de luz filtrada refletida do pigmento. O modelo CMY é um modelo subtrativo que usa como cores primárias o ciano (C, cyan), magenta (M, magenta) e amarelo (Y, yellow). Pigmentos nessas cores filtram (absorvem) a luz nas cores vermelha, verde e azul, respectivamente. Assim, quando os pigmentos são aplicados sobre uma superfície branca, a cor preta resulta da mistura dessas cores primárias em igual quantidade, enquanto a cor branca corresponde à ausência dos pigmentos. Em impressoras é comum o uso do modelo CMYK que inclui o pigmento preto (K, black) de modo a melhorar o contraste. Isso ocorre porque os pigmentos CMY utilizados em impressão geralmente não absorvem perfeitamente as cores complementares RGB. Além disso, o uso de um pigmento preto ajuda a reduzir So consumo das tintas CMY. Sistema CIE 1931 Os modelos RGB ou CMYK por si só não definem a colorimetria das cores primárias. Um mesmo valor de cor RGB pode parecer diferente dependendo das características das fontes de luz ou dos pigmentos que geram as cores primárias. Para possibilitar a correta reprodução de cores entre diferentes dispositivos, é necessário que o modelo de cor tenha como referencial um espaço de cor absoluto no qual as cores possam ser caracterizadas por medidas colorimétricas tais como comprimento de onda e intensidade. Se tivermos fontes de luz capazes de gerar cores primárias monocromáticas em diferentes intensidades, podemos modelar um espaço de cor absoluto através de uma medição empírica de quais intensidades de cores primárias são necessárias para produzir cada cor espectral. William David Wright (Wright 1929) e John Guild (Guild 1932) realizaram tal experimento na década de 1920. Os resultados foram padronizados no sistema CIE 1931, utilizado atualmente como sistema padrão de correspondência de cores (Smith and Guild 1931) (CIE 1932). Nos experimentos de Wright e Guild, as intensidades de três fontes de luz monocromática (700 nm, 546.1 nm, e 435.8 nm) foram ajustadas em condições de visão fotópica de modo que a combinação das três cores imitasse cada cor do espectro visível. O experimento foi repetido com diferentes pessoas, obtendo resultados semelhantes, padronizado pelo resultado mostrado na figura 3.12. Figura 3.12: Funções de correspondência CIE RGB (fonte). Curiosamente, alguns valores são negativos. Isso ocorre porque há cores espectrais que não podem ser obtidas a partir da combinação de apenas três cores primárias. Quando esses casos aconteciam durante o experimento, o sujeito da experimentação adicionava vermelho, verde ou azul à cor espectral de referência até conseguir obter uma correspondência. A adição na cor de referência corresponde efetivamente a uma subtração nas primárias, produzindo os valores negativos. A escala das funções de correspondência supõe que as cores primárias possuem o mesmo brilho. Entretanto, segundo a função de eficiência luminosa \\(V(\\lambda)\\), a visão humana percebe o brilho de forma mais eficiente para comprimentos de onda na região do amarelo-esverdeado. As funções de correspondência podem ser ajustadas para as luminâncias absolutas usando as escalas 1, 4.5907 e 0.0601, respectivamente. Os valores das funções R, G e B resultantes dessa escala são chamados de valores de triestímulo: \\[ \\begin{align} R &amp;= \\bar{r}(\\lambda) \\times 1,\\\\ G &amp;= \\bar{g}(\\lambda) \\times 4.5907,\\\\ B &amp;= \\bar{b}(\\lambda) \\times 0.0601.\\\\ \\end{align} \\] A combinação dos valores de triestímulo resulta em um espaço tridimensional. Para facilitar a visualização, a dimensão que corresponde unicamente a variações de intensidade pode ser ignorada. Isso pode ser feito através de uma normalização de \\(R\\), \\(G\\) e \\(B\\) de modo que a soma dos valores seja sempre igual a um: \\[ r = \\frac{R}{R+G+B},\\\\ g = \\frac{G}{R+G+B},\\\\ b = \\frac{B}{R+G+B},\\\\ \\,\\\\ r+g+b = 1. \\] Valores de \\(r\\), \\(g\\) e \\(b\\) são chamados de valores de cromaticidade, pois representam a informação da cor independente da intensidade. Podemos nos concentrar apenas em \\(r\\) e \\(g\\), pois \\(b\\) é uma combinação dos dois primeiros (\\(b=1-r-g\\)). Desse modo, podemos desenhar um gráfico bidimensional de cromaticidade no plano \\(rg\\) como mostra a figura 3.13. Pontos sobre a borda da região colorida indicam coordenadas de cromaticidade que correspondem a cores espectrais. Pontos dentro da região colorida correspondem às cores não-espectrais que podem ser obtidas a partir da combinação de cores espectrais. Os pontos na região triangular colorida no primeiro quadrante correspondem às cores que podem ser obtidas a partir da combinação das cores primárias. Em particular, as cores primárias vermelho, verde e azul estão situadas nas coordenadas (1,0), (0,1) e (0,0), respectivamente. A região colorida com coordenadas negativas corresponde a cores reais, mas que não podem ser alcançadas através da combinação das cores primárias utilizadas. Figura 3.13: Diagrama de cromaticidade CIE rg (fonte). Para obter funções de correspondência de cores com valores unicamente positivos, a CIE criou o espaço de cor CIE XYZ, que usa cores primárias imaginárias XYZ no lugar de RGB. Esse espaço é obtido a partir de uma transformação linear das funções de correspondência CIE RGB. Além de tornar as funções positivas, a transformação é feita de tal forma que a função \\(\\bar{y}(\\lambda)\\) deste novo espaço corresponda exatamente à função de eficiência luminosa \\(V(\\lambda)\\) na visão fotópica. As funções de correspondência resultantes são mostradas na figura 3.14. Figura 3.14: Funções de correspondência CIE XYZ (fonte). Da mesma forma que podemos obter um diagrama de cromaticidade CIE rg a partir da normalização das funções de correspondência CIE RGB, podemos também obter um diagrama de cromaticidade xy com a normalização \\[ x = \\frac{X}{X+Y+Z},\\\\ y = \\frac{Y}{X+Y+Z},\\\\ z = \\frac{Z}{X+Y+Z}.\\\\ \\,\\\\ x+y+z = 1. \\] O diagrama de cromaticidade xy é mostrado na figura 3.15. O triângulo dentro da região colorida mostra a região formada pelas primárias \\(R\\), \\(G\\), \\(B\\) do experimento de Wright e Guild. As cores que podem ser obtidas a partir da combinação de primárias é chamada de gama de cores (em inglês, color gamut) e corresponde à área do triângulo. Figura 3.15: Diagrama de cromaticidade CIE xy (fonte). Observação No diagrama de cromaticidade CIE rg (figura 3.13), a transformação de CIE RGB para CIE XYZ leva os pontos \\(C_r\\), \\(C_g\\) e \\(C_b\\) (vértices do triângulo vermelho) para os pontos \\((1,0)\\), \\((0,1)\\), \\((0,0)\\) do diagrama de cromaticidade CIE xy (figura 3.15). O espaço formado pelas coordenadas \\(x\\), \\(y\\) e \\(Y\\) é chamado de espaço CIE xyY. Nesse espaço, as duas primeiras coordenadas (\\(x\\), \\(y\\)) correspondem às cromaticidades, isto é, às variações de cor independente de luminosidade. A terceira coordenada (\\(Y\\)) corresponde apenas a variações de luminosidade. O espaço CIE xyZ tem sido utilizado como referência para caracterizar de forma precisa os diferentes espaços de cor utilizados em dispositivos de exibição e impressão. A figura 3.16 mostra alguns dos principais espaços de cor utilizados na indústria. O padrão mais comum em monitores de computador é o espaço sRGB, criado pela HP e Microsoft em 1996 e depois tornado padrão pela Comissão Eletrotécnica Internacional (IEC, International Electrotechnical Commission) como o padrão IEC 61966-2-1:1999. Se um monitor é compatível com o espaço sRGB, isso significa que ele é capaz de reproduzir a gama de cores desse espaço. Para todo monitor é possível determinar um perfil de correspondência de cores do modelo RGB para as cores correspondentes no espaço CIE XYZ (essa correspondência é chamada de perfil ICC). Desse modo é possível estabelecer uma correspondência exata entre cores do monitor e cores de outro dispositivo, garantindo a reprodução fiel das cores. Figura 3.16: Gamas de cores de diferentes espaços de cor (fonte). Referências "],["vectorxraster.html", "3.2 Vetorial x matricial", " 3.2 Vetorial x matricial Em computação gráfica, é comum trabalharmos com dois tipos de representações de gráficos: a representação vetorial, utilizada na descrição de formas 2D e 3D compostas por primitivas geométricas, e a representação matricial, utilizada em imagens digitais e definição de texturas. O processo de converter representações vetoriais em representações matriciais desempenha um papel central no pipeline de processamento gráfico, uma vez que a representação matricial é a representação final de uma imagem nos dispositivos de exibição. Essa conversão matricial, também chamada de rasterização (raster conversion ou scan conversion), é implementada em hardware nas GPUs atuais. A figura 3.17 ilustra o resultado da conversão de uma representação vetorial em representação matricial. As formas geométricas à esquerda estão representadas originalmente no formato SVG (Scalable Vector Graphics), que é o formato padrão de gráficos vetoriais nos navegadores Web. A imagem à direita é um arranjo bidimensional de valores de cor, resultado da renderização das formas SVG em uma imagem digital (neste caso, uma imagem de baixa resolução). Figura 3.17: Rasterização de um círculo e triângulo. Observação A figura 3.17 é apenas ilustrativa. Rigorosamente falando, a imagem da esquerda também está no formato matricial. O navegador converte automaticamente o código SVG em comandos da API gráfica que fazem com que a GPU renderize a imagem que vemos na tela. A rasterização ocorre durante este processamento. A imagem à direita não precisa passar pelo processo de renderização pois já é uma imagem digital em seu formato nativo. Representação vetorial Na representação vetorial, os gráficos são descritos em termos de primitivas geométricas. Por exemplo, o formato SVG é um formato de descrição de gráficos vetoriais 2D através de sequências de comandos de desenho. Uma forma 2D pode ser descrita através da definição de um “caminho” (path) composto por uma sequência de passos de movimentação de uma caneta virtual sobre um plano. Os principais passos utilizados são comandos do tipo MoveTo, LineTo e ClosePath: MoveTo (denotado por M ou m em SVG11) move a caneta virtual para uma nova posição na área de desenho, como se ela fosse levantada da superfície e posicionada em outro local; LineTo (L ou l) traça um segmento de reta da posição atual da caneta até uma nova posição, que passa a ser a nova posição da caneta; Em uma sequência de comandos LineTo, o comando ClosePath (Z ou z) traça um segmento de reta que fecha o caminho da posição atual da caneta ao ponto inicial. Observe o código SVG a seguir que resulta no desenho do triângulo visto mais abaixo: &lt;svg width=&quot;250&quot; height=&quot;210&quot;&gt; &lt;path d=&quot;M125 0 L0 200 L250 200 Z&quot; stroke=&quot;black&quot; fill=&quot;lightgray&quot; /&gt; &lt;/svg&gt; No rótulo &lt;svg&gt;, os atributos width=\"250\" e height=\"210\" definem que a área de desenho tem largura 250 e altura 210. Por padrão, a origem fica no canto superior esquerdo. O eixo horizontal (\\(x\\)) é positivo para a direita, e o eixo vertical (\\(y\\)) é positivo para baixo. O atributo d do rótulo &lt;path&gt; contém os comandos de desenho do caminho. M125 0 move a caneta virtual para a posição (125,0). Em seguida, L0 200 traça um segmento da posição atual até a posição (0, 200), que passa a ser a nova posição da caneta. L250 200 traça um novo segmento até (250, 200). O comando Z fecha o caminho até a posição inicial em (125, 0), completando o triângulo. O atributo stroke=\"black\" define a cor do traço como preto, e fill=\"lightgray\" define a cor de preenchimento como cinza claro: O formato SVG também suporta a descrição de curvas, arcos, retângulos, círculos, elipses, entre outras primitivas geométricas. Comandos similares são suportados em outros formatos de gráficos vetoriais, como o EPS (Encapsulated PostScript), PDF (Portable Document Format), AI (Adobe Illustrator Artwork) e DXF (AutoCAD Drawing Exchange Format). Representação vetorial no OpenGL No OpenGL, a representação vetorial é utilizada para definir a geometria que será processada durante a renderização. Todas as primitivas geométricas são definidas a partir de vértices que representam posições em um espaço, e atributos adicionais definidos pelo programador (por exemplo, a cor do vértice). Esses vértices são armazenados em arranjos ordenados que são processados em um fluxo de vértices no pipeline de renderização especificado pelo OpenGL. Os vértices podem ser utilizados para formar diferentes primitivas. Por exemplo, o uso do identificador GL_TRIANGLES na função de renderização glDrawArrays faz com que seja formado um triângulo a cada grupo de três vértices do arranjo de vértices. Assim, se o arranjo tiver seis vértices (em uma sequência de 0 a 5), serão formados dois triângulos: um triângulo com os vértices 0, 1, 2, e outro com os vértices 3, 4, 5. Para o mesmo arranjo de vértices, GL_POINTS faz com que o pipeline de renderização interprete cada vértice como um ponto separado, e GL_LINE_STRIP faz com que o pipeline de renderização forme uma sequência de segmentos (uma polilinha) conectando os vértices. A figura 3.18 ilustra a formação dessas primitivas para um arranjo de seis vértices no plano. A numeração indica a ordem dos vértices no arranjo. Figura 3.18: Formando diferentes primitivas do OpenGL com um mesmo arranjo de vértices. A figura 3.19 mostra como a geometria das primitivas pode mudar (com exceção de GL_POINTS) caso os vértices estejam em uma ordem diferente no arranjo. Figura 3.19: A ordem dos vértices no arranjo altera a geometria das primitivas. Veremos com mais detalhes o uso de primitivas no próximo capítulo quando abordaremos as diferentes etapas de processamento do pipeline de renderização do OpenGL. Observação Até a década de 2010, a maneira mais comum de renderizar primitivas no OpenGL era através de comandos do modo imediato de renderização, como a seguir (em C/C++): glColor3f(0.83f, 0.83f, 0.83f); // Light gray color glBegin(GL_TRIANGLES); glVertex2i(-1, -1); glVertex2i( 1, -1); glVertex2i( 0, 1); glEnd(); Nesse código, a função glColor3f informa que a cor dos vértices que estão prestes a ser definidos será um cinza claro, como no triângulo desenhado com SVG. O sufixo 3f de glColor3f indica que os argumentos são três valores do tipo float. Entre as funções glBegin e glEnd é definida a sequência de vértices. Cada chamada a glVertex2i define as coordenadas 2D de um vértice (o sufixo 2i indica que as coordenadas são compostas por dois números inteiros). Como há três vértices e a primitiva é identificada com GL_TRIANGLES, será desenhado um triângulo cinza similar ao triângulo desenhado com SVG, porém sem o contorno preto12. O sistema de coordenadas nativo do OpenGL não é o mesmo da área de desenho do formato SVG. No OpenGL, a origem é o centro da janela de visualização, sendo que o eixo \\(x\\) é positivo à direita e o eixo \\(y\\) é positivo para cima. Além disso, para que a primitiva possa ser vista, as coordenadas dos vértices precisam estar entre -1 e 1 (em ponto flutuante). Para desenhar o triângulo colorido do exemplo “Hello, World!”, como visto na seção 1.5, poderíamos utilizar o seguinte código: glBegin(GL_TRIANGLES); glColor3f(1.0f, 0.0f, 0.0f); // Red glVertex2f(0.0f, 0.5f); glColor3f(1.0f, 0.0f, 1.0f); // Magenta glVertex2f(0.5f, -0.5f); glColor3f(0.0f, 0.0f, 1.0f); // Green glVertex2f(-0.5f, -0.5f); glEnd(); Observe que, antes da definição de cada vértice, é definida a sua cor. Quando o triângulo é processado na GPU, as cores em cada vértice são interpoladas bilinearmente (em \\(x\\) e em \\(y\\)) ao longo da superfície do triângulo, formando um gradiente de cores. Em nossos programas usando a ABCg, bastaria colocar esse código na função membro onPaint de nossa classe derivada de abcg::OpenGLWindow. Internamente o OpenGL utilizaria um pipeline de renderização de função fixa (pipeline não programável) para desenhar o triângulo. No entanto, se compararmos com o código atual do projeto no subdiretório abcg\\examples\\helloworld, perceberemos que não há nenhum comando glBegin, glVertex* ou glColor*. Isso acontece porque o código acima é obsoleto. As funções do modo imediato foram retiradas do OpenGL na versão 3.1 (de 2009). Ainda é possível habilitar um “perfil de compatibilidade” (compatibility profile) para usar funções obsoletas do OpenGL, mas esse perfil não é recomendado para código atual. Por isso, não o utilizaremos neste curso. Atualmente, para desenhar primitivas com o OpenGL, o arranjo ordenado de vértices precisa ser enviada previamente à GPU juntamente com programas chamados shaders que definem como os vértices serão processados e como os pixels serão preenchidos após a rasterização. Desenhar um simples triângulo preenchido no OpenGL não é tão simples como antigamente, mas essa dificuldade é compensada pela maior eficiência e flexibilidade obtida com a possibilidade de programar o comportamento da GPU. Representação matricial Na representação matricial, também chamada de representação raster, as imagens são compostas por arranjos bidimensionais de elementos discretos e finitos chamados de pixels (picture elements). Um pixel contém uma informação de amostra de cor e corresponde ao menor elemento que compõe a imagem. A resolução da imagem é o número de linhas e colunas do arranjo bidimensional. Esse é o formato utilizado nos arquivos GIF (Graphics Interchange Format), TIFF (Tag Image File Format), PNG (Portable Graphics Format), JPEG e BMP. A figura 3.20 mostra uma imagem digital e um detalhe ampliado. Figura 3.20: Imagem digital de 300x394 pixels e detalhe ampliado de 38x38 pixels. Observação Embora os pixels ampliados da figura 3.20 sejam mostrados como pequenos quadrados coloridos, um pixel não tem necessariamente o formato de um quadrado. Um pixel é apenas uma amostra de cor e pode ser exibido em diferentes formatos de acordo com o dispositivo de exibição. Uma imagem digital pode ser armazenada como um mapa de bits (bitmap). A quantidade de cores que podem ser representadas em um pixel – a profundidade da cor (color depth) – depende do número de bits designados a cada pixel. Em uma imagem binária, cada pixel é representado por apenas 1 bit. Desse modo, a imagem só pode ter duas cores, como preto (para os bits com estado 0) e branco (para os bits com estado 1). A figura 3.21 mostra uma imagem binária em formato BMP, que é um formato simples utilizado para armazenar mapas de bits. Figura 3.21: Imagem binária. A imagem da figura 3.21 foi gerada a partir de outra imagem de maior profundidade de cor (figura 3.25) usando o algoritmo Floyd-Steinberg de dithering (Floyd and Steinberg 1976). Dithering é o processo de introduzir um ruído ou padrão de pontilhado que atenua a percepção de artefatos no formato de bandas resultantes da quantização da cor (color banding). A figura 3.22 mostra esse efeito em uma imagem colorida. A imagem da esquerda é a imagem original, com 24 bits de profundidade de cor. A imagem do centro teve a profundidade de cor reduzida para 4 bits (16 cores). É possível perceber as bandas de cor no gradiente do céu. Na imagem da direita, a profundidade de cor também foi reduzida para 4 bits, mas o uso de dithering reduz a percepção das variações bruscas de tom. Figura 3.22: Redução de bandas de cor com dithering. Esquerda: imagem original de 24 bits/pixel. Centro: redução para 4 bits/pixel. Direita: redução para 4 bits/pixel usando dithering. Em imagens com profundidade de cor de 8 bits, cada pixel pode assumir um valor de 0 a 255. Esse valor pode ser interpretado como um nível de luminosidade para, por exemplo, descrever imagens monocromáticas de 256 tons de cinza (figura 3.23). Figura 3.23: Imagem monocromática de 8 bits por pixel. Uma outra possibilidade é fazer com que cada valor corresponda a um índice de uma paleta de cores que determina qual será a cor do pixel. Em imagens de 8 bits, a paleta de cores é uma tabela de 256 cores, sendo que cada cor é definida por 3 bytes, um para cada componente de cor RGB (vermelho, verde, azul). Esse formato de cor indexada foi o formato predominante em computadores pessoais na década de 1990, quando o hardware gráfico só conseguia exibir um máximo de 256 cores simultâneas no modo VGA (Video Graphics Array). O formato GIF, criado em 1987, utiliza cores indexadas. A figura 3.24 exibe uma imagem GIF e sua paleta correspondente de 256 cores. Figura 3.24: Imagem de 8 bits com cores indexadas (esquerda) e paleta utilizada (direita). Atualmente, as imagens digitais coloridas usam o formato true color no qual cada pixel tem 24 bits (3 bytes, um para cada componente de cor RGB), sem o uso de paleta de cor (figura 3.25). Isso possibilita a exibição de \\(2^{24}\\) cores simultâneas (aproximadamente 16 milhões). Figura 3.25: Imagem de 24 bits por pixel. Em arquivos de imagens, também é comum o uso de 32 bits por pixel (4 bytes), sendo 3 bytes para as componentes de cor e 1 byte para definir o nível de opacidade do pixel. Isso permite realizar composição de imagens sobrepostas, por exemplo, misturando cores de uma imagem A sobre uma imagem B, usando o valor de opacidade como peso da combinação. Geralmente, os valores de intensidade de cor de um pixel são representados por números inteiros. Entretanto, imagens podem ser especificadas em um formato HDR (high dynamic range) no qual cada componente de cor pode ter até 32 bits em formato de ponto flutuante, permitindo alcançar uma faixa mais ampla de intensidades. As GPUs atuais fornecem suporte a um variado conjunto de formatos de bits, incluindo suporte a mapas de bits compactados e tipos de dados em formato de ponto flutuante de 16 e 32 bits. Referências "],["es.html", "3.3 Dispositivos de E/S", " 3.3 Dispositivos de E/S A seguir apresentamos uma visão geral de conceitos e tecnologias relacionadas a dispositivos de entrada e saída utilizados em sistemas gráficos. Dispositivos de entrada Um sistema gráfico possui um ou mais dispositivos de entrada que permitem ao usuário interagir com os modelos de dados gráficos. O mais tradicional dispositivo de entrada é o dispositivo de teclado. O teclado produz um código (scancode) composto por um byte ou sequência de bytes que identifica cada tecla pressionada e liberada. O teclado virtual de um smartphone ou tablet é também um dispositivo de teclado. Embora não possua teclas físicas, o resultado dos toques na tela ou conversão de uma anotação manuscrita em texto é um conjunto de scancodes que corresponde aos mesmos caracteres de teclas de um teclado físico (figura 3.26). Os códigos de um dispositivo de teclado podem ser interpretados como direções de movimentação de um cursor de desenho para permitir a interação com dados gráficos. Figura 3.26: Teclado virtual de um iPad (fonte). Além do teclado, é comum que um sistema gráfico tenha também pelo menos um dispositivo apontador, como o mouse, capaz de fornecer dados de movimentação ou posicionamento sobre uma superfície, geralmente mapeados para uma posição na tela. O mouse produz dados de deslocamento em duas direções ortogonais que correspondem ao movimento horizontal (\\(x\\)) e vertical (\\(y\\)) do dispositivo13. Como os dados produzidos são apenas deslocamentos, e não posições, o mouse é considerado um dispositivo de posicionamento relativo. Entretanto, os deslocamentos em \\(x\\) e \\(y\\) podem ser interpretados como velocidades e acumulados ao longo do tempo para determinar a posição de um cursor na tela. Outros dispositivos populares de posicionamento relativo são os touch pads, trackballs, joysticks e gamepads (figura 3.27). Tais dispositivos também possuem botões que podem ser configurados da mesma forma que as teclas de um teclado. Figura 3.27: Dispositivos apontadores de posicionamento relativo. Da esquerda para a direita: trackball (fonte), joystick (fonte), gamepad (fonte). Dispositivos apontadores como a tela sensível ao toque (touch screen) e a mesa digitalizadora (graphics tablet) são capazes de fornecer dados de posicionamento absoluto (figura 3.28). Os toques produzidos com o dedo ou com uma caneta de toque (stylus pen) produzem dados que correspondem a um par de coordenadas sobre a superfície de desenho, além de um valor que corresponde à pressão aplicada. Esses dispositivos também podem ser configurados para gerar dados de posicionamento relativo e detecção de gestos de arrasto (swipe e drag and drop) através do rastreamento dos pontos de pressão. Telas sensíveis ao toque frequentemente também são capazes de detectar múltiplos toques simultâneos, permitindo a detecção de gestos mais complexos como pinça (pinch) e rotação (rotate). Figura 3.28: Mesa digitalizadora com caneta (fonte). Dispositivos de saída Um sistema gráfico possui pelo menos um dispositivo de saída para exibição de gráficos. Esses dispositivos podem ser do tipo vetorial ou matricial. Dispositivos vetoriais O primeiro dispositivo de exibição utilizado em computador foi o CRT vetorial, que é o mesmo tipo de tecnologia utilizada nas telas dos antigos osciloscópios analógicos (figura 3.29). Figura 3.29: Osciloscópio analógico com CRT vetorial (fonte). No CRT vetorial, um canhão de elétrons emite um feixe de elétrons que incide sobre uma tela revestida por um material fotoluminescente (fósforo). Um conjunto de placas defletoras eletromagnéticas permite alterar a posição horizontal (\\(x\\)) e vertical (\\(y\\)) de incidência do feixe, de modo que gráficos de linhas e curvas podem ser traçados na tela. Em um sistema gráfico, a posição de incidência do feixe pode ser descrita por comandos do tipo MoveTo e LineTo (figuras 3.30 e 3.31). Como o brilho do fósforo tem persistência baixa, na ordem de milissegundos, é preciso redesenhar o traço continuamente. Figura 3.30: Desenhando um triângulo em um CRT vetorial. A sequência de passos de 1 a 4 precisa ser repetida continuamente para manter a imagem na tela. Figura 3.31: Jogo estilo “Asteroides” (“Space Rocks”) sendo exibido em um CRT vetorial de um antigo osciloscópio (fonte). Dispositivos de exibição vetorial não conseguem desenhar de forma adequada áreas preenchidas. Além disso, a velocidade de geração do desenho é proporcional à quantidade de primitivas e ao comprimento dos caminhos, impondo um limite à complexidade do desenho. Por essas desvantagens, CRTs vetoriais tornaram-se obsoletos e foram substituídos inteiramente pelos dispositivos matriciais. Dispositivos matriciais O primeiro dispositivo de exibição matricial utilizado em computadores também foi o CRT (Noll 1971). No CRT matricial, o feixe de elétrons é direcionado por deflexão eletromagnética e varre continuamente a tela de cima para baixo, da esquerda para direita. A cada linha percorrida, o canhão de elétrons é desligado momentaneamente e religado no início da próxima linha (retraço horizontal). Ao completar a varredura no canto inferior direito, o canhão de elétrons é desligado e direcionado para o ponto inicial, no canto superior esquerdo (retraço vertical). O feixe de elétrons é então religado e uma nova varredura é feita, iniciando um novo quadro de exibição. Esse processo é feito continuamente, a uma taxa que, nos televisores antigos, era sincronizada com a frequência da rede elétrica: 50 Hz ou 60 Hz14. Durante a varredura, a intensidade do feixe é controlada por um sinal analógico de vídeo. Esse sinal pode ser produzido por um conversor digital-analógico a partir de uma imagem digital, reproduzindo na tela os pontos que formam a imagem (figura 3.32). Figura 3.32: Varredura de um quadro em um CRT matricial. CRTs coloridos utilizam três canhões de elétrons, um para cada componente de cor RGB. A tela é coberta por um padrão de fósforos nessas cores, em grupos de três. Uma máscara ou grelha metálica próxima da tela (shadow mask, slot mask ou apperture grille, dependendo da tecnologia utilizada) assegura que cada tipo de fósforo recebe elétrons apenas do canhão correspondente. A figura 3.33 mostra o detalhe ampliado da tela de um CRT de TV e um CRT de computador, mostrando o padrão das tríades RGB formadas pelo slot mask (no CRT de TV) e shadow mask (no CRT de computador). Uma vez que os padrões são muito pequenos e cobrem a tela por completo, o usuário percebe a combinação das cores primárias que resultam na cor da imagem. A figura 3.34 mostra o detalhe ampliado de uma letra “e” exibida em um CRT de TV que usa a tecnologia de apperture grille (tecnologia Trinitron, da Sony), e o detalhe ampliado de um cursor em um CRT de computador. Figura 3.33: Padrões de fósforos RGB em CRTs. Esquerda: slot mask em um CRT de TV. Direita: shadow mask em um CRT de PC. (fonte) Figura 3.34: Detalhes ampliados de telas de CRT. Esquerda: letra ‘e’ em um CRT de TV Sony Trinitron (fonte). Direita: cursor na tela de um CRT de computador (fonte). Os CRTs não são mais utilizados desde meados de 2000 e foram substituídos pelos monitores LCD (liquid-crystal display), que utilizam sinais digitais de vídeo. Até a metade de 2010 eram também comuns os monitores de tela de plasma. Nessa tecnologia, tensões aplicadas em eletrodos de endereçamento de linhas e colunas energizam um gás (geralmente néon e xenônio) contido em minúsculas células envoltas em painéis de vidro. O fundo das células é coberto por fósforo nas cores RGB, de modo que cada grupo de 3 cores forma um pixel. Como em uma lâmpada fluorescente, o gás ionizado se torna um plasma emissor de luz ultravioleta que faz com que os fósforos emitam a luz visível que forma as cores da imagem (figura 3.35). Figura 3.35: Estrutura de uma tela de plasma (fonte). A tecnologia LCD é a mais utilizada nos dispositivos de exibição atuais. Uma tela de LCD é composta por um sanduíche de vários painéis (figura 3.36). Na parte de trás dos painéis, lâmpadas fluorescentes ou LEDs emitem uma luz branca que é espalhada uniformemente por um painel difusor. Essa luz incide sobre um filtro que só permite passar luz polarizada em uma direção. Na frente dos painéis há uma outra camada que só permite passar a luz polarizada na direção ortogonal ao primeiro filtro, de modo que o resultado é o bloqueio total da luz. Para controlar eletronicamente a passagem da luz, entre os dois filtros é colocado um substrato de vidro contendo uma camada de cristais líquidos e eletrodos e/ou transistores que alteram a orientação dos cristais – e com isso a polarização da luz – através de campos elétricos. Uma camada de filtros de cor divide a tela em pixels compostos de três subpixels, um para cada componente RGB, coincidentes com a camada de eletrodos. Desse modo, a passagem de luz em cada subpixel é controlada individualmente para formar a imagem final. O conteúdo da tela LCD é atualizado continuamente, geralmente a uma taxa de 60 Hz, mas em monitores mais recentes essa taxa pode chegar a 240 Hz. Figura 3.36: Estrutura de uma tela de LCD (fonte). A tecnologia OLED (organic light-emitting diodes) tem se popularizado em telas de smart TVs e smartphones e tem a promessa de substituir a tecnologia de LCD. Telas OLED não utilizam luz de fundo, pois cada subpixel emite sua própria luz: cada subpixel é um LED no qual a camada eletroluminescente é um filme de compostos orgânicos. A figura 3.37 mostra o detalhe ampliado de uma tela com tecnologia AMOLED (active-matrix organic light-emitting diode) que utiliza transistores de filme fino para manter o fluxo de corrente em cada subpixel. Figura 3.37: Detalhe da tela AMOLED de um smartphone Google Nexus One. Foto por Matthew Rollings (fonte). Telas OLED obtêm níveis mais profundos de preto e melhor contraste em ambientes escuros quando comparadas com as telas de LCD. A tecnologia tem ainda outras vantagens, como a possibilidade de ser utilizada em telas mais finas, flexíveis e transparentes (figura 3.38). Por outro lado, telas OLED possuem algumas desvantagens em relação à tecnologia LCD, como menor tempo de vida e maior risco de burn-in (marcação permanente da tela após a exibição de uma mesma imagem por longo período). Figura 3.38: Smartphones com tela OLED flexível (fonte). Embora as tecnologias LCD, Plasma e OLED não usem um canhão de elétrons para varrer a tela, o princípio geral de varredura de quadros de exibição continua sendo empregado. Em particular, o chamado tempo de apagamento vertical (VBI, vertical blanking interval), que é o intervalo entre a varredura de um quadro e o quadro subsequente, continua fazendo parte dessas tecnologias. Os sinais digitais utilizados (por exemplo, DVI ou HDMI) contém um sinal de VBI como parte de seu fluxo de dados. Referências "],["framebuffer.html", "3.4 Framebuffer", " 3.4 Framebuffer O framebuffer é uma área contígua de memória de vídeo utilizada para armazenar a imagem que será mostrada no dispositivo de exibição. O hardware gráfico lê continuamente o conteúdo do framebuffer e atualiza o dispositivo de exibição, tipicamente a uma taxa entre 60 e 240 Hz nos monitores de LCD. Nos primeiros PCs e em sistemas gráficos mais antigos, o framebuffer fazia parte da memória do sistema que poderia ser acessada diretamente pela CPU. Nos PCs do início da década de 1990, o framebuffer podia ser acessado com um simples ponteiro para o endereço 0xA000 no chamado “modo 13h” do controlador VGA (um modo gráfico de cores indexadas de 8 bits com resolução de 320x200). Atualmente, o framebuffer é acessado através da GPU e, em GPUs dedicadas, está localizado na memória RAM da placa gráfica. Em hardware compatível com OpenGL (o que inclui todas as GPUs atuais), o framebuffer pode ser composto por diversos buffers. Pelo menos um deles é um color buffer (buffer de cor) no qual cada pixel contém uma informação de cor, geralmente no formato RGB (24 bits) ou RGBA (32 bits). Um framebuffer pode ter vários buffers de cor associados. Por exemplo, em implementações que suportam visão estereoscópica, podemos ter um buffer de cor para a tela da visão esquerda e outro para a tela da visão direita. Na técnica de double buffering (descrita no fim da seção), são utilizados dois buffers de cor: o backbuffer, que é um buffer off-screen no qual a imagem é renderizada antes de ser exibida na tela, e o frontbuffer, que recebe o conteúdo do backbuffer ao fim da renderização para exibição na tela. Em renderização estéreo, cada lado esquerdo e direito pode ter o seu backbuffer e frontbuffer (ou outros buffers mais). Cada buffer de cor do framebuffer também pode ser associado a: Um depth buffer (buffer de profundidade), no qual cada pixel contém uma informação de profundidade utilizada no teste de profundidade. O teste de profundidade faz parte da implementação da técnica de Z-buffering de determinação de superfícies visíveis. A informação de profundidade pode ser um inteiro ou ponto flutuante de 16, 24 ou 32 bits (geralmente 24 bits). Um stencil buffer (buffer de estêncil), utilizado no teste de estêncil para operações de mascaramento e composição de imagens. No buffer estêncil, cada pixel contém um inteiro sem sinal, de 1, 4, 8 ou 16 bits (geralmente 8 bits). Screen tearing A taxa de atualização do dispositivo de exibição (chamada de vertical refresh rate) é controlada pelo hardware gráfico. Entretanto, a taxa em que a GPU atualiza o framebuffer pode ser bem maior que a taxa de atualização do dispositivo. Essa taxa é o número de quadros por segundo (FPS) que o processador gráfico é capaz de renderizar. Se o framebuffer for atualizado muito rapidamente, o hardware gráfico pode começar a atualizar o dispositivo de exibição com o conteúdo de um quadro de exibição e terminar com o conteúdo de outro, mais recente. Essa quebra entre os quadros de exibição gera um defeito na imagem conhecido como screen tearing, ou simplesmente tearing (figura 3.39). Figura 3.39: Screen tearing. (fonte). Vsync Para reduzir o problema de tearing, a GPU pode sincronizar o desenho do framebuffer com o tempo de apagamento vertical (VBI), que é o intervalo de tempo definido entre o fim da varredura de um quadro de exibição e o início da varredura do quadro seguinte. Essa sincronização efetivamente limita o número de FPS à frequência do monitor em Hz. Esse processo de sincronização é chamado de sincronização vertical ou Vsync (vertical synchronization). Em monitores mais recentes, compatíveis com as tecnologias G-SYNC da NVIDIA, e FreeSync da AMD, é possível fazer a sincronização na direção contrária: a frequência do monitor é ajustada pela GPU de acordo com a taxa de FPS. Multiple buffering O uso de VSync resolve o problema do tearing quando os quadros são renderizados em uma frequência mais alta que a frequência do monitor. Porém, quando a taxa de atualização do framebuffer é menor que a frequência do monitor, o conteúdo parcial do novo quadro pode ser misturado com o conteúdo do quadro anterior, gerando tearing. Uma solução para esse caso é usar a técnica de double buffering. Double buffering consiste em utilizar dois framebuffers: o backbuffer e o frontbuffer (figura 3.40). A GPU renderiza os gráficos apenas no backbuffer. Enquanto isso, o hardware gráfico atualiza o dispositivo de exibição com o conteúdo do frontbuffer. No próximo VBI, se a renderização no backbuffer ainda não tiver terminado, o mesmo frontbuffer é exibido novamente. Caso contrário, o backbuffer e o frontbuffer trocam de lugar, e agora o frontbuffer torna-se o backbuffer, e vice-versa. Desse modo, o hardware gráfico terá agora no frontbuffer um quadro completo para ser exibido na tela. Figura 3.40: Double buffering. Os casos em que a GPU precisa usar o mesmo frontbuffer (porque o backbuffer ainda está sendo desenhado) não são desejáveis pois podem ser percebidos como travamentos da imagem (stuttering), e resultam em uma diminuição da média de quadros por segundo. Isso pode ser melhorado com a adição de mais um backbuffer, no chamado triple buffering. O resultado é uma espécie de fila circular de buffers chamada de swapchain (figura 3.41). No triple buffering, dois quadros consecutivos são desenhados nos backbuffers. No próximo intervalo de apagamento vertical, a fila avança e o backbuffer mais antigo toma o lugar do frontbuffer, que por sua vez torna-se um backbuffer. Esse princípio de uso de múltiplos backbuffers (multiple buffering) pode ser estendido para ainda mais buffers. Entretanto, quanto maior o número de backbuffers, maior o atraso (lag) entre o quadro mais novo renderizado e o quadro que está sendo exibido na tela. Figura 3.41: Triple buffering. "],["sierpinski.html", "3.5 Triângulo de Sierpinski", " 3.5 Triângulo de Sierpinski Nesta atividade usaremos pela primeira vez comandos de desenho do OpenGL. Usaremos esses comandos para desenhar um triângulo de Sierpinski composto de pontos no plano. O triângulo de Sierpinski é um fractal que pode ser gerado por um tipo de sistema dinâmico chamado de sistema de função iterada (iterated function system, ou IFS). Esse processo pode ser implementado através de um algoritmo conhecido como jogo do caos. Para “jogar” o jogo do caos, começamos definindo três pontos \\(A\\), \\(B\\) e \\(C\\) não colineares. Por exemplo, \\(A=(0, 1)\\), \\(B=(-1, -1)\\) e \\(C=(1, -1)\\): Além dos pontos \\(A\\), \\(B\\) e \\(C\\), definimos mais um ponto \\(P\\) em uma posição aleatória do plano. Com \\(A\\), \\(B\\), \\(C\\) e \\(P\\) definidos, o jogo do caos consiste nos seguintes passos: Mova \\(P\\) para o ponto médio entre \\(P\\) e um dos pontos \\(A\\), \\(B\\), \\(C\\) escolhido de forma aleatória; Volte ao passo 1. Para gerar o triângulo de Sierpinski, basta desenhar \\(P\\) a cada iteração. O jogo não tem fim, mas quanto maior o número de iterações, mais pontos serão desenhados e mais detalhes terá o fractal (figura 3.42). Figura 3.42: Triângulo de Sierpinski desenhado com 1.000, 10.000 e 100.000 iterações em uma área de 210x210 pixels. Implementaremos o jogo do caos usando a mesma estrutura dos projetos firstapp (seção 2.3) e tictactoe (seção 2.4). O procedimento será simples: para cada chamada de onPaint, faremos uma iteração do jogo e desenharemos um ponto na posição \\(P\\) usando um comando de renderização do OpenGL. Os pontos desenhados serão acumulados no framebuffer e o resultado será a visualização da construção do fractal. Configuração inicial Crie o subdiretório abcg/examples/sierpinski e modifique o arquivo abcg/examples/CMakeLists.txt para incluir essa nova pasta. Comente as linhas de add_subdirectory dos projetos anteriores para que eles não sejam compilados: # add_subdirectory(helloworld) # add_subdirectory(firstapp) # add_subdirectory(tictactoe) add_subdirectory(sierpinski) Crie o arquivo abcg/examples/sierpinski/CMakeLists.txt com o seguinte conteúdo, similar ao que utilizamos nos projetos anteriores: project(sierpinski) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Crie também os arquivos main.cpp, window.cpp e window.hpp em abcg/examples/sierpinski. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp ficará como a seguir: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings({.samples = 2, .doubleBuffering = false}); window.setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Esse código é parecido com o main.cpp dos projetos firstapp e tictactoe. As únicas diferenças estão nas funções chamadas nas linhas 8 e 9: window.setOpenGLSettings({.samples = 2, .doubleBuffering = false}); window.setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); setOpenGLSettings é uma função membro de abcg::OpenGLWindow, classe base de nossa classe customizada Window. setOpenGLSettings recebe uma estrutura abcg::OpenGLSettings com as configurações de inicialização do OpenGL. Essas configurações são usadas pela SDL no momento da criação de um “contexto do OpenGL” que representa o framebuffer vinculado à janela e todo o estado interno do OpenGL: O atributo samples = 2 faz com que o framebuffer suporte suavização de serrilhado (antialiasing) das primitivas do OpenGL; O atributo .doubleBuffering = false é uma configuração de criação do contexto do OpenGL que faz com que a técnica de double buffering seja desativada. Desse modo, conseguiremos desenhar os pontos incrementalmente no framebuffer para formar o desenho do fractal. Se não desativarmos o double buffering, o OpenGL poderá apagar o conteúdo do backbuffer para cada novo quadro de exibição (isso acontece com a versão para web), e então só visualizaremos o último ponto desenhado. No estrutura abcg::WindowSettings passada como argumento de setWindowSettings, usamos showFullscreenButton = false para desativar a exibição do botão de tela cheia. Afinal, não queremos que o botão obstrua o desenho do fractal. Mesmo sem o botão, o modo janela pode ser alternado com o modo de tela cheia através da tecla F11. window.hpp Na definição da classe Window, substituiremos novas funções virtuais de abcg::OpenGLWindow e definiremos novas variáveis que serão utilizadas para atualizar o jogo do caos e desenhar o ponto na tela: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOVertices{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec2, 3&gt; const m_points{{{0, 1}, {-1, -1}, {1, -1}}}; glm::vec2 m_P{}; void setupModel(); }; #endif Observe que, além de usarmos as funções onCreate, onPaint e onPaintUI, estamos agora substituindo mais duas funções virtuais de abcg::OpenGLWindow: onResize é chamada pela ABCg sempre que o tamanho da janela é alterado. O novo tamanho é recebido pelo parâmetro size, que é um vetor 2D de inteiros glm::ivec2 disponibilizado pela biblioteca OpenGL Mathematics (GLM)15. Na nossa aplicação, quando a ABCg chamar onResize, faremos uma cópia de size em m_viewportSize (linha 17). Isso será feito para que, em onPaint, possamos configurar o tamanho da área de desenho (viewport) do OpenGL para ser o mesmo tamanho da janela da aplicação. O conceito de viewport será aprofundado mais adiante. onDestroy é chamada pela ABCg quando a janela é destruída, isto é, no fim da aplicação. Essa é a função complementar de onCreate, usada para liberar os recursos do OpenGL que foram alocados em onCreate ou durante a aplicação. Da linha 17 a 25 temos a definição das variáveis da classe: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOVertices{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec2, 3&gt; const m_points{{{0, 1}, {-1, -1}, {1, -1}}}; glm::vec2 m_P{}; m_VAO, m_VBOVertices e m_program são identificadores de recursos alocados pelo OpenGL, geralmente armazenados na memória da placa de vídeo. Esses recursos correspondem aos shaders que definem o comportamento da renderização, e ao arranjo ordenado de vértices utilizado para montar as primitivas geométricas no pipeline de renderização. No nosso caso, o arranjo de vértices contém apenas um vértice e equivale ao ponto \\(P\\) que queremos desenhar. m_viewportSize, como já vimos, serve para armazenar o tamanho da janela da aplicação que é recebido em onResize. m_randomEngine é um objeto do gerador de números pseudoaleatórios da biblioteca padrão do C++ (note o uso do #include &lt;random&gt; na linha 4). Esse objeto é utilizado para sortear a posição inicial de \\(P\\) e qual ponto (\\(A\\), \\(B\\) ou \\(C\\)) será escolhido em cada iteração do jogo do caos. m_points é um arranjo que contém a posição dos pontos \\(A\\), \\(B\\) e \\(C\\). As coordenadas dos pontos são descritas por uma estrutura glm::vec2 que representa um vetor 2D com coordenadas do tipo float. m_P é a posição do ponto \\(P\\). Além da definição das variáveis, na linha 27 é declarada a função Window::setupModel que cria os recursos identificados por m_VAO e m_VBOVertices. A função é chamada sempre que um novo ponto \\(P\\) precisa ser desenhado. window.cpp Primeiro vamos implementar a lógica do jogo sem desenhar na tela. Em seguida, incluiremos o código que usa o OpenGL para desenhar os pontos. Começaremos com a definição de Window::onCreate(). Como essa função é chamada apenas uma vez na inicialização da aplicação, colocaremos aqui o código que inicia o gerador de números pseudoaleatórios e sorteia as coordenadas iniciais de \\(P\\) (que no código é m_P): #include &quot;window.hpp&quot; void Window::onCreate() { // Start pseudorandom number generator auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly pick a pair of coordinates in the range [-1; 1) std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } O gerador m_randomEngine é iniciado usando como semente o tempo do sistema. As coordenadas de m_P são iniciadas como valores sorteados do intervalo \\([-1, 1)\\). O intervalo não precisa ser este necessariamente, mas fazendo assim garantimos que o ponto inicial será visto na tela16. Na configuração padrão do OpenGL, só conseguimos visualizar as primitivas gráficas que estão situadas entre as coordenadas \\((-1, -1)\\) e \\((1, 1)\\). Na configuração padrão, a coordenada \\((-1, -1)\\) é mapeada ao canto inferior esquerdo da janela, e a coordenada \\((1, 1)\\) é mapeada ao canto superior direito. Esse mapeamento poderá ser modificado posteriormente com a função glViewport. Vamos agora implementar o passo iterativo do jogo. Faremos isso em Window::onPaint. Assim, cada quadro de exibição corresponderá a uma iteração: void Window::onPaint() { // Randomly pick the index of a triangle vertex std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); auto const index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex position m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Neste trecho de código, index é um índice do arranjo m_points. Assim, m_points.at(index) é um dos pontos \\(A\\), \\(B\\) ou \\(C\\) que definem os vértices do triângulo. Observe que utilizamos uma distribuição uniforme para sortear o índice (std::uniform_int_distribution). Isso é importante para que o fractal seja desenhado como esperado. A nova posição de m_P é calculada como o ponto médio entre m_P e o ponto de m_points. O código comentado pode ser utilizado para imprimir no terminal as novas coordenadas de m_P. Isso conclui a implementação da lógica do jogo. O resto do código será para desenhar m_P como um ponto na tela. No OpenGL anterior à versão 3.1, isso seria tão simples quanto acrescentar o seguinte código em Window::onPaint: glBegin(GL_POINTS); glVertex2f(m_P.x, m_P.y); glEnd(); Entretanto, como vimos na seção 3.2, esse código é obsoleto e não é mais suportado em muitos drivers e plataformas. Precisaremos seguir os seguintes passos para desenhar um simples ponto na tela: Criar um “buffer de vértices” como um recurso do OpenGL. Esse recurso é chamado VBO (Vertex Buffer Object) e corresponde ao arranjo ordenado de vértices utilizado pela GPU para montar as primitivas que serão renderizadas. No nosso caso, o buffer de vértices só precisa ter um vértice, que é a coordenada do ponto que queremos desenhar. A variável m_VBOVertices é um inteiro que identifica esse recurso. Programar o comportamento do pipeline de renderização. Isso é feito compilando e ligando um par de shaders que fica armazenado na GPU como um único “programa de shader”, identificado pela variável m_program. No OpenGL, os shaders são escritos na linguagem GLSL (OpenGL Shading Language), que é parecida com a linguagem C, mas possui novos tipos de dados e operações. Especificar como o buffer de vértices será lido pelo programa de shader. No nosso código, o estado dessa configuração é armazenado como um objeto do OpenGL chamado VAO (Vertex Array Object), identificado pela variável m_VAO. Somente após alocar e ativar esses recursos é que podemos iniciar o pipeline de renderização, chamando uma função de desenho em Window::onPaint. Não se preocupe se tudo isso está parecendo muito complexo nesse momento. Nos próximos capítulos revisitaremos cada etapa diversas vezes até nos familiarizarmos com todo o processo. Por enquanto, utilizaremos o código já pronto. Primeiro, defina Window::setupModel como a seguir: void Window::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_VBOVertices); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_VBOVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Esse código cria o VBO (m_VBOVertices) e VAO (m_VAO) usando a posição atual de m_P. Agora, modifique onCreate para o seguinte código final: void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes.at(0), sizes.at(1)); // Start pseudorandom number generator auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly pick a pair of coordinates in the range [-1; 1) std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } Nesta função, vertexShader e fragmentShader são strings que contêm o código-fonte dos shaders. vertexShader é o código do chamado vertex shader, que programa o processamento de vértices na GPU. fragmentShader é o código do fragment shader, que programa o processamento de pixels na GPU (ou, mais precisamente, o processamento de fragmentos, que são conjuntos de atributos que representam uma amostra de geometria rasterizada). A compilação e ligação dos shaders é feita pela função abcg::createOpenGLProgram. Se acontecer algum erro de compilação, a mensagem de erro será exibida no console e uma exceção será lançada. Note que limpamos o buffer de cor com a cor preta, usando glClearColor e glClear (linhas 27 e 28). Observe o trecho de código entre as diretivas de pré-processamento: #if !defined(__EMSCRIPTEN__) glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else Esse código só será compilado quando não usarmos o Emscripten, isto é, quando o binário for compilado para desktop. No OpenGL para desktop, o comando da linha 32 é necessário para que o tamanho do ponto que será desenhado possa ser definido no vertex shader. Quando o código é compilado com o Emscripten, podemos considerar que a definição do tamanho do ponto no vertex shader é suportada por padrão, pois o OpenGL utilizado nesse caso é o OpenGL ES (o WebGL usa um subconjunto de funções do OpenGL ES). Observe, no código do vertex shader, que o tamanho do ponto é definido com gl_PointSize = 2.0 (isto é, dois pixels). Os tamanhos válidos dependem do que é suportado pelo hardware. Para imprimir no console os tamanhos mínimo e máximo, usamos glGetFloatv neste trecho de código: std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes.at(0), sizes.at(1)); A função glGetFloatv com o identificador GL_POINT_SIZE_RANGE (para OpenGL desktop) e GL_ALIASED_POINT_SIZE_RANGE (para OpenGL ES) preenche o arranjo sizes com os tamanhos mínimo e máximo suportados. Em seguida, fmt::print mostra os valores no console. Voltando agora à implementação de Window::onPaint, o código final ficará assim: void Window::onPaint() { // Create OpenGL buffers for drawing the point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_VAO); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly pick the index of a triangle vertex std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); auto const index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex position m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Na linha 52, setupModel cria os recursos do OpenGL necessários para desenhar um ponto na posição atual de m_P. Na linha 55, glViewport configura o mapeamento entre o sistema de coordenadas no qual nossos pontos foram definidos (coordenadas normalizadas do dispositivo, ou NDC, de normalized device coordinates), e o sistema de coordenadas da janela (window coordinates), em pixels, com origem no canto inferior esquerdo da janela da aplicação. A figura 3.43 ilustra como fica configurado o mapeamento entre coordenadas em NDC para coordenadas da janela, supondo uma chamada a glViewport(x, y, w, h), onde x, y, w e h são inteiros dados em pixels da tela. Na figura, o chamado viewport do OpenGL é a janela formada pelo retângulo entre os pontos \\((x,y)\\) e \\((x+w,y+h)\\). No nosso código com glViewport(0, 0, m_viewportSize.x, m_viewportSize.y), o ponto \\((-1,-1)\\) em NDC é mapeado para o pixel \\((0, 0)\\) da janela (canto inferior esquerdo), e o ponto \\((1,1)\\) em NDC é mapeado para o pixel \\((0,0)\\) + (m_viewportSize.x, m_viewportSize.y). Isso faz com que o viewport ocupe toda a janela da aplicação. Figura 3.43: Mapeamento das coordenadas normalizadas no dispositivo (NDC) para coordenadas da janela usando glViewport(x, y, w, h). Com o viewport devidamente configurado, iniciamos o pipeline de renderização neste trecho: // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_VAO); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); Na linha 58, glUseProgram ativa os shaders compilados no programa m_program. Na linha 60, glBindVertexArray ativa o VAO (m_VAO), que contém as especificações de como o arranjo de vértices (VBO) será lido no vertex shader atualmente ativo. Ao ativar o VAO, também é ativado automaticamente o VBO identificado por m_VBO. Finalmente, na linha 63, glDrawArrays inicia o pipeline de renderização usando os shaders e o VBO ativos. O primeiro argumento (GL_POINTS) indica que os vértices do arranjo de vértices devem ser tratados como pontos. O segundo argumento (0) é o índice inicial dos vértices no VBO. O terceiro argumento (1) informa quantos vértices devem ser processados. O processamento no pipeline de renderização é realizado de forma paralela e assíncrona com a CPU. Isto é, glDrawArrays retorna imediatamente, enquanto a GPU continua trabalhando em paralelo renderizando a geometria no framebuffer. Após o comando de renderização, nas linhas 66 e 68 temos comandos para desativar o VAO e os shaders. Essa desativação é opcional pois, de qualquer forma, o mesmo VAO e os mesmos shaders serão utilizados na próxima chamada de Window::onPaint. Ainda assim, é uma boa prática de programação desativá-los logo após seu uso. Vamos agora definir a função Window::onResize, assim: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } Como vimos, resizeGL é chamada sempre que a janela da aplicação muda de tamanho. Observe que simplesmente armazenamos o tamanho da janela em m_viewportSize. Como m_viewportSize será usado em glViewport, garantimos que o viewport sempre ocupará toda a janela da aplicação. Observe que também chamamos glClear para apagar o buffer de cor. Essa é uma forma de reiniciar o desenho do fractal. Afinal, estragaríamos o triângulo de Sierpinski se continuássemos desenhando sobre o fractal de tamanho anterior. A função Window::onDestroy é definida da seguinte maneira: void Window::onDestroy() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOVertices); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Os comandos glDelete* liberam os recursos alocados em Window::setupModel. Para finalizar, definiremos Window::onPaintUI usando o seguinte código: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 81)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } Na linha 83 chamamos o onPaintUI da classe base, responsável por mostrar o contador de FPS. Normalmente, o botão de tela cheia também seria desenhado, mas desabilitamos sua exibição em main.cpp. O código nas linhas 85 a 94 cria uma pequena janela da ImGui (com título em branco e sem decorações) contendo um botão “Clear window” que chama glClear sempre que pressionado. Isso é tudo! Construa a aplicação para ver o resultado: O projeto completo pode ser baixado deste link. Exercícios Em Window::onPaint, utilizamos std::uniform_int_distribution para sortear os vértices do triângulo usando uma distribuição uniforme. Veja o que acontece caso a distribuição não seja uniforme. Em particular, troque std::uniform_int_distribution por std::discrete_distribution e use pesos diferentes para cada vértice. Por exemplo, use std::discrete_distribution&lt;int&gt; intDistribution({30, 50, 20}) para que os índices 0, 1, e 2 tenham respectivamente 30%, 50%, e 20% de chance de serem sorteados. Faça com que os pesos da distribuição discreta do item anterior possam ser configurados pelo usuário através de sliders da ImGui. Além disso, faça com que a soma dos três pesos seja sempre 100. A biblioteca GLM fornece estruturas e funções de operações matemáticas compatíveis com a especificação da linguagem de shaders do OpenGL (GLSL). Por exemplo, glm::ivec2 representa um vetor de dois valores do tipo int. Na linguagem de shaders também existe ivec2, mas ele existe como um tipo de dado nativo.↩︎ O ponto inicial poderia ser simplesmente um ponto fixo ou um dos vértices, mas nosso código segue o algoritmo original descrito no início da seção.↩︎ "],["pipeline.html", "4 Pipeline gráfico", " 4 Pipeline gráfico O pipeline gráfico ou pipeline de renderização é um modelo conceitual de descrição da sequência de passos que a GPU utiliza para transformar um modelo matemático em uma imagem digital. O termo “pipeline” é utilizado porque o processamento é realizado em uma sequência de etapas alimentadas por um fluxo de dados, sendo que cada etapa processa novos dados tão logo tenha enviado a saída à etapa seguinte. Algumas etapas de processamento do pipeline podem ser programadas por shaders. Outras etapas são fixas e não podem ser programadas. A aplicação é responsável por configurar previamente os dados gráficos na memória acessada pela GPU antes de dar início ao processamento do pipeline. Em geral, os dados gráficos descrevem elementos de uma cena tridimensional tais como representações dos objetos da cena, descrição de materiais e fontes de luz, além de transformações geométricas que podem ser utilizadas para definir como os objetos serão dispostos no espaço e projetados no plano de imagem de uma câmera virtual. As superfícies dos objetos de uma cena são geralmente representadas por malhas de triângulos. Para cada superfície podem ser associados atributos que descrevem suas propriedades, como por exemplo as propriedades físicas do material que a superfície pretende simular. Os shaders podem ler esses atributos e simular a interação entre cada superfície com a luz emitida pelas fontes de luz ou refletida de outras superfícies. Além dos dados que descrevem a cena, a aplicação também é responsável por inicializar o pipeline com os shaders que serão utilizados durante a renderização. O pipeline é iniciado pela aplicação através de um ou mais comandos de desenho (draw calls). Esse processamento do pipeline na GPU é realizado de forma assíncrona com a CPU. O pipeline típico especificado pelas APIs gráficas envolve etapas que compreendem o processamento geométrico, a rasterização e o processamento de fragmentos (figura 4.1): Figura 4.1: Etapas de um pipeline gráfico. Processamento geométrico: envolve operações realizadas sobre vértices, como transformações afins e transformações projetivas que serão abordadas em capítulos futuros. O processamento geométrico pode envolver também a criação de geometria e o refinamento de malhas. Ao final desse processamento é feito o recorte ou descarte das primitivas geométricas que estão fora de um volume de visualização. Rasterização: compreende a conversão matricial das primitivas. O resultado é um conjunto de amostras de primitivas. Durante o processamento no pipeline, o termo fragmento é frequentemente utilizado para designar essas amostras no lugar de pixel. Cada fragmento é uma coleção de valores que inclui atributos interpolados a partir dos vértices e a posição \\((x,y,z)\\) da amostra em coordenadas da janela (o valor \\(z\\) é considerado a “profundidade” do fragmento). O pixel é o valor final da cor no buffer de cor, que pode ser uma combinação da cor de vários fragmentos. Processamento de fragmentos: envolve operações realizadas sobre cada fragmento para determinar sua cor e outros atributos. A cor pode ser determinada através da avaliação de modelos de iluminação que levam em conta os atributos de iluminação fornecidos pela aplicação, tais como atributos das fontes de luz, e descrições de detalhes das superfícies através de mapas de bits chamados de texturas. Após essas operações são realizados testes de descarte de fragmentos e combinação de cores entre os fragmentos processados e os pixels já existentes no framebuffer. O resultado é armazenado em diferentes buffers do framebuffer: buffers de cor, buffer de profundidade e buffer de estêncil. "],["dados-gráficos.html", "4.1 Dados gráficos", " 4.1 Dados gráficos O processamento de um pipeline gráfico começa com a definição dos dados gráficos pela aplicação. Esses dados são frequentemente representações de objetos – abstrações de objetos do mundo real – dispostos em uma cena virtual tridimensional. Uma cena é tipicamente composta por: Objetos com superfícies descritas através de modelos geométricos tais como malhas de triângulos, nuvens de pontos, equações paramétricas ou equações implícitas. Propriedades dos materiais que compõem as superfícies dos objetos. Essas propriedades descrevem a forma como uma luz incidente na superfície é refletida, absorvida e/ou transmitida. Fontes de luz descritas por informações como intensidade, direção de propagação da luz e fatores de atenuação. Alternativamente, uma fonte de luz pode ser um objeto com superfície de material emissivo. Uma câmera virtual descrita por informações que permitem definir um ponto de vista na cena, tais como posição da câmera, orientação e campo de visão. A câmera virtual é uma abstração de uma câmera ou observador do mundo real. Em uma câmera de verdade, a imagem é formada a partir da energia luminosa que atravessa as lentes e é captada pelo filme ou sensor durante um tempo de exposição. Poderíamos tentar simular de forma precisa o comportamento de uma câmera real, calculando a intensidade de luz de cada comprimento de onda que adentra a abertura da câmera vindo de diferentes direções, atravessa o sistema de lentes e então incide em cada subpixel RGB do sensor da câmera durante um tempo de exposição. Entretanto, uma simulação com esse grau de precisão seria extremamente custosa e inviável mesmo em hardware gráfico atual. Precisamos simplificar de forma significativa este processo, especialmente para aplicações de síntese de imagens em tempo real. Em muitas aplicações, essa aproximação pode ser até mesmo fisicamente incorreta, e a produção da percepção de sombreamento (shading) dos objetos pode ser suficiente. Para a maioria das aplicações, é comum considerar que a câmera virtual é uma câmera pinhole ideal (figura 4.2). A câmera pinhole é uma câmera que não possui lentes. A luz passa por um pequeno furo (chamado de centro de projeção) e incide sobre um filme ou sensor localizado no fundo da câmera (o plano de imagem). A abertura do campo de visão pode ser ajustada mudando a distância focal, que é a distância entre o centro de projeção e o plano de imagem. Na câmera pinhole ideal, a abertura do furo é infinitamente pequena e as imagens formadas são perfeitamente nítidas (isto é, em foco). Efeitos de difração são ignorados nesse modelo. Figura 4.2: Camera pinhole ideal. Para evitar ter de lidar com a imagem invertida formada no plano de imagem da câmera pinhole, podemos considerar que o plano de imagem está localizado na frente do centro de projeção, o que seria impossível de fazer numa câmera real. Podemos também ajustar arbitrariamente a distância focal sem preocupação com limitações físicas. A distância focal pode até mesmo ser infinita, se desejarmos uma projeção paralela. Por fim, podemos considerar que o plano de imagem é o framebuffer. Nessa configuração, é comum considerar que o centro de projeção corresponde ao olho do observador, como mostra a figura (figura 4.3). Figura 4.3: Câmera virtual com plano de imagem na frente do centro de projeção. Para determinar a cor de cada pixel do framebuffer para um determinado ponto de vista da câmera virtual, podemos considerar duas abordagens de renderização: ray casting e rasterização. Essas abordagens são apresentadas na seção a seguir. "],["ray-casting-x-rasterização.html", "4.2 Ray casting x rasterização", " 4.2 Ray casting x rasterização Ray casting e rasterização são duas abordagens distintas de se renderizar uma cena, e resultam em pipelines também distintos. Ray casting consiste em lançar raios que saem do centro de projeção, atravessam os pixels da tela e intersectam os objetos da cena. A rasterização faz o caminho inverso: os objetos da cena são projetados na tela na direção do centro de projeção, e são então convertidos em pixels. Neste curso usaremos apenas a rasterização, que é a forma de renderização utilizada na maioria das aplicações gráficas interativas. É também a única abordagem de renderização suportada atualmente no pipeline gráfico do OpenGL. Entretanto, é importante observar que novos pipelines baseados em traçado de raios (uma forma de ray casting) têm sido incorporados às APIs gráficas e tendem a conquistar cada vez mais espaço em síntese de imagens em tempo real. Ray casting Na sua forma mais simples, o algoritmo de ray casting (Roth 1982) consiste nos seguintes passos (figura 4.4): Figura 4.4: Ray casting. Para cada pixel do framebuffer: Calcule o raio \\(R\\) que sai do centro de projeção e passa pelo pixel. Seja \\(P\\) a interseção mais próxima (se houver) de \\(R\\) com um objeto da cena. Faça com que a cor do pixel seja a cor calculada em \\(P\\). Em ray casting, cada pixel é visitado apenas uma vez. Entretanto, para cada pixel visitado, potencialmente todos os objetos da cena podem ser consultados para calcular a interseção mais próxima. Assim, o principal custo da geração de imagem usando ray casting está relacionado ao cálculo das interseções. Estruturas de dados de subdivisão espacial podem ser utilizadas para que seja possível descartar rapidamente a geometria não intersectada pelo raio e com isso diminuir o número de testes de interseção. Algumas dessas estruturas de aceleração utilizadas em ray casting são a k-d tree (Bentley 1975), octree (Meagher 1980), e a hierarquia de volumes delimitantes (BVH, bounding volume hierarchies). Embora o ray casting seja conceitualmente simples, exige a manutenção da estrutura de aceleração na memória do renderizador. Essa limitação tem sido cada vez menos significativa nas GPUs mais recentes, mas ray casting ainda é pouco utilizado em síntese de imagens em tempo real. Observação Para produzir imagens fotorrealistas, a cor em \\(P\\) deve ser calculada através da integração da energia luminosa que incide sobre o ponto vindo de todas as direções da cena, e da determinação da quantidade dessa energia que é refletida na direção do pixel na tela. Isso pode ser feito de diferentes formas e em diferentes níveis de aproximação. Uma aproximação pouco acurada, mas muito eficiente, é avaliar a equação de um modelo de iluminação local como o modelo de reflexão de Phong (Phong 1973) ou Blinn-Phong (Blinn 1977) que considera que a cor em uma superfície é determinada unicamente pela luz que incide diretamente sobre a superfície, e não pela luz indireta refletida por outros objetos. Outra aproximação, mais acurada porém bem menos eficiente, é a técnica de traçado de raios recursivo (Whitted 1979) que consiste em lançar novos raios a partir de \\(P\\) (figura 4.5). Esses raios são: Um raio de sombra (shadow ray) em direção a cada fonte de luz, para saber se \\(P\\) encontra-se na sombra em relação à fonte de luz correspondente; Um raio de reflexão (reflection ray) na direção espelhada em relação ao vetor normal à superfície em \\(P\\); Um raio de refração (refraction ray) que atravessa a superfície do objeto, caso o objeto seja transparente. Figura 4.5: Traçado de raios recursivo. Os raios de reflexão e refração podem intersectar outros objetos, e novos raios podem ser gerados a partir desses pontos de interseção, recursivamente, de tal modo que a cor final refletida em \\(P\\) é formada por uma combinação da energia luminosa representada por todos os raios. Rasterização Em oposição ao ray casting, a rasterização é centrada no processamento de primitivas em vez de pixels. Cada primitiva é projetada no plano de imagem e rasterizada em seguida (figura 4.6): Figura 4.6: Rasterização. Para cada primitiva da cena: Projete a primitiva no plano de imagem. Rasterize a primitiva projetada. Modifique o framebuffer com a cor calculada em cada pixel da primitiva, exceto se o pixel do framebuffer já tiver sido preenchido anteriormente com uma primitiva mais próxima do plano de imagem. Na etapa 3, a cor do pixel é geralmente avaliada através de um modelo de iluminação local como o modelo de Blinn-Phong. Outras técnicas podem ser utilizadas para melhorar a aproximação da luz refletida no ponto amostrado, mas não há lançamento de raios ou testes de interseção como no ray casting. A rasterização é mais adequada para implementação em hardware, pois cada iteração do laço principal só precisa armazenar a primitiva que está sendo processada, juntamente com o conteúdo do framebuffer. Como resultado, o processamento de transformação geométrica de vértices e a conversão matricial podem ser paralelizados de forma massiva, como de fato ocorre nas GPUs. Referências "],["glpipeline.html", "4.3 Pipeline do OpenGL", " 4.3 Pipeline do OpenGL A figura 4.7 mostra um diagrama dos estágios de processamento do pipeline gráfico do OpenGL (fundo amarelo, à esquerda) e de como os dados gráficos (fundo cinza, à direita) interagem com cada estágio. As etapas programáveis são mostradas com fundo preto (vertex shader e fragment shader). No lado esquerdo há uma ilustração do resultado de cada etapa para a renderização de um triângulo colorido. Figura 4.7: Pipeline gráfico do OpenGL. Observação Para simplificar, algumas etapas do pipeline foram omitidas, tais como: O geometry shader, utilizado para o processamento de geometria após a montagem de primitivas; Os shaders de tesselação (tessellation control shader e tessellation evaluation shader), utilizados para subdivisão de primitivas; O compute shader, utilizado para processamento de propósito geral (GPGPU). Essas etapas não serão utilizadas nas atividades da disciplina pois, no momento, não fazem parte do subconjunto do OpenGL ES (OpenGL for Embedded Systems) utilizado pelo WebGL 2.0. Entretanto, são etapas frequentemente utilizadas em aplicações para OpenGL desktop. Consulte a especificação do OpenGL 4.6 para ter acesso ao pipeline da versão mais recente para desktop. Aplicação Antes de iniciar o processamento, a aplicação deve especificar o formato dos dados gráficos e enviar esses dados à memória que será acessada durante a renderização. A aplicação também deve configurar as etapas programáveis do pipeline, compostas pelo vertex shader e fragment shader. Os shaders devem ser compilados, ligados e ativados previamente. A geometria a ser processada é especificada através de um arranjo ordenado de vértices. O tipo de primitiva que será formada a partir desses vértices é determinado no comando de renderização. As primitivas suportadas pelo OpenGL são descritas a seguir e mostradas na figura 4.8: GL_POINTS: cada vértice forma um ponto que será desenhado na tela como um pixel ou como um quadrilátero centralizado no vértice. O tamanho do ponto/quadrilátero pode ser definido pelo usuário17; GL_LINES: cada grupo de dois vértices forma um segmento de reta; GL_LINE_STRIP: os vértices são conectados em ordem para formar uma polilinha; GL_LINE_LOOP: os vértices são conectados em ordem para formar uma polilinha, e o último vértice forma um segmento com o primeiro vértice, formando um laço; GL_TRIANGLES: cada grupo de três vértices forma um triângulo; GL_TRIANGLE_STRIP: os vértices formam uma faixa de triângulos com arestas compartilhadas; GL_TRIANGLE_FAN: os vértices formam um leque de triângulos de modo que todos os triângulos compartilham o primeiro vértice. Figura 4.8: Primitivas do OpenGL. Cada vértice do arranjo de vértices de entrada é composto por um conjunto de atributos definidos pela aplicação. Cada atributo pode ser um único valor ou um conjunto de valores. A forma como esses valores são interpretados depende exclusivamente do que é definido no vertex shader. Geralmente, considera-se que cada vértice tem pelo menos uma posição 2D \\((x,y)\\) ou 3D \\((x,y,z)\\). Outros atributos comuns para cada vértice são o vetor normal, cor e coordenadas de textura. Para ser utilizado pelo pipeline, o arranjo de vértices deve ser armazenado na memória como um recurso chamado Vertex Buffer Object (VBO). Cada atributo de vértice pode ser armazenado como um VBO separado, mas também é possível deixar todos os atributos em um único VBO (interleaved data). Cabe à aplicação especificar o formato dos dados de cada VBO e como eles serão lidos pelo vertex shader. Isso deve ser feito sempre antes da chamada do comando de renderização, para todos os VBOs. Alternativamente, essa configuração pode ser feita apenas uma vez e armazenada em um Vertex Array Object (VAO), bastando então ativar o VAO antes de cada comando de desenho. Além da criação dos VBOs, a aplicação pode criar variáveis globais, chamadas de variáveis uniformes (uniform variables), que podem ser lidas pelo vertex shader e fragment shader. Essa é uma outra forma de enviar dados ao pipeline. As variáveis uniformes contêm dados apenas de leitura e que não variam de vértice para vértice, por isso o nome “uniforme”. Por exemplo, uma matriz de transformação geométrica pode ser armazenada como uma variável uniforme pois todos os vértices serão transformados por essa matriz durante o processamento no vertex shader (isto é, a matriz de transformação é a mesma para todos os vértices). Também é possível criar buffers de dados uniformes (Uniform Buffer Objects, ou UBOs) para enviar arranjos de dados. A especificação do OpenGL garante ser possível enviar pelo menos 16KB de dados no formato de UBOs, mas é comum os drivers oferecerem suporte a até 64KB. A aplicação também pode enviar dados ao pipeline usando buffers de texturas (buffer textures). Os valores dos texels dessas texturas podem ser lidos no vertex shader e no fragment shader como se fossem valores de arranjos unidimensionais. Esses valores podem ser interpretados como cores RGBA normalizadas entre 0 e 1 ou como valores arbitrários em ponto flutuante de até 32 bits. Uma forma mais recente e flexível de enviar dados uniformes é através dos Shader Storage Buffer Objects (SSBOs). O tamanho de um SSBO pode ser de até 128MB segundo a especificação, mas na maioria das implementações pode ser tão grande quanto a memória de vídeo disponível. Além disso, esse recurso pode ser utilizado tanto para leitura quanto escrita. Há muitas formas de enviar e receber dados da GPU. Entretanto, para deixarmos as coisas mais simples, usaremos neste curso apenas os recursos mais básicos, como VBOs, VAOs e variáveis uniformes. Vertex shader Os shaders do OpenGL são programas escritos na linguagem OpenGL Shading Language (GLSL). GLSL é similar à linguagem C, mas utiliza novas palavras-chave, novos tipos de dados, qualificadores e operações. A versão mais recente da GLSL é a versão 4.6. Entretanto, usaremos a especificação GLSL ES 3.0 para garantir a compatibilidade com WebGL 2.0. Os documentos de especificação dessas duas versões podem ser acessados pelos links a seguir: OpenGL ES Shading Language 3.0: versão compatível com WebGL 2.0. OpenGL Shading Language 4.6: versão mais recente com suporte a geometry shaders, tessellation shaders, compute shaders, mas sem compatibilidade com WebGL 2.0. O vertex shader processa cada vértice individualmente. Entretanto, esse processamento é paralelizado de forma massiva na GPU. Cada execução de um vertex shader acessa apenas os atributos do vértice que está sendo processado. Não há como compartilhar o estado do processamento de um vértice com os demais vértices. A entrada do vertex shader é um conjunto de atributos de vértice definidos pelo usuário. Esses atributos são alimentados pelo pipeline de acordo com os VBOs atualmente ativos. A saída do vertex shader é também um conjunto de atributos de vértice definidos pelo usuário. Esses atributos podem ser diferentes dos atributos de entrada. Além de escrever o resultado nos atributos de saída, é esperado (mas não obrigatório) que o vertex shader preencha uma variável embutida gl_Position com a posição final do vértice em um sistema de coordenadas homogêneas 4D \\((x, y, z, w)\\) chamado de espaço de recorte (clip space). Nos próximos estágios, a geometria das primitivas será determinada com bases nessas coordenadas. Veremos mais detalhes sobre os diferentes sistemas de coordenadas do OpenGL em capítulos futuros. Exemplo A seguir é exibido o código-fonte de um vertex shader: #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition.x, inPosition.y * 1.5, 0, 1); fragColor = inColor / 2; } A primeira linha contém a diretiva de pré-processamento #version que identifica a versão da especificação GLSL utilizada. A diretiva deve ser escrita obrigatoriamente na primeira linha do shader, sem espaços anteriores ou linhas em branco. Neste exemplo, #version 300 es corresponde à especificação GLSL ES 3.0. Se quisermos escrever um shader que use funcionalidades mais recentes do OpenGL – ainda que quebrando a compatibilidade com WebGL 2.0 – devemos mudar essa diretiva para a versão correspondente. Por exemplo, a especificação GLSL 4.0, introduzida com o OpenGL 4.0, oferece suporte a shaders de tesselação. Um shader com a diretiva #version 400 é um shader compatível com essa especificação. A especificação mais recente é a 4.6 (#version 460). Nas linhas 3 e 4 são definidas as variáveis que receberão os atributos de entrada. Essas variáveis são identificadas com o qualificador in18: inPosition é uma tupla de dois elementos (vec2) que recebe uma posição 2D (a posição do vértice). inColor é uma tupla de quatro elementos (vec4) que recebe componentes de cor RGBA (a cor do vértice). O vertex shader tem apenas um atributo de saída, definido na linha 6 através da variável fragColor com o qualificador out. A função main é chamada para cada vértice processado. Para cada chamada, inPosition e inColor recebem os atributos do vértice. Na linha 9, a variável embutida gl_Position é preenchida com \\((x, \\frac{3}{2}y,0,1)\\), onde \\(x\\) e \\(y\\) são as coordenadas da posição 2D de entrada. Isso significa que a geometria sofrerá uma escala não uniforme: será “esticada” verticalmente. Na linha 10, a variável de saída recebe a cor de entrada com a intensidade de cada componente RGBA dividida por dois. Isso significa que a cor de saída terá a metade da intensidade da cor de entrada. Montagem de primitivas A montagem de primitivas recebe os atributos de vértices processados pelo vertex shader e monta as primitivas de acordo com o que é informado na chamada do comando de renderização. As primitivas geradas são formadas por pontos, segmentos ou triângulos. As primitivas da figura 4.8 são sempre decompostas em uma dessas três primitivas básicas. Por exemplo, se a primitiva informada pela aplicação é GL_LINE_STRIP, a polilinha será desmembrada em uma sequência de segmentos individuais. Recorte Na etapa de recorte, as primitivas que estão fora do volume de visão (fora do viewport) são descartadas ou recortadas. Por exemplo, se a ponta de um triângulo estiver fora do volume de visão, o triângulo será recortado e formará um quadrilátero, que é então decomposto em dois triângulos. Os atributos dos vértices a mais gerados no recorte são obtidos através da interpolação linear dos atributos dos vértices originais. O recorte também pode operar sobre planos de recorte definidos pelo usuário no vertex shader. Após o recorte, ocorre a divisão perspectiva, que consiste na conversão das coordenadas homogêneas 4D \\((x, y, z, w)\\) em coordenadas cartesianas 3D \\((x, y, z)\\). Isso é feito dividindo \\(x\\), \\(y\\) e \\(z\\) por \\(w\\). O sistema de coordenadas resultante é chamado de coordenadas normalizadas do dispositivo (normalized device coordinates, ou NDC). Em NDC, todas as primitivas após o recorte estão situadas dentro de um volume de visão canônico: um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Ainda nesta etapa, as componentes \\(x\\) e \\(y\\) das coordenadas em NDC são mapeadas para o sistema de coordenadas da janela (chamado de espaço da janela, ou window space), em pixels. Esse mapeamento é configurado pelo comando glViewport. O valor \\(z\\) é mapeado de \\([-1, 1]\\) para \\([0, 1]\\) por padrão, mas isso pode ser configurado com glDepthRange. Rasterização Todas as primitivas contidas no volume de visão canônico passam por uma conversão matricial, na ordem em que foram processadas nas etapas anteriores. O resultado da rasterização de cada primitiva é um conjunto de fragmentos que representam amostras da primitiva no espaço da tela. Um fragmento pode ser interpretado como um pixel em potencial. A cor final de cada pixel no framebuffer poderá ser determinada por um fragmento ou pela combinação de vários fragmentos. Cada fragmento é descrito por dados como: Posição \\((x, y, z)\\) em coordenadas da janela19, sendo que \\(z\\) é a profundidade do fragmento (por padrão, um valor no intervalo \\([0, 1]\\)). Como cada fragmento tem uma profundidade, é possível determinar qual fragmento está “mais na frente” quando vários fragmentos são mapeados para a mesma posição \\((x, y)\\) da janela. Assim, a cor do pixel pode ser determinada apenas pelo fragmento mais próximo. Os demais podem ser descartados pois estão sendo escondidos pelo fragmento mais próximo. Atributos interpolados a partir dos vértices da primitiva. Isso inclui todos os atributos definidos na saída do vertex shader. Por exemplo, se a saída do vertex shader devolve um atributo de cor RGB para cada vértice (uma tupla de três valores), então cada fragmento terá também uma cor RGB, com valores obtidos através da interpolação (geralmente linear) dos atributos definidos nos vértices. Fragment shader O fragment shader é um programa que processa cada fragmento individualmente após a rasterização. A entrada do fragment shader é o mesmo conjunto de atributos definidos pelo usuário na saída do vertex shader. É possível acessar também outros atributos pré-definidos que compõem o conjunto de dados de cada fragmento. Por exemplo, a posição do fragmento pode ser acessada através de uma variável embutida chamada gl_FragCoord. A saída do fragment shader geralmente é uma cor em formato RGBA (uma tupla de quatro valores), mas é possível produzir também mais de uma cor caso o pipeline tenha sido configurado para renderizar simultaneamente em vários buffers de cor. O fragment shader também pode alterar as propriedades do fragmento através de variáveis embutidas. Por exemplo, a profundidade pode ser modificada através de gl_FragDepth. Exemplo A seguir é exibido o código-fonte do fragment shader que acompanha o vertex shader mostrado no exemplo anterior: #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = vec4(fragColor.r, fragColor.r, fragColor.r, 1); } Na primeira linha temos a identificação da versão do shader, que é GLSL ES 3.0. A instrução na linha 3 especifica qual é a precisão numérica padrão para o tipo float neste shader. A precisão pode ser lowp (baixa precisão: 8 bits ou mais), mediump (média precisão: 10 bits ou mais) ou highp (alta precisão: 16 bits ou mais). No vertex shader não precisamos especificar uma precisão pois o padrão já é highp. Essas indicações de precisão são apenas dicas ao driver, e geralmente só produzem alguma diferença quando a plataforma é um dispositivo móvel. Para desktop, e mesmo no navegador rodando em um desktop, a precisão geralmente é highp em todos os casos. Esse fragment shader só tem um atributo de entrada, definido na linha 5 pela variável fragColor. O atributo de entrada é a cor RGBA correspondente ao atributo de saída do vertex shader. A saída do fragment shader também é uma cor RGBA, definida pela variável outColor. A função main é chamada para cada fragmento processado. Para cada chamada, fragColor recebe o atributo do fragmento, que é o atributo de saída do vertex shader, mas interpolado entre os vértices da primitiva. Por exemplo, se a primitiva é um segmento formado por um vértice de cor RGB branca \\((1,1,1)\\) e outro vértice de cor preta \\((0,0,0)\\), o fragmento produzido no ponto médio do segmento terá a cor cinza \\((0.5, 0.5, 0.5)\\). Na linha 10, outColor recebe uma cor RGBA na qual as componentes RGB são uma replicação da componente R da cor de entrada. Isso significa que a cor resultante é um tom de cinza que corresponde à intensidade de vermelho da cor original. Se esse fragment shader e o vertex shader do exemplo anterior fossem utilizados no projeto “Hello, World!” da ABCg (seção 1.5), o triângulo resultante seria igual ao mostrado à direita na figura 4.9. Observe o efeito da mudança de escala da geometria (feita no vertex shader) e modificação das cores (intensidade reduzida pela metade no vertex shader, e conversão para tons de cinza no fragment shader). Figura 4.9: Renderização do triângulo do projeto “Hello, World!” com os shaders originais (esquerda) e shaders utilizados nos exemplos (direita). Operações de fragmentos Após o processamento no fragment shader, cada fragmento passa por uma sequência de testes que podem resultar em seu descarte. Se o fragmento falhar em algum desses testes, ele será ignorado e não contribuirá para a cor do pixel final. O teste de propriedade de pixel (pixel ownership test) verifica se o fragmento corresponde a um pixel do framebuffer que está de fato visível no sistema de janelas. Por exemplo, se uma outra janela estiver sobrepondo a janela do OpenGL, os fragmentos mapeados para a área sobreposta serão descartados. O teste de tesoura (scissor test), quando ativado com glEnable, descarta fragmentos que estão fora de um retângulo definido no espaço da janela pela função glScissor. Por exemplo, usando o código a seguir, o teste de tesoura será ativado e serão descartados todos os fragmentos que estiverem fora do retângulo definido pelas coordenadas \\((50,30)\\) a \\((250,130)\\) pixels no espaço da janela (o pixel de coordenada \\((0,0)\\) corresponde ao canto inferior esquerdo da janela): glEnable(GL_SCISSOR_TEST); glScissor(50, 30, 200, 100); O teste de estêncil (stencil test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação entre um valor de estêncil do fragmento (um número inteiro, geralmente de 8 bits) e o valor de estêncil do buffer de estêncil (stencil buffer), que é um dos buffers do framebuffer. Por exemplo, no código a seguir, glStencilFunc estabelece que o teste deve comparar se o valor de estêncil do fragmento é maior que 5. Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_STENCIL_TEST); glStencilFunc(GL_GREATER, 5, 0xFF) O teste de profundidade (depth test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação do valor de profundidade do fragmento (valor \\(z\\) no espaço da janela) com o valor de profundidade armazenado atualmente no buffer de profundidade (depth buffer). Com o teste de profundidade é possível fazer com que só os fragmentos mais próximos sejam exibidos. Por exemplo, no código a seguir, glDepthFunc faz com que o teste de profundidade compare se o valor de profundidade do fragmento é menor que o valor do buffer de profundidade (GL_LESS é a comparação padrão). Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LESS); Muitos desses testes podem ser realizados antes do fragment shader, em uma otimização chamada de early per-fragment test, suportada pela maioria das GPUs atuais. Por exemplo, se o fragment shader não modificar gl_FragDepth, é possível fazer o teste de profundidade logo após a rasterização, evitando o processamento de um fragmento que já se sabe que não contribuirá para a formação da imagem. Se o fragmento passou por todos os testes e não foi descartado, sua cor será utilizada para modificar o pixel correspondente no(s) buffer(s) de cor. Mesmo que o fragmento não tenha passado por todos os testes, é possível que o buffer de estêncil e buffer de profundidade sejam modificados. Esse comportamento pode ser determinado pela aplicação. Também é possível usar operações de mascaramento para permitir, por exemplo, que somente as componentes RG da cor RGB sejam escritas no buffer de cor. Color blending Antes do buffer de cor ser modificado, é possível fazer com que a cor do fragmento que está sendo renderizado (chamada de cor de origem) seja misturada com a cor atual do buffer de cor (chamada de cor de destino), em uma operação de mistura de cor (color blending). Por exemplo, considere o código a seguir: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND) habilita o modo de mistura de cor. As funções glBlendEquation e glBlendFunc configuram a mistura de cor para que cada nova componente de cor (R, G, B, e A) do buffer de cor seja calculada como \\(C=C_sF_s + C_dF_d\\), onde: \\(C_s\\) é a cor de origem (source color): \\(C_s=[R_s, G_s, B_s, A_s]\\); \\(C_d\\) é a cor de destino (destination color): \\(C_d=[R_d, G_d, B_d, A_d]\\); \\(F_s\\) (primeiro parâmetro de glBlendFunc) é o fator de mistura da cor de origem: para GL_SRC_ALPHA, \\(F_s=A_s\\); \\(F_d\\) (segundo parâmetro de glBlendFunc) é o fator de mistura da cor de destino: para GL_ONE_MINUS_SRC_ALPHA, \\(F_d=1-A_s\\); O resultado para glBlendEquation(GL_FUNC_ADD) e glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA) é \\(C=C_sA_s + C_d(1-A_s)\\), isto é, uma interpolação linear entre a cor de origem (\\(C_s\\)) e cor de destino (\\(C_d\\)), usando a componente A de origem (\\(A_s\\)) como parâmetro de interpolação. Se \\(A_s=0\\), mantém-se a cor atual do buffer de cor. Se \\(A_s=1\\), o buffer de cor é substituído pela cor de origem. Se \\(A_s=0.5\\), a nova cor é uma média entre a cor de destino e a cor de origem. Outras equações de combinação de cores podem ser obtidas usando os seguintes argumentos com glBlendEquation: Argumento Equação de combinação GL_FUNC_ADD \\(C=C_sF_s + C_dF_d\\) GL_FUNC_SUBTRACT \\(C=C_sF_s - C_dF_d\\) GL_FUNC_REVERSE_SUBTRACT \\(C=C_dF_d - C_sF_s\\) GL_MIN \\(C=\\min(C_sF_s, C_dF_d)\\) GL_MAX \\(C=\\max(C_sF_s, C_dF_d)\\) Em glBlendFunc podem ser usados os seguintes argumentos, tanto para \\(F_s\\) quanto \\(F_d\\): Argumento \\(F_s\\) ou \\(F_d\\) GL_ZERO \\((0,0,0,0)\\) GL_ONE \\((1,1,1,1)\\) GL_SRC_COLOR \\((R_s, G_s, B_s, A_s)\\) GL_ONE_MINUS_SRC_COLOR \\((1,1,1,1) - (R_s, G_s, B_s, A_s)\\) GL_DST_COLOR \\((R_d, G_d, B_d, A_d)\\) GL_ONE_MINUS_DST_COLOR \\((1,1,1,1) - (R_d, G_d, B_d, A_d)\\) GL_SRC_ALPHA \\((A_s, A_s, A_s, A_s)\\) GL_ONE_MINUS_SRC_ALPHA \\((1,1,1,1) - (A_s, A_s, A_s, A_s)\\) GL_DST_ALPHA \\((A_d, A_d, A_d, A_d)\\) GL_ONE_MINUS_DST_ALPHA \\((1,1,1,1) - (A_d, A_d, A_d, A_d)\\) GL_CONSTANT_COLOR \\((R_c, G_c, B_c, A_c)\\) GL_ONE_MINUS_CONSTANT_COLOR \\((1,1,1,1) - (R_c, G_c, B_c, A_c)\\) GL_CONSTANT_ALPHA \\((A_c, A_c, A_c, A_c)\\) GL_ONE_MINUS_CONSTANT_ALPHA \\((1,1,1,1) - (A_c, A_c, A_c, A_c)\\) GL_SRC_ALPHA_SATURATE \\((i,i,i,1)\\), onde \\(i=\\min(A_s, 1-A_d)\\) Nessa tabela, \\((R_c, G_c, B_c, A_c)\\) é uma cor que pode ser especificada com glBlendColor (o padrão é \\((0,0,0,0)\\)). O tamanho do ponto pode ser definido através da função glPointSize (suportada apenas no OpenGL desktop) ou pela variável built-in gl_PointSize no vertex shader (forma recomendada, compatível com OpenGL, OpenGL ES e WebGL).↩︎ Neste exemplo, o nome das variáveis de entrada também começa com o prefixo in, mas isso é só uma convenção.↩︎ A posição de cada fragmento também inclui o valor recíproco da coordenada \\(w\\) no espaço de recorte.↩︎ "],["coloredtriangles.html", "4.4 Triângulos coloridos", " 4.4 Triângulos coloridos Na seção 3.5, renderizamos pontos (GL_POINTS) para gerar o Triângulo de Sierpinski. Neste projeto, desenharemos triângulos (GL_TRIANGLES). Para cada quadro de exibição, renderizaremos um triângulo colorido com coordenadas 2D aleatórias dentro do viewport que ocupa toda a janela da aplicação. O resultado ficará como a seguir: Durante o desenvolvimento desta atividade veremos com mais detalhes os comandos do OpenGL utilizados para especificar os dados gráficos e configurar o pipeline. Configuração inicial Repita a configuração inicial dos projetos anteriores e mude o nome do projeto para coloredtriangles. O arquivo abcg/examples/CMakeLists.txt ficará assim (com a compilação desabilitada para os projetos anteriores): # add_subdirectory(helloworld) # add_subdirectory(firstapp) # add_subdirectory(tictactoe) # add_subdirectory(sierpinski) add_subdirectory(coloredtriangles) O arquivo abcg/examples/coloredtriangles/CMakeLists.txt ficará assim: project(coloredtriangles) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Como nos projetos anteriores, crie os arquivos main.cpp, window.cpp e window.hpp em abcg/examples/coloredtriangles. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é praticamente idêntico ao do projeto sierpinski: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings( {.samples = 2, .doubleBuffering = false}); window.setWindowSettings( {.width = 600, .height = 600, .title = &quot;Colored Triangles&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } window.hpp A definição da classe Window também é parecida com a do projeto anterior: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOPositions{}; GLuint m_VBOColors{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec4, 3&gt; m_colors{{{0.36f, 0.83f, 1.00f, 1}, {0.63f, 0.00f, 0.61f, 1}, {1.00f, 0.69f, 0.30f, 1}}}; void setupModel(); }; #endif No projeto sierpinski utilizamos apenas um VBO (m_VBOVertices). Dessa vez usaremos dois VBOs: um para a posição dos vértices (m_VBOPositions) e outro para as cores (m_VOBColors). O arranjo m_vertexColors contém as cores RGBA que serão copiadas para m_VBOColors. São três cores, uma para cada vértice do triângulo. window.cpp Vamos primeiro incluir o cabeçalho window.hpp e a definição de Window::onCreate: #include &quot;window.hpp&quot; void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } O código é praticamente idêntico ao do projeto anterior. Observe o conteúdo da string em vertexShader. A string é inicializada com um raw string literal. O código do shader é o texto que está entre R\"gl( e )gl\": #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Este vertex shader define dois atributos de entrada: inPosition, que recebe a posição 2D do vértice, e inColor que recebe a cor do vértice em formato RGBA. A saída, fragColor, é também uma cor RGBA. Na função main, a posição \\((x,y)\\) do vértice é repassada sem modificações para gl_Position. A conversão de \\((x,y)\\) em coordenadas cartesianas para \\((x,y,0,1)\\) em coordenadas homogêneas preserva a geometria do triângulo20. A cor do atributo de entrada também é repassada sem modificações para o atributo de saída. Vejamos agora o fragment shader: #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } O fragment shader é mais simples. O atributo de entrada (fragColor) é copiado sem modificações para o atributo de saída (outColor). A compilação e ligação dos shaders é feita pela chamada a abcg::createOpenGLProgram na linha 26. O resultado é m_program, um número inteiro que identifica o programa de shader composto pelo par de vertex/fragment shader. Para ativar o programa no pipeline, devemos chamar glUseProgram(m_program). Para desativá-lo, podemos ativar outro programa (se existir) ou chamar glUseProgram(0). A função Window::onPaint é definida assim: void Window::onPaint() { setupModel(); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Novamente, o código é similar ao utilizado no projeto sierpinski. O comando de renderização, glDrawArrays, dessa vez usa GL_TRIANGLES e 3 vértices, sendo que o índice inicial dos vértices no arranjo é 0. Isso significa que o pipeline desenhará apenas um triângulo. Em Window::onPaintUI, usaremos controles de UI da ImGui para criar uma pequena janela de edição das três cores dos vértices: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { auto const widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportSize.x - widgetSize.x - 5, m_viewportSize.y - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_colors.at(0).x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_colors.at(1).x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_colors.at(2).x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } As funções ImGui::SetNextWindowPos e ImGui::SetNextWindowSize definem a posição e tamanho da janela da ImGui que está prestes a ser criada na linha 70. A janela é inicializada com alguns flags para que ela não possa ser redimensionada (ImGuiWindowFlags_NoResize) e não tenha a barra de título (ImGuiWindowFlags_NoTitleBar). Os controles ImGui::ColorEdit3 também são criados com flags para desabilitar o color picker (ImGuiColorEditFlags_NoPicker) e os tooltips (ImGuiColorEditFlags_NoTooltip), pois eles podem atrapalhar o desenho dos triângulos. A definição de Window::onResize é idêntica à do projeto sierpinski. A definição de Window::onDestroy também é bem semelhante e libera os recursos do pipeline: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onDestroy() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Vamos agora à função Window::setupModel. A definição completa é dada a seguir. Em seguida faremos uma análise detalhada de cada trecho: void Window::setupModel() { abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Create array of random vertex positions std::uniform_real_distribution rd(-1.5f, 1.5f); std::array&lt;glm::vec2, 3&gt; const positions{ {{rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}}}; // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_colors), m_colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } As linhas 96 a 98 liberam os VBOs e o VAO, caso tenham sido criados anteriormente: abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); É importante fazer isso, pois a função Window::setupModel é chamada continuamente em Window::onPaint. Se não liberarmos os recursos, em algum momento eles consumirão toda a memória da GPU e CPU21. As linhas 100 a 105 criam um arranjo com as posições dos vértices. Esse arranjo será copiado para o VBO m_VBOPositions. Também precisamos copiar as cores para m_VBOColors, mas nesse caso não precisamos criar um novo arranjo pois já temos m_colors definido como membro de Window: // Create array of random vertex positions std::uniform_real_distribution rd(-1.5f, 1.5f); std::array&lt;glm::vec2, 3&gt; const positions{ {{rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}}}; As coordenadas das posições dos vértices são valores float pseudoaleatórios no intervalo \\([-1.5, 1.5)\\). Vimos no projeto anterior que, para uma primitiva ser vista no viewport, ela precisa ser especificada entre \\([-1, -1]\\) e \\([1, 1]\\). Logo, nossos triângulos terão partes que ficarão para fora da janela. O pipeline se encarregará de recortar os triângulos e mostrar apenas os fragmentos que estão dentro do viewport. Nas linhas 107 a 119 são criados os VBOs (um para as posições 2D, outro para as cores RGBA): // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_colors), m_colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); glGenBuffers cria o identificador de um objeto de buffer (buffer object). Um objeto de buffer é um arranjo de dados alocado pelo OpenGL, geralmente na memória da GPU. glBindBuffer com o argumento GL_ARRAY_BUFFER vincula o objeto de buffer a um buffer de atributos de vértices. Isso define o objeto de buffer como um objeto de buffer de vértice (VBO). O objeto de buffer pode ser desvinculado com glBindBuffer(0), ou vinculando outro objeto de buffer. glBufferData aloca a memória e inicializa o buffer com o conteúdo copiado de um ponteiro alocado na CPU. O primeiro parâmetro indica o tipo de objeto de buffer utilizado. O segundo parâmetro é o tamanho do buffer em bytes. O terceiro parâmetro é um ponteiro para o primeiro elemento do arranjo contendo os dados que serão copiados. O quarto parâmetro é uma “dica” fornecida ao driver de como o buffer será utilizado. Essas dicas podem ser: GL_STATIC_DRAW significa que o buffer será modificado apenas uma vez, mas utilizado muitas vezes. GL_STREAM_DRAW significa que o buffer será modificado apenas uma vez e utilizado algumas poucas vezes. GL_DYNAMIC_DRAW significa que o buffer será modificado repetidamente, e utilizado também muitas vezes. As modificações do buffer podem ser feitas com comandos como glBufferData, glBufferSubData e glMapBuffer. O sufixo DRAW nesses identificadores significa que os dados são escritos pela aplicação e utilizados em um comando de desenho. Além do DRAW é possível usar o sufixo READ (os dados são escritos pela GPU e lidos pela aplicação) e COPY (os dados são escritos pela GPU e utilizados em um comando de desenho). Após a cópia dos dados com o glBufferData, o arranjo do qual os dados foram copiados não é mais necessário e pode ser destruído. No nosso código, positions é um arranjo alocado na pilha e portanto é liberado no fim do escopo da função. As linhas 121 a 124 usam glGetAttribLocation para obter o número de localização de cada atributo de entrada do vertex shader de m_program: // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; O resultado de positionAttribute será 0, pois o vertex shader define inPosition com layout(location = 0). Da mesma forma, colorAttribute será 1, pois o vertex shader define inColor com layout(location = 1). Poderíamos omitir esse código e escrever os valores 0 e 1 diretamente no código como valores hard-coded, mas é recomendável fazer a consulta da localização com glGetAttribLocation. Agora que sabemos a localização dos atributos inPosition e inColor no vertex shader, podemos especificar como os dados de cada VBO serão mapeados para esses atributos. Isso é feito nas linhas 126 a 145 a seguir: // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); Na linha 127, glGenVertexArray cria um VAO que, como vimos no projeto sierpinski, armazena o estado da especificação de vinculação dos VBOs com o vertex shader. Neste projeto, essa especificação é feita nas linhas 132 a 142. Em Window::onPaint, quando vinculamos esse VAO com glBindVertexArray, o estado da configuração dos VBOs com o programa de shader é recuperado automaticamente (isto é, a configuração feita nas linhas 132 a 142). Na nossa aplicação isso não é incrivelmente útil: as linhas 132 a 142 já são executadas para todo quadro de exibição, pois chamamos Window::setupModel logo antes de glDrawArrays. Mas, em aplicações futuras, chamaremos Window::setupModel apenas uma vez (por exemplo, em Window::onCreate). Geralmente, o modelo geométrico é definido apenas uma vez e nunca mais é alterado (ou é raramente alterado). Nesse caso, o VAO é útil para que não tenhamos de configurar manualmente a ligação dos VBOs com os atributos do vertex shader para todo quadro de exibição. Ajustando a taxa de atualização Neste momento, se compilarmos e executarmos a aplicação, perceberemos que os triângulos coloridos estão sendo gerados muito rapidamente. Isso ocorre porque um novo triângulo é desenhado a cada chamada a Window::onPaint, na maior taxa possível que a GPU consegue suportar. Geralmente isso é muito acima da taxa de atualização do monitor. Uma tentativa de solucionar isso é habilitar o modo Vsync (sincronização vertical) através da seguinte modificação em main.cpp: window.setOpenGLSettings( {.samples = 2, .vSync = true, .doubleBuffering = false}); Com a sincronização vertical ativada, as chamadas de Window::onPaint serão sincronizadas com a taxa de atualização do monitor, geralmente 60 Hz. O problema é que essa solução muito provavelmente não funcionará em nossa aplicação. Muitos drivers de vídeo só permitem habilitar o modo Vsync quando o double buffering está habilitado, o que não é nosso caso. Precisamos de outra estratégia para ajustar a velocidade da geração de novos triângulos. Uma solução é usar um temporizador e desenhar um novo triângulo apenas se um intervalo de tempo tiver decorrido desde o desenho do último triângulo. É isso que faremos a seguir. Adicione abcg::Timer m_timer como variável membro de Window. abcg::Timer é uma classe da ABCg que implementa um temporizador usando std::chrono, da biblioteca padrão do C++. Quando o objeto m_timer é criado, o horário de sua inicialização é armazenado internamente. Podemos chamar m_timer.elapsed() sempre que quisermos saber quantos segundos se passaram desde o início do temporizador. Podemos também chamar m_timer.restart() para reiniciar o temporizador. Modifique Window::onPaint com o seguinte código: void Window::onPaint() { // Check whether to render the next triangle if (m_timer.elapsed() &lt; 1.0 / 20) return; m_timer.restart(); setupModel(); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 41, verificamos se o temporizador já passou de 0.05 segundos (1.0 / 20). Se ainda não, precisamos aguardar mais algum tempo antes de desenhar um novo triângulo, e por isso retornamos na linha 42. Em algum momento, depois de algumas chamadas de Window::onPaint, o temporizador terá passado 0.05 segundos. Neste caso, reiniciamos o temporizador na linha 43 e continuamos com a execução das instruções do restante do código para desenhar um novo triângulo. Desse modo, a taxa de geração de triângulos será fixada em 20 por segundo. Execute o código novamente e veja o resultado. O projeto completo pode ser baixado deste link. Exercício Habilite o modo de mistura de cor usando o código mostrado na seção 4.3. Inclua o código a seguir em Window::onCreate: abcg::glEnable(GL_BLEND); abcg::glBlendEquation(GL_FUNC_ADD); abcg::glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); Além disso, mude a componente A (alpha) das cores RGBA de m_colors. Por exemplo, com a definição a seguir, os triângulos ficarão 50% transparentes: std::array&lt;glm::vec4, 3&gt; m_colors{{{0.36f, 0.83f, 1.00f, 0.5f}, {0.63f, 0.00f, 0.61f, 0.5f}, {1.00f, 0.69f, 0.30f, 0.5f}}}; Exercício Modifique o projeto coloredtriangles para suportar novas funcionalidades: Geração de cores aleatórias nos vértices; Possibilidade de desenhar cada triângulo com uma cor sólida; Ajuste do intervalo de tempo entre a renderização de cada triângulo. Um exemplo é dado a seguir: O conceito de coordenadas homogêneas será abordado futuramente, quando trabalharmos com transformações geométricas 3D.↩︎ Em geral, destruir e criar os VBOs a cada quadro de exibição não é muito eficiente. É preferível criar o VBO apenas uma vez e, se necessário, modificá-lo com glBufferData a cada quadro. Em nossa aplicação, optamos por chamar setupModel a cada Window::onPaint apenas para manter o código mais simples.↩︎ "],["game.html", "5 Desenvolvendo um jogo 2D", " 5 Desenvolvendo um jogo 2D Neste capítulo, usaremos o pipeline gráfico do OpenGL e funções das bibliotecas auxiliares da ABCg (GLM, ImGui, SDL) para desenvolver um jogo com gráficos 2D. Veremos como especificar a geometria de objetos de cena usando diferentes formatos de Vertex Buffer Objects (VBOs), como trabalhar com mais de um par de vertex/fragment shaders, e como usar variáveis uniformes (uniform variables) para modificar a cor, posição e orientação dos objetos sem precisar reconstruir os VBOs. Também trabalharemos com aspectos não relacionados ao pipeline, mas que são necessários para dar animação e interação ao jogo: Como responder a eventos do mouse e teclado; Como calcular a colisão entre os objetos do jogo; Como usar temporizadores para animar os objetos. O resultado será um jogo estilo Asteroids, como mostrado a seguir. Para simplificar, o jogo não terá efeitos sonoros22. A nave pode ser controlada pelo teclado ou mouse: A orientação é ajustada pela posição do mouse, setas para os lados ou teclas A e D; A aceleração é ativada com o botão direito do mouse, seta para cima ou tecla W; Os tiros são disparados com o botão esquerdo do mouse ou barra de espaço. Observação Como a página do jogo está embutida neste livro online, só é possível controlar a nave pelo mouse. Para a aplicação ter o foco do teclado, abra o link original. Antes de começar a desenvolver o jogo, acompanharemos na seção 5.1 o passo a passo de desenvolvimento de um projeto mais simples, que desenha polígonos regulares coloridos em posições aleatórias da janela. O projeto explora conceitos que serão aplicados no jogo. Em particular, os polígonos regulares servirão de base para criarmos os asteroides e os tiros da espaçonave. Também usaremos temporizadores e variáveis uniformes de forma muito parecida com o que veremos nesse primeiro projeto. É possível incluir sons usando as funções de áudio da SDL, mas neste curso não utilizaremos tais funcionalidades.↩︎ "],["regularpolygons.html", "5.1 Polígonos regulares", " 5.1 Polígonos regulares Este projeto é um aprimoramento do projeto coloredtriangles da seção 4.4. No lugar de desenharmos triângulos (GL_TRIANGLES), desenharemos polígonos regulares 2D formados por leques de triângulos (GL_TRIANGLE_FAN). Para cada quadro de exibição, será renderizado um polígono regular colorido em uma posição aleatória do viewport. O número de lados de cada polígono também será escolhido aleatoriamente. A aplicação ficará como a seguir: Configuração inicial A configuração inicial é a mesma dos projetos anteriores. Apenas mude o nome do projeto para regularpolygons e inclua a linha add_subdirectory(regularpolygons) em abcg/examples/CMakeLists.txt. O arquivo abcg/examples/regularpolygons/CMakeLists.txt ficará assim: project(regularpolygons) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Este projeto também terá os arquivos main.cpp, window.cpp e window.hpp. main.cpp O conteúdo aqui é praticamente idêntico ao do projeto coloredtriangles. Continuaremos com o double buffering desabilitado de modo a desenhar cada novo polígono sobre o conteúdo anterior do framebuffer. #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings({.samples = 2, .doubleBuffering = false}); window.setWindowSettings({ .width = 600, .height = 600, .title = &quot;Regular Polygons&quot;, }); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } window.hpp Aqui também há poucas mudanças em relação ao projeto anterior: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOPositions{}; GLuint m_VBOColors{}; GLuint m_program{}; std::default_random_engine m_randomEngine; abcg::Timer m_timer; int m_delay{200}; void setupModel(int sides); }; #endif Observe que há novamente dois VBOs: um para a posição e outro para a cor dos vértices (linhas 20 e 21). Na linha 27, a variável m_delay é utilizada para especificar o intervalo de tempo, em milissegundos, entre a renderização dos polígonos. window.cpp A definição de Window::onCreate é a mesma do projeto anterior, mas com o conteúdo do vertex shader modificado: #include &quot;window.hpp&quot; void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); } No projeto coloredtriangles, o vertex shader estava assim: #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Neste projeto o código está assim: #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } A principal mudança é o uso de duas variáveis uniformes, identificadas pela palavra-chave uniform. São elas: translation: um fator de translação (deslocamento) da geometria; scale: um fator de escala da geometria. O conteúdo de translation e scale é definido em onPaint antes da chamada do comando de renderização. Lembre-se que variáveis uniformes são variáveis globais do shader que não mudam de valor de um vértice para outro, ao contrário do que ocorre com as variáveis inPosition e inColor. Observe o conteúdo de main: void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } A posição original do vértice (inPosition) é multiplicada por scale e somada com translation para gerar uma nova posição (newPosition), que é a posição final do vértice passada para gl_Position. Na expressão inPosition * scale + translation, inPosition * scale resulta na aplicação do fator de escala nas coordenadas \\(x\\) e \\(y\\) do vértice. Como isso é feito para cada vértice da geometria, o resultado será a mudança do tamanho do objeto. Se o fator de escala for 1, não haverá mudança de escala. Se for 0.5, o tamanho do objeto será reduzido pela metade em \\(x\\) e em \\(y\\). Se for 2.0, o tamanho será dobrado em \\(x\\) e em \\(y\\), e assim por diante. O resultado de inPosition * scale é somado com translation. Isso significa que, após a mudança de escala, a geometria será deslocada no plano pelas coordenadas \\((x,y)\\) da translação. Ao aplicar a escala e a translação do vertex shader, podemos usar um mesmo VBO para renderizar o objeto em posições e escalas diferentes, como mostra a figura 5.1: Figura 5.1: Resultado da transformação dos vértices de um triângulo usando diferentes fatores de escala e translação. Observação O uso de variáveis uniformes e transformações geométricas no vertex shader pode reduzir significativamente o consumo de memória dos dados gráficos. Para citar um exemplo, suponha que queremos renderizar uma cena no estilo do jogo Minecraft composta por 100 mil cubos. A estratégia mais ingênua para renderizar essa cena é criar um único VBO contendo os vértices dos 100 mil cubos. Se usarmos GL_TRIANGLES, cada lado do cubo terá de ser renderizado como 2 triângulos, isto é, precisaremos de 6 vértices (3 vértices por triângulo). Como um cubo tem 6 lados, teremos então 36 vértices por cubo. Logo, nosso VBO de 100 mil cubos terá 3600000 vértices23. Ao usar variáveis uniformes, podemos criar um VBO para apenas um cubo e renderizar esse cubo 100 mil vezes, cada um com um fator de escala e translação diferente. No fim, o número de vértices processados será igual, mas o uso de memória terá uma redução de 5 ordens de magnitude! Vamos agora à definição de Window::onPaintGL: void Window::onPaint() { if (m_timer.elapsed() &lt; m_delay / 1000.0) return; m_timer.restart(); // Create a regular polygon with number of sides in the range [3,20] std::uniform_int_distribution intDist(3, 20); auto const sides{intDist(m_randomEngine)}; setupModel(sides); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); // Pick a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution rd1(-1.0f, 1.0f); glm::vec2 const translation{rd1(m_randomEngine), rd1(m_randomEngine)}; auto const translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Pick a random scale factor (1% to 25%) std::uniform_real_distribution rd2(0.01f, 0.25f); auto const scale{rd2(m_randomEngine)}; auto const scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); // Render abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 44, o tempo contado por m_timer é comparado com m_delay. Se o tempo ainda não atingiu m_delay, a função retorna. Caso contrário, o temporizador é reiniciado na linha 46 e a execução continua nas linhas seguintes. Na linha 51, setupModel(sides) é chamada para criar o VBO de um polígono regular de sides lados. O número de lados é escolhido aletoriamente do intervalo \\([3,20]\\). Nas linhas 57 a 68 são definidos os valores das variáveis uniformes do shader: // Pick a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution rd1(-1.0f, 1.0f); glm::vec2 const translation{rd1(m_randomEngine), rd1(m_randomEngine)}; auto const translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Pick a random scale factor (1% to 25%) std::uniform_real_distribution rd2(0.01f, 0.25f); auto const scale{rd2(m_randomEngine)}; auto const scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); Na linha 59, translation contém coordenadas 2D aleatórias no intervalo \\([-1,1)\\). Na linha 66, scale é um fator de escala aleatório no intervalo \\([0.01, 0.25)\\). Nas linhas 60 e 67, translationLocation e scaleLocation contêm os identificadores de localização das variáveis uniformes do shader. Esse valores são obtidos com glGetUniformLocation passando o identificador do programa de shader como primeiro argumento (m_program) e o nome da variável uniforme como segundo argumento. A atribuição dos valores das variáveis uniformes é feita nas linhas 62 e 68. As funções glUniform* têm como primeiro parâmetro a localização da variável uniforme que será modificada, seguida de uma lista de parâmetros que depende do sufixo no fim de glUniform: Em glUniform2fv, 2fv significa que a variável uniforme é um arranjo de tuplas de dois valores do tipo float, isto é, um arranjo de vec2. Nesse caso, o segundo parâmetro é a quantidade de vec2 que deverá ser copiada para a variável uniforme do shader. O argumento é 1 porque translation não é apenas um vec2. O terceiro parâmetro é o endereço do primeiro elemento do conjunto de dados que será copiado. Em glUniform1f, 1f significa que a variável uniforme é apenas um valor do tipo float. Nesse caso, o segundo parâmetro é simplesmente o valor float. Observação O formato geral de glUniform é glUniform{1|2|3|4}{f|i|ui}[v]: {1|2|3|4} define o número de componentes do tipo de dado: 1 para float, int, unsigned int e bool; 2 para vec2, ivec2, uvec2, bvec2; 3 para vec3, ivec3, uvec3, bvec3; 4 para vec4, ivec4, uvec4, bvec4. {f|i|ui} define o tipo de dado de cada componente: f para float, vec2, vec3, vec4; i para int, ivec2, ivec3, ivec4; ui para unsigned int, uvec2, uvec3, uvec4. Tanto f, i e ui podem ser usados para copiar dados para variáveis uniformes booleanas (bool, bvec2, bvec3, bvec4). Nesse caso, true é qualquer valor diferente de zero. Se o v final não é especificado, então {1|2|3|4} é também o número de parâmetros após o identificador de localização. Por exemplo: // Variável uniform é um float ou bool glUniform1f(loc, 3.14f); // Variável uniform é um unsigned int ou bool glUniform1ui(loc, 42); // Variável uniform é um vec2 ou bvec2 glUniform2f(loc, 0.0f, 10.5f); // Variável uniform é um ivec4 ou bvec4 glUniform4i(loc, -1, 2, 10, 3); Se o v é especificado, o segundo parâmetro é o número de elementos do arranjo, e o terceiro parâmetro é o ponteiro para os dados. Por exemplo: // Variável uniform é um float ou bool float pi{3.14f}; glUniform1fv(loc, 1, &amp;pi); // Variável uniform é um unsigned int ou bool unsigned int answer{42}; glUniform1uiv(loc, 1, &amp;answer); // Variável uniform é um vec2 ou bvec2 glm::vec2 foo{0.0f, 10.5f}; glUniform2fv(loc, 1, &amp;foo.x); // Variável uniform é um ivec4[2] ou bvec4[2] std::array bar{glm::ivec4{-1, 2, 10, 3}, glm::ivec4{7, -5, 1, 90}}; glUniform4iv(loc, 2, &amp;bar.at(0).x); Nas linhas 71 a 73 temos a chamada ao comando de renderização: // Render abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); O VAO é vinculado na linha 71. O resultado é a ativação e configuração da ligação dos VBOs com o programa de shader. O comando de renderização é chamado na linha 72. Observe o uso da constante GL_TRIANGLE_FAN. O número de vértices é sides + 2 porque definiremos nossos polígonos de tal modo que o número de vértices será sempre o número de lados mais dois, como mostra a figura 5.2 para a definição de um pentágono: Figura 5.2: Pentágono formado por um leque de sete vértices. No pentágono, o vértice de índice 6 tem a mesma posição do vértice de índice 1 para “fechar” o leque de triângulos. Na verdade, o leque poderia definir um pentágono com apenas cinco vértices, como mostra a figura 5.3: Figura 5.3: Pentágono formado por um leque de cinco vértices. A escolha de manter o vértice de índice 0 no centro é proposital pois permite simular um efeito de gradiente de cor parecido com um gradiente radial. Para isto, basta atribuir uma cor ao vértice 0, e outra cor aos demais vértices. Como os atributos dos vértices são interpolados linearmente pelo rasterizador para cada fragmento gerado, o resultado será um degradê de cor. A figura 5.4 mostra um exemplo usando amarelo no vértice central e azul nos demais vértices: Figura 5.4: Pentágono com gradiente de cor formado através da interpolação do atributo de cor dos vértices. Continuando com a definição das funções membro da classe Window, definiremos Window::onPaintUI usando o código a seguir. O código é bem parecido com o do projeto anterior. A diferença é que, no lugar de ImGui::ColorEdit3, criaremos um slider para controlar o valor de m_delay e criaremos um botão para limpar a janela: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { auto const widgetSize{ImVec2(200, 72)}; ImGui::SetNextWindowPos(ImVec2(m_viewportSize.x - widgetSize.x - 5, m_viewportSize.y - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto const windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); ImGui::PushItemWidth(140); ImGui::SliderInt(&quot;Delay&quot;, &amp;m_delay, 0, 200, &quot;%d ms&quot;); ImGui::PopItemWidth(); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(-1, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } A definição de Window::onResize e Window::onDestroy é idêntica à do projeto anterior: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onDestroy() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Vamos agora à definição da função Window::setupModel. O código completo é mostrado abaixo, mas analisaremos cada trecho em seguida: void Window::setupModel(int sides) { // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Select random colors for the radial gradient std::uniform_real_distribution rd(0.0f, 1.0f); glm::vec3 const color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; glm::vec3 const color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions; std::vector&lt;glm::vec3&gt; colors; // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices auto const step{M_PI * 2 / sides}; for (auto const angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec3), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } No início da função, os VBOs e o VAO são liberados caso tenham sido criados anteriormente: // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); Em seguida, temos o código que cria os vértices do polígono regular (arranjos positions e colors): // Select random colors for the radial gradient std::uniform_real_distribution rd(0.0f, 1.0f); glm::vec3 const color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; glm::vec3 const color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions; std::vector&lt;glm::vec3&gt; colors; // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices auto const step{M_PI * 2 / sides}; for (auto const angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); Duas cores RGB são sorteadas nas linhas 124 e 126. color1 é utilizada na definição do vértice do centro (linhas 137), e color2 é utilizada para os demais vértices (linha 143). Nas linhas 140 a 144, a posição dos vértices é calculada usando a equação paramétrica de um círculo unitário na origem: \\[ \\begin{eqnarray} x&amp;=&amp;cos(t),\\\\ y&amp;=&amp;sin(t), \\end{eqnarray} \\] onde \\(t\\) é o ângulo (angle) que varia de \\(0\\) a \\(2\\pi\\) usando um tamanho do passo (step) igual à divisão de \\(2\\pi\\) pelo número de lados do polígono. A definição dos VBOs é semelhante à forma utilizada no projeto anterior. Nas linhas 175 a 185 é definido como os dados dos VBOs serão mapeados para a entrada do vertex shader. Vamos nos concentrar na definição do mapeamento de m_VBOPositions (o mapeamento de m_VBOColors é similar): abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); Na linha 175, glEnableVertexAttribArray habilita o atributo de posição do vértice (inPosition) para ser utilizado durante a renderização. Em seguida, glBindBuffer vincula o VBO m_VBOPositions, que contém os dados das posições dos vértices. Na linha 177, glVertexAttribPointer define como os dados do VBO serão mapeados para o atributo. Lembre-se que o VBO é apenas um arranjo linear de bytes copiados pela função glBufferData. Com glVertexAttribPointer, informamos ao OpenGL como esses bytes devem ser mapeados para uma variável de atributo de entrada do vertex shader. A assinatura de glVertexAttribPointer é a seguinte: void glVertexAttribPointer(GLuint index, GLint size, GLenum type, GLboolean normalized, GLsizei stride, const void * pointer); Os parâmetros são: index: índice do atributo que será modificado. No nosso caso (linha 175) é positionAttribute. size: número de componentes do atributo. No nosso caso é 2 pois inPosition é um vec2, isto é, um atributo de dois componentes. type: tipo de dado de cada valor do VBO. Usamos GL_FLOAT pois cada coordenada \\(x\\) e \\(y\\) do VBO de posições é um float. normalized: flag que indica se valores inteiros devem ser normalizados para \\([-1,1]\\) (para valores com sinal) ou \\([0,1]\\) (para valores sem sinal) quando forem enviados ao atributo. Usamos GL_FALSE porque nossas coordenadas são valores do tipo float; stride: é o número de bytes entre o início do atributo de um vértice e o início do atributo do próximo vértice no arranjo linear. O argumento 0 que utilizamos indica que não há bytes extras entre uma posição \\((x,y)\\) e a posição \\((x,y)\\) do vértice seguinte. pointer: apesar do nome, não é um ponteiro, mas um deslocamento em bytes a partir do início do arranjo linear, e que corresponde à posição do primeiro componente do atributo. Usamos nullptr, que corresponde a zero, pois não há bytes extras no início do VBO antes da primeira posição \\((x,y)\\). Observação Os parâmetros stride e pointer de glVertexAttribPointer podem ser utilizados para especificar o mapeamento de VBOs que contém dados intercalados (interleaved data). Nosso m_VBOPositions não usa dados intercalados. O arranjo contém apenas posições \\((x,y)\\) em sequência. Assim, para um triângulo (três vértices), o VBO representa um arranjo no formato \\[[\\underline{x,\\; y},\\; \\; x,\\; y,\\; \\; x,\\; y],\\] onde cada grupo de \\((x, y)\\) é a posição de um vértice, e tanto \\(x\\) quanto \\(y\\) são do tipo float24. Da mesma forma, m_VBOColors não usa dados intercalados. Para a definição das cores dos vértices de um triângulo, o arranjo tem o formato: \\[[\\underline{r,\\; g,\\; b},\\; \\; r,\\; g,\\; b,\\; \\; r,\\; g,\\; b],\\] onde cada grupo de \\((r,g,b)\\) define a cor de um vértice, e \\(r\\), \\(g\\) e \\(b\\) também são do tipo float. Quando os dados não são intercalados, podemos especificar 0 como argumento de stride, que é o que fizemos. Além disso, pointer também é 0. Suponha agora que os dados tenham sido intercalados em um único VBO no seguinte formato: \\[[\\underline{x,\\; y,\\; r,\\; g,\\; b},\\; \\; x,\\; y,\\; r,\\; g,\\; b,\\; \\; x,\\; y,\\; r,\\; g,\\; b].\\] Agora, o atributo de posição \\((x,y)\\) tem um stride que corresponde à quantidade de bytes contida em \\((x,y,r,g,b)\\). Esse valor é 20 se cada float tiver 4 bytes (5 \\(\\times\\) 4 = 20 bytes). pointer continua sendo 0, pois não há deslocamento no início do arranjo. O atributo de cor \\((r,g,b)\\) também tem um stride de 20 bytes. Entretanto, pointer (deslocamento a partir do início) precisa ser 8, pois \\(x\\) e \\(y\\) ocupam 8 bytes antes do início do primeiro grupo de \\((r,g,b)\\). Suponha agora um único VBO no formato a seguir: \\[[\\underline{x,\\; y},\\; \\; x,\\; y,\\; \\; x,\\; y,\\; \\; \\underline{r,\\; g,\\; b},\\; \\; r,\\; g,\\; b,\\; \\; r,\\; g,\\; b].\\] Nesse VBO, o stride da posição pode ser 0, pois após um grupo de \\((x,y)\\) há imediatamente outro \\((x,y)\\)25. O stride da cor também pode ser 0 pelo mesmo raciocínio. Entretanto, o pointer para o atributo de cor precisa ser 24 (3 vértices \\(\\times\\) 8 bytes por vértices = 24 bytes), pois o primeiro grupo de \\((r,g,b)\\) ocorre apenas depois de três grupos de \\((x,y)\\). Com todas essas opções de formatação de VBOs, não há uma forma mais certa ou mais recomendada de organizar os dados. É possível que algum driver use algum formato de forma mais eficiente, mas isso só pode ser determinado através de medição de tempo. Na prática, use o formato que melhor fizer sentido para o caso de uso. Para simplificar, fizemos as contas supondo 4 bytes por float, mas lembre-se sempre de usar sizeof(float) pois o tamanho de um float pode variar dependendo da arquitetura. O código completo de window.cpp é mostrado a seguir: #include &quot;window.hpp&quot; void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; uniform vec2 translation; uniform float scale; out vec4 fragColor; void main() { vec2 newPosition = inPosition * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = inColor; } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); } void Window::onPaint() { if (m_timer.elapsed() &lt; m_delay / 1000.0) return; m_timer.restart(); // Create a regular polygon with number of sides in the range [3,20] std::uniform_int_distribution intDist(3, 20); auto const sides{intDist(m_randomEngine)}; setupModel(sides); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); // Pick a random xy position from (-1,-1) to (1,1) std::uniform_real_distribution rd1(-1.0f, 1.0f); glm::vec2 const translation{rd1(m_randomEngine), rd1(m_randomEngine)}; auto const translationLocation{ abcg::glGetUniformLocation(m_program, &quot;translation&quot;)}; abcg::glUniform2fv(translationLocation, 1, &amp;translation.x); // Pick a random scale factor (1% to 25%) std::uniform_real_distribution rd2(0.01f, 0.25f); auto const scale{rd2(m_randomEngine)}; auto const scaleLocation{abcg::glGetUniformLocation(m_program, &quot;scale&quot;)}; abcg::glUniform1f(scaleLocation, scale); // Render abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, sides + 2); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { auto const widgetSize{ImVec2(200, 72)}; ImGui::SetNextWindowPos(ImVec2(m_viewportSize.x - widgetSize.x - 5, m_viewportSize.y - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto const windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); ImGui::PushItemWidth(140); ImGui::SliderInt(&quot;Delay&quot;, &amp;m_delay, 0, 200, &quot;%d ms&quot;); ImGui::PopItemWidth(); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(-1, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onDestroy() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } void Window::setupModel(int sides) { // Release previous resources, if any abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Select random colors for the radial gradient std::uniform_real_distribution rd(0.0f, 1.0f); glm::vec3 const color1{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; glm::vec3 const color2{rd(m_randomEngine), rd(m_randomEngine), rd(m_randomEngine)}; // Minimum number of sides is 3 sides = std::max(3, sides); std::vector&lt;glm::vec2&gt; positions; std::vector&lt;glm::vec3&gt; colors; // Polygon center positions.emplace_back(0, 0); colors.push_back(color1); // Border vertices auto const step{M_PI * 2 / sides}; for (auto const angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); colors.push_back(color2); } // Duplicate second vertex positions.push_back(positions.at(1)); colors.push_back(color2); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, colors.size() * sizeof(glm::vec3), colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } O código completo do projeto pode ser baixado deste link. Agora que vimos como usar variáveis uniformes para fazer transformações geométricas no vertex shader e como organizar os dados de um VBO de diferentes maneiras, vamos ao jogo! Como veremos posteriomente, é possível reduzir esse número com o uso de geometria indexada, mas ainda assim o consumo de memória seria alto para este caso.↩︎ Os dados que correspondem ao primeiro vértice são sublinhados para facilitar a visualização. Nos exemplos subsequentes, os dados do primeiro vértice também são sublinhados.↩︎ O stride nesse caso também pode ser 8 bytes (4 bytes para \\(x\\), mais 4 para \\(y\\)), mas o argumento 0 serve para indicar que os atributos estão agrupados de forma “apertada”.↩︎ "],["asteroids.html", "5.2 Asteroids", " 5.2 Asteroids O cenário do jogo Asteroids será composto pelos seguintes objetos: Uma nave espacial, formada por GL_TRIANGLES; Asteroides, formados por GL_TRIANGLE_FAN; Tiros, formados por GL_TRIANGLE_FAN; Estrelas de fundo, formadas por GL_POINTS. Como nas aplicações feitas até agora, trabalharemos somente com gráficos 2D. As coordenadas de todos os objetos do jogo serão especificadas no chamado NDC (espaço normalizado do dispositivo). Como vimos na seção 4.3, para que as primitivas sejam renderizadas, as coordenadas em NDC devem estar dentro do volume de visão canônico, que é um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Também vimos que coordenadas em NDC são mapeadas para o espaço da janela, de modo que o ponto \\((-1,-1)\\) é mapeado para o canto inferior esquerdo do viewport, e \\((1,1)\\) é mapeado para o canto superior direito, de acordo com o especificado em glViewport. A figura 5.5 ilustra o posicionamento de objetos da cena recortados pela região visível do NDC. Figura 5.5: Objetos de cena na região visível do NDC. No jogo Asteroids original, a nave se movimenta pela tela enquanto a câmera virtual permanece fixa. Quando a nave sai dos limites da tela, reaparece no lado oposto. No nosso projeto, a nave se manterá fixa no centro da tela, enquanto todo o resto se moverá ao seu redor. O espaço será finito como no Asteroids original, e terá o tamanho da região que vai de \\((-1,-1)\\) a \\((1,1)\\). Se um asteroide sair do lado esquerdo da tela, reaparecerá no lado direito (observe que isso acontece na figura 5.5). Um truque simples para obter o efeito de replicação do espaço é renderizar a cena nove vezes, uma vez para célula de uma grade 3x3 na qual apenas a célula do meio corresponde à região de \\((-1,-1)\\) a \\((1,1)\\). Isso é ilustrado na figura 5.6: Figura 5.6: Replicando a cena em torno da região visível do NDC. Não é necessário replicar os objetos que não saem da tela, como a nave. No nosso caso, os tiros também não serão replicados e deixarão de existir assim que saírem da tela. Embora esse truque de replicação de cena funcione bem para este jogo simples, em cenas mais complexas é recomendável fazer testes de proximidade para não desenhar os objetos que estão totalmente fora da área visível. Isso evita processamento desnecessário no pipeline gráfico. Organização do projeto Nosso jogo possui vários objetos de cena, e portanto possui potencialmente vários VBOs, VAOs e variáveis de propriedades desses objetos. Precisamos pensar bem em como organizar tudo isso. O código pode ficar bastante confuso se definirmos tudo na classe Window como fizemos nos projetos anteriores. Organização das classes Para organizar melhor o projeto, separaremos os elementos de cena do jogo nas seguintes classes: Ship: classe que representará a nave espacial (VAO, VBO e atributos como translação, orientação e velocidade). StarLayers: classe que gerenciará as camadas de estrelas usadas para fazer o efeito de paralaxe de fundo26. StarLayers conterá um arranjo de objetos do tipo StarLayer, sendo que cada StarLayer definirá o VBO de pontos de uma camada de estrelas. Bullets: classe que gerencia os tiros. A classe terá uma lista de instâncias de uma estrutura Bullet, sendo que cada Bullet representará as propriedades de um tiro (translação, velocidade, etc). Todos os tiros compartilharão um mesmo VBO definido como membro de Bullets. Asteroids: classe que gerenciará os asteroides. Asteroids conterá uma lista de instâncias de uma estrutura Asteroid, sendo que cada Asteroid definirá o VBO e as propriedades de um asteroide. As classes Ship, StarLayers, Bullets e Asteroids terão funções públicas create, paint e destroy que serão chamadas respectivamente nas funções onCreate, onPaint e onDestroy de Window. Definiremos também uma classe GameData para permitir o compartilhamento dos dados de estado do jogo entre Window e as outras classes. Organização dos arquivos O diretório de projeto abcg/examples/asteroids terá a seguinte estrutura: asteroids/ │ asteroids.cpp │ asteroids.hpp │ bullets.cpp │ bullets.hpp │ CMakeLists.txt │ gamedata.hpp │ main.cpp │ window.hpp │ window.cpp │ ship.cpp │ ship.hpp │ starlayers.cpp │ starlayers.hpp │ └───assets/ │ Inconsolata-Medium.ttf │ objects.frag │ objects.vert │ stars.frag └ stars.vert O subdiretório assets terá os seguintes arquivos de recursos utilizados no jogo: Inconsolata-Medium.ttf: fonte Inconsolata utilizada na mensagem “Game Over” e “You Win”. É a mesma fonte que utilizados no projeto tictactoe (seção 2.4). stars.vert e stars.frag: código-fonte do vertex shader e fragment shader utilizados para renderizar as estrelas. objects.vert e objects.frag: código-fonte do vertex shader e fragment shader utilizados em todos os outros objetos: nave, asteroides e tiros. Poderíamos continuar definindo os shaders através de strings, como fizemos até agora, mas o projeto fica mais organizado desta nova forma. Observação Quando o projeto é compilado para WebAssembly, o conteúdo de assets é transformado em um arquivo .data no diretório public. Assim, os arquivos resultantes de um projeto chamado proj são: proj.data: arquivo de recursos (assets); proj.js: arquivo JavaScript que deve ser chamado pelo html; proj.wasm: binário WebAssembly. Configuração inicial Em abcg/examples, crie o subdiretório asteroids. No arquivo abcg/examples/CMakeLists.txt, inclua a linha add_subdirectory(asteroids). Crie o arquivo abcg/examples/asteroids/CMakeLists.txt com o seguinte conteúdo: project(asteroids) add_executable(${PROJECT_NAME} main.cpp window.cpp asteroids.cpp bullets.cpp ship.cpp starlayers.cpp) enable_abcg(${PROJECT_NAME}) Crie todos os arquivos .cpp e .hpp (de asteroids.cpp até starlayers.cpp). Por enquanto eles ficarão vazios. Crie o subdiretório assets e baixe/copie a fonte .ttf. Crie também os arquivos .frag e .vert. Vamos editá-los em seguida. main.cpp Não há nada de realmente novo no conteúdo de main.cpp. Apenas desativaremos o contador de FPS e o botão de tela cheia. O código ficará assim: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings({.samples = 4}); window.setWindowSettings({ .width = 600, .height = 600, .showFPS = false, .showFullscreenButton = false, .title = &quot;Asteroids&quot;, }); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } gamedata.hpp Neste arquivo definiremos uma estrutura GameData que descreve o estado atual do jogo e o estado dos dispositivos de entrada: #ifndef GAMEDATA_HPP_ #define GAMEDATA_HPP_ #include &lt;bitset&gt; enum class Input { Right, Left, Down, Up, Fire }; enum class State { Playing, GameOver, Win }; struct GameData { State m_state{State::Playing}; std::bitset&lt;5&gt; m_input; // [fire, up, down, left, right] }; #endif m_state pode ser: State::Playing: a aplicação está em modo de jogo, com a nave respondendo aos comandos do jogador; State::GameOver: o jogador perdeu. Nesse caso a nave não é exibida e não responde aos comandos do jogador; State::Win: o jogador ganhou. A nave também não é exibida neste estado. m_input é uma máscara de 5 bits setados em resposta a eventos dos dispositivos de entrada. Por exemplo, o bit 0 corresponde a Input::Right e está setado enquanto o usuário pressiona a seta para a direita ou a tecla D. Esse estado é atualizado pela função membro Window::onEvent que veremos adiante. A classe Window manterá um objeto GameData que será compartilhado com outras classes (Ship, Bullets, Asteroids, etc) sempre que elas precisarem ler ou modificar o estado do jogo. objects.vert Esse é o shader utilizado na renderização da nave, asteroides e tiros. O conteúdo será como a seguir: #version 300 es layout(location = 0) in vec2 inPosition; uniform vec4 color; uniform float rotation; uniform float scale; uniform vec2 translation; out vec4 fragColor; void main() { float sinAngle = sin(rotation); float cosAngle = cos(rotation); vec2 rotated = vec2(inPosition.x * cosAngle - inPosition.y * sinAngle, inPosition.x * sinAngle + inPosition.y * cosAngle); vec2 newPosition = rotated * scale + translation; gl_Position = vec4(newPosition, 0, 1); fragColor = color; } Observe que os vértices só possuem um atributo inPosition do tipo vec2. Esse atributo corresponde à posição \\((x,y)\\) do vértice. A saída do vertex shader é uma cor RGBA definida pela variável uniforme color. Isso significa que, usando esse shader, todos os vértices terão a mesma cor. O código de main é similar ao do vertex shader do projeto regularpolygons, mas dessa vez a posição é modificada não apenas por um fator de escala e translação, mas também por uma rotação. As linhas 13 a 16 fazem com que a posição inPosition seja rodada pelo ângulo rotation (em radianos) no sentido anti-horário. O resultado é uma nova posição rotated que é então transformada pela escala e translação (linha 18). Em capítulos futuros, veremos a teoria das transformações geométricas e os passos necessários para se chegar à expressão das linhas 15 e 16. Observação Todos os objetos do jogo são desenhados em tons de cinza, mas não há nada nos shaders que impeça que utilizemos cores. O aspecto preto e branco do jogo é só uma escolha estética para lembrar o antigo Asteroids do arcade. objects.frag O conteúdo desse fragment shader que acompanha objects.vert é o mesmo dos projetos anteriores. A cor de entrada é copiada para a cor de saída: #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } Nave Para simplificar, faremos primeiramente o código para desenhar e animar a nave. Em seguida incluiremos o código das estrelas, asteroides, e por fim os tiros e a detecção de colisões. window.hpp O conteúdo de window.hpp ficará como a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; #include &quot;asteroids.hpp&quot; #include &quot;bullets.hpp&quot; #include &quot;ship.hpp&quot; #include &quot;starlayers.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onEvent(SDL_Event const &amp;event) override; void onCreate() override; void onUpdate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_objectsProgram{}; GameData m_gameData; Ship m_ship; abcg::Timer m_restartWaitTimer; ImFont *m_font{}; std::default_random_engine m_randomEngine; void restart(); }; #endif m_objectsProgram é o identificador do par de shaders objects.vert e objects.frag. m_gameData é a instância de GameData com o estado do jogo e dos dispositivos de entrada. m_ship é a instância da classe Ship que gerencia a nave. m_restartWaitTimer é um temporizador utilizado para fazer com que o jogo seja reiniciado em cinco segundos após o fim de jogo. m_font representa a fonte que será utilizada pela ImGui para escrever “Game Over” e “Win” na tela. Observe que há uma nova função membro onEvent que substitui uma função virtual da classe base. onEvent é chamada pela ABCg sempre que ocorre algum evento da SDL, incluindo os eventos dos dispositivos de entrada. Neste projeto, usaremos onEvent para atualizar a máscara de bits m_input de m_gameData. Na linha 38 há a declaração de uma função membro restart. Essa função será chamada sempre que quisermos reiniciar o estado do jogo. Há também a função membro onUpdate na linha 17. onUpdate é chamada pela ABCg imediatamente antes de cada onPaint. Nessa função incluiremos o código responsável por atualizar a posição dos objetos da cena para torná-la animada. Observação Poderíamos ter colocado o código de onUpdate em onPaint diretamente, mas o programa fica mais organizado fazendo essa separação entre a animação e a renderização. onPaint não tem exatamente a mesma funcionalidade de onUpdate: onPaint não é chamada quando a janela está minimizada, enquanto onUpdate é chamada incondicionalmente. window.cpp Em window.cpp, começaremos com a definição de Window::onEvent: #include &quot;window.hpp&quot; void Window::onEvent(SDL_Event const &amp;event) { // Keyboard events if (event.type == SDL_KEYDOWN) { if (event.key.keysym.sym == SDLK_SPACE) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Fire)); if (event.key.keysym.sym == SDLK_UP || event.key.keysym.sym == SDLK_w) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Up)); if (event.key.keysym.sym == SDLK_DOWN || event.key.keysym.sym == SDLK_s) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Down)); if (event.key.keysym.sym == SDLK_LEFT || event.key.keysym.sym == SDLK_a) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Left)); if (event.key.keysym.sym == SDLK_RIGHT || event.key.keysym.sym == SDLK_d) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Right)); } if (event.type == SDL_KEYUP) { if (event.key.keysym.sym == SDLK_SPACE) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Fire)); if (event.key.keysym.sym == SDLK_UP || event.key.keysym.sym == SDLK_w) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Up)); if (event.key.keysym.sym == SDLK_DOWN || event.key.keysym.sym == SDLK_s) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Down)); if (event.key.keysym.sym == SDLK_LEFT || event.key.keysym.sym == SDLK_a) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Left)); if (event.key.keysym.sym == SDLK_RIGHT || event.key.keysym.sym == SDLK_d) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Right)); } // Mouse events if (event.type == SDL_MOUSEBUTTONDOWN) { if (event.button.button == SDL_BUTTON_LEFT) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Fire)); if (event.button.button == SDL_BUTTON_RIGHT) m_gameData.m_input.set(gsl::narrow&lt;size_t&gt;(Input::Up)); } if (event.type == SDL_MOUSEBUTTONUP) { if (event.button.button == SDL_BUTTON_LEFT) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Fire)); if (event.button.button == SDL_BUTTON_RIGHT) m_gameData.m_input.reset(gsl::narrow&lt;size_t&gt;(Input::Up)); } if (event.type == SDL_MOUSEMOTION) { glm::ivec2 mousePosition; SDL_GetMouseState(&amp;mousePosition.x, &amp;mousePosition.y); glm::vec2 direction{mousePosition.x - m_viewportSize.x / 2, -(mousePosition.y - m_viewportSize.y / 2)}; m_ship.m_rotation = std::atan2(direction.y, direction.x) - M_PI_2; } } Os dados do evento são descritos pelo parâmetro event que é uma estrutura SDL_Event da SDL. Os eventos do teclado são divididos em eventos de pressionamento de teclas (SDL_KEYDOWN), tratados nas linhas 5 a 16, e liberação de teclas (SDL_KEYUP), nas linhas 17 a 28. De acordo com as teclas pressionadas/liberadas, os bits de m_gameData.m_input são setados/resetados. Os eventos do mouse são divididos em eventos de pressionamento (SDL_MOUSEBUTTONDOWN) e liberação (SDL_MOUSEBUTTONUP) dos botões, e eventos de movimentação do mouse (SDL_MOUSEMOTION). Quando ocorre a movimentação do mouse, as coordenadas \\((x,y)\\) do cursor são lidas com SDL_GetMouseState (linha 45) e convertidas para um vetor direction (linhas 47 e 48) definido pelo ponto que sai do centro da janela e vai até a posição do cursor. A coordenada \\(y\\) é invertida na linha 48 porque, no sistema de coordenadas do mouse, o eixo \\(y\\) é positivo para baixo, enquanto que no OpenGL é positivo para cima. Na linha 50, o ângulo de rotação da nave é definido como o ângulo subentendido pelo vetor direction. A função atan2 retorna o ângulo entre o vetor direction e o eixo \\(x\\) positivo. Desse valor é subtraído \\(\\pi/2\\) (90 graus) pois a orientação inicial da nave já é de 90 graus (a nave está “olhando” na direção do eixo \\(y\\) e não na direção do eixo \\(x\\)). Vamos agora à definição de Window::onCreate: void Window::onCreate() { auto const assetsPath{abcg::Application::getAssetsPath()}; // Load a new font auto const filename{assetsPath + &quot;Inconsolata-Medium.ttf&quot;}; m_font = ImGui::GetIO().Fonts-&gt;AddFontFromFileTTF(filename.c_str(), 60.0f); if (m_font == nullptr) { throw abcg::RuntimeError(&quot;Cannot load font file&quot;); } // Create program to render the other objects m_objectsProgram = abcg::createOpenGLProgram({{.source = assetsPath + &quot;objects.vert&quot;, .stage = abcg::ShaderStage::Vertex}, {.source = assetsPath + &quot;objects.frag&quot;, .stage = abcg::ShaderStage::Fragment}}); abcg::glClearColor(0, 0, 0, 1); #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); #endif // Start pseudo-random number generator m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); restart(); } Nas linhas 57 a 62 é carregada a fonte TrueType da pasta assets. O tamanho da fonte é definido como 60.0f na chamada à função AddFontFromFileTTF da ImGui. Internamente, a ImGui renderiza cada letra em uma textura que pode ser utilizada posteriormente em onPaintUI para produzir texto com essa fonte e tamanho. Esses dados ficam armazenados em m_font. Na linha 66 é chamada a função createOpenGLProgram da ABCg para compilar e ligar os shaders objects.vert e objects.frag. O restante do código de Window::onCreate contém funções que já usamos em projetos anteriores. Na linha 81 é chamada a função membro restart que reinicia o estado do jogo. A definição de Window::restart ficará como a seguir: void Window::restart() { m_gameData.m_state = State::Playing; m_ship.create(m_objectsProgram); } A função Window::onUpdate será definida como segue: void Window::onUpdate() { auto const deltaTime{gsl::narrow_cast&lt;float&gt;(getDeltaTime())}; // Wait 5 seconds before restarting if (m_gameData.m_state != State::Playing &amp;&amp; m_restartWaitTimer.elapsed() &gt; 5) { restart(); return; } m_ship.update(m_gameData, deltaTime); } Na linha 91, getDeltaTime é uma função membro de abcg::OpenGLWindow que retorna o tempo que se passou, em segundos, desde a última chamada a onPaint (é o “delta” de tempo entre os quadros de exibição). Na linha 100, esse tempo é passado para a função update da nave, junto com o estado do jogo em m_gameData para atualizar a animação da nave. Importante O valor retornado por abcg::OpenGLWindow::getDeltaTime deve ser utilizado para fazer com que a animação dos objetos seja sincronizada pelo tempo e não pela velocidade do computador. Suponha que queremos animar o deslocamento de um asteroide, de x = 0 para a coordenada x = 10 em dois segundos. Para fazer isso, poderíamos incluir a seguinte expressão em onUpdate: x += (10.0 / 2.0) * getDeltaTime(); Isso faz com que x cresça a uma taxa de 5 unidades por segundo, independentemente da velocidade do computador. Em um computador lento, que renderiza a uma taxa de, digamos, 10 FPS, getDeltaTime retornará 0.1 a cada quadro. Em um computador com GPU mais rápida, que renderiza a uma taxa de 100 FPS, getDeltaTime retornará 0.01 a cada quadro. Nos dois casos, a acumulação de getDeltaTime durante um segundo será exatamante 1. Em resumo, sempre utilize getDeltaTime para atualizar os parâmetros de uma animação. Nunca suponha que a velocidade de renderização será fixa. A definição de Window::onPaint ficará assim: void Window::onPaint() { abcg::glClear(GL_COLOR_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); m_ship.paint(m_gameData); } A nave é renderizada usando a função paint que implementaremos em Ship. Em Window::onPaintUI, mostraremos o texto de “Game Over” e “Win” de acordo com o estado do jogo: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { auto const size{ImVec2(300, 85)}; auto const position{ImVec2((m_viewportSize.x - size.x) / 2.0f, (m_viewportSize.y - size.y) / 2.0f)}; ImGui::SetNextWindowPos(position); ImGui::SetNextWindowSize(size); ImGuiWindowFlags const flags{ImGuiWindowFlags_NoBackground | ImGuiWindowFlags_NoTitleBar | ImGuiWindowFlags_NoInputs}; ImGui::Begin(&quot; &quot;, nullptr, flags); ImGui::PushFont(m_font); if (m_gameData.m_state == State::GameOver) { ImGui::Text(&quot;Game Over!&quot;); } else if (m_gameData.m_state == State::Win) { ImGui::Text(&quot;*You Win!*&quot;); } ImGui::PopFont(); ImGui::End(); } } Nas linhas 119 a 121 definimos os flags da janela da ImGui usando ImGuiWindowFlags_NoBackground para a janela ficar invisível e ImGuiWindowFlags_NoInputs para não receber entrada do teclado. O conteúdo da janela continua visível, de modo que ImGui::Text mostrará o texto sobreposto na tela. Todo o texto exibido entre ImGui::PushFont(m_font) (linha 123) e ImGui::PopFont() (linha 131), usa a fonte m_font, que é a fonte Inconsolata de tamanho 60 criada em Window::onCreate. As funções Window::onResize e Window::onDestroy ficarão como a seguir: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onDestroy() { abcg::glDeleteProgram(m_objectsProgram); m_ship.destroy(); } Não há nada de novo em Window::onResize. É o mesmo código já utilizado em outros projetos. Em Window::onDestroy, m_objectsProgram é liberado e a função destroy de m_ship é chamada para liberar os recursos da nave. ship.hpp A definição da classe Ship ficará como a seguir: #ifndef SHIP_HPP_ #define SHIP_HPP_ #include &quot;abcgOpenGL.hpp&quot; #include &quot;gamedata.hpp&quot; class Ship { public: void create(GLuint program); void paint(GameData const &amp;gameData); void destroy(); void update(GameData const &amp;gameData, float deltaTime); glm::vec4 m_color{1}; float m_rotation{}; float m_scale{0.125f}; glm::vec2 m_translation{}; glm::vec2 m_velocity{}; abcg::Timer m_trailBlinkTimer; abcg::Timer m_bulletCoolDownTimer; private: GLuint m_program{}; GLint m_translationLoc{}; GLint m_colorLoc{}; GLint m_scaleLoc{}; GLint m_rotationLoc{}; GLuint m_VAO{}; GLuint m_VBO{}; GLuint m_EBO{}; }; #endif As propriedades da nave são definidas nas linhas 15 a 19: cor, ângulo de rotação, fator de escala, translação e vetor de velocidade. Os temporizadores definidos nas linhas 21 e 224 são utilizados para controlar o tempo de piscagem dos foguetes da nave em aceleração, e o tempo de espera entre um tiro e outro. As variáveis nas linhas 26 a 29 receberão o resultado de glGetUniformLocation para as variáveis uniformes do vertex shader. Isso será feito em Ship::create. ship.cpp Aqui definiremos Ship::create, Ship::paint, Ship::destroy e Ship::update. A definição de Ship::create no início do arquivo contém o código que cria o VBO e o VAO da nave: #include &quot;ship.hpp&quot; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; #include &lt;glm/gtx/rotate_vector.hpp&gt; void Ship::create(GLuint program) { destroy(); m_program = program; // Get location of uniforms in the program m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); // Reset ship attributes m_rotation = 0.0f; m_translation = glm::vec2(0); m_velocity = glm::vec2(0); // clang-format off std::array positions{ // Ship body glm::vec2{-02.5f, +12.5f}, glm::vec2{-15.5f, +02.5f}, glm::vec2{-15.5f, -12.5f}, glm::vec2{-09.5f, -07.5f}, glm::vec2{-03.5f, -12.5f}, glm::vec2{+03.5f, -12.5f}, glm::vec2{+09.5f, -07.5f}, glm::vec2{+15.5f, -12.5f}, glm::vec2{+15.5f, +02.5f}, glm::vec2{+02.5f, +12.5f}, // Cannon (left) glm::vec2{-12.5f, +10.5f}, glm::vec2{-12.5f, +04.0f}, glm::vec2{-09.5f, +04.0f}, glm::vec2{-09.5f, +10.5f}, // Cannon (right) glm::vec2{+09.5f, +10.5f}, glm::vec2{+09.5f, +04.0f}, glm::vec2{+12.5f, +04.0f}, glm::vec2{+12.5f, +10.5f}, // Thruster trail (left) glm::vec2{-12.0f, -07.5f}, glm::vec2{-09.5f, -18.0f}, glm::vec2{-07.0f, -07.5f}, // Thruster trail (right) glm::vec2{+07.0f, -07.5f}, glm::vec2{+09.5f, -18.0f}, glm::vec2{+12.0f, -07.5f}, }; // Normalize for (auto &amp;position : positions) { position /= glm::vec2{15.5f, 15.5f}; } std::array const indices{0, 1, 3, 1, 2, 3, 0, 3, 4, 0, 4, 5, 9, 0, 5, 9, 5, 6, 9, 6, 8, 8, 6, 7, // Cannons 10, 11, 12, 10, 12, 13, 14, 15, 16, 14, 16, 17, // Thruster trails 18, 19, 20, 21, 22, 23}; // clang-format on // Generate VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate EBO abcg::glGenBuffers(1, &amp;m_EBO); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); // Get location of attributes in the program GLint positionAttribute{abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); // End of binding to current VAO abcg::glBindVertexArray(0); } Observe que destroy é chamada logo no início da função para liberar o VBO e o VAO anterior, caso existam. Nas linhas 12 a 15, a localização das variáveis uniformes é salva para ser utilizada posteriormente na renderização. O arranjo positions (linhas 23 a 48) contém a posição dos vértices da nave. São 24 vértices (de 0 a 23), como mostra a figura 5.7. Esses vértices são ligados para formar 14 triângulos. A imagem foi dividida em duas para facilitar a visualização, pois há sobreposição de triângulos: Os vértices de 0 a 9 definem o corpo da nave; Os vértices de índices 10 a 17 definem os canhões de tiro; Os vértices de 18 a 23 definem dois triângulos que representam o fogo dos foguetes. Esses dois triângulos só serão desenhados quando a nave estiver acelerando. Figura 5.7: Geometria da nave. As coordenadas \\(x\\) em positions estão no intervalo \\([-15.5, 15.5]\\). Nas linhas 51 a 53, esse intervalo é mapeado para \\([-1,1]\\) de modo que a nave fique dentro da região visível do viewport. Após a normalização, a nave já poderá ser vista no viewport, mas ainda estará muito grande. Para chegar ao tamanho final, a escala é ajustada no shader usando m_scale, que é 0.125 (valor definido na linha 17 de ship.h). Observação Poderíamos ter definido positions diretamente com os valores normalizados. Não há um motivo especial para as coordenadas originais da nave variarem de -15.5 a 15.5. Só é assim porque esse foi o tamanho utilizado quando a nave foi modelada pela primeira vez em um software de edição de gráficos vetoriais. Na figura 5.7, todos os vértices ligados por linhas pontilhadas fazem parte de mais de um triângulo. Por exemplo, o vértice de índice 0 é compartilhado por quatro triângulos. Para evitar ter de criar quatro vértices na mesma posição, um para cada triângulo, vamos usar o conceito de geometria indexada. Nas linhas 55 a 70 é definido um arranjo de índices aos vértices. Cada grupo de três índices define um triângulo (compare com a figura 5.7). O VBO que contém os dados de positions é criado nas linhas 74 a 78. Nas linhas 81 a 85 é criado um element buffer object (EBO), que é o buffer de índices do VBO. O EBO armazena os dados de indices. Quando o comando de renderização é chamado com GL_TRIANGLES, cada sequência de três índices do EBO é utilizada para criar um triângulo. No restante do código de create, o VAO é criado para salvar a vinculação do VBO e o mapeamento do VBO para o atributo de entrada do vertex shader. Note que o VAO também salva a vinculação do EBO na linha 102. Quando o VAO for utilizado em paint, toda a configuração entre as linhas 94 a 105 será aplicada ao pipeline. O código de Ship::paint ficará como a seguir: void Ship::paint(const GameData &amp;gameData) { if (gameData.m_state != State::Playing) return; abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glUniform1f(m_scaleLoc, m_scale); abcg::glUniform1f(m_rotationLoc, m_rotation); abcg::glUniform2fv(m_translationLoc, 1, &amp;m_translation.x); // Restart thruster blink timer every 100 ms if (m_trailBlinkTimer.elapsed() &gt; 100.0 / 1000.0) m_trailBlinkTimer.restart(); if (gameData.m_input[static_cast&lt;size_t&gt;(Input::Up)]) { // Show thruster trail during 50 ms if (m_trailBlinkTimer.elapsed() &lt; 50.0 / 1000.0) { abcg::glEnable(GL_BLEND); abcg::glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); // 50% transparent abcg::glUniform4f(m_colorLoc, 1, 1, 1, 0.5f); abcg::glDrawElements(GL_TRIANGLES, 14 * 3, GL_UNSIGNED_INT, nullptr); abcg::glDisable(GL_BLEND); } } abcg::glUniform4fv(m_colorLoc, 1, &amp;m_color.r); abcg::glDrawElements(GL_TRIANGLES, 12 * 3, GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Note que paint retorna imediatamente caso o jogo não esteja no estado State::Playing. A escala, ângulo de rotação e translação da nave são enviadas às variáveis uniformes nas linhas 115 a 117. Na linha 120, o temporizador m_trailBlinkTimer é reiniciado a cada 100 milissegundos. Ele é utilizado quando a nave está acelerando, de modo a mostrar o rastro de fogo do foguete durante 50 milissegundos, intercalado com uma pausa também de 50 milissegundos. Para desenhar o rastro de fogo, todos os 14 triângulos da geometria da nave são exibidos usando transparência de 50% (linhas 125 a 133). Isso é necessário pois não é possível desenhar só os dois triângulos finais. Para isso precisaríamos ter de criar um outro VBO, o que daria mais trabalho. Na linha 138 é renderizado o corpo da nave sem o rastro do foguete, isto é, só são renderizados os primeiros 12 triângulos. Nessa renderização, a nave é renderizada com cor opaca, sobrepondo a renderização anterior da nave 50% transparente. Note que a função de renderização é glDrawElements. Essa função deve ser utilizada no lugar de glDrawArrays sempre que quisermos desenhar geometria indexada. A função Ship::destroy contém apenas a liberação do VBO, EBO e VAO: void Ship::destroy() { abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Na função Ship::update é onde são atualizados os atributos de orientação e velocidade da nave de acordo com o estado de GameData::m_input: void Ship::update(GameData const &amp;gameData, float deltaTime) { // Rotate if (gameData.m_input[gsl::narrow&lt;size_t&gt;(Input::Left)]) m_rotation = glm::wrapAngle(m_rotation + 4.0f * deltaTime); if (gameData.m_input[gsl::narrow&lt;size_t&gt;(Input::Right)]) m_rotation = glm::wrapAngle(m_rotation - 4.0f * deltaTime); // Apply thrust if (gameData.m_input[gsl::narrow&lt;size_t&gt;(Input::Up)] &amp;&amp; gameData.m_state == State::Playing) { // Thrust in the forward vector auto const forward{glm::rotate(glm::vec2{0.0f, 1.0f}, m_rotation)}; m_velocity += forward * deltaTime; } } A rotação (linhas 153 a 156) é feita a uma taxa de 4 radianos por segundo (note o uso do deltaTime). A função glm::wrapAngle é utilizada para fazer com que o valor sempre fique no intervalo circular \\([0,2\\pi]\\). O #include &lt;glm/gtx/fast_trigonometry.hpp&gt; na linha 3 de ship.cpp serve para incluir essa função da biblioteca GLM. A aceleração é atualizada no if da linha 159. Na linha 162 é calculado um vetor que indica a direção para onde a nave está apontando (vetor forward). Isso é obtido aplicando uma rotação no vetor \\((0, 1)\\)27 usando o ângulo atual da nave. Uma vez calculado o vetor forward, ele é adicionado ao vetor de velocidade m_velocity na linha 163. Observe novamente o uso do deltaTime. Como forward é um vetor unitário, a velocidade será incrementada em uma unidade por segundo. Observe que o #include na linha 4 de ship.cpp (#include &lt;glm/gtx/rotate_vector.hpp&gt;) inclui a função glm::rotate da biblioteca GLM. Visualmente, a nave ficará presa no centro do viewport, mas os outros objetos da cena (asteroides, estrelas e tiros) usarão m_velocity (na verdade, -m_velocity) para serem deslocados. Se o código for construído neste momento, o resultado será como mostrado a seguir (use o link original caso queria controlar a nave pelo teclado): O código do projeto pode ser baixado deste link. Estrelas As estrelas serão desenhadas como pontos (GL_POINTS) e usarão os shaders stars.vert e stars.frag definidos a seguir. stars.vert #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec3 inColor; uniform vec2 translation; uniform float pointSize; out vec4 fragColor; void main() { gl_PointSize = pointSize; gl_Position = vec4(inPosition.xy + translation, 0, 1); fragColor = vec4(inColor, 1); } Os atributos de entrada são uma posição \\((x,y)\\) (inPosition) e uma cor RGB (inColor). Em main, a cor de entrada é copiada para o atributo de saída (fragColor) como uma cor RGBA onde A é 1. A posição do ponto é deslocada por translation, e o tamanho do ponto é definido por pointSize. stars.frag #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { float intensity = 1.0 - length(gl_PointCoord - vec2(0.5)) * 2.0; outColor = fragColor * intensity; } O processamento principal deste shader ocorre na definição da variável intensity. Para compreendermos o que está acontecendo, lembre-se primeiro que o tamanho de um ponto (gl_PointSize) é dado em pixels. O ponto é na verdade um quadrado centralizado na posição de cada ponto. O fragment shader explora esse fato para exibir um gradiente radial no quadrado de modo a simular o formato circular de uma estrela. A variável embutida gl_PointCoord contém as coordenadas do fragmento dentro do quadrado. Na configuração padrão, \\((0,0)\\) é o canto superior esquerdo, e \\((1,1)\\) é o canto inferior direito (figura 5.8). Figura 5.8: Quadrado gerado em torno de um ponto de GL_POINTS, e coordenadas de gl_PointCoord dentro do quadrado formado. A expressão length(gl_PointCoord - vec2(0.5)) calcula a distância euclidiana até o centro do quadrado. Na direção em \\(x\\) e \\(y\\), essa distância está no intervalo \\([0,0.5]\\). A distância é convertida em uma intensidade de luz armazenada em intensity, sendo que a intensidade é máxima (1) no centro do quadrado. A cor de saída é multiplicada por essa intensidade. Se o quadrado for branco, o resultado será como o mostrado na figura 5.9). Figura 5.9: Gradiente radial produzido por stars.frag no quadrado de um ponto definido com GL_POINTS. Atualizando window.hpp Para a implementação das estrelas, precisamos definir em Window o identificador dos shaders m_starsProgram e a instância de StarLayers. O código atualizado ficará como a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; #include &quot;asteroids.hpp&quot; #include &quot;bullets.hpp&quot; #include &quot;ship.hpp&quot; #include &quot;starlayers.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onEvent(SDL_Event const &amp;event) override; void onCreate() override; void onUpdate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_starsProgram{}; GLuint m_objectsProgram{}; GameData m_gameData; Ship m_ship; StarLayers m_starLayers; abcg::Timer m_restartWaitTimer; ImFont *m_font{}; std::default_random_engine m_randomEngine; void restart(); }; #endif Atualizando window.cpp Precisamos atualizar também as funções membro de Window: Em Window::onCreate, inclua o seguinte código para compilar os novos shaders: // Create program to render the stars m_starsProgram = abcg::createOpenGLProgram({{.source = assetsPath + &quot;stars.vert&quot;, .stage = abcg::ShaderStage::Vertex}, {.source = assetsPath + &quot;stars.frag&quot;, .stage = abcg::ShaderStage::Fragment}}); Em Window::restart, inclua a chamada a StarLayers::create junto com a chamada a Ship::create feita anteriormente: m_starLayers.create(m_starsProgram, 25); m_ship.create(m_objectsProgram); Em Window::onUpdate, chame a função StarLayers::update depois de Ship::update, assim: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); Em Window::onPaint, chame StarLayers::paint antes de Ship::paint, assim: m_starLayers.paint(); m_ship.paint(m_gameData); Por fim, modifique Window::onDestroy da seguinte forma: void Window::onDestroy() { abcg::glDeleteProgram(m_starsProgram); abcg::glDeleteProgram(m_objectsProgram); m_ship.destroy(); m_starLayers.destroy(); } starlayers.hpp A definição da classe StarLayers ficará assim: #ifndef STARLAYERS_HPP_ #define STARLAYERS_HPP_ #include &lt;array&gt; #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class StarLayers { public: void create(GLuint program, int quantity); void paint(); void destroy(); void update(const Ship &amp;ship, float deltaTime); private: GLuint m_program{}; GLint m_pointSizeLoc{}; GLint m_translationLoc{}; struct StarLayer { GLuint m_VAO{}; GLuint m_VBO{}; float m_pointSize{}; int m_quantity{}; glm::vec2 m_translation{}; }; std::array&lt;StarLayer, 5&gt; m_starLayers; std::default_random_engine m_randomEngine; }; #endif Nas linhas 24 a 31 é definida a estrutura StarLayer. Ela contém o VBO e VAO dos pontos que formam uma “camada” de estrelas. Além disso contém o tamanho (m_pointSize) e quantidade (m_quantity) dos pontos, e um fator de translação (m_translation) utilizado para deslocar todos os pontos da camada (isto é, todos os vértices do VBO). Na linha 33 é definido um arranjo de cinco objetos StarLayer, pois renderizaremos cinco camadas sobrepostas de estrelas. starlayers.cpp O arquivo começa com a definição de StarLayers::create: #include &quot;starlayers.hpp&quot; void StarLayers::create(GLuint program, int quantity) { destroy(); // Initialize pseudorandom number generator and distributions m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); std::uniform_real_distribution distPos(-1.0f, 1.0f); std::uniform_real_distribution distIntensity(0.5f, 1.0f); auto &amp;re{m_randomEngine}; // Shortcut m_program = program; // Get location of uniforms in the program m_pointSizeLoc = abcg::glGetUniformLocation(m_program, &quot;pointSize&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; for (auto &amp;&amp;[index, layer] : iter::enumerate(m_starLayers)) { // Create data for the stars of this layer layer.m_pointSize = 10.0f / (1.0f + index); layer.m_quantity = quantity * (gsl::narrow&lt;int&gt;(index) + 1); layer.m_translation = {}; std::vector&lt;glm::vec3&gt; data; for ([[maybe_unused]] auto _ : iter::range(0, layer.m_quantity)) { data.emplace_back(distPos(re), distPos(re), 0); data.push_back(glm::vec3(distIntensity(re))); } // Generate VBO abcg::glGenBuffers(1, &amp;layer.m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, layer.m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, data.size() * sizeof(glm::vec3), data.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Create VAO abcg::glGenVertexArrays(1, &amp;layer.m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(layer.m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, layer.m_VBO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, sizeof(glm::vec3) * 2, nullptr); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glVertexAttribPointer(colorAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(glm::vec3) * 2, reinterpret_cast&lt;void *&gt;(sizeof(glm::vec3))); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } } O laço da linha 24 itera sobre cada elemento de m_starLayers. A expressão na linha 26 faz com que os pontos tenham tamanho 10 na 1ª camada, 5 na 2ª camada, 2.5 na 3ª camada, e assim sucessivamente. Na linha 27, a quantidade de pontos é dobrada a cada camada. Na linha 30 é criado um arranjo data com dados dos pontos da camada. Os dados ficarão intercalados no formato \\[[\\underline{x,y,0,r,g,b},x,y,0,r,g,b,\\dots],\\] onde \\((x,y,0)\\) é a posição do ponto, e \\((r,g,b)\\) é a cor do ponto. Dentro do laço, as coordenadas \\(x\\) e \\(y\\) são escolhidas de forma aleatória dentro do intervalo \\([-1,1)\\). A cor é um tom de cinza escolhido aleatoriamente do intervalo \\([0.5,1)\\). Os dados de data são copiados para o VBO através de glBufferData na linha 39. Observe nas linhas 49 a 57 como é feito o mapeamento do VBO com os atributos inPosition (do tipo vec2) e inColor (do tipo vec4) do vertex shader. O stride do VBO é sizeof(glm::vec3) * 2 (isto é, dois vec3). Na linha 56, o deslocamento no início do VBO é sizeof(glm::vec3) (isto é, apenas um vec3). A conversão de tipo é necessária porque o parâmetro de deslocamento é do tipo void const * ao invés de um inteiro (é assim por razões históricas). A definição de StarLayers::paint ficará como a seguir: void StarLayers::paint() { abcg::glUseProgram(m_program); abcg::glEnable(GL_BLEND); abcg::glBlendFunc(GL_ONE, GL_ONE); for (auto const &amp;layer : m_starLayers) { abcg::glBindVertexArray(layer.m_VAO); abcg::glUniform1f(m_pointSizeLoc, layer.m_pointSize); for (auto const i : {-2, 0, 2}) { for (auto const j : {-2, 0, 2}) { abcg::glUniform2f(m_translationLoc, layer.m_translation.x + j, layer.m_translation.y + i); abcg::glDrawArrays(GL_POINTS, 0, layer.m_quantity); } } abcg::glBindVertexArray(0); } abcg::glDisable(GL_BLEND); abcg::glUseProgram(0); } Observe que os pontos são desenhados com o modo de mistura de cor habilitado. Na linha 68, a definição da função de mistura com fatores GL_ONE faz com que as cores produzidas pelo fragment shader sejam somadas com as cores atuais do framebuffer. Isso produz um efeito cumulativo de intensidade da luz quando estrelas de camadas diferentes são renderizadas na mesma posição. Os laços aninhados nas linhas 74 e 75 produzem índices i e j que são usados em layer.m_translation para replicar o desenho das estrelas em uma grade 3x3 em torno da região visível do NDC, como vimos no início da seção. Na linha 86, o modo de mistura de cor é desabilitado para não afetar a renderização dos outros objetos de cena que são totalmente opacos. Em StarLayers::destroy são liberados os VBOs e VAOs de todas as instâncias de StarLayer: void StarLayers::destroy() { for (auto &amp;layer : m_starLayers) { abcg::glDeleteBuffers(1, &amp;layer.m_VBO); abcg::glDeleteVertexArrays(1, &amp;layer.m_VAO); } } Vamos agora à definição de StarLayers::update: void StarLayers::update(const Ship &amp;ship, float deltaTime) { for (auto &amp;&amp;[index, layer] : iter::enumerate(m_starLayers)) { auto const layerSpeedScale{1.0f / (index + 2.0f)}; layer.m_translation -= ship.m_velocity * deltaTime * layerSpeedScale; // Wrap-around if (layer.m_translation.x &lt; -1.0f) layer.m_translation.x += 2.0f; if (layer.m_translation.x &gt; +1.0f) layer.m_translation.x -= 2.0f; if (layer.m_translation.y &lt; -1.0f) layer.m_translation.y += 2.0f; if (layer.m_translation.y &gt; +1.0f) layer.m_translation.y -= 2.0f; } } A translação (m_translation) de cada camada é atualizada na linha 101 de acordo com a velocidade da nave. Se a nave está indo para a frente, então a camada de estrelas deve ir para trás: por isso a subtração. A velocidade é multiplicada por um fator de escala layerSpeedScale para fazer com que a primeira camada seja mais rápida que a segunda, e assim sucessivamente para produzir o efeito de paralaxe. Nas linhas 104 a 111 há uma série de condicionais que testam se os pontos saíram dos limites da região visível do NDC. Se saíram, são deslocados para o lado oposto. Nesse momento, o jogo ficará como a seguir (link original): O código pode ser baixado deste link. Asteroides Para incluir a implementação dos asteroides, vamos primeiramente atualizar OpenGLWindow. Atualizando window.hpp Adicione a definição de m_asteroids junto às definições dos outros objetos (m_ship e m_starLayers), assim: Asteroids m_asteroids; Ship m_ship; StarLayers m_starLayers; Atualizando window.cpp Em Window::restart, chame create de m_asteroids junto com a chamada a create dos objetos anteriores: m_starLayers.create(m_starsProgram, 25); m_ship.create(m_objectsProgram); m_asteroids.create(m_objectsProgram, 3); Em Window::onUpdate, chame update de m_asteroids após update de m_ship: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); m_asteroids.update(m_ship, deltaTime); Em Window::onPaint, chame paint de m_asteroids logo após paint de m_starLayers: m_starLayers.paint(); m_asteroids.paint(); m_ship.paint(m_gameData); Em Window::onDestroy, chame destroy de m_asteroids junto com a chamada a destroy dos outros objetos: m_asteroids.destroy(); m_ship.destroy(); m_starLayers.destroy(); Observação A ordem em que a função paint de cada objeto é chamada é importante porque o objeto renderizado por último será desenhado sobre os anteriores que já estão no framebuffer. Essa forma de renderizar os objetos na ordem do mais distante para o mais próximo é chamada de “algoritmo do pintor” pois é similar ao modo como um pintor desenha sobre uma tela: os elementos mais ao fundo são desenhados antes dos elementos mais à frente. asteroids.hpp A definição da classe Asteroids ficará assim: #ifndef ASTEROIDS_HPP_ #define ASTEROIDS_HPP_ #include &lt;list&gt; #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class Asteroids { public: void create(GLuint program, int quantity); void paint(); void destroy(); void update(const Ship &amp;ship, float deltaTime); struct Asteroid { GLuint m_VAO{}; GLuint m_VBO{}; float m_angularVelocity{}; glm::vec4 m_color{1}; bool m_hit{}; int m_polygonSides{}; float m_rotation{}; float m_scale{}; glm::vec2 m_translation{}; glm::vec2 m_velocity{}; }; std::list&lt;Asteroid&gt; m_asteroids; Asteroid makeAsteroid(glm::vec2 translation = {}, float scale = 0.25f); private: GLuint m_program{}; GLint m_colorLoc{}; GLint m_rotationLoc{}; GLint m_translationLoc{}; GLint m_scaleLoc{}; std::default_random_engine m_randomEngine; std::uniform_real_distribution&lt;float&gt; m_randomDist{-1.0f, 1.0f}; }; #endif Entre as linhas 19 a 31 é definida a estrutura Asteroid. Cada asteroide tem seu próprio VAO e VBO. Além disso, Asteroid possui uma velocidade angular, uma cor, número de lados, ângulo de rotação, escala, translação, vetor de velocidade, e um flag m_hit que indica se o asteroide foi acertado por um tiro. Na linha 33 é definida uma lista de objetos Asteroid. O número de elementos dessa lista será modificado de acordo com os asteroides que forem acertados pelos tiros. Cada vez que um asteroide for acertado, ele será retirado da lista. Entretanto, se o asteroide for grande, simularemos que ele foi quebrado em vários pedaços fazendo com asteroides menores sejam inseridos na lista. A função makeAsteroid declarada na linha 35 será utilizada para criar um novo asteroide para ser inserido na lista m_asteroids. Os parâmetros translation e scale permitirão configurar a posição e o fator de escala do novo asteroide. asteroids.cpp O arquivo começa com a definição de Asteroids::create: #include &quot;asteroids.hpp&quot; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; void Asteroids::create(GLuint program, int quantity) { destroy(); m_randomEngine.seed( std::chrono::steady_clock::now().time_since_epoch().count()); m_program = program; // Get location of uniforms in the program m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); // Create asteroids m_asteroids.clear(); m_asteroids.resize(quantity); for (auto &amp;asteroid : m_asteroids) { asteroid = makeAsteroid(); // Make sure the asteroid won&#39;t collide with the ship do { asteroid.m_translation = {m_randomDist(m_randomEngine), m_randomDist(m_randomEngine)}; } while (glm::length(asteroid.m_translation) &lt; 0.5f); } } Na linha 21, a lista de asteroides é iniciada com uma quantidade quantity de objetos do tipo Asteroid. Essa lista é então iterada no laço da linha 23 e o conteúdo de cada asteroide é substituído por Asteroids::makeAsteroid. No laço das linhas 27 a 30 é escolhida uma posição aleatória para o asteroide, mas uma posição longe o suficiente da nave: não queremos que o jogo comece com o asteroide colidindo com a nave! A definição de Asteroids::paint ficará como a seguir: void Asteroids::paint() { abcg::glUseProgram(m_program); for (auto const &amp;asteroid : m_asteroids) { abcg::glBindVertexArray(asteroid.m_VAO); abcg::glUniform4fv(m_colorLoc, 1, &amp;asteroid.m_color.r); abcg::glUniform1f(m_scaleLoc, asteroid.m_scale); abcg::glUniform1f(m_rotationLoc, asteroid.m_rotation); for (auto i : {-2, 0, 2}) { for (auto j : {-2, 0, 2}) { abcg::glUniform2f(m_translationLoc, asteroid.m_translation.x + j, asteroid.m_translation.y + i); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, asteroid.m_polygonSides + 2); } } abcg::glBindVertexArray(0); } abcg::glUseProgram(0); } A lista m_asteroids é iterada e cada asteroide é renderizado 9 vezes (em uma grade 3x3), como fizemos com as estrelas. Em Asteroids::destroy são liberados os VBOs e VAOs dos asteroides: void Asteroids::destroy() { for (auto &amp;asteroid : m_asteroids) { abcg::glDeleteBuffers(1, &amp;asteroid.m_VBO); abcg::glDeleteVertexArrays(1, &amp;asteroid.m_VAO); } } Vamos agora à definição de Asteroids::update: void Asteroids::update(const Ship &amp;ship, float deltaTime) { for (auto &amp;asteroid : m_asteroids) { asteroid.m_translation -= ship.m_velocity * deltaTime; asteroid.m_rotation = glm::wrapAngle( asteroid.m_rotation + asteroid.m_angularVelocity * deltaTime); asteroid.m_translation += asteroid.m_velocity * deltaTime; // Wrap-around if (asteroid.m_translation.x &lt; -1.0f) asteroid.m_translation.x += 2.0f; if (asteroid.m_translation.x &gt; +1.0f) asteroid.m_translation.x -= 2.0f; if (asteroid.m_translation.y &lt; -1.0f) asteroid.m_translation.y += 2.0f; if (asteroid.m_translation.y &gt; +1.0f) asteroid.m_translation.y -= 2.0f; } } Na linha 68, a translação (m_translation) de cada asteroide é modificada pelo vetor de velocidade da nave, como fizemos com as estrelas. Na linha 69, a rotação é atualizada de acordo com a velocidade angular. Na linha 71, a translação do asteroide é modificada novamente, mas agora considerando a velocidade do próprio asteroide. As condicionais das linhas 74 a 80 fazem com que as coordenadas de m_translation permaneçam no intervalo circular de -1 a 1. A definição de Asteroids::makeAsteroid ficará como a seguir: Asteroids::Asteroid Asteroids::makeAsteroid(glm::vec2 translation, float scale) { Asteroid asteroid; auto &amp;re{m_randomEngine}; // Shortcut // Randomly pick the number of sides std::uniform_int_distribution randomSides(6, 20); asteroid.m_polygonSides = randomSides(re); // Get a random color (actually, a grayscale) std::uniform_real_distribution randomIntensity(0.5f, 1.0f); asteroid.m_color = glm::vec4(randomIntensity(re)); asteroid.m_color.a = 1.0f; asteroid.m_rotation = 0.0f; asteroid.m_scale = scale; asteroid.m_translation = translation; // Get a random angular velocity asteroid.m_angularVelocity = m_randomDist(re); // Get a random direction glm::vec2 const direction{m_randomDist(re), m_randomDist(re)}; asteroid.m_velocity = glm::normalize(direction) / 7.0f; // Create geometry data std::vector&lt;glm::vec2&gt; positions{{0, 0}}; auto const step{M_PI * 2 / asteroid.m_polygonSides}; std::uniform_real_distribution randomRadius(0.8f, 1.0f); for (auto const angle : iter::range(0.0, M_PI * 2, step)) { auto const radius{randomRadius(re)}; positions.emplace_back(radius * std::cos(angle), radius * std::sin(angle)); } positions.push_back(positions.at(1)); // Generate VBO abcg::glGenBuffers(1, &amp;asteroid.m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, asteroid.m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;asteroid.m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(asteroid.m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, asteroid.m_VBO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); return asteroid; } Na linha 105 é escolhida uma velocidade angular aleatória no intervalo \\([-1, 1)\\). Essa velocidade será interpretada como sendo em radianos por segundo. Na linha 109 é escolhido um vetor unitário aleatório para definir a velocidade do asteroide. As componentes do vetor são divididas por 7 de modo que cada asteroide inicie com uma velocidade de 1/7 unidades de espaço por segundo. O restante do código cria a geometria do asteroide. O código é bem parecido com o que foi utilizado para criar o polígono regular no projeto regularpolygons. A diferença é que agora usamos a equação paramétrica do círculo com raio \\(r\\) \\[ \\begin{eqnarray} x&amp;=&amp;r cos(t),\\\\ y&amp;=&amp;r sin(t), \\end{eqnarray} \\] e selecionamos um \\(r\\) aleatório do intervalo \\([0.8, 1)\\) para cada vértice do polígono. Nesse momento, o jogo ficará como a seguir (link original): O código pode ser baixado deste link. Tiros e colisões Até agora o jogo ainda não tem detecção de colisão com os asteroides. Faremos isso a seguir. Atualizando window.hpp Adicione a definição de m_bullets junto à definição dos outros objetos: Asteroids m_asteroids; Bullets m_bullets; Ship m_ship; StarLayers m_starLayers; Adicione também a declaração das seguintes funções adicionais de Window: void checkCollisions(); void checkWinCondition(); checkCollisions será utilizada para verificar as colisões; checkWinCondition será utilizada para verificar se o jogador ganhou (isto é, se não há mais asteroides). Atualizando window.cpp Em Window::restart, inclua a chamada à função create de m_bullets junto com create dos objetos anteriores, assim: m_starLayers.create(m_starsProgram, 25); m_ship.create(m_objectsProgram); m_asteroids.create(m_objectsProgram, 3); m_bullets.create(m_objectsProgram); Em Window::onUpdate, chame update de m_bullets em qualquer lugar após a chamada de update de m_ship. Por exemplo: m_ship.update(m_gameData, deltaTime); m_starLayers.update(m_ship, deltaTime); m_asteroids.update(m_ship, deltaTime); m_bullets.update(m_ship, m_gameData, deltaTime); Após esses updates, inclua a seguinte condicional que chama as funções de detecção de colisão e verificação da condição de vitória se o jogo está no estado State::Playing: if (m_gameData.m_state == State::Playing) { checkCollisions(); checkWinCondition(); } Em Window::onPaint, chame paint de m_bullets logo após a chamada de paint de m_asteroids: m_starLayers.paint(); m_asteroids.paint(); m_bullets.paint(); m_ship.paint(m_gameData); Em Window::onDestroy, chame destroy de m_bullets junto com destroy dos outros objetos: m_asteroids.destroy(); m_bullets.destroy(); m_ship.destroy(); m_starLayers.destroy(); Vamos agora definir Window::checkCollisions como a seguir: void Window::checkCollisions() { // Check collision between ship and asteroids for (auto const &amp;asteroid : m_asteroids.m_asteroids) { auto const asteroidTranslation{asteroid.m_translation}; auto const distance{ glm::distance(m_ship.m_translation, asteroidTranslation)}; if (distance &lt; m_ship.m_scale * 0.9f + asteroid.m_scale * 0.85f) { m_gameData.m_state = State::GameOver; m_restartWaitTimer.restart(); } } // Check collision between bullets and asteroids for (auto &amp;bullet : m_bullets.m_bullets) { if (bullet.m_dead) continue; for (auto &amp;asteroid : m_asteroids.m_asteroids) { for (auto const i : {-2, 0, 2}) { for (auto const j : {-2, 0, 2}) { auto const asteroidTranslation{asteroid.m_translation + glm::vec2(i, j)}; auto const distance{ glm::distance(bullet.m_translation, asteroidTranslation)}; if (distance &lt; m_bullets.m_scale + asteroid.m_scale * 0.85f) { asteroid.m_hit = true; bullet.m_dead = true; } } } } // Break asteroids marked as hit for (auto const &amp;asteroid : m_asteroids.m_asteroids) { if (asteroid.m_hit &amp;&amp; asteroid.m_scale &gt; 0.10f) { std::uniform_real_distribution randomDist{-1.0f, 1.0f}; std::generate_n(std::back_inserter(m_asteroids.m_asteroids), 3, [&amp;]() { glm::vec2 const offset{randomDist(m_randomEngine), randomDist(m_randomEngine)}; auto const newScale{asteroid.m_scale * 0.5f}; return m_asteroids.makeAsteroid( asteroid.m_translation + offset * newScale, newScale); }); } } m_asteroids.m_asteroids.remove_if([](auto const &amp;a) { return a.m_hit; }); } } Nas linhas 174 a 184 é feita a detecção de colisão entre a nave e cada asteroide: // Check collision between ship and asteroids for (auto const &amp;asteroid : m_asteroids.m_asteroids) { auto const asteroidTranslation{asteroid.m_translation}; auto const distance{ glm::distance(m_ship.m_translation, asteroidTranslation)}; if (distance &lt; m_ship.m_scale * 0.9f + asteroid.m_scale * 0.85f) { m_gameData.m_state = State::GameOver; m_restartWaitTimer.restart(); } } A detecção de colisão é feita através da comparação da distância euclidiana (glm::distance) entre as coordenadas de translação dos objetos. Essas coordenadas podem ser consideradas como a posição do centro dos objetos na cena (como ilustrado pelos pontos \\(P_s\\) e \\(P_a\\) na figura 5.10: Figura 5.10: Detecção de colisão entre nave e asteroide através da comparação de distância entre círculos. \\(P_s\\) e \\(P_a\\) também podem ser considerados como centros de círculos. O fator de escala de cada objeto corresponde ao raio do círculo (\\(r_s\\) e \\(r_a\\)). Assim, podemos detectar a colisão através de uma simples comparação da distância \\(|P_s-P_a|\\) com a soma dos fatores de escala. Só há colisão se a distância for menor ou igual a \\(r_s+r_a\\). Esse tipo de teste é bem mais simples e eficiente (embora menos preciso) do que comparar a interseção entre os triângulos que formam os objetos. Note, na linha 180, que \\(r_s\\) e \\(r_a\\) são de fato os fatores m_scale de cada objeto, mas multiplicados por 0.9f (para a nave) e 0.85f (para o asteroide). Isso é feito para diminuir um pouco o raio dos círculos e fazer com que exista uma tolerância de sobreposição antes de ocorrer a colisão. Veja, na figura 5.10, que dessa forma os objetos não ficam inscritos nos círculos. Os valores 0.9 e 0.85 foram determinados empiricamente. O laço for da linha 187 itera sobre os tiros. Dentro desse laço há outro (linha 191) que itera sobre os asteroides para verificar se há colisão entre cada tiro e os asteroides: // Check collision between bullets and asteroids for (auto &amp;bullet : m_bullets.m_bullets) { if (bullet.m_dead) continue; for (auto &amp;asteroid : m_asteroids.m_asteroids) { for (auto const i : {-2, 0, 2}) { for (auto const j : {-2, 0, 2}) { auto const asteroidTranslation{asteroid.m_translation + glm::vec2(i, j)}; auto const distance{ glm::distance(bullet.m_translation, asteroidTranslation)}; if (distance &lt; m_bullets.m_scale + asteroid.m_scale * 0.85f) { asteroid.m_hit = true; bullet.m_dead = true; } } } } A verificação da interseção é calculada novamente através da comparação da distância entre círculos. Note que os testes de distância são feitos dentro de laços aninhados parecidos com os que foram utilizados para replicar a renderização dos asteroides na grade 3x3 em torno da região visível do viewport. De fato, o teste de colisão de um tiro com um asteroide precisa considerar essa replicação, pois um asteroide que está saindo à esquerda do viewport pode ser atingido por um tiro no lado oposto, à direita. Se um tiro acertou um asteroide, o m_hit do asteroide e o m_dead do tiro tornam-se true. Observe agora as linhas 207 a 219: // Break asteroids marked as hit for (auto const &amp;asteroid : m_asteroids.m_asteroids) { if (asteroid.m_hit &amp;&amp; asteroid.m_scale &gt; 0.10f) { std::uniform_real_distribution randomDist{-1.0f, 1.0f}; std::generate_n(std::back_inserter(m_asteroids.m_asteroids), 3, [&amp;]() { glm::vec2 const offset{randomDist(m_randomEngine), randomDist(m_randomEngine)}; auto const newScale{asteroid.m_scale * 0.5f}; return m_asteroids.makeAsteroid( asteroid.m_translation + offset * newScale, newScale); }); } } Neste código, os asteroides com m_hit == true são testados para verificar se são suficientemente grandes (m_scale &gt; 0.10f). Se sim, três novos asteroides menores são criados e inseridos na lista m_asteroids.m_asteroids. std::generate é uma função da biblioteca padrão do C++. Ela chama um número de vezes (nesse caso 3 vezes) a expressão lambda passada como último argumento. O valor de retorno da expressão lambda é inserido no fim de m_asteroids.m_asteroids usando a função auxiliar std::back_inserter. Na linha 221, os asteroides que estavam com m_hit == true são removidos da lista usando std::remove_if: m_asteroids.m_asteroids.remove_if( [](const Asteroids::Asteroid &amp;a) { return a.m_hit; }); Isso é tudo para a detecção de colisão. Vamos agora à definição de Window::checkWinCondition, que ficará como a seguir: void Window::checkWinCondition() { if (m_asteroids.m_asteroids.empty()) { m_gameData.m_state = State::Win; m_restartWaitTimer.restart(); } } A vitória ocorre quando a lista de asteroides está vazia. Nesse caso, o estado do jogo é modificado para State::Win e o temporizador m_restartWaitTimer é reiniciado. Como resultado, o jogo será reiniciado após cinco segundos (essa verificação é feita em Window::onUpdate). Enquanto isso, o texto de vitória será exibido em Window::onPaintUI. bullets.hpp A definição da classe Bullets ficará como a seguir: #ifndef BULLETS_HPP_ #define BULLETS_HPP_ #include &lt;list&gt; #include &quot;abcgOpenGL.hpp&quot; #include &quot;gamedata.hpp&quot; #include &quot;ship.hpp&quot; class OpenGLWindow; class Bullets { public: void create(GLuint program); void paint(); void destroy(); void update(Ship &amp;ship, const GameData &amp;gameData, float deltaTime); struct Bullet { bool m_dead{}; glm::vec2 m_translation{}; glm::vec2 m_velocity{}; }; std::list&lt;Bullet&gt; m_bullets; float m_scale{0.015f}; private: GLuint m_program{}; GLint m_colorLoc{}; GLint m_rotationLoc{}; GLint m_translationLoc{}; GLint m_scaleLoc{}; GLuint m_VAO{}; GLuint m_VBO{}; }; #endif Nas linhas 20 a 24 é definida a estrutura Bullet. Observe que o VAO e VBO não está em Bullet, mas em Bullets, pois todos os tiros utilizam o mesmo VBO. Na linha 26 é definida a lista de tiros atualmente na cena. O número de elementos desta lista será alterado de acordo com a quantidade de tiros visíveis. Na linha 28 é definido o fator de escala comum a todos os tiros. bullets.cpp O arquivo começa com a definição de Bullets::create: #include &quot;bullets.hpp&quot; #include &lt;glm/gtx/rotate_vector.hpp&gt; void Bullets::create(GLuint program) { destroy(); m_program = program; // Get location of uniforms in the program m_colorLoc = abcg::glGetUniformLocation(m_program, &quot;color&quot;); m_rotationLoc = abcg::glGetUniformLocation(m_program, &quot;rotation&quot;); m_scaleLoc = abcg::glGetUniformLocation(m_program, &quot;scale&quot;); m_translationLoc = abcg::glGetUniformLocation(m_program, &quot;translation&quot;); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; m_bullets.clear(); // Create geometry data auto const sides{10}; std::vector&lt;glm::vec2&gt; positions{{0, 0}}; auto const step{M_PI * 2 / sides}; for (auto const angle : iter::range(0.0, M_PI * 2, step)) { positions.emplace_back(std::cos(angle), std::sin(angle)); } positions.push_back(positions.at(1)); // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, positions.size() * sizeof(glm::vec2), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } A lista de tiros é inicializada como vazia na linha 20. O restante do código é para criar o VBO que será compartilhado por todos os tiros. O VBO contém vértices de um polígono regular de 10 lados e usa o mesmo código que utilizamos no projeto regularpolygons. A definição de Bullets::paint ficará como a seguir: void Bullets::paint() { abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glUniform4f(m_colorLoc, 1, 1, 1, 1); abcg::glUniform1f(m_rotationLoc, 0); abcg::glUniform1f(m_scaleLoc, m_scale); for (auto const &amp;bullet : m_bullets) { abcg::glUniform2f(m_translationLoc, bullet.m_translation.x, bullet.m_translation.y); abcg::glDrawArrays(GL_TRIANGLE_FAN, 0, 12); } abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Todos os tiros têm a mesma cor (linha 58), ângulo de rotação (linha 59) e fator de escala (linha 60). A lista de tiros é iterada no laço das linhas 62 a 67. Cada tiro é renderizado como um GL_TRIANGLE_FAN. Em Bullets::destroy é liberado o VBO e VAO: void Bullets::destroy() { abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Vamos agora à definição de Bullets::update: void Bullets::update(Ship &amp;ship, const GameData &amp;gameData, float deltaTime) { // Create a pair of bullets if (gameData.m_input[gsl::narrow&lt;size_t&gt;(Input::Fire)] &amp;&amp; gameData.m_state == State::Playing) { // At least 250 ms must have passed since the last bullets if (ship.m_bulletCoolDownTimer.elapsed() &gt; 250.0 / 1000.0) { ship.m_bulletCoolDownTimer.restart(); // Bullets are shot in the direction of the ship&#39;s forward vector auto const forward{glm::rotate(glm::vec2{0.0f, 1.0f}, ship.m_rotation)}; auto const right{glm::rotate(glm::vec2{1.0f, 0.0f}, ship.m_rotation)}; auto const cannonOffset{(11.0f / 15.5f) * ship.m_scale}; auto const bulletSpeed{2.0f}; Bullet bullet{.m_dead = false, .m_translation = ship.m_translation + right * cannonOffset, .m_velocity = ship.m_velocity + forward * bulletSpeed}; m_bullets.push_back(bullet); bullet.m_translation = ship.m_translation - right * cannonOffset; m_bullets.push_back(bullet); // Moves ship in the opposite direction ship.m_velocity -= forward * 0.1f; } } for (auto &amp;bullet : m_bullets) { bullet.m_translation -= ship.m_velocity * deltaTime; bullet.m_translation += bullet.m_velocity * deltaTime; // Kill bullet if it goes off screen if (bullet.m_translation.x &lt; -1.1f) bullet.m_dead = true; if (bullet.m_translation.x &gt; +1.1f) bullet.m_dead = true; if (bullet.m_translation.y &lt; -1.1f) bullet.m_dead = true; if (bullet.m_translation.y &gt; +1.1f) bullet.m_dead = true; } // Remove dead bullets m_bullets.remove_if([](auto const &amp;p) { return p.m_dead; }); } Um par de tiros é criado a cada disparo. O temporizador m_bulletCoolDownTimer é utilizado para fazer com que os disparos ocorram em intervalos de no mínimo 250 milissegundos. Observe, na linha 102, que subtraímos da velocidade da nave o vetor de direção dos tiros. Isso produz um efeito de recuo da nave. Quanto mais tiros são disparados, mais a nave será deslocada para trás. Nas linhas 106 a 119 são atualizadas as coordenadas de translação de cada tiro. Nas linhas 107 e 108, os tiros são atualizados de acordo com a velocidade da nave e a velocidade do próprio tiro. Nas linhas 110 a 118, verificamos se o tiro saiu da tela. Se sim, o flag m_dead torna-se true. A comparação é feita com -1.1f/+1.1f no lugar de -1.0f/+1.0f para ter certeza que todo o polígono do tiro (e não só seu centro) saiu da tela. Na linha 122, todos os tiros com m_dead == true são removidos da lista. Isso é tudo! Eis o jogo completo (link original): O código do projeto completo pode ser baixado deste link. As estrelas das camadas superiores se moverão mais rapidamente do que as estrelas das camadas inferiores, dando a sensação de profundidade do espaço.↩︎ O vetor é \\((0, 1)\\) pois a nave está alinhada ao eixo \\(y\\) positivo em sua orientação original.↩︎ "],["geometry.html", "6 Espaços e geometria", " 6 Espaços e geometria Em computação gráfica, utilizamos escalares, pontos e vetores para representar quantidades, posições e direções. Através da combinação de operações sobre esses elementos podemos representar objetos geométricos e realizar o processamento necessário para a síntese de imagens. Este capítulo faz uma revisão dos conceitos fundamentais relacionados aos espaços abstratos que definem operações sobre pontos e vetores: Espaço vetorial (seção 6.1). Espaço afim (seção 6.2). Espaço euclidiano (seção 6.3). Tais conceitos formarão a base teórica para o uso de transformações geométricas nas atividades dos próximos capítulos. O final do capítulo (seção 6.4) contém uma atividade de prática de programação: veremos como carregar modelos geométricos tridimensionais formados por malhas de triângulos no formato Wavefront OBJ. Veremos também como pré-processar esses modelos de modo a assegurar que eles estejam contidos no volume de visão. "],["vector.html", "6.1 Espaço vetorial", " 6.1 Espaço vetorial Um espaço vetorial contém um conjunto de escalares e um conjunto de vetores. Usaremos letras minúsculas \\((a, b, \\dots)\\) para denotar escalares, e letras em negrito \\((\\mathbf{u}, \\mathbf{v}, \\dots)\\) para denotar vetores. Um escalar é um número real ou complexo que representa uma quantidade ou medida. Um vetor é um objeto abstrato de um conjunto \\(V\\) fechado para as seguintes operações: Adição de vetor com vetor: \\[ \\mathbf{u}+\\mathbf{v}. \\] Essa operação é comutativa e associativa: \\[ \\begin{align} \\mathbf{u}+\\mathbf{v} &amp;= \\mathbf{v}+\\mathbf{u},\\\\ \\mathbf{u}+(\\mathbf{v}+\\mathbf{w}) &amp;= (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}. \\end{align} \\] Podemos definir a subtração de dois vetores através da adição: \\[ \\begin{align} \\mathbf{u}-\\mathbf{v} &amp;= \\mathbf{u}+(-1)\\mathbf{v}. \\end{align} \\] Multiplicação de escalar com vetor: \\[ a \\mathbf{v}. \\] Essa operação é distributiva e associativa: \\[ \\begin{align} a (\\mathbf{u} + \\mathbf{v}) &amp;= a \\mathbf{u} + a \\mathbf{v},\\\\ (a + b) \\mathbf{u} &amp;= a \\mathbf{u} + b \\mathbf{u},\\\\ a(b \\mathbf{u}) &amp;= (a b) \\mathbf{u},\\\\ ab\\mathbf{u} &amp;= ba\\mathbf{u}. \\end{align} \\] No espaço vetorial há um vetor nulo denotado por \\(\\mathbf{0}\\). O vetor nulo é o elemento neutro na adição de vetor com vetor: \\[ \\begin{align} \\mathbf{v}+\\mathbf{0} &amp;= \\mathbf{v}, \\quad \\forall \\mathbf{v} \\in V. \\end{align} \\] Todo vetor possui um elemento inverso aditivo \\((-1)\\mathbf{v}\\), denotado por \\(-\\mathbf{v}\\), de modo que: \\[\\mathbf{v}+(-\\mathbf{v}) = \\mathbf{0}.\\] Além disso, \\[0\\mathbf{v} = \\mathbf{0}.\\] Combinação e independência linear Uma combinação linear de \\(n\\) vetores \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) é um vetor na forma \\[\\mathbf{v}=a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2 + \\cdots + a_n\\mathbf{v}_n.\\] Em uma combinação linear, os vetores \\(\\{\\mathbf{v}_i\\}\\) são linearmente independentes se \\[a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2 + \\cdots + a_n\\mathbf{v}_n = \\mathbf{0}\\] se e somente se \\[a_1 = a_2 = \\cdots = a_n = 0.\\] Dimensão e base A dimensão de um espaço vetorial corresponde ao maior número de vetores linearmente independentes encontrados naquele espaço. Em um espaço vetorial de dimensão \\(n\\), qualquer conjunto de \\(n\\) vetores linearmente independentes forma uma base. Seja \\(\\{\\mathbf{v}_i\\}\\) uma base em \\(V\\). Então, qualquer vetor \\(\\mathbf{v} \\in V\\) pode ser expresso unicamente como uma combinação linear \\[\\mathbf{v}=a_1\\mathbf{v_1} + a_2\\mathbf{v_2} + \\dots + a_n\\mathbf{v_n}.\\] Os escalares \\(\\{a_i\\}\\) formam uma representação de \\(\\mathbf{v}\\) com relação à base \\(\\{\\mathbf{v}_i\\}\\). Vetores geométricos Até agora, definimos vetores apenas como entidades abstratas do espaço vetorial. Em computação gráfica, estamos geralmente interessados em vetores geométricos, isto é, vetores que representam uma direção e uma magnitude (comprimento) e que podem ser representados visualmente como segmentos direcionados em um espaço coordenado de números reais como no \\(\\mathbb{R}^2\\) (figura 6.1). Figura 6.1: Vetores como segmentos direcionados. Nessa representação em que vetor é sinônimo de segmento direcionado, a adição de vetor com vetor (\\(\\mathbf{u}+\\mathbf{v}\\)) pode ser feita graficamente pela chamada “regra do polígono” como mostra a figura 6.2: desenhamos \\(\\mathbf{v}\\) partindo do fim de \\(\\mathbf{u}\\), e obtemos a soma traçando o segmento que parte do início de \\(\\mathbf{u}\\) e vai até o fim de \\(\\mathbf{v}\\). Figura 6.2: Adição entre vetores geométricos. A multiplicação de escalar com vetor também tem uma interpretação geométrica. O comprimento de \\(a\\mathbf{u}\\) (\\(a \\neq 0\\), \\(\\mathbf{u}\\neq\\mathbf{0}\\)) é igual ao comprimento de \\(\\mathbf{u}\\) multiplicado pelo módulo de \\(a\\). Assim, se \\(|a| \\in (0, 1)\\), o vetor diminui de comprimento. Se \\(|a|=1\\), o comprimento se mantém. Se \\(|a|&gt;1\\), o comprimento aumenta. Em todos os casos, se o sinal do escalar é negativo, o vetor muda de sentido. A figura 6.3 ilustra a multiplicação de escalar com vetor para um exemplo com \\(a&gt;1\\) e \\(a=-1\\). Figura 6.3: Multiplicação de escalar com vetor geométrico. Frequentemente, precisaremos usar vetores para representar deslocamentos de um ponto a outro do espaço. Entretanto, no espaço vetorial não existe o conceito de “ponto no espaço”. Sem essa noção de localização, os vetores agrupados no lado esquerdo da figura 6.4 são indistinguíveis dos vetores exibidos no lado direito. Os vetores são os mesmos, pois são caracterizados unicamente por sua direção e comprimento. Figura 6.4: Vetores idênticos no espaço vetorial. Para representarmos um vetor como um deslocamento a partir de um ponto no espaço, precisamos de um espaço afim. Além disso, para podermos usar o conceito de comprimento ou distância, precisamos de um espaço afim que use uma estrutura métrica, como o espaço euclidiano. "],["affine.html", "6.2 Espaço afim", " 6.2 Espaço afim Um espaço afim contém, além do espaço vetorial de mesma dimensão, um conjunto de pontos. Usararemos letras maiúsculas \\((P, Q, \\dots)\\) para denotar pontos. Além das operações com vetores, é definida a operação de diferença entre pontos, que resulta em um vetor: \\[\\mathbf{u}=P-Q.\\] O vetor \\(\\mathbf{u}\\) representa o deslocamento (translação) de \\(Q\\) para \\(P\\) (figura 6.5). Figura 6.5: Vetor como deslocamento de um ponto \\(Q\\) para um ponto \\(P\\). A partir da diferença entre pontos podemos definir a operação de adição de ponto e vetor, que resulta em um ponto deslocado pelo vetor: \\[P=Q+\\mathbf{u}.\\] Combinação afim Com as operações de diferença entre pontos e adição de ponto e vetor, podemos fazer combinações entre pontos para formar novos pontos. Por exemplo, considere a seguinte operação entre os pontos \\(P\\) e \\(Q\\): \\[R=Q+a(P-Q).\\] Se \\(a=0\\), então \\(R=Q\\); Se \\(a=1\\), então \\(R=P\\). Valores de \\(a \\in [0, 1]\\) produzem pontos entre o segmento que conecta \\(P\\) e \\(Q\\) (figura 6.6). As setas representam os diferentes vetores \\(a(P-Q)\\). Figura 6.6: Pontos produzidos com diferentes valores de \\(a\\) em \\(R=Q+a(P-Q)\\). Podemos usar essa expressão para definir uma nova e importante operação básica sobre pontos. Primeiro, note que \\[R=Q+a(P-Q)\\] é equivalente a \\[R=a P+(1-a)Q.\\] Uma combinação afim entre pontos \\(P\\) e \\(Q\\) é definida como \\[R=a P + b Q,\\] onde \\[a + b = 1.\\] De forma mais geral, uma combinação afim entre os pontos \\(P_1, P_2, \\dots, P_n\\) é qualquer soma na forma \\[a_1 P_1 + a_2 P_2 + \\dots + a_n P_n,\\] onde \\[a_1 + a_2 + \\dots + a_n = 1.\\] Se restringirmos os escalares \\(a_i\\) de uma combinação afim para o intervalo \\([0, 1]\\), temos uma combinação convexa. A união de todas as combinações convexas de um conjunto de pontos é chamada de casco convexo ou fecho convexo (figura 6.7). Figura 6.7: Casco convexo de um conjunto de pontos. Se \\(P_1\\), \\(P_2\\) e \\(P_3\\) forem quaisquer três pontos não colineares, então a combinação afim \\[P=a_1 P_1 + a_2 P_2 + a_3 P_3\\] pode ser usada para gerar qualquer ponto no plano que contém \\(P_1\\), \\(P_2\\) e \\(P_3\\). A união de todas as combinações convexas de \\(P_1\\), \\(P_2\\) e \\(P_3\\) é o conjunto de pontos do triângulo \\(\\triangle P_1 P_2 P_3\\). Se \\(a_1, a_2,a_3 \\geq 0\\) e \\(a_1+a_2+a_3=1\\), esses três escalares são chamados de coordenadas baricêntricas (geralmente denotadas por \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\)). Um ponto no triângulo pode ser unicamente determinado pela combinação de quaisquer duas coordenadas baricêntricas. Como ilustrado na figura 6.8, as coordenadas baricêntricas representam razões entre as áreas de subtriângulos e a área do triângulo. No pipeline de renderização, o rasterizador usa coordenadas baricêntricas para calcular a interpolação dos atributos de vértices em relação aos fragmentos amostrados no triângulo. Figura 6.8: Coordenadas baricêntricas de um triângulo. "],["euclidean.html", "6.3 Espaço euclidiano", " 6.3 Espaço euclidiano O espaço euclidiano é um espaço afim que inclui a operação de produto escalar, também chamada de produto interno. O produto escalar produz um escalar a partir de dois vetores. Com isso é possível definir conceitos como distância e ângulo. O espaço euclidiano inclui o espaço vetorial de números reais \\(\\mathbb{R}^n\\). Seus elementos são \\(n\\)-tuplas de números reais \\((a_1, a_2, \\dots, a_n)\\) que denotam coordenadas do sistema cartesiano. A figura 6.9 ilustra a representação de um ponto em coordenadas cartesianas no espaço euclidiano de três dimensões (3D). Figura 6.9: Representação de um ponto no espaço euclidiano 3D. As operações do espaço vetorial são definidas como: \\[ \\begin{align} \\mathbf{u}+\\mathbf{v}&amp;=(u_1 + v_1, u_2 + v_2, \\dots, u_n + v_n),\\\\ a \\mathbf{v}&amp;=(av_1, av_2, \\dots, av_n). \\end{align} \\] As operações afins são definidas como: \\[ \\begin{align} P-Q &amp;= (p_1 - q_1, p_2 - q_2, \\dots, p_n - q_n),\\\\ P+\\mathbf{u} &amp;= (p_1 + u_1, p_2 + u_2, \\dots, p_n + u_n). \\end{align} \\] Quadro de referência Em aplicações de computação gráfica, é comum desejarmos representar pontos e vetores em relação a diferentes quadros de referência. Por exemplo, em uma cena tridimensional que representa uma sala de aula, a geometria da sala pode ser definida em relação a uma base com eixos ortogonais nas direções da largura, comprimento e altura da sala, além de um ponto de referência (origem) localizado no centro da sala. Entretanto, cada mesa e cada cadeira podem ter seu próprio referencial local, com origem e eixos diferentes do quadro global. Esse uso de diferentes quadros de referência é conveniente para posicionarmos e orientarmos os objetos da cena de forma intuitiva em relação a outros objetos e diferentes pontos de vista. Um quadro de referência de um espaço euclidiano de \\(n\\) dimensões é composto por: Um ponto \\(P_0\\) que representa a origem do quadro; Uma base composta por \\(n\\) vetores linearmente independentes \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\). O quadro padrão é o quadro composto por \\[P_0=(0, \\dots, 0),\\] e a base formada pelo conjunto de tuplas \\[ \\begin{align} \\mathbf{e}_1 &amp;= (1, 0, 0, \\dots, 0),\\\\ \\mathbf{e}_2 &amp;= (0, 1, 0, \\dots, 0),\\\\ &amp;\\;\\;\\vdots\\\\ \\mathbf{e}_n &amp;= (0, 0, 0, \\dots, 1),\\\\ \\end{align} \\] No \\(\\mathbb{R}^3\\), essa base é denotada por vetores \\(\\hat{\\mathbf{i}}\\), \\(\\hat{\\mathbf{j}}\\), \\(\\hat{\\mathbf{k}}\\): \\[ \\begin{align} \\hat{\\mathbf{i}} &amp;= (1, 0, 0),\\\\ \\hat{\\mathbf{j}} &amp;= (0, 1, 0),\\\\ \\hat{\\mathbf{k}} &amp;= (0, 0, 1). \\end{align} \\] Dado um quadro \\(\\{P_0, \\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\), um vetor \\[\\mathbf{u}=(u_1, \\dots, u_n)\\] pode ser escrito unicamente como \\[\\mathbf{u}=u_1 \\mathbf{v}_1 + u_2 \\mathbf{v}_2 + \\cdots + u_n \\mathbf{v}_n.\\] Um ponto \\[P=(p_1, \\dots, p_n)\\] pode ser escrito unicamente como \\[P=P_0 + p_1\\mathbf{v}_1 + p_2\\mathbf{v}_2 + \\cdots + p_n\\mathbf{v}_n.\\] Veremos futuramente como realizar mudança de coordenadas entre diferentes quadros através da composição de matrizes de transformação. Produto escalar Sejam \\(\\mathbf{u}=(u_1, \\dots, u_n)\\) e \\(\\mathbf{v}=(v_1, \\dots, v_n)\\) dois vetores do \\(\\mathbb{R}^n\\). O produto escalar, denotado por \\(\\mathbf{u} \\cdot \\mathbf{v}\\), é a soma da multiplicação componente a componente das tuplas. O resultado é, portanto, um escalar: \\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\\] As seguintes propriedades se aplicam: \\(\\mathbf{u} \\cdot \\mathbf{v}= \\mathbf{v} \\cdot \\mathbf{u}\\). \\((a \\mathbf{u} + b \\mathbf{v}) \\cdot \\mathbf{w} = a \\mathbf{u} \\cdot \\mathbf{w} + b \\mathbf{v} \\cdot \\mathbf{w}\\). \\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\), e \\(\\mathbf{v} \\cdot \\mathbf{v} = 0\\) se e somente se \\(\\mathbf{v}=\\mathbf{0}\\). \\(\\mathbf{0} \\cdot \\mathbf{0} = 0\\). Ortogonalidade Se \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\), então \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) são ortogonais, isto é, os vetores são perpendiculares entre si. Quando todos os vetores de uma base são ortogonais, temos uma base ortogonal. A base \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_n\\) de \\(\\mathbb{R}^n\\) é um exemplo de base ortogonal, pois \\(\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0\\) para \\(i \\neq j\\). Comprimento e distância No espaço euclidiano, a magnitude ou comprimento de um vetor \\(\\mathbf{u}=(u_1, u_2, \\dots, u_n)\\) é definida pela norma euclidiana: \\[ \\begin{align} |\\mathbf{u}| &amp;= \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\\\ &amp; = \\sqrt{u_1^2 + u_2^2 + \\cdots + u_n^2}. \\end{align} \\] A norma euclidiana permite calcular a distância entre pontos. Como \\(P - Q\\) é um vetor de deslocamento de \\(Q=(q_1, \\dots, q_n)\\) para \\(P=(p_1, \\dots, p_n)\\), a distância entre os dois pontos pode ser calculada como \\[ \\begin{align} |P - Q| &amp;= \\sqrt{(P-Q) \\cdot (P-Q)}\\\\ &amp;= \\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}. \\end{align} \\] A figura 6.10 mostra a distância euclidiana entre dois pontos \\(P=(p_x, p_y)\\) e \\(Q=(q_x, q_y)\\). Observe sua relação com o teorema de Pitágoras. Figura 6.10: Distância entre dois pontos no plano. Normalização Se \\(|\\mathbf{v}|=1\\), dizemos que \\(\\mathbf{v}\\) é um vetor unitário. Podemos transformar qualquer vetor \\(\\mathbf{v}\\) não nulo em um vetor unitário na mesma direção, denotado por \\(\\hat{\\mathbf{v}}\\), se dividirmos todos os elementos de \\(\\mathbf{v}\\) por seu comprimento: \\[\\hat{\\mathbf{v}}=\\frac{\\mathbf{v}}{|\\mathbf{v}|}.\\] A figura 6.11 ilustra o resultado da normalização de vetores no \\(\\mathbb{R}^2\\). Observe que os vetores normalizados desenhados a partir da origem ficam inscritos em um círculo unitário. Figura 6.11: Um conjunto de vetores (esquerda) e seus correspondentes vetores normalizados (direita). Sempre que possível trabalharemos com vetores unitários. O uso de vetores unitários simplifica o cálculo do sombreamento e iluminação de superfícies. Quando dois vetores unitários são ortogonais, dizemos que os vetores são ortonormais. A base padrão \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_n\\) do \\(\\mathbb{R}^n\\) é uma base ortonormal pois possui vetores de base unitários (isto é, \\(|\\mathbf{e}_i|=1\\)) e ortogonais entre si. Ângulo entre vetores O produto escalar entre dois vetores \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) não nulos é proporcional ao cosseno do ângulo \\(\\theta\\) formado entre esses vetores: \\[\\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos \\theta.\\] Logo, \\[\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}||\\mathbf{v}|}.\\] Se os vetores são ortogonais, \\(\\cos \\theta = 0\\). Se os vetores são paralelos e na mesma direção, \\(\\cos \\theta = 1\\). O menor ângulo não negativo entre dois vetores (\\(\\theta \\in [0, \\pi]\\)) pode ser calculado como: \\[\\theta = \\cos^{-1} \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}||\\mathbf{v}|} \\right).\\] Note que, para vetores unitários, a expressão é mais simples: \\[\\cos \\theta = \\mathbf{u} \\cdot \\mathbf{v}\\] e \\[\\theta = \\cos^{-1}(\\mathbf{u} \\cdot \\mathbf{v}).\\] A relação entre \\(\\theta\\) e \\(\\mathbf{u} \\cdot \\mathbf{v}\\) é como segue: \\[ \\begin{align} \\nonumber \\textbf{u} \\cdot \\textbf{v} \\begin{cases} &gt;0, \\quad \\text{para }0 \\leq \\theta &lt; \\frac{\\pi}{2} \\\\[4pt] =0, \\quad \\text{para }\\theta = \\frac{\\pi}{2} \\\\[4pt] &lt;0, \\quad \\text{para } \\frac{\\pi}{2} &lt; \\theta \\leq \\pi \\end{cases} \\end{align} \\] A figura 6.12 mostra exemplos dos diferentes valores do produto escalar usando vetores no plano. Figura 6.12: Valor do produto escalar e ângulo entre vetores. Projeção ortogonal Dado um vetor \\(\\mathbf{w}\\) e um vetor \\(\\mathbf{v}\\) não nulo, podemos decompor \\(\\mathbf{w}\\) como uma soma de dois vetores, sendo um paralelo a \\(\\mathbf{v}\\) e outro ortogonal a \\(\\mathbf{v}\\) (figura 6.13): \\[\\mathbf{w} = a\\mathbf{v} + \\mathbf{u},\\] \\(a \\mathbf{v}\\) é o vetor paralelo, chamado de projeção de \\(\\mathbf{w}\\) sobre \\(\\mathbf{v}\\), sendo que \\[a = \\frac{\\mathbf{w} \\cdot \\mathbf{v}}{\\mathbf{v} \\cdot \\mathbf{v}}.\\] \\(\\mathbf{u}\\) é o vetor ortogonal a \\(\\mathbf{v}\\) (isto é, \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\)) e \\[\\mathbf{u}=\\mathbf{w}-a \\mathbf{v}.\\] Figura 6.13: Projeção ortogonal de um vetor sobre outro. Note que, se \\(\\mathbf{v}\\) é um vetor unitário, \\[a=\\mathbf{w} \\cdot \\hat{\\mathbf{v}}=|\\mathbf{w}|\\cos \\theta,\\] onde \\(\\theta\\) é o ângulo entre \\(\\mathbf{w}\\) e \\(\\hat{\\mathbf{v}}\\). Produto vetorial Sejam \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) dois vetores do \\(\\mathbb{R}^3\\). O produto vetorial ou produto cruzado de \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) é definido como \\[\\mathbf{u} \\times \\mathbf{v} = |\\mathbf{u}| |\\mathbf{v}| \\sin(\\theta) \\hat{\\mathbf{n}},\\] onde \\(\\hat{\\mathbf{n}}\\) é um vetor unitário ortogonal a \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\), e \\(\\theta\\) é o ângulo entre \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\). Assim, \\(\\mathbf{u} \\times \\mathbf{v}\\) é um vetor ortogonal aos dois vetores, com magnitude \\(|\\mathbf{u} \\times \\mathbf{v}| = |\\mathbf{u}| |\\mathbf{v}| |\\sin \\theta|\\) como mostra a figura 6.14. Figura 6.14: Produto vetorial. A direção do vetor ortogonal é dada pela regra da mão direita: usando a mão direita, se o indicador apontar na direção de \\(\\mathbf{u}\\) e o dedo médio apontar na direção de \\(\\mathbf{v}\\), o vetor ortogonal \\(\\mathbf{u} \\times \\mathbf{v}\\) apontará na direção do dedão (figura 6.15). Figura 6.15: Direção do produto vetorial segundo a regra da mão direita (imagem modificada do original). O produto vetorial é anticomutativo, isto é, \\[\\mathbf{u} \\times \\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u}).\\] Assim, se a ordem dos operandos for invertida, o vetor ortogonal apontará para a direção oposta, como mostra a figura 6.16 (pela regra da mão direita, o dedão apontará para baixo). Figura 6.16: A ordem dos operandos determina a direção do vetor ortogonal. O produto vetorial é calculado como \\[ \\mathbf{u} \\times \\mathbf{v} = (u_y v_z - u_z v_y, u_z v_x - u_x v_z, u_x v_y - u_y v_x). \\] Essa operação corresponde ao determinante de ordem 3: \\[ \\mathbf{u} \\times \\mathbf{v} = \\begin{vmatrix} \\hat{\\mathbf{i}} &amp; \\hat{\\mathbf{j}} &amp; \\hat{\\mathbf{k}} \\\\ u_x &amp; u_y &amp; u_z \\\\ v_x &amp; v_y &amp; v_z \\end{vmatrix}. \\] Usando expansão de cofatores: \\[ \\begin{align} \\mathbf{u} \\times \\mathbf{v} &amp;= \\begin{vmatrix} u_y &amp; u_z \\\\ v_y &amp; v_z \\end{vmatrix}\\hat{\\mathbf{i}} - \\begin{vmatrix} u_x &amp; u_z \\\\ v_x &amp; v_z \\end{vmatrix}\\hat{\\mathbf{j}} + \\begin{vmatrix} u_x &amp; u_y \\\\ v_x &amp; v_y \\end{vmatrix}\\hat{\\mathbf{k}}\\\\ &amp;= (u_y v_z - u_z v_y)\\hat{\\mathbf{i}}-(u_x v_z - u_z v_x)\\hat{\\mathbf{j}}+(u_x v_y - u_y v_x)\\hat{\\mathbf{k}}\\\\ &amp;= (u_y v_z - u_z v_y, u_z v_x - u_x v_z, u_x v_y - u_y v_x). \\end{align} \\] Uma vez que o vetor ortogonal tem tamanho proporcional ao seno do ângulo entre os vetores, o produto vetorial de dois vetores paralelos é o vetor nulo: \\[ \\begin{align} \\mathbf{u} \\times \\mathbf{u} &amp;= \\mathbf{0},\\\\ -\\mathbf{u} \\times \\mathbf{u} &amp;= \\mathbf{0}. \\end{align} \\] A magnitude do produto vetorial corresponde à área do paralelogramo formado pelos vetores, como mostra a figura 6.17. Figura 6.17: Magnitude do produto vetorial como área do paralelogramo formado pelos vetores. Como resultado, a área do triângulo com lados definidos pelos vetores corresponde à metade da magnitude do produto vetorial (figura 6.18). Figura 6.18: Área do triângulo formado pelos vetores. Então, para calcular a área de um triângulo \\(\\triangle P_1 P_2 P_3\\), podemos formar dois vetores a partir de um ponto em comum (por exemplo, \\(P_1\\)): \\[ \\mathbf{u}=P_3-P_1,\\\\ \\mathbf{v}=P_2-P_1, \\] e obter a área com \\[ |\\mathbf{u} \\times \\mathbf{v}|/2=|(P_3-P_1)\\times(P_2-P_1)|/2. \\] Uma generalização do produto vetorial – o chamado produto externo – é particularmente útil para o cálculo de coordenadas baricêntricas. Seja \\(\\triangle P_1 P_2 P_3\\) um triângulo com pontos \\(P_1\\), \\(P_2\\) e \\(P_3\\) no \\(\\mathbb{R}^2\\). Se \\(\\mathbf{u}=P_3-P_1\\), \\(\\mathbf{v}=P_2-P_1\\), a área do paralelogramo formado por \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) é o produto externo \\(\\mathbf{u}\\wedge\\mathbf{v}\\). Essa operação corresponde ao determinante \\[ \\begin{align} \\mathbf{u}\\wedge\\mathbf{v} &amp;= \\begin{vmatrix} u_x &amp; u_y \\\\ v_x &amp; v_y \\end{vmatrix}\\\\ &amp;= u_x v_y - u_y v_x. \\end{align} \\] O uso do determinante tem a vantagem de produzir uma “área com sinal”. Se \\(\\mathbf{u}\\) e \\(\\mathbf{v}\\) estão orientados no sentido anti-horário, a área será positiva. Caso contrário, será negativa. Isso pode ser utilizado para verificar se um dado ponto \\(P\\) está dentro do triângulo. Em particular, \\(P\\) está dentro do triângulo \\(\\triangle P_1 P_2 P_3\\) com pontos orientados no sentido anti-horário se e somente se \\(w_1 \\geq 0\\), \\(w_2 \\geq 0\\) e \\(w_3 \\geq 0\\), onde \\[ w_1 = (P-P_2)\\wedge(P_3-P_2) = 2 \\times \\textrm{área}(\\triangle P_2 P_3 P),\\\\ w_2 = (P-P_3)\\wedge(P_1-P_3) = 2 \\times \\textrm{área}(\\triangle P_3 P_1 P),\\\\ w_3 = (P-P_1)\\wedge(P_2-P_1) = 2 \\times \\textrm{área}(\\triangle P_1 P_2 P). \\] As coordenadas baricêntricas de \\(P\\) são os pesos \\(w_1\\), \\(w_2\\), \\(w_3\\) divididos pela área total do paralelogramo28: \\[ \\alpha = w_1 / (2 \\times \\textrm{área}(\\triangle P_1 P_2 P_3)),\\\\ \\beta = w_2 / (2 \\times \\textrm{área}(\\triangle P_1 P_2 P_3)),\\\\ \\gamma = w_3 / (2 \\times \\textrm{área}(\\triangle P_1 P_2 P_3)).\\\\ \\] Outras propriedades do produto vetorial são dadas a seguir: \\(\\mathbf{u} \\times (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u}\\times\\mathbf{v})+(\\mathbf{u}\\times\\mathbf{w})\\). \\((a\\mathbf{u}) \\times \\mathbf{v} = \\mathbf{u} \\times (a \\mathbf{v}) = a (\\mathbf{u} \\times \\mathbf{v})\\). \\(\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w}) = (\\mathbf{u} \\times \\mathbf{v}) \\cdot \\mathbf{w}\\). \\(\\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u} \\cdot \\mathbf{w})\\mathbf{v} - (\\mathbf{u} \\cdot \\mathbf{v})\\mathbf{w}\\). Além disso, \\[ \\begin{align} \\hat{\\mathbf{i}} \\times \\hat{\\mathbf{j}} = \\hat{\\mathbf{k}},\\\\ \\hat{\\mathbf{j}} \\times \\hat{\\mathbf{k}} = \\hat{\\mathbf{i}},\\\\ \\hat{\\mathbf{k}} \\times \\hat{\\mathbf{i}} = \\hat{\\mathbf{j}}. \\end{align} \\] Vetor normal Um vetor normal, ou simplesmente normal, é um vetor perpendicular ao plano que tangencia uma superfície em um dado ponto. Em computação gráfica, vetores normais são essenciais para o cálculo correto do sombreamento e iluminação de superfícies. Se considerarmos a superfície de uma esfera de raio \\(r&gt;0\\) dada pela equação \\[x^2+y^2+z^2=r^2,\\] a normal de um ponto \\(P=(x,y,z)\\) sobre essa esfera é, por exemplo, o vetor \\(\\mathbf{n}=(2x, 2y, 2z)\\) (figura 6.19). O vetor no sentido oposto, \\(-\\mathbf{n}\\), também é um vetor normal. Entretanto, em superfícies fechadas como a esfera, geralmente estamos interessados nas normais que apontam para fora da superfície. Figura 6.19: Vetor normal em um ponto na superfície de uma esfera. O cálculo do vetor normal em superfícies suaves frequentemente exige o uso de ferramentas de geometria diferencial. Por exemplo, em uma superfície definida implicitamente como uma função level set \\(f(x,y,z)=c\\), o vetor normal é calculado através do gradiente \\[ \\begin{align} \\mathbf{n} = \\nabla f(x,y,z) = \\frac{\\partial f}{\\partial x}\\hat{\\mathbf{i}} + \\frac{\\partial f}{\\partial y}\\hat{\\mathbf{j}} + \\frac{\\partial f}{\\partial z}\\hat{\\mathbf{k}}. \\end{align} \\] De fato, para a esfera centralizada na origem, \\[f(x,y,z)=x^2+y^2+z^2\\] e \\[ \\begin{align} \\mathbf{n} = \\nabla f(x,y,z) &amp;= 2x\\,\\hat{\\mathbf{i}} + 2y\\,\\hat{\\mathbf{j}} + 2z\\,\\hat{\\mathbf{k}}\\\\ &amp;= (2x, 2y, 2z). \\end{align} \\] Entretanto, neste curso não trabalharemos com superfícies implícitas. Utilizaremos apenas superfícies formadas por malhas de triângulos, uma vez que o pipeline gráfico do OpenGL trabalha apenas com triângulos. Se quisermos renderizar uma esfera, teremos de usar uma malha triangular que aproxime essa esfera. A figura 6.20 ilustra esse caso. Cada faceta da malha é visualizada como um quadrilátero, mas essa malha pode ser descrita unicamente por triângulos, uma vez que cada quadrilátero pode ser formado por dois triângulos. Figura 6.20: Vetor normal em uma malha que aproxima a superfície de uma esfera. Em geral, dada uma malha de triângulos, não temos acesso à representação implícita ou paramétrica da superfície que a malha tenta aproximar. Assim, no caso geral, a normal \\(n\\) mostrada na figura 6.20 precisa ser calculada utilizando unicamente os triângulos. Figura 6.21: Calculando o vetor normal de um triângulo. Para calcular a normal de um triângulo \\(\\triangle ABC\\), basta definirmos dois vetores sobre o plano do triângulo, e então calcularmos o produto vetorial entre eles. Os vetores podem ser obtidos através da subtração dos vértices que formam quaisquer duas arestas (figura 6.21): \\[ \\begin{align} \\mathbf{u}&amp;=A-C,\\\\ \\mathbf{v}&amp;=B-C,\\\\ \\mathbf{n}&amp;=\\mathbf{u} \\times \\mathbf{v}.\\\\ \\end{align} \\] Em geral, desejaremos trabalhar com normais unitárias: \\[ \\begin{align} \\hat{\\mathbf{n}}&amp;=\\frac{\\mathbf{u} \\times \\mathbf{v}}{|\\mathbf{u} \\times \\mathbf{v}|}.\\\\ \\end{align} \\] Como o triângulo é uma superfície planar, o vetor normal é o mesmo para todos os pontos do triângulo. Entretanto, sob arestas e vértices compartilhados por diferentes triângulos, podemos ter mais de um vetor normal. Observe na figura 6.22 o detalhe ampliado da esfera da figura 6.20. Cada face, formada por dois triângulos (mostrados pelo tracejado), é uma superfície planar. Portanto, cada face tem o mesmo vetor normal para todos os pontos. Por outro lado, o ponto \\(P\\) é compartilhado por quatro faces (seis triângulos). Qual das normais (\\(\\mathbf{n}_1\\), \\(\\mathbf{n}_2\\), \\(\\mathbf{n}_3\\), \\(\\mathbf{n}_4\\)) deve ser utilizada em \\(\\mathbf{n}_p\\)? Figura 6.22: Detalhe ampliado de uma esfera aproximada por uma malha. Como a malha de triângulos aproxima uma superfície suave, podemos calcular um vetor normal em \\(P\\) como uma média dos vetores normais de todos os \\(n\\) triângulos que usam \\(P\\). Uma forma simples de fazer isso é através da normalização da soma dessas normais \\[ \\begin{align} \\hat{\\mathbf{n}}_p&amp;=\\frac{\\sum_{i=1}^n \\mathbf{n}_i}{|\\sum_{i=1}^n \\mathbf{n}_i|},\\\\ \\end{align} \\] onde \\(\\mathbf{n}_i\\) é o vetor normal do \\(n\\)-ésimo triângulo que usa \\(P\\). O resultado é um vetor normalizado chamado de normal de vértice. A figura 6.23 ilustra, em um corte bidimensional, como a normal de vértice \\(P\\) aproxima a superfície suave mostrada no tracejado. Figura 6.23: Normal de vértice como aproximação da normal da superfície suave. Podemos utilizar este método sempre que soubermos que a malha aproxima uma superfície suave. Esse método é ideal para ser utilizado com geometria indexada, pois os vértices compartilhados têm em comum tanto a posição como a normal de vértice. Entretanto, se a malha representar um objeto com quinas, tal como um cubo ou pirâmide, então cada face precisará ser desenhada com vértices não compartilhados, pois queremos evidenciar a descontinuidade da superfície. Se esse fosse o caso da geometria ilustrada na figura 6.22, quatro vértices teriam de ser utilizados em \\(P\\): um para cada face (quadrilátero formado por dois triângulos). Os vértices teriam a mesma posição de \\(P\\), mas cada um usaria a normal da face correspondente. Poderíamos calcular os pesos e o fator de normalização usando a área do triângulo ao invés da área do paralelogramo, mas o que importa aqui é a razão entre as áreas, que é a mesma.↩︎ "],["objmodel.html", "6.4 Lendo um modelo 3D", " 6.4 Lendo um modelo 3D Nos capítulos anteriores vimos como usar o pipeline gráfico do OpenGL para renderizar primitivas formadas a partir de arranjos ordenados de vértices. Em particular, conseguimos representar diferentes formas no plano através de malhas de triângulos. Tomemos, como exemplo, o polígono da figura 6.24: Figura 6.24: Um polígono de seis lados. O polígono pode ser convertido em uma malha de triângulos através de uma triangulação. Uma possível triangulação é mostrada na figura 6.25: Figura 6.25: Triangulação de um polígono de seis lados. Os triângulos \\(T_1\\) a \\(T_4\\) podem ser renderizados com glDrawArrays(GL_TRIANGLES, ...) usando um VBO formado por um arranjo de posições de vértices: std::array&lt;glm::ve2, 12&gt; positions{// Triângulo T_1 {-3, 3}, {-4,-2}, { 1,-4}, // Triângulo T_2 {-3, 3}, { 1,-4}, { 4,-1}, // Triângulo T_3 {-3, 3}, { 4,-1}, { 4, 2}, // Triângulo T_4 {-3, 3}, { 4, 2}, { 1, 4}}; ... glDrawArrays(GL_TRIANGLES, 0, 12); Uma desvantagem dessa representação é que há repetição de coordenadas. Por exemplo, a coordenada \\((-3,3)\\) é repetida quatro vezes, uma para cada triângulo. Felizmente, como a triangulação forma um leque, podemos usar GL_TRIANGLE_FAN com um arranjo mais compacto: std::array&lt;glm::ve2, 6&gt; positions{{-3, 3}, {-4,-2}, { 1,-4}, { 4,-1}, { 4, 2}, { 1, 4}}; ... glDrawArrays(GL_TRIANGLE_FAN, 0, 6); Outra possibilidade é usar geometria indexada. A figura 6.26 mostra uma possível indexação dos vértices do polígono da figura 6.25. Os índices são numerados de 0 a 5: Figura 6.26: Geometria indexada. Na geometria indexada, um arranjo ordenado de posições de vértices é armazenado no VBO, e um arranjo de números inteiros que representam os índices para esses vértices é armazenado no EBO: std::array&lt;glm::vec2, 6&gt; positions{{-3, 3}, // Vértice 0 {-4,-2}, // Vértice 1 { 1,-4}, // Vértice 2 { 4,-1}, // Vértice 3 { 4, 2}, // Vértice 4 { 1, 4}}; // Vértice 5 std::array indices{0, 1, 2, // Triângulo T_1 0, 2, 3, // Triângulo T_2 0, 3, 4, // Triângulo T_3 0, 4, 5}; // Triângulo T_4 ... glDrawElements(GL_TRIANGLES, 12, GL_UNSIGNED_INT, nullptr); A geometria indexada é o formato mais utilizado para descrever malhas de triângulos. É, por exemplo, o formato utilizado nos modelos OBJ que utilizaremos nas atividades da disciplina. Até agora, só utilizamos formas no plano. Entretanto, não há qualquer limitação no OpenGL que nos impeça de representar geometria no espaço. Basta especificarmos a posição dos vértices como coordenadas \\((x,y,z)\\) do espaço euclidiano usando tuplas de três elementos com glm::vec3. Nesta seção descreveremos um passo a passo de construção de uma aplicação de leitura de modelos 3D no formato OBJ. Neste formato, os dados são gravados em formato texto e são fáceis de serem lidos. Veja a seguir o conteúdo de um arquivo OBJ contendo a definição de um cubo unitário centralizado na origem. Observe que há inicialmente a definição dos 8 vértices do cubo, e então a definição dos índices das 12 faces. Neste arquivo, cada face é um triângulo (o cubo tem seis 6 lados e cada lado é formado por 2 faces coplanares). # object Box v -0.5000 -0.5000 0.5000 v -0.5000 -0.5000 -0.5000 v 0.5000 -0.5000 -0.5000 v 0.5000 -0.5000 0.5000 v -0.5000 0.5000 0.5000 v 0.5000 0.5000 0.5000 v 0.5000 0.5000 -0.5000 v -0.5000 0.5000 -0.5000 # 8 vertices o Box g Box f 1 3 2 f 3 1 4 f 5 7 6 f 7 5 8 f 1 6 4 f 6 1 5 f 4 7 3 f 7 4 6 f 3 8 2 f 8 3 7 f 2 5 1 f 5 2 8 # 12 faces Antes de iniciarmos o passo a passo de leitura do modelo 3D, veremos o conceito de orientação de triângulos e como isso pode afetar a renderização. Se os triângulos de um modelo 3D estiverem com orientação diferente do esperado, o modelo pode ser renderizado de forma incorreta. Orientação e face culling A direção do vetor normal pode ser utilizada para definir a orientação de uma superfície, isto é, qual é o “lado da frente” da superfície. Entretanto, reservaremos o uso dos vetores normais para o cálculo da iluminação de superfícies. Há uma forma mais simples de determinar a orientação de uma superfície quando triângulos são utilizados. A orientação de um triângulo pode ser definida pela ordem em que seus vértices estão ordenados no arranjo de vértices quando o triângulo é visto de frente. Só há duas orientações possíveis (figura 6.27): Sentido horário (clockwise ou CW): os vértices estão orientados no sentido horário quando o triângulo é visto de frente; Sentido anti-horário (counterclockwise ou CCW): os vértices estão orientados no sentido anti-horário quando o triângulo é visto de frente. Figura 6.27: Sentidos de orientação de um triângulo. Volte no exemplo anterior de geometria indexada (figura 6.26) e observe como os índices de cada triângulo \\(T_1\\) a \\(T_4\\) estão ordenados no sentido anti-horário (CCW). Por padrão, o OpenGL considera que o lado da frente é o lado orientado no sentido anti-horário (CCW). Isso pode ser modificado com a função glFrontFace, usando o argumento GL_CW ou GL_CCW, como a seguir: glFrontFace(GL_CW): para indicar que o lado da frente tem vértices no sentido horário; glFrontFace(GL_CCW): para indicar que o lado da frente tem vértices no sentido anti-horário (padrão). CCW é a orientação padrão do OpenGL porque essa é também a orientação padrão utilizada na matemática. Por exemplo, os ângulos medidos no plano cartesiano são medidos no sentido anti-horário e seguem a regra da mão direita (figura 6.28): \\(0\\) radianos (\\(0^{\\circ}\\)) aponta para o eixo \\(x\\) positivo (para a direita); \\(\\frac{\\pi}{2}\\) radianos (\\(90^{\\circ}\\)) aponta para o eixo \\(y\\) positivo (para cima); \\(\\pi\\) radianos (\\(180^{\\circ}\\)) aponta para o eixo \\(x\\) negativo (para a esquerda). Figura 6.28: Direção convencional dos ângulos em um eixo de rotação (fonte). Há algumas vantagens em saber qual é o lado da frente de um triângulo: Podemos desenhar cada lado com uma cor ou efeito diferente. No fragment shader, a variável embutida gl_FrontFacing é uma variável booleana que é true sempre que o fragmento pertencer a um triângulo visto de frente, e false caso contrário. Podemos fazer uso da técnica de face culling, também chamada de back-face culling. Face culling Face culling é uma técnica que consiste em descartar todas as faces (triângulos no OpenGL) que não estão de frente em relação ao plano de projeção. O uso de face culling pode aumentar de forma considerável a eficiência da renderização, pois os triângulos podem ser removidos antes da rasterização, poupando tempo de processamento. Se a malha de triângulos formar um sólido opaco e fechado, então o face culling pode remover cerca de metade dos triângulos. Esse é o caso da malha que aproxima uma esfera. Na figura 6.29, parte das faces da frente da malha foram deslocadas para revelar as faces voltadas para trás, em vermelho. Essas faces em vermelho podem ser removidas completamente se a esfera estiver fechada, pois elas estarão totalmente encobertas pelas faces da frente. Figura 6.29: Faces voltadas para trás (em vermelho). O descarte de primitivas usando face culling pode ser feito automaticamente pelo pipeline gráfico do OpenGL, após a etapa de recorte de primitivas, e imediatamente antes da rasterização. Podemos ativar o face culling através de glEnable(GL_CULL_FACE) e desativá-lo através de glDisable(GL_CULL_FACE). Por padrão, o face culling está desativado. A função glCullFace pode ser utilizada para especificar qual lado deve ser descartado quando o face culling estiver habilitado. Por exemplo: glCullFace(GL_FRONT): para descartar os triângulos que estão de frente quando projetados no viewport; glCullFace(GL_BACK): para descartar os triângulos que estão voltados para trás quando projetados no viewport (padrão). glCullFace(GL_FRONT_AND_BACK): para descartar todos os triângulos, mas ainda renderizar pontos e segmentos. Configuração inicial A configuração inicial do nosso leitor de arquivos OBJ é semelhante a dos projetos anteriores. No arquivo abcg/examples/CMakeLists.txt, inclua a linha: add_subdirectory(loadmodel) Crie o subdiretório abcg/examples/loadmodel e o arquivo abcg/examples/loadmodel/CMakeLists.txt, que ficará assim: project(loadmodel) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Crie os arquivos main.cpp, window.cpp e window.hpp. Crie o subdiretório abcg/examples/loadmodel/assets. Dentro dele, crie os arquivos loadmodel.frag e loadmodel.vert. Baixe o arquivo bunny.zip e descompacte-o em assets. O conteúdo é o arquivo bunny.obj que contém o modelo 3D de um coelho de cerâmica escaneado: o Stanford Bunny (figura 6.30). O modelo utilizado aqui não é o original, mas uma versão processada e simplificada no MeshLab. Figura 6.30: Modelo real do “Stanford Bunny” (fonte). O conteúdo de abcg/examples/loadmodel ficará com a seguinte estrutura: loadmodel/ │ CMakeLists.txt │ main.cpp │ window.hpp │ window.cpp │ └───assets/ │ bunny.obj │ loadmodel.frag └ loadmodel.vert main.cpp O conteúdo de main.cpp é bem similar ao dos projetos anteriores. Não há nada de realmente novo aqui: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings({.samples = 4}); window.setWindowSettings({ .width = 600, .height = 600, .title = &quot;Load Model&quot;, }); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } loadmodel.vert O vertex shader ficará como a seguir: #version 300 es layout(location = 0) in vec3 inPosition; uniform float angle; void main() { float sinAngle = sin(angle); float cosAngle = cos(angle); gl_Position = vec4(inPosition.x * cosAngle + inPosition.z * sinAngle, inPosition.y, inPosition.z * cosAngle - inPosition.x * sinAngle, 1.0); } Só há um atributo de entrada, inPosition, que é a posição \\((x,y,z)\\) do vértice. Observe que não há atributo de saída. A cor dos fragmentos será determinada unicamente no fragment shader. A variável uniforme angle (linha 5) é um ângulo em radianos que será incrementado continuamente para produzir uma animação de rotação. No código de main, gl_Position recebe inPosition transformado através de uma operação de rotação em torno do eixo \\(y\\) pelo ângulo angle. A transformação de inPosition (posição \\(x,y,z\\)) para gl_Position (posição \\(x&#39;,y&#39;,z&#39;,1\\)) é como segue: \\[ \\begin{align} x&#39; &amp;= x \\cos(\\theta) + z \\sin(\\theta),\\\\ y&#39; &amp;= y,\\\\ z&#39; &amp;= z \\cos(\\theta) - x \\sin(\\theta). \\end{align} \\] Os fundamentos de transformação geométrica, incluindo rotação, serão abordados no próximo capítulo. Veremos também que será possível simplificar esse código através do uso de operações matriciais. loadmodel.frag O conteúdo do fragment shader ficará assim: #version 300 es precision mediump float; out vec4 outColor; void main() { float i = 1.0 - gl_FragCoord.z; if (gl_FrontFacing) { outColor = vec4(i, i, i, 1); } else { outColor = vec4(i, 0, 0, 1); } } A variável i é um valor de intensidade de cor, calculado a partir da componente z da variável embutida gl_FragCoord. gl_FragCoord é um vec4 que contém a posição do fragmento no espaço da janela. As componentes \\(x\\) e \\(y\\) são a posição do fragmento na janela, em pixels. A componente \\(z\\) é a “profundidade” do fragmento, que varia de 0 (mais perto) a 1 (mais distante). Lembre-se que esse é o valor \\(z\\) que estava no intervalo \\([-1, 1]\\) em coordenadas normalizadas do dispositivo (NDC) e que, após a rasterização, foi mapeado para \\([0, 1]\\) no espaço da janela (o mapeamento pode ser controlado com glDepthRange). A cor de saída depende do valor da variável embutida gl_FrontFacing, que indica se o fragmento pertence a uma face de frente ou de trás. Se é true (frente), a cor de saída é um tom de cinza dado pelo valor de i. Caso contrário (trás), a cor de saída é um tom de vermelho. O resultado será um modelo renderizado em tons de cinza (frente) ou vermelho (trás). Quanto maior for a profundidade do fragmento, menor será sua intensidade. Com isso conseguiremos distinguir melhor a forma e o volume do objeto. window.hpp A definição da classe Window ficará assim: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; struct Vertex { glm::vec3 position{}; friend bool operator==(Vertex const &amp;, Vertex const &amp;) = default; }; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBO{}; GLuint m_EBO{}; GLuint m_program{}; float m_angle{}; int m_verticesToDraw{}; std::vector&lt;Vertex&gt; m_vertices; std::vector&lt;GLuint&gt; m_indices; void loadModelFromFile(std::string_view path); void standardize(); }; #endif Primeiro, observe a estrutura Vertex definida nas linhas 6 a 10: struct Vertex { glm::vec3 position{}; friend bool operator==(Vertex const &amp;, Vertex const &amp;) = default; }; Essa estrutura define os atributos que compõem um vértice. Há apenas uma posição \\((x,y,z)\\) e a definição de um operador de igualdade (==) que verifica se dois vértices são iguais. Os objetos to tipo Vertex serão os elementos de uma tabela hash implementada com std::unordered_map. Durante a leitura do modelo OBJ, a tabela hash será utilizada para verificar se há algum vértice com posição repetida (por isso o operador de igualdade). Através disso conseguiremos criar uma geometria indexada o mais compacta possível. Veremos mais sobre isso na implementação da função de leitura do modelo OBJ. A variável m_angle (linha 28) é o ângulo de rotação que será enviado à variável uniforme do vertex shader. A variável m_verticesToDraw (linha 29) é a quantidade de vértices do VBO que será processada pela função de renderização, glDrawElements. O valor de m_verticesToDraw será controlado por um slider da ImGui. Assim conseguiremos controlar quantos triângulos queremos renderizar. Nas linhas 31 e 32, m_vertices e m_indices são os arranjos de vértices e índices lidos do arquivo OBJ. Esses são os dados que serão enviados ao VBO (m_VBO) e EBO (m_EBO). O carregamento do arquivo OBJ será feito pela função Window::loadModelFromFile (linha 34). A função Window::standardize (linha 35) será chamada após Window::loadModelFromFile e servirá para centralizar o modelo na origem e normalizar as coordenadas de todos os vértices no intervalo \\([-1, 1]\\). window.cpp O início de window.cpp começa como a seguir: #include &quot;window.hpp&quot; #include &lt;glm/gtx/fast_trigonometry.hpp&gt; #include &lt;unordered_map&gt; // Explicit specialization of std::hash for Vertex template &lt;&gt; struct std::hash&lt;Vertex&gt; { size_t operator()(Vertex const &amp;vertex) const noexcept { auto const h1{std::hash&lt;glm::vec3&gt;()(vertex.position)}; return h1; } }; Nas linhas 7 a 12 há uma especialização explícita de std::hash para a nossa estrutura Vertex. Isso é necessário para que possamos usar Vertex como chave de uma tabela hash (como std::unordered_map). Por padrão, std::hash gera valores de hashing (do tipo std::size_t) a partir de tipos de dados mais simples, como char, int e float. Como queremos usar um Vertex, precisamos definir como o valor de hashing será gerado. Isso é feito através da sobrecarga do operador de chamada de função () na linha 8. O valor de hashing é o h1 da linha 11, gerado a partir da posição do vértice. A biblioteca GLM implementa sua própria especialização de std::hash para glm::vec3 (definido no cabeçalho glm/gtx/hash.cpp). Na verdade, para este projeto poderíamos ter usado glm::vec3 diretamente no lugar de Vertex. Entretanto, em projetos futuros ampliaremos o número de atributos de Vertex e nossas chaves serão um pouco mais complexas. Esse código será reutilizado e ficará maior nos próximos projetos. Vamos agora à definição de Window::onCreate: void Window::onCreate() { auto const &amp;assetsPath{abcg::Application::getAssetsPath()}; abcg::glClearColor(0, 0, 0, 1); // Enable depth buffering abcg::glEnable(GL_DEPTH_TEST); // Create program m_program = abcg::createOpenGLProgram({{.source = assetsPath + &quot;loadmodel.vert&quot;, .stage = abcg::ShaderStage::Vertex}, {.source = assetsPath + &quot;loadmodel.frag&quot;, .stage = abcg::ShaderStage::Fragment}}); // Load model loadModelFromFile(assetsPath + &quot;bunny.obj&quot;); standardize(); m_verticesToDraw = m_indices.size(); // Generate VBO abcg::glGenBuffers(1, &amp;m_VBO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_vertices.at(0)) * m_vertices.size(), m_vertices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate EBO abcg::glGenBuffers(1, &amp;m_EBO); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); abcg::glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(m_indices.at(0)) * m_indices.size(), m_indices.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBO); auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; abcg::glEnableVertexAttribArray(positionAttribute); abcg::glVertexAttribPointer(positionAttribute, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_EBO); // End of binding to current VAO abcg::glBindVertexArray(0); } Na linha 20, o teste de profundidade é habilitado. Isso faz com que os fragmentos sejam descartados durante a renderização com base na comparação de sua profundidade com o valor atual do buffer de profundidade. Precisamos usar o teste de profundidade pois, em cenas 3D, geralmente é inviável ordenar e renderizar os triângulos do mais distante para o mais próximo, como fizemos com os objetos do projeto asteroids (seção 5.2) usando o “algoritmo do pintor”. Na linha 30 carregamos o arquivo bunny.obj. Internamente, Window::loadModelFromFile armazena os vértices e índices em m_vertices e m_indices. Na linha 31, Window::standardize modifica esses dados para fazer com que a geometria caiba no volume de visão do pipeline gráfico, que é o cubo de tamanho \\(2 \\times 2 \\times 2\\) centralizado em \\((0,0,0)\\) no espaço normalizado do dispositivo (figura 6.31). Figura 6.31: Volume de visão em coordenadas normalizadas do dispositivo. O restante do código de Window::onCreate contém a criação do VAO, VBO e EBO usando os dados de m_vertices e m_indices. A definição de Window::loadModelFromFile será como a seguir: void Window::loadModelFromFile(std::string_view path) { tinyobj::ObjReader reader; if (!reader.ParseFromFile(path.data())) { if (!reader.Error().empty()) { throw abcg::RuntimeError( fmt::format(&quot;Failed to load model {} ({})&quot;, path, reader.Error())); } throw abcg::RuntimeError(fmt::format(&quot;Failed to load model {}&quot;, path)); } if (!reader.Warning().empty()) { fmt::print(&quot;Warning: {}\\n&quot;, reader.Warning()); } auto const &amp;attributes{reader.GetAttrib()}; auto const &amp;shapes{reader.GetShapes()}; m_vertices.clear(); m_indices.clear(); // A key:value map with key=Vertex and value=index std::unordered_map&lt;Vertex, GLuint&gt; hash{}; // Loop over shapes for (auto const &amp;shape : shapes) { // Loop over indices for (auto const offset : iter::range(shape.mesh.indices.size())) { // Access to vertex auto const index{shape.mesh.indices.at(offset)}; // Vertex position auto const startIndex{3 * index.vertex_index}; auto const vx{attributes.vertices.at(startIndex + 0)}; auto const vy{attributes.vertices.at(startIndex + 1)}; auto const vz{attributes.vertices.at(startIndex + 2)}; Vertex const vertex{.position = {vx, vy, vz}}; // If map doesn&#39;t contain this vertex if (!hash.contains(vertex)) { // Add this index (size of m_vertices) hash[vertex] = m_vertices.size(); // Add this vertex m_vertices.push_back(vertex); } m_indices.push_back(hash[vertex]); } } } A variável reader (linha 72) é um objeto da classe tinyobj::ObjReader, da biblioteca TinyObjLoader, responsável pela leitura e parsing do arquivo. O resultado é um conjunto de malhas (shapes) e um conjunto de atributos de vértices (attributes). Cada malha pode ser um objeto, ou apenas parte de um objeto. No nosso caso, trataremos todas as malhas como um único objeto. Para mais detalhes sobre a estrutura utilizada pelo TinyObjLoader, consulte a documentação. Na linha 92 definimos a tabela hash que será utilizada para fazer a consulta de vértices não repetidos. Embora o formato OBJ utilize geometria indexada, durante a leitura do modelo é possível que tenhamos vértices em uma mesma posição, embora com índices diferentes. Isso acontece porque os vértices podem diferir em relação a outros atributos além da posição. Por exemplo, dois vértices podem ter a mesma posição no espaço, mas cada um pode ter uma cor diferente. Neste projeto, cada vértice só contém o atributo de posição. Podemos então simplificar o modelo mantendo apenas um índice para cada posição de vértice. Cada vértice lido do modelo OBJ será inserido na tabela hash usando a posição \\((x,y,z)\\) como chave, e a ordem de leitura do vértice (isto é, seu índice) como valor associado à chave. Se o vértice está sendo inserido na tabela hash pela primeira vez, ele é inserido também na lista de vértices m_vertices, que contém os dados do VBO. Por sua vez, a lista de índices m_indices (que contém os dados do EBO) será formada pela inserção do valor associado à chave do vértice que está sendo lido. No fim, teremos uma lista de vértices não repetidos (m_vertices), e uma lista de índices (m_indices) a esses vértices. No laço da linha 96, o conjunto de malhas (shapes) é iterado para ler todos os triângulos e vértices. A posição de cada vértice é lida nas linhas 104 a 106, nas variáveis vx, vy e vz, e utilizada para criar o vértice vertex na linha 108. Na linha 111 verificamos se o vértice atual existe na tabela hash. Se não existir, ele é incluído na tabela e em m_vertices. Na linha 118, o índice do vértice atual é inserido em m_indices. O índice é o valor da tabela hash para a chave de vertex. A definição da função Window::standardize é dada a seguir: void Window::standardize() { // Center to origin and normalize bounds to [-1, 1] // Get bounds glm::vec3 max(std::numeric_limits&lt;float&gt;::lowest()); glm::vec3 min(std::numeric_limits&lt;float&gt;::max()); for (auto const &amp;vertex : m_vertices) { max = glm::max(max, vertex.position); min = glm::min(min, vertex.position); } // Center and scale auto const center{(min + max) / 2.0f}; auto const scaling{2.0f / glm::length(max - min)}; for (auto &amp;vertex : m_vertices) { vertex.position = (vertex.position - center) * scaling; } } As maiores e menores coordenadas \\(x\\), \\(y\\) e \\(z\\) dos vértices são calculadas no laço da linha 129. Esses valores determinam os limites de uma caixa delimitante do modelo geométrico. O centro dessa caixa e um fator de escala de normalização são calculados nas linhas 135 e 136. No laço da linha 141, essas variáveis são utilizadas para centralizar o modelo na origem e mudar sua escala de modo que o modelo fique contido no volume de visão em NDC, isto é, todos os vértices terão coordenadas no intervalo \\([-1, 1]\\). Definiremos Window::onPaint como a seguir: void Window::onPaint() { // Animate angle by 15 degrees per second auto const deltaTime{gsl::narrow_cast&lt;float&gt;(getDeltaTime())}; m_angle = glm::wrapAngle(m_angle + glm::radians(15.0f) * deltaTime); // Clear color buffer and depth buffer abcg::glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); // Update uniform variable auto const angleLocation{abcg::glGetUniformLocation(m_program, &quot;angle&quot;)}; abcg::glUniform1f(angleLocation, m_angle); // Draw triangles abcg::glDrawElements(GL_TRIANGLES, m_verticesToDraw, GL_UNSIGNED_INT, nullptr); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 145, o valor de m_angle é incrementado a uma taxa de 15 graus por segundo. Observe, na linha 148, que glClear agora usa GL_DEPTH_BUFFER_BIT além de GL_COLOR_BUFFER_BIT. Isso é necessário para limpar o buffer de profundidade antes de renderizar o quadro atual. O restante do código é similar ao que já usamos em projetos anteriores. Vamos agora à definição de Window::onPaintUI: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); // Create window for slider { ImGui::SetNextWindowPos(ImVec2(5, m_viewportSize.y - 94)); ImGui::SetNextWindowSize(ImVec2(m_viewportSize.x - 10, -1)); ImGui::Begin(&quot;Slider window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); // Create a slider to control the number of rendered triangles { // Slider will fill the space of the window ImGui::PushItemWidth(m_viewportSize.x - 25); static auto n{m_verticesToDraw / 3}; ImGui::SliderInt(&quot; &quot;, &amp;n, 0, m_indices.size() / 3, &quot;%d triangles&quot;); m_verticesToDraw = n * 3; ImGui::PopItemWidth(); } ImGui::End(); } // Create a window for the other widgets { auto const widgetSize{ImVec2(172, 62)}; ImGui::SetNextWindowPos(ImVec2(m_viewportSize.x - widgetSize.x - 5, 5)); ImGui::SetNextWindowSize(widgetSize); ImGui::Begin(&quot;Widget window&quot;, nullptr, ImGuiWindowFlags_NoDecoration); static bool faceCulling{}; ImGui::Checkbox(&quot;Back-face culling&quot;, &amp;faceCulling); if (faceCulling) { abcg::glEnable(GL_CULL_FACE); } else { abcg::glDisable(GL_CULL_FACE); } // CW/CCW combo box { static std::size_t currentIndex{}; std::vector&lt;std::string&gt; const comboItems{&quot;CW&quot;, &quot;CCW&quot;}; ImGui::PushItemWidth(70); if (ImGui::BeginCombo(&quot;Front face&quot;, comboItems.at(currentIndex).c_str())) { for (auto const index : iter::range(comboItems.size())) { auto const isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index).c_str(), isSelected)) currentIndex = index; if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } ImGui::PopItemWidth(); if (currentIndex == 0) { abcg::glFrontFace(GL_CW); } else { abcg::glFrontFace(GL_CCW); } } ImGui::End(); } } No escopo da linha 177 é definido o slider que controla o número de triângulos que será renderizado. Na linha 199 é criada uma caixa de seleção (checkbox) de ativação do back-face culling (estamos usando o padrão do glCullFace, que é GL_BACK). O resultado da variável booleana faceCulling é utilizado para ativar ou desativar o face culling nas linhas 202 e 204. Uma caixa de combinação (combo box) com as opções CW e CCW é definida no escopo a partir da linha 208. Nas linhas 227 e 229, a função glFrontFace é chamada com GL_CW ou GL_CCW de acordo com o que foi selecionado pelo usuário. Como a opção CW é a primeira opção da caixa, a aplicação iniciará com GL_CW. O conteúdo restante de window.cpp é similar ao utilizado nos projetos anteriores: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; } void Window::onDestroy() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_EBO); abcg::glDeleteBuffers(1, &amp;m_VBO); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Baixe o código completo do projeto a partir deste link. Outros modelos OBJ estão disponíveis neste link. "],["trabalhos-finais.html", "Trabalhos finais", " Trabalhos finais 2022.3 Boid Simulator, por Gabriel Jorge Menezes e Rodrigo Cabrera Castaldoni (WebAssembly, GitHub). BorgCube Game, por Aline Conquista de Almeida Ferreira e Lucca Segantim Jacomassi (WebAssembly, GitHub). Cidade - Vista Aérea, por Sérgio Maracajá Junior e Vinícius Lacerda Gonsalez (WebAssembly, GitHub). Cube, por Lucas Guimarães Miranda (WebAssembly, GitHub). Earth Visualization, por Heberton Miranda Rodrigues da Silva (WebAssembly, GitHub). Galaxy Explorer, por Gustavo Gualberto Rocha de Lemos (WebAssembly, GitHub). Golf C++, por Jairo da Silva Freitas Júnior (WebAssembly, GitHub). Roll The Ball 2, por Vitor Costa de Farias (WebAssembly, GitHub). RollerCoaster, por Diego Sousa Santos (WebAssembly, GitHub). Simulação de Construção Aleatória de Terreno, por Henrique Augusto Santos Batista e Paulo Henrique Eiji Hayashida (WebAssembly, GitHub). Simulação de jogo de plataforma em primeira pessoa, por Daniel Fusimoto Pires (WebAssembly, GitHub). Sistema Solar, por Beatriz Favini Chicaroni e Thais Amorim Souza (WebAssembly, GitHub). Sistema Solar, por Kaio Vinicius Souza Santos e Mateus Flosi Molero (WebAssembly, (GitHub). Sistema Solar, por Rafael Correia de Lima e Renan Gonçalves Miranda (WebAssembly, GitHub). Solar System Final, por Thaina Abade Siemerink (WebAssembly, GitHub). Storm Cube, por Lucas Zanoni de Oliveira (WebAssembly, GitHub). Taz Maluco, por Henrique Augusto de Oliveira e Jefferson Leite Rodrigues Dantas (WebAssembly, GitHub). 2021.3 ABC Wave (WebAssembly, GitHub) Avoid Asteroids (WebAssembly, GitHub) Baleia!! (WebAssembly, GitHub) Bouncing Ball (WebAssembly, GitHub) Crab World (WebAssembly, GitHub) Dice 3D 2.0 (WebAssembly, GitHub) Dino Wants Money (WebAssembly, GitHub) Earth (WebAssembly, GitHub) Earth and Moon (WebAssembly, GitHub) Futebol_v2 (Vídeo, GitHub) Jumping Block V2 (WebAssembly, GitHub) Labirinto 3D (WebAssembly, GitHub) OpenGL Lo-Fi (WebAssembly, GitHub) Pines (WebAssembly, GitHub) Sem título (GitHub) Solar System (WebAssembly, GitHub) 2021.1 AMOGUS_Selector (WebAssembly, GitHub) Animal’s House - Primeiros passos da língua inglesa (WebAssembly, GitHub) Ball around a wolf (WebAssembly, GitHub) Block Tube V2 (WebAssembly, GitHub) Boneco de Gelo e Fogo (Vídeo, GitHub) Boids 3D (WebAssembly, GitHub) Bunnies (WebAssembly, GitHub) Campo de futebol (WebAssembly, GitHub) Cube place (WebAssembly, GitHub) Dark Maze (GitHub) Deformador de objetos (WebAssembly, GitHub) Dragão Midas (WebAssembly, GitHub) Ducket League (WebAssembly, GitHub) Endless Runner (Vídeo, GitHub) Estátua do vigésimo presidente americano James Garfield (WebAssembly, GitHub) Floresta 3D (WebAssembly, GitHub) Fruits! (WebAssembly, GitHub) Haunted Maze 3D (WebAssembly, GitHub) Karambit Viewer (WebAssembly, GitHub) Lookat - Tea Party! (WebAssembly, GitHub) Maze (WebAssembly, GitHub) Mini Earth (WebAssembly, GitHub) Mirror (WebAssembly, GitHub) OpenCraft (WebAssembly, GitHub) Os patos (GitHub) Planet Tour (WebAssembly, GitHub) Renderização 3D de um labirinto (WebAssembly, GitHub) RUN! from UFABC (WebAssembly, GitHub) Scape Room (GitHub) Screensaver: Deathstar (WebAssembly, GitHub) Simulação de avião (WebAssembly, GitHub) Sistema Solar (WebAssembly, GitHub) Skull Screensaver 2 (WebAssembly, GitHub) spaceTrip (WebAssembly, GitHub) The Tree Log Challenge (Vídeo, GitHub) Three Bodies Viewer v2 (WebAssembly, GitHub) Tour na neve (WebAssembly, GitHub) Trenó SP (WebAssembly, GitHub) Uma sala 3D repleta de cubos com diversas texturas (Vídeo, GitHub) "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
