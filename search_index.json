[["index.html", "MCTA008-17 Computação Gráfica Apresentação", " MCTA008-17 Computação Gráfica Harlen Batagelo (harlen.batagelo@ufabc.edu.br) Universidade Federal do ABC 3º quadrimestre de 2022 Apresentação Caro/a estudante, Este site contém as notas de aula da disciplina MCTA008-17 Computação Gráfica para o 3º quadrimestre de 2022. O conteúdo foi elaborado originalmente para o ensino remoto dos Quadrimestres Suplementares seguindo o formato de um livro interativo voltado ao estudo autodirigido. Neste quadrimestre ele será usado como material de apoio às aulas presenciais. Os capítulos estão divididos em tópicos teóricos sobre o processo de geração de imagens no computador, e partes práticas de desenvolvimento de aplicações gráficas interativas usando C++ com a API gráfica OpenGL. Para informações sobre o cronograma das atividades e critérios de avaliação, consulte o plano de ensino disponível no Moodle. Bons estudos! — Harlen "],["pré-requisitos.html", "Pré-requisitos", " Pré-requisitos Para acompanhar a disciplina é recomendável ter conhecimento prévio do conteúdo abordado nas disciplinas de Algoritmos e Estruturas de Dados I e Geometria Analítica. As atividades práticas avaliativas serão desenvolvidas na linguagem C++. Embora não seja necessário ter fluência em C++, é recomendável ter proficiência em programação em C e familiaridade com conceitos básicos de programação orientada a objetos. Também é recomendável ter familiaridade com o Git e com o uso de alguma plataforma de hospedagem de repositórios Git tal como o GitHub, GitLab ou Bitbucket. Você deverá ser capaz de gerenciar seus próprios repositórios ao longo do quadrimestre. Atividades práticas Para realizar as atividades práticas é necessário ter um computador com sistema operacional 64 bits (Windows 10 ou superior, Linux ou macOS) e placa de vídeo compatível com OpenGL 3.3 ou superior. O OpenGL 3.3 é suportado em placas gráficas da família Nvidia GeForce 400 (2010) ou mais recentes, AMD Radeon HD 5000 (2009) em diante e Intel HD Graphics a partir dos processadores Intel de 7ª geração (2012). Caso a sua placa de vídeo seja de uma geração a partir de 2012, provavelmente ela deve suportar OpenGL 3.3. Se não suportar, há a possibilidade de simular o processamento gráfico em software através do driver Gallium llvmpipe da biblioteca Mesa. Visualizando este site Parte do conteúdo deste site requer um navegador com suporte a WebGL 2.0. Para informações detalhadas sobre o suporte do seu navegador a WebGL 2.0, consulte o WebGL Report. Dica Para garantir a visualização correta do conteúdo WebGL 2.0, utilize a versão mais recente do Google Chrome ou Mozilla Firefox. Além disso, use o navegador em um computador desktop ou laptop. Embora o site funcione em tablets e smartphones, pode ser difícil interagir com o conteúdo WebGL nesses dispositivos. Dependendo das configurações de DPI utilizadas no sistema de janelas, podem ocorrer problemas de redimensionamento dos elementos de interface no Chrome e em navegadores baseados no Chromium, como o Microsoft Edge ou Brave. Por exemplo, o cubo exibido acima pode ser redimensionado e as arestas podem apresentar distorções, parecendo mais serrilhadas que o normal: No Chrome, isso pode ser resolvido iniciando o navegador com a opção /high-dpi-support=1 /force-device-scale-factor=1 na linha de comando, ou então incluindo essas opções no atalho, ou simplesmente ajustando o zoom. Importante O Apple Safari possui suporte a WebGL 2.0 a partir da versão 15, mas o desempenho das aplicações pode ser muito baixo quando comparado a outros navegadores. Consulte o suporte a WebGL 2.0 em diferentes navegadores em https://caniuse.com/webgl2. "],["config.html", "1 Configuração do ambiente", " 1 Configuração do ambiente Neste capítulo veremos como configurar o ambiente de desenvolvimento para realizar as atividades práticas no computador. Qualquer que seja a plataforma – Linux, macOS ou Windows – será necessário instalar as seguintes ferramentas e bibliotecas conforme as instruções que veremos nas próximas seções: CMake, para automatizar a geração de scripts de compilação e ligação de forma independente de plataforma. Emscripten, para compilar código C++ e gerar binário em WebAssembly de modo a executar as aplicações no navegador. Git, para clonar do GitHub o repositório do SDK do Emscripten e da biblioteca de desenvolvimento que usaremos na disciplina. Também é recomendável usar o Git para o controle de versão das atividades que serão feitas ao longo do quadrimestre. GLEW, para carregamento das funções da API gráfica OpenGL. Simple DirectMedia Layer (SDL) 2.0, para gerenciamento de dispositivos de vídeo, dispositivos de entrada, áudio, entre outros componentes de hardware. SDL_image 2.0, para leitura de arquivos de imagem. Precisaremos também de um compilador recente com suporte a C++20, como o GCC 11, Clang 13, ou MSVC 19. Acompanhe nas seções a seguir o passo a passo da instalação desses recursos de acordo com o sistema operacional utilizado: Seção 1.1 para instalação no Linux; Seção 1.2 para instalação no macOS; Seção 1.3 para instalar no Windows. Não é necessário usar um IDE ou editor específico de código-fonte para o desenvolvimento das atividades. A compilação pode ser disparada através de scripts de linha de comando. Entretanto, como um exemplo, veremos na seção 1.4 como fazer a configuração básica do Visual Studio Code para o desenvolvimento de aplicações C++ com CMake. Na seção 1.5 veremos como instalar o framework (ABCg) criado especialmente para esta disciplina. O framework será utilizado em todas as atividades do curso para facilitar o desenvolvimento das aplicações gráficas. Dica Preferencialmente, configure o ambiente de desenvolvimento no sistema operacional nativo. Entretanto, caso o seu computador tenha recursos de processamento e memória suficientes, é possível configurar todo o ambiente em uma máquina virtual. O VMware Workstation Player (Windows e Linux) e VMWare Fusion Player (macOS) possuem suporte a aceleração gráfica 3D e são adequados para desenvolver as atividades da disciplina. Tanto o VMWare Workstation Player quanto o Fusion Player podem ser utilizados gratuitamente através de uma licença de uso pessoal. O Windows Subsystem for Linux 2 (WSL 2) também suporta aceleração gráfica através do Direct3D 12. É possível também configurar o ambiente de desenvolvimento em um container Docker. Entretanto, o suporte a gráficos com aceleração de hardware em um container Docker exige o uso do Linux e do NVIDIA Container Toolkit, que só funciona com GPUs da NVIDIA. "],["linux.html", "1.1 Linux", " 1.1 Linux As ferramentas e bibliotecas necessárias para o desenvolvimento das atividades da disciplina estão disponíveis nos repositórios de pacotes das principais distribuições Linux. A seguir veremos como instalar os pacotes na versão para desktop do Ubuntu 22.04 LTS. Há pacotes equivalentes em outras distribuições. Em um terminal, execute os passos a seguir. Atualize o sistema: sudo apt update &amp;&amp; sudo apt upgrade Instale o pacote build-essential (GCC, GDB, Make, etc): sudo apt install build-essential Instale o CMake e Git: sudo apt install cmake git Instale as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0: sudo apt install libglew-dev libsdl2-dev libsdl2-image-dev Dica É recomendável instalar também as ferramentas Clang-Tidy e ClangFormat. Elas podem ser utilizadas em linha de comando ou no seu editor de código de preferência para fazer a formatação automática e linting de código C++. Instale o Clang-Tidy e ClangFormat com o seguinte comando: sudo apt install clang-tidy clang-format Outra ferramenta recomendada é o Ccache. Ela serve para acelerar a recompilação das atividades através da manutenção de um cache das compilações anteriores: Instale o pacote ccache: sudo apt install ccache Atualize os links simbólicos dos compiladores instalados: sudo /usr/sbin/update-ccache-symlinks Execute o comando a seguir para fazer com que o caminho do Ccache seja prefixado ao PATH sempre que um novo terminal for aberto: echo &#39;export PATH=&quot;/usr/lib/ccache:$PATH&quot;&#39; &gt;&gt; ~/.bashrc Reabra o terminal ou execute source ~/.bashrc. Para testar se o Ccache está ativado, execute o comando which g++. A saída deverá incluir o caminho /usr/lib/ccache/, como a seguir: /usr/lib/ccache/g++ Verificando o OpenGL O suporte ao OpenGL vem integrado no kernel do Linux através dos drivers de código aberto da biblioteca Mesa (drivers Intel/AMD/Nouveau). Para as placas da NVIDIA e AMD há a possibilidade de instalar os drivers proprietários do repositório nonfree (repositório restricted no Ubuntu), ou diretamente dos sites dos fabricantes: AMD ou NVIDIA. Os drivers proprietários, especialmente os da NVIDIA, geralmente têm desempenho superior aos de código aberto. Para verificar a versão do OpenGL suportada pelos drivers instalados, instale primeiro o pacote mesa-utils: sudo apt install mesa-utils Execute o comando: glxinfo | grep version O resultado deverá ser parecido com o seguinte: server glx version string: 1.4 client glx version string: 1.4 GLX version: 1.4 Max core profile version: 4.1 Max compat profile version: 4.1 Max GLES1 profile version: 1.1 Max GLES[23] profile version: 2.0 OpenGL core profile version string: 4.1 (Core Profile) Mesa 21.0.3 OpenGL core profile shading language version string: 4.10 OpenGL version string: 4.1 (Compatibility Profile) Mesa 21.0.3 OpenGL shading language version string: 4.10 OpenGL ES profile version string: OpenGL ES 2.0 Mesa 21.0.3 OpenGL ES profile shading language version string: OpenGL ES GLSL ES 1.0.16 Importante A versão em OpenGL version string ou OpenGL core profile version string deve ser 3.3 ou superior. Caso não seja, instale os drivers proprietários e certifique-se de que sua placa de vídeo tem os requisitos necessários para o OpenGL 3.3. Atualizando o GCC As atividades da disciplina farão uso de um framework de desenvolvimento que exige um compilador com suporte a C++20. Esse requisito é atendido se instalarmos uma versão recente do GCC, como o GCC 11 ou posterior. A versão padrão do GCC no Ubuntu 22.04 LTS é compatível com C++20. Então, se você seguiu as instruções de instalação até aqui usando o Ubuntu 22.04 ou uma versão mais recente, não é necessário atualizar o GCC. Caso sua versão do Ubuntu seja mais antiga que a 22.04, verifique no terminal a saída de g++ --version. Por exemplo, no Ubuntu 20.04 o GCC padrão é o 9.3, e a saída será assim: g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Neste caso é necessário instalar uma versão mais recente. Para isso, adicione primeiro o PPA ubuntu-toolchain-r/test: sudo apt install software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test Agora podemos instalar o GCC 11: sudo apt install gcc-11 g++-11 A instalação do GCC 11 não substitui a versão mais antiga já instalada. Entretanto, é necessário criar links simbólicos de gcc e g++ para a versão mais recente. Uma forma simples de fazer isso é através do comando update-alternatives. Primeiro, execute o comando a seguir para definir um valor de prioridade (neste caso, 100) para o GCC 11: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11 Use o comando a seguir para definir um valor de prioridade mais baixo (por exemplo, 90) para a versão anterior do GCC, que neste exemplo é a versão 9: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 Agora, execute o comando a seguir para escolher qual versão do GCC instalada será utilizada por padrão: sudo update-alternatives --config gcc Na lista de versões instaladas, selecione o GCC 11 caso ainda não esteja selecionado. Isso criará os links simbólicos. Para testar se a versão correta do GCC está sendo utilizada, execute g++ --version novamente. A saída deverá ser parecida com a seguinte: g++ (Ubuntu 11.2.0-1ubuntu1~20.04) 11.2.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Instalando o Emscripten Vá a um diretório onde queira instalar o SDK do Emscripten. Por exemplo, o diretório home: cd Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.18 (d5ca9bba6513763d5bdddbd0efff759332bd85d7) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante O passo 6 precisa ser refeito sempre que um novo terminal for aberto. Para não precisarmos fazer isso manualmente, automatize o processo com o seguinte comando: echo &#39;source &quot;./emsdk/emsdk_env.sh&quot; &gt; /dev/null 2&gt;&amp;1&#39; &gt;&gt; ~/.bashrc Isso adiciona source \"./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1\" ao final de ~/.bashrc, fazendo com que o script emsdk_env.sh seja executado automaticamente toda vez que um novo terminal for aberto (ajuste o caminho caso o Emscripten não esteja em ~/emsdk). Uma curiosidade: o trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e o erro padrão (stderr), isto é, o script é executado em modo silencioso. "],["macos.html", "1.2 macOS", " 1.2 macOS Em um terminal, execute os passos a seguir: Execute o comando gcc. Se o GCC não estiver instalado, aparecerá uma caixa de diálogo solicitando a instalação das ferramentas de desenvolvimento de linha de comando. Clique em “Install”. Esse procedimento também instalará outras ferramentas, como o Make e Git. Para verificar se o GCC foi instalado, execute gcc --version. A saída deverá ser parecida com a seguinte (note que o GCC é apenas um atalho para o Apple Clang): Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1 Apple clang version 12.0.0 (clang-1200.0.32.28) Target: x86_64-apple-darwin19.6.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin Se o procedimento acima não funcionar (as instruções acima foram testadas no macOS Catalina), baixe o Command Line Tools for Xcode usando sua conta de desenvolvedor do Apple Developer, ou execute xcode-select --version no terminal. Em versões mais antigas do macOS pode ser necessário instalar o Xcode. Para instalar os demais pacotes de bibliotecas e ferramentas, instale o Homebrew com o seguinte comando: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Instale o CMake: brew install cmake Instale o GLEW, SDL 2.0 e SDL_image 2.0: brew install glew brew install sdl2 brew install sdl2_image Está pronto! Dica Opcionalmente, instale o Ccache para acelerar a recompilação das atividades: Instale o Ccache usando o Homebrew: brew install ccache Anote a saída de echo $(brew --prefix) (por exemplo, /usr/local). Abra o modo de edição do PATH: sudo nano /etc/paths Insira como primeira linha o caminho $(brew --prefix)/opt/ccache/libexec, onde $(brew --prefix) é a saída do passo 2. Por exemplo, /usr/local/opt/ccache/libexec. Salve (Ctrl+X e Y) e reinicie o terminal. Para testar, digite which gcc. A saída deverá ser um caminho que inclui o Ccache, como a seguir: /usr/local/opt/ccache/libexec/gcc Instalando o Emscripten Vá ao diretório home: cd Clone o repositório do SDK do Emscripten: git clone https://github.com/emscripten-core/emsdk.git Entre no diretório recém-criado: cd emsdk Baixe e instale o SDK atualizado (latest): ./emsdk install latest Ative o SDK latest para o usuário atual: ./emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: source ./emsdk_env.sh Execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.18 (d5ca9bba6513763d5bdddbd0efff759332bd85d7) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal. Como alternativa, insira o comando a seguir na última linha de ~/.zshrc (se estiver usando o shell Zsh no macOS Catalina ou posterior) ou ~/.bashrc (se estiver usando o shell Bash em versões anteriores) para que o script seja executado automaticamente toda vez que um terminal for aberto: source ./emsdk/emsdk_env.sh &gt; /dev/null 2&gt;&amp;1 O trecho &gt; /dev/null 2&gt;&amp;1 serve para omitir a saída padrão (stdout) e o erro padrão (stderr). "],["windows.html", "1.3 Windows", " 1.3 Windows Para a instalação das ferramentas e bibliotecas de desenvolvimento no Windows, utilizaremos o MSYS2. MSYS2 é um ambiente de terminal tipo Unix com acesso a um repositório de ferramentas e bibliotecas de desenvolvimento de aplicações nativas em Windows através do gerenciador de pacotes pacman. Essas ferramentas e bibliotecas incluem o CMake, Git, GLEW, SDL 2.0 e SDL_image 2.0, entre outras que vamos utilizar na disciplina. O MSYS2 também permite instalar o MinGW-W64. Com isso conseguiremos usar o compilador GCC para gerar binários nativos para o Windows 64 bits e poderemos também usar o GDB para depurar nossos programas. Por que não o Visual Studio? Poderíamos instalar as ferramentas de desenvolvimento sem precisar do MSYS2. O Visual Studio é uma alternativa ao GCC e tem a vantagem de ser um IDE completo. A desvantagem é que a instalação das dependências é mais complexa, intrusiva e suscetível a incompatibilidades decorrentes das diferentes configurações do Windows. Daremos preferência ao MSYS2 pois a instalação é mais simples e a experiência de uso das ferramentas é mais parecida com a das outras plataformas. Siga os passos a seguir para instalar o MSYS2 e as ferramentas/bibliotecas de desenvolvimento: Baixe o instalador de https://www.msys2.org, execute-o e siga as instruções de instalação. Abra o shell do MSYS2 (aplicativo “MSYS2 MSYS” no menu Iniciar) e execute o seguinte comando: pacman -S git mingw-w64-x86_64-ccache mingw-w64-x86_64-cmake mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb mingw-w64-x86_64-ninja mingw-w64-x86_64-glew mingw-w64-x86_64-SDL2 mingw-w64-x86_64-SDL2_image Isso instalará as ferramentas Git, Ccache, CMake, Ninja (o Ninja substitui o GNU Make no Windows), GCC e GDB (do MinGW-W64), e as bibliotecas GLEW, SDL 2.0 e SDL_image 2.0. Opcionalmente, instale o pacote mingw-w64-x86_64-glslang com o comando a seguir. Isso instalará a ferramenta glslangValidator que poderá ser usada como linter de código na linguagem GLSL (linguagem de shader que será utilizada na disciplina): pacman -S mingw-w64-x86_64-glslang Ao terminar a instalação, feche o shell do MSYS2. Baixe o instalador do Python 3 para Windows 64 bits e execute-o. No restante do texto usaremos como exemplo a versão 3.10, mas qualquer versão a partir da 3.6 é suportada. Durante a instalação, certifique-se de ativar a opção “Add Python 3.10 to PATH”. Após a instalação, abra o Prompt de Comando (cmd.exe) e execute o comando where python para exibir os diferentes caminhos em que o python é alcançado pela variável de ambiente Path. O resultado deverá ser parecido com o seguinte: C:\\&gt;where python C:\\Users\\ufabc\\AppData\\Local\\Programs\\Python\\Python310\\python.exe C:\\Users\\ufabc\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe O primeiro caminho exibido deve ser o caminho do executável do Python que acabou de ser instalado. Neste caso está correto, pois o Python foi instalado no local padrão do instalador que é %LocalAppData%\\Programs\\Python\\Python310 (neste exemplo, o nome do usuário é ufabc). No menu inicial, procure por “Editar variáveis de ambiente para a sua conta”. Edite a variável Path do usuário atual e inclua os caminhos para mingw64\\bin e usr\\bin do MSYS2. Por exemplo, se o MSYS2 foi instalado em C:\\msys64, inclua os seguintes caminhos no Path1: C:\\msys64\\usr\\bin C:\\msys64\\mingw64\\bin Edite a ordem dos caminhos de tal forma que os caminhos do Python sejam listados antes dos caminhos do MSYS2, como mostra a figura a seguir. Essa ordem é importante pois o MSYS2 também instala o Python 3, mas queremos usar o Python do Windows: Para testar se o MSYS2 foi instalado corretamente, abra o Prompt de Comando e execute o comando g++ --version. A saída deverá ser parecida com a seguinte: g++ (Rev5, Built by MSYS2 project) 12.1.0 Copyright (C) 2022 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Verificando o OpenGL O suporte ao OpenGL vem integrado ao Windows. Apenas certifique-se de instalar os drivers mais recentes da sua placa de vídeo. Instalando o Emscripten Abra o Prompt de Comando em algum caminho onde queira instalar a pasta do SDK do Emscripten (por exemplo, C:\\). Note the o terminal deve ser o Prompt de Comando e não o PowerShell. Clone o repositório do SDK: git clone https://github.com/emscripten-core/emsdk.git Entre na pasta recém-criada: cd emsdk Baixe e instale o SDK atualizado (latest): emsdk install latest Ative o SDK latest para o usuário atual: emsdk activate latest Configure as variáveis de ambiente e PATH do compilador para o terminal atual: emsdk_env.bat Para testar se a instalação foi bem-sucedida, execute o comando emcc --version. A saída deverá ser parecida com a seguinte: emcc (Emscripten gcc/clang-like replacement + linker emulating GNU ld) 3.1.18 (d5ca9bba6513763d5bdddbd0efff759332bd85d7) Copyright (C) 2014 the Emscripten authors (see AUTHORS.txt) This is free and open source software under the MIT license. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Importante Refaça o passo 6 sempre que abrir um terminal ou execute o comando a seguir para registrar permanentemente as variáveis de ambiente no registro do sistema: emsdk_env.bat --permanent Caso não queira modificar o registro do sistema, crie um atalho para cmd.exe e use a opção /k para executar o arquivo emsdk_env.bat sempre que o Prompt de Comando for aberto. Por exemplo: cmd.exe /k &quot;C:\\emsdk\\emsdk_env.bat&quot; Mude esse caminho de acordo com o local onde emsdk_env.bat foi instalado. A variável Path existe tanto nas variáveis do usuário quanto nas variáveis do sistema. A modificação do Path do usuário geralmente é suficiente para uma instalação correta. Se ocorrer algum erro ao seguir as instruções de construção do “Hello, World!” na seção 1.5 (erro de construção ou de DLL não encontrada), experimente alterar o Path do sistema ao invés do usuário.↩︎ "],["vscode.html", "1.4 Visual Studio Code", " 1.4 Visual Studio Code Para desenvolver as atividades não é necessário usar um IDE ou editor em particular. Os códigos podem ser escritos em qualquer editor de texto não formatado e a compilação pode ser feita em linha de comando. Entretanto, é recomendável utilizar um editor/IDE como o CLion, Emacs, Sublime Text, Vim, Visual Studio Code, ou outro semelhante que seja capaz de oferecer funcionalidades de preenchimento automático de código, detecção de erros, ajuda sensível ao contexto e integração de construção com o CMake. A seguir veremos como configurar o Visual Studio Code (VS Code) para deixá-lo pronto para o desenvolvimento das atividades. O procedimento é bem simples e é o mesmo no Linux, macOS e Windows: Instale o VS Code através do instalador disponível em https://code.visualstudio.com/ ou através do gerenciador de pacotes de seu sistema operacional. Abra o editor e, na janela de extensões (Ctrl+Shift+X), instale a extensão C/C++ Extension Pack. Isso já é o suficiente para começarmos a trabalhar. Ainda precisaremos de algumas configurações extras para habilitar a depuração de código, mas veremos isso na seção a seguir (1.5). Caso você queira usar outro editor ou IDE, consulte a documentação específica de seu editor sobre como fazer a integração com o CMake e sobre como usar o GDB/LLDB para depurar código. Importante Qualquer que seja o IDE/editor utilizado, certifique-se de que o CMake e GCC estejam instalados e visíveis no PATH de acordo com as instruções mostradas nas seções anteriores. Dicas Em sistemas que possuem as ferramentas extras do Clang para linting e formatação de código como o Clang-Tidy e ClangFormat, é possível instalar extensões para fazer análise estática em tempo real e formatar o código automaticamente sempre que um arquivo é salvo. Uma dessas extensões é o clangd, baseado no servidor de mesmo nome do LLVM. Como vimos na seção 1.1, o Clang-Tidy e ClangFormat podem ser instalados no Ubuntu com os seguintes comandos: sudo apt install clang-tidy clang-format No Windows com MSYS2, abra o shell do MSYS2 e execute o seguinte comando: pacman -S mingw-w64-x86_64-clang-tools-extra Para ativar a formatação automática de código sempre que o arquivo for salvo, abra o arquivo JSON contendo as configurações do usuário do VS Code (opção “Preferences: Open User Settings (JSON)” da paleta de comandos) e adicione a chave \"editor.formatOnSave\": true. Para a análise estática em tempo real de código GLSL, instale a extensão GLSL Lint. Isso ajudará a evitar erros de sintaxe comuns na programação dos shaders em GLSL. Essa extensão instala também o Shader languages support for VS Code para habilitar o syntax highlighting e autocomplete do código GLSL. Explore o Visual Studio Marketplace para instalar outras extensões de sua preferência. Por exemplo, se você é fã do Emacs, instale o VSCodeEmacs para emular as teclas do Emacs no VS Code. Se você prefere o Vim, instale o VSCodeVim. E se você só quer relaxar um pouco, instale o Chrome Dinosaur Game! "],["abcg.html", "1.5 ABCg", " 1.5 ABCg Para facilitar o desenvolvimento das atividades práticas utilizaremos a biblioteca ABCg desenvolvida especialmente para esta disciplina. A ABCg permite a prototipagem rápida de aplicações gráficas interativas 3D em C++ capazes de rodar tanto no desktop (binário nativo) quanto no navegador (binário WebAssembly). Internamente a ABCg utiliza a biblioteca SDL para gerenciar o acesso a dispositivos de entrada (mouse/teclado/gamepad) e saída (vídeo e áudio) de forma independente de plataforma, e a biblioteca GLEW para acesso às funções da API gráfica OpenGL. Além disso, a API do Emscripten é utilizada sempre que a aplicação é compilada para gerar binário WebAssembly. A ABCg é mais propriamente um framework do que uma biblioteca, pois ela gerencia o fluxo de trabalho da aplicação. Nosso código deverá ser construído em torno da estrutura fornecida pela ABCg. Do ponto de vista do desenvolvedor, essa estrutura é apenas uma leve camada de abstração das APIs utilizadas. Por exemplo, é possível acessar diretamente as funções da API gráfica OpenGL. De fato, faremos isso na maior parte das vezes. Outras bibliotecas também utilizadas e que podem ser acessadas diretamente são: CPPIterTools: para o suporte a laços range-based em C++ usando funções do tipo range, enumerate e zip similares às da linguagem Python; Dear ImGui: para gerenciamento de widgets de interface gráfica do usuário, tais como janelas, botões e caixas de edição; {fmt}: como alternativa mais eficiente ao stdio da linguagem C (printf, scanf, etc) e iostreams do C++ (std::cout, std::cin, etc), e para formatação de strings com uma sintaxe similar às f-strings do Python; Guidelines Support Library (GSL): para uso de funções e tipos de dados recomendados pelo C++ Core Guidelines; OpenGL Mathematics (GLM): para suporte a operações de transformação geométrica com vetores e matrizes; tinyobjloader: para a leitura de modelos 3D no formato Wavefront OBJ. A seguir veremos como instalar e compilar a ABCg junto com um exemplo de uso. Instalação Em um terminal, clone o repositório do GitHub: git clone https://github.com/hbatagelo/abcg.git Observação A release mais recente da ABCg também pode ser baixada como um arquivo compactado de https://github.com/hbatagelo/abcg/releases/latest. Atenção No Windows, certifique-se de clonar/descompactar o repositório em um diretório cujo nome não contenha espaços ou caracteres especiais. Por exemplo, clone/descompacte em C:\\cg em vez de C:\\computação gráfica. O repositório tem a estrutura mostrada a seguir. Para simplificar, os arquivos e subdiretórios .git* foram omitidos: abcg │ .clang-format │ .clang-tidy │ build.bat │ build.sh | build-vs.bat │ build-wasm.bat │ build-wasm.sh | CHANGELOG.md │ CMakeLists.txt | DockerFile │ LICENSE │ README.md │ runweb.bat │ runweb.sh │ └───abcg │ │ ... │ └───cmake │ │ ... │ └───examples │ │ ... │ └───public │ ... Os arquivos .clang-format e .clang-tidy são arquivos de configuração utilizados pelas ferramentas ClangFormat e Clang-Tidy caso estejam instaladas. Os arquivos build.* são scripts de compilação em linha de comando. Note que há scripts correspondentes com extensão .bat para usar no Prompt de Comando do Windows 2: build.sh/build.bat: para compilar a ABCg e os exemplos em binários nativos usando o compilador padrão (no nosso caso, o GCC); build-wasm.sh/build-wasm.bat: similar ao build.sh, mas para gerar binário em WebAssembly dentro do subdiretório public; build-vs.bat: similar ao build.bat, mas usando o compilador do Visual Studio 2022 ao invés do GCC do MSYS2. O arquivo CMakeLists.txt é o script de compilação do CMake. Os arquivos runweb.sh e runweb.bat podem ser usados para criar um servidor web local para servir o conteúdo de public. Os subdiretórios são os seguintes: abcg contém o código-fonte da ABCg e suas dependências. cmake contém scripts auxiliares de configuração do CMake. examples contém um exemplo de uso da ABCg: o “Hello, World!”; public contém os códigos HTML para exibir o “Hello, World!” no navegador. Observação O “Hello, World!” pode usar tanto a API gráfica OpenGL (código-fonte em examples/opengl) quanto a API gráfica Vulkan (código-fonte em examples/vulkan). Nesta disciplina usaremos o OpenGL em todas as atividades. Em particular, usaremos o OpenGL 3.3. Assim poderemos possível construir aplicações para desktop e web usando o mesmo código-fonte. Por este motivo, os scripts CMake da ABCg estão configurados para usar o OpenGL por padrão. Compilando na linha de comando Execute o script build.sh (Linux/macOS) ou build.bat (Windows) para iniciar o processo de configuração e construção da versão OpenGL do “Hello, World!”. A saída será similar a esta (o exemplo a seguir mostra a saída no Ubuntu): -- The C compiler identification is GNU 11.2.0 -- The CXX compiler identification is GNU 11.2.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/lib/ccache/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/lib/ccache/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done Using ccache -- Found OpenGL: /usr/lib/x86_64-linux-gnu/libOpenGL.so -- Found GLEW: /usr/include (found version &quot;2.2.0&quot;) -- Looking for pthread.h -- Looking for pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success -- Found Threads: TRUE -- Found SDL2: /usr/lib/x86_64-linux-gnu/libSDL2main.a;/usr/lib/x86_64-linux-gnu/libSDL2.so -- Found SDL2_image: /usr/lib/x86_64-linux-gnu/libSDL2_image.so -- Configuring done -- Generating done -- Build files have been written to: /home/ufabc/abcg/build ... [100%] Linking CXX executable ../../bin/helloworld [100%] Built target helloworld Ao final, os binários estarão disponíveis no subdiretório build. A biblioteca estática estará em build/abcg/libabcg.a e o executável do exemplo “Hello, World!” estará em build/bin/helloworld. Para testar, execute o helloworld. No Linux/macOS: ./build/bin/helloworld/helloworld No Windows: .\\build\\bin\\helloworld\\helloworld.exe | cat Importante No Windows, a saída deve sempre ser redirecionada para cat ou tee. Se isso não for feito, nenhuma saída de texto será exibida no terminal. Isso se deve a um bug do MSYS2. Observação Observe o conteúdo de build.sh (build.bat contém instruções equivalentes): #!/bin/bash set -euo pipefail BUILD_TYPE=${1:-Release} CMAKE_EXTRA_ARGS=${2:-&quot;&quot;} # Reset build directory rm -rf build mkdir -p build &amp;&amp; cd build # Configure cmake -DCMAKE_BUILD_TYPE=&quot;$BUILD_TYPE&quot; &quot;$CMAKE_EXTRA_ARGS&quot; .. # Build if [[ &quot;$OSTYPE&quot; == &quot;darwin&quot;* ]]; then # macOS NUM_PROCESSORS=&quot;$(sysctl -n hw.ncpu)&quot; else NUM_PROCESSORS=&quot;$(nproc)&quot; fi cmake --build . --config &quot;$BUILD_TYPE&quot; -- -j &quot;$NUM_PROCESSORS&quot; A variável BUILD_TYPE é Release por padrão, mas pode ser modificada passando a string Debug, MinSizeRel ou RelWithDebInfo como primeiro argumento de build.sh. A opção Debug ou RelWithDebInfo é utilizada quando queremos gerar símbolos de depuração. O modo Release (padrão) ou MinSizeRel devem ser usados quando queremos gerar um binário otimizado e sem arquivos de símbolos de depuração. Em particular, Release otimiza em favor do código mais rápido, enquanto MinSizeRel otimiza em favor do binário de menor tamanho. build.sh/build.bat também aceita um segundo argumento que corresponde a uma string a ser passada como argumento do CMake durante a configuração do projeto. Por exemplo, para compilar no modo Release e com a API gráfica OpenGL podemos executar build.sh Release -DGRAPHICS_API=OpenGL (neste caso basta executar build.sh, pois essas já são as configurações padrão). Observe que o script apaga o subdiretório build antes de criá-lo novamente. Portanto, não salve arquivos dentro de build pois eles serão apagados na próxima compilação! A geração dos binários usando o CMake é composta de duas etapas: a configuração (cmake -DCMAKE_BUILD_TYPE=$BUILD_TYPE ..) e a construção (cmake --build . --config $BUILD_TYPE). A configuração gera os scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja). A construção dispara a compilação e ligação usando tais scripts. Todos os arquivos gerados na configuração e construção ficam armazenados no subdiretório build. Compilando no VS Code Primeiramente, apague o subdiretório build caso você já tenha compilado a ABCg via linha de comando na seção anterior. No VS Code, selecione o menu “File &gt; Open Folder…” e abra a pasta abcg. Na primeira vez que a pasta é aberta aparecerá uma caixa de mensagem perguntando se você confia nos autores do projeto. Esperamos que você selecione “Yes” : No canto inferior direito da janela aparecerá uma notificação perguntando se você quer configurar o projeto. Selecione “Yes”. Ao fazer isso, será feita uma varredura no sistema para identificar os compiladores e toolchains disponíveis. Uma lista dos “kits” de compilação encontrados aparecerá na parte superior da janela. O exemplo a seguir é o resultado exibido no Ubuntu: Selecione um kit compatível, como o GCC 11.2 ou mais recente. Ao fazer isso, o CMake iniciará o processo de configuração do projeto. Esse processo gera, dentro de um subdiretório build, os arquivos que serão utilizados pelo sistema de construção nativo. Caso queira invocar manualmente a configuração do CMake, acesse a paleta de comandos (Ctrl+Shift+P) e digite a opção “CMake: Configure”. Se aparecer uma notificação pedindo para configurar o projeto sempre que ele for aberto, selecione “Yes”: Após o término da configuração, é possível que apareça uma outra notificação solicitando permissão para configurar o Intellisense. Selecione “Allow”. Além disso, pode aparecer também uma notificação sobre o uso do arquivo compile_commands.json, como mostrado a seguir. Selecione “Yes” novamente: O arquivo compile_commands.json é gerado automaticamente pelo CMake. Ele contém os comandos de compilação e o caminho de cada unidade de tradução utilizada no projeto. O IntelliSense (ou clangd) utiliza as informações desse arquivo para habilitar as referências cruzadas. Importante A construção dos projetos usando o CMake é feita em duas etapas: Configuração: consiste na geração dos scripts do sistema de compilação nativo (por exemplo, arquivos Makefile ou Ninja); Construção: consiste no disparo da compilação e ligação usando os scripts gerados na configuração, além da execução de etapas de pré e pós-construção definidas nos scripts dos arquivos CMakeLists.txt. Tanto os arquivos da configuração quanto os da construção (binários) são gravados no subdiretório build. Geralmente a configuração só precisa ser feita uma vez e depois refeita caso o subdiretório build tenha sido apagado, ou após a alteração do kit de compilação, ou ainda após a alteração do build type (por exemplo, de Debug para Release). As informações de configuração ficam armazenadas em um arquivo CMakeCache.txt dentro de build. Como indicado na figura abaixo, na barra de status há botões para selecionar o build type/configurar, selecionar o kit de compilação, e construir a aplicação. A opção de construir já se encarrega de configurar o projeto caso os arquivos de configuração ainda não tenham sido gerados. Essas opções também estão disponíveis na paleta de comandos. Os comandos são: “CMake: Select Variant”: para selecionar um build type; “CMake: Select a Kit”: para selecionar um kit de compilação; “CMake: Configure”: para configurar o projeto usando o kit e o build type atual; “CMake: Build”: para construir o projeto. “CMake: Clean Rebuild”: para apagar as configurações anteriores do CMake, reconfigurar e reconstruir o projeto. Observação Os build types permitidos no CMake são: Debug para gerar binários não otimizados e com arquivos de símbolos de depuração. Esse é o build type padrão (ao contrário de build.sh e build.bat, que usam o modo Release por padrão); RelWithDebInfo para gerar arquivos de símbolos de depuração com binários otimizados; Release para gerar binários otimizados e favorecer código mais rápido. Essa opção não gera os arquivos de símbolos de depuração; MinSizeRel, semelhante ao Release, mas a otimização tenta gerar binário de menor tamanho. Para compilar e gerar os binários, tecle F7 ou clique em “Build” na barra de status. O progresso será exibido na janela “Output” do CMake/Build. Se a construção terminar com sucesso, a última linha de texto da janela Output será: [build] Build finished with exit code 0 Os arquivos gerados na construção ficam armazenados no subdiretório build, da mesma forma como ocorre na compilação via linha de comando. Para testar, abra um terminal e execute ./build/bin/helloworld/helloworld (Linux/macOS) ou .\\build\\bin\\helloworld\\helloworld.exe (Windows). Atenção A configuração do CMake gerada a partir do VS Code não é necessariamente a mesma gerada usando os scripts de linha de comando: o compilador pode ser diferente, ou o build type pode ser diferente. Se em algum momento você construir o projeto via linha de comando usando os scripts .sh ou .bat e depois quiser construir pelo editor, apague o subdiretório build antes de retornar ao VS Code. Isso forçará uma nova configuração do CMake e evitará erros de incompatibilidade entre as configurações. Na primeira vez que um arquivo com extensão .cpp for aberto no editor, algumas notificações poderão aparecer. Experimente abrir, por exemplo, examples\\helloworld\\main.cpp. Se a extensão clangd foi instalada como sugerido na seção 1.4, a caixa de mensagem a seguir será exibida. Selecione “Install”. A seguinte mensagem também poderá aparecer. Neste caso, selecione “Disable IntelliSense”: Ao final dessas configurações, reinicie o editor. Isso pode ser feito rapidamente selecionando a opção “Developer: Reload Window” da paleta de comandos. Depurando no VS Code Podemos depurar facilmente nossas aplicações com GDB ou LLDB usando a interface do VS Code. Após construir um projeto com build type Debug ou RelWithDebInfo, devemos abrir um de seus arquivos com extensão .cpp. Isso é necessário para fazer com que o VS Code identifique que queremos configurar a depuração de um projeto em linguagem C++. Por exemplo, para configurar a depuração do projeto “Hello, World!”, abra o arquivo examples\\helloworld\\main.cpp. Selecione a opção “Run and Debug” na barra de atividades (Ctrl+Shift+D). Em seguida, clique na opção “create a launch.json file”: Se aparecer um drop-down list de seleção do ambiente de depuração, selecione “C++ (GDB/LLDB)”. Isso criará o arquivo launch.json. O arquivo também será aberto no editor. Copie e cole em launch.json o conteúdo exibido a seguir. Este é uma exemplo para depurar o “Hello, World!” no Linux ou macOS usando o GDB. Um exemplo de configuração para o Windows é mostrado mais adiante: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${fileDirname}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;Set Disassembly Flavor to Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Observe que o valor da chave program aponta para o executável do projeto: ${workspaceFolder}/build/bin/helloworld/helloworld. Observação ${workspaceFolder} é uma variável pré-definida do VS Code que contém o caminho da pasta do projeto. Consulte a documentação para informações sobre outras variáveis disponíveis. A listagem a seguir mostra um exemplo de launch.json para depurar o “Hello, World!” no Windows usando o GDB do MSYS2: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/build/bin/helloworld/helloworld.exe&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;miDebuggerPath&quot;: &quot;C:\\\\msys64\\\\mingw64\\\\bin\\\\gdb.exe&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;Set Disassembly Flavor to Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ] } ] } Veja que o valor da chave miDebuggerPath contém o caminho completo do GDB, que é C:\\msys64\\mingw64\\bin\\gdb.exe supondo que o MSYS2 tenha sido instalado em C:\\msys64. O valor da chave externalConsole pode ser modificado para true caso você prefira que um novo terminal seja aberto durante a depuração. Consulte a documentação sobre depuração para informações sobre outras opções e informações gerais sobre como depurar código no editor. Após modificar o arquivo launch.json, selecione novamente a opção “Run” na barra de atividades ou aperte F5 para iniciar o programa no modo de depuração. Reedite o arquivo launch.json sempre que mudar o nome do executável que você queira depurar. Observação No VS Code para Windows, configure o terminal padrão para “Command Prompt” no lugar de “PowerShell”, uma vez que nossos scripts são compatíveis apenas com o Prompt de Comando. Para fazer isso, abra a paleta de comandos (Ctrl+Shift+P), acesse o comando “Terminal: Select Default Profile” e então selecione “Command Prompt”. Compilando para WASM Podemos compilar as aplicações ABCg para WebAssembly (WASM) de modo a executá-las diretamente no navegador. A construção é feita via linha de comando usando o toolchain Emscripten. Acompanhe a seguir como construir o “Hello, World!” para WASM e como testá-lo no navegador: Em um terminal (shell ou Prompt de Comando), ative as variáveis de ambiente do Emscripten (script emsdk_env.sh/emsdk_env.bat do SDK). Após isso, o compilador emcc deverá estar visível no PATH; No diretório abcg, execute build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Isso fará com que o CMake inicie a configuração do projeto e a construção dos binários. Os arquivos resultantes serão gravados em abcg/public. Em nosso caso, esses arquivos são helloworld.data (arquivo de dados/assets), helloworld.js (arquivo JavaScript) e helloworld.wasm (binário WebAssembly); Execute o script runweb.sh (Linux/macOS) ou runweb.bat (Windows) para rodar um servidor web local. O conteúdo de public ficará disponível em http://localhost:8080/; Abra a página http://localhost:8080/helloworld.html que chama o script helloworld.js recém-criado. A página HTML não faz parte do processo de construção e foi criada previamente. O resultado será semelhante ao exibido a seguir: uma aplicação mostrando um triângulo colorido e uma caixa de diálogo com alguns controles de interface. A pequena janela de texto abaixo da janela da aplicação é uma área de texto em HTML que mostra o conteúdo do terminal. Aqui, são exibidas algumas informações sobre o OpenGL (versão utilizada, fornecedor do driver, etc). Observação O subdiretório public contém, além do helloworld.html: full_window.html: para exibir o “Hello, World!” ocupando a janela inteira do navegador; full_window_console.html: idêntico ao anterior, mas com a sobreposição das mensagens do console na tela. Nos próximos capítulos veremos como construir novas aplicações usando a ABCg. Revisão de C++ Se você está mais habituado com programação em C ou C++ anterior ao C++11, o código das atividades da disciplina usando a ABCg poderá parecer pouco familiar. Revisaremos nesta seção alguns dos conceitos de C++ que podem gerar dúvidas em programadores que vieram da linguagem C, como a conversão explícita de tipos usando named casts, inicialização uniforme com {} e uso da palavra-chave auto. A aplicação desses conceitos segue as boas práticas de programação indicadas no C++ Core Guidelines. Dica Aproveite as primeiras semanas de aula para se familiarizar com os conceitos do chamado “C++ moderno” (C++11 em diante). Isso facilitará o entendimento do código da ABCg nos próximos capítulos. Uma referência rápida (cheatsheet) ao C++ moderno está disponível em https://github.com/AnthonyCalandra/modern-cpp-features. Um excelente livro é o A Tour of C++, de Bjarne Stroustrup. Há também recursos gratuitos como os sites learncpp.com e tutorialspoint.com. A documentação da Microsoft sobre C++ é uma opção em português. Há uma referência sobre a linguagem C++ e sobre a biblioteca C++ padrão. Consulte também o C++ Core Guidelines para ficar a par das boas práticas de programação. Uma referência mais completa e aprofundada da linguagem está disponível em cppreference.com. Algumas partes estão traduzidas para o português. Named casts Observe os seguintes exemplos de conversões explícitas de tipos (casts) usando a sintaxe tradicional oriunda da linguagem C: int a = (int)7.9; // double para int float b = (float)1 / 3; // int para float unsigned *pa = (unsigned *)&amp;a; // int* para unsigned* void foo(Base *b) { Derived *pd = (Derived *)b; // Base* para Derived* // ... } Estes casts “estilo C” têm uma sintaxe concisa, mas nem sempre traduzem de forma clara a intenção do programador. Além disso, a conversão é feita por conta e risco do programador. Não há verificação da validade da conversão, por exemplo, na conversão de Base* (ponteiro de uma classe base) para Derived* (ponteiro para uma classe derivada de Base). C++ procura tornar as conversões explícitas mais expressivas e seguras através de quatro tipos de casts chamados de named casts: static_cast para conversões entre tipos básicos cuja validade pode ser verificada em tempo de compilação (por exemplo, float para int e vice-versa); const_cast para conversões entre tipos de diferentes qualificações (por exemplo, de int const para int); reinterpret_cast para conversões entre tipos representados por diferentes padrões de bits na memória (por exemplo, int* para char*); dynamic_cast para conversões de ponteiros e referências a objetos com polimorfismo dinâmico. Os exemplos anteriores podem ser escritos assim com named casts: int a = static_cast&lt;int&gt;(7.9); // double para int float b = static_cast&lt;float&gt;(1) / 3; // int para float unsigned *pa = reinterpret_cast&lt;unsigned *&gt;(&amp;a); // int* para unsigned* void foo(Base *b) { // Não compila se Base não for uma classe polimórfica Derived *pd = dynamic_cast&lt;Derived *&gt;(b); // Base* para Derived* // ... } A sintaxe é mais verbosa, mas a intenção do programador fica mais clara e o código torna-se mais seguro. Por exemplo, dynamic_cast retorna nulo se b (ponteiro para Base) não puder ser convertido para Derived* (por exemplo, se o objeto apontado por b é de uma classe incompatível com Derived). Esse erro passaria despercebido com o cast estilo C. O ideal é usarmos a menor quantidade possível de casts. Em geral só precisaremos usar static_cast, e reinterpret_cast nas atividades do curso. Entretanto, os códigos das atividades vão um pouco mais além e seguem a diretriz Pro.safety do C++ Core Guidelines. Essa diretriz recomenda o uso de gsl::narrow_cast e gsl::narrow no lugar de static_cast para conversões com estreitamento: gsl::narrow_cast é o mesmo que static_cast. gsl::narrow é o mesmo que static_cast quando não há perda de informação na conversão. Se houver perda de informação, ocorre um erro de compilação. Conversões com estreitamento são aquelas em que existe a possiblidade de perda de informação (por exemplo, float para int, ou double para float). Considere o seguinte exemplo de substituição de static_cast por gsl::narrow em uma conversão com estreitamento: // float x = static_cast&lt;float&gt;(7.9); // Antes: OK float x = gsl::narrow&lt;float&gt;(7.9); // Depois: intenção mais clara Poderíamos usar static_cast, mas o uso de gsl::narrow deixa nossa intenção mais clara: esperamos que a conversão de 7.9 (um double) para float não perca informação. Afinal, float tem precisão suficiente para representar uma casa decimal. Se por algum motivo houver perda de informação, gsl::narrow gerará um erro de compilação e então saberemos que algo está errado3. Agora considere o seguinte exemplo de substituição de static_cast por gsl::narrow_cast: // int x = static_cast&lt;int&gt;(7.9f); // Antes: OK int x = gsl::narrow_cast&lt;int&gt;(7.9f); // Depois: intenção mais clara Nesse caso também poderíamos usar static_cast, mas gsl::narrow_cast deixa nossa intenção mais clara: esperamos que a conversão de 7.9f (um float) para um int perca informação. Afinal, a parte fracionária será perdida. gsl::narrow_cast indica que estamos ciente dessa perda e que isso não é um problema para nós. Em resumo, em conversões com estreitamento, isto é, com possível perda de informação, o uso de gsl::narrow e gsl::narrow_cast mostra melhor a intenção do programador. Ademais, de nossa parte, faz com que tenhamos que pensar melhor se estamos usando o static_cast corretamente. Inicialização uniforme Nas atividades do curso usaremos a chamada inicialização uniforme sempre que possível. A inicialização uniforme consiste no uso de {} no lugar de = ou () para inicializar objetos. Segundo a diretriz ES.23 do C++ Core Guidelines, “as regras para inicialização com {} são mais simples, mais gerais, menos ambíguas, e mais seguras do que outras formas de inicialização”. Observe os exemplos abaixo sem inicialização com {}, isto é, usando apenas = ou (): int a = 0; // Inicializa a com 0 int b(42); // Inicializa b com 42 double c(); // Declaração de uma função (ambíguo) int d = 7.9; // Inicializa implicitamente com 7 (inseguro) double pi = 3.1415f; // Inicializa implicitamente com 3.1415 std::vector&lt;int&gt; v1(3); // Inicializa v1 com 3 elementos de valor 0 (ambíguo) std::vector&lt;int&gt; v2(3, 2); // Inicializa v2 com 3 elementos de valor 2 (ambíguo) Compare com os exemplos abaixo com inicialização com {}: int a{}; // Inicializa a com valor default (0) int b{42}; // Inicializa b com 42 double c{}; // Inicializa c com valor default (0.0) // int d{7.9}; // ERRO: conversão com estreitamento int d{gsl::narrow_cast&lt;int&gt;(7.9)}; // OK: conversão explícita de double para int double pi{3.1415f}; // OK: conversão implícita sem estreitamento std::vector v1{3}; // Inicializa v1 com um único elemento 3 std::vector v2{3, 2}; // Inicializa v2 com os elementos 3 e 2 Observe que a inicialização com {} proíbe conversões implícitas com estreitamento. A inicialização uniforme nos obriga a usar conversões explícitas sempre que houver a possibilidade de perda de informação. Por um lado, isso deixa o código mais verboso, como no uso do gsl::narrow_cast no exemplo anterior. Por outro lado, deixa evidente a intenção do programador e evita bugs decorrentes de conversões implícitas. Palavra-chave auto A palavra-chave auto permite a dedução automática do tipo de uma variável a partir de sua inicialização. Usaremos auto sempre que possível para evitar repetições redundantes de tipos de dados. Essa é uma recomendação da diretiva ES.11 do C++ Core Guidelines. Alguns exemplos de uso de auto são mostrados a seguir: auto a{42}; // int auto b{7.9f}; // float auto c{1.0 / 3}; // double // Os exemplos a seguir funcionam com qualquer tipo de dado em v std::vector v{3, 2}; auto e{v[0]}; // Cópia de v[0] auto &amp;f{v[0]}; // Referência a v[0]) auto const &amp;g{v[0]}; // Referência apenas de leitura auto *h{&amp;v[0]}; // Ponteiro para v[0] auto it{v.begin()}; // std::vector&lt;int&gt;::iterator // Ponteiro inteligente auto p{std::make_unique&lt;int[]&gt;(1024)}; // std::unique_ptr&lt;int[]&gt; // Expressão lambda auto addTen{[](auto x) { return x + 10; }}; auto x{0U}; // unsigned auto y{addTen(x)}; // unsigned o PowerShell não é suportado! Use apenas o Prompt de Comando.↩︎ Por exemplo, gsl::narrow&lt;T&gt;(3.1415926535) deve funcionar em uma plataforma em que T é um ponto de flutuante de 48 bits (exótico, mas possível), mas falha em um plataforma em que T tem 32 bits.↩︎ "],["intro.html", "2 Introdução", " 2 Introdução Computação gráfica (CG) é o conjunto de métodos e técnicas para a construção, manipulação, armazenagem e exibição de imagens por meio de um computador (ISO 2015). A computação gráfica tem suas origens no desenvolvimento dos primeiros computadores eletrônicos equipados com dispositivos de exibição. A partir do final da década de 1940, o computador experimental Whirlwind I do MIT (Instituto de Tecnologia de Massachusetts) e o sistema SAGE (Semi-Automatic Ground Environment) da Força Aérea dos Estados Unidos, foram os primeiros a utilizar dispositivos de exibição do tipo CRT (cathod-ray tube, ou tubo de raios catódicos) para exibir gráficos vetoriais compostos de linhas e pontos. O uso do termo “computação gráfica” é um pouco mais recente. William Fetter, projetista gráfico da Boeing, utilizou o termo pela primeira vez em 1960 por sugestão de seu supervisor Verne L. Hudson, para descrever seu trabalho. Fetter utilizava gráficos tridimensionais no computador para criar um modelo estilizado de corpo humano que ficou conhecido como “Boeing Man” (figura 2.1). O modelo, composto de curvas e segmentos, era utilizado em simulações de ergonomia do piloto na cabine do avião. Figura 2.1: “Boeing Man” desenhado por William Fetter em um IBM 7094 (fonte). O acrônimo CGI (Computer-Generated Imagery) é frequentemente utilizado para se referir à geração de imagens e efeitos visuais em computador com aplicações em arte, entretenimento, simulação e visualização científica. Uma forma de CGI bastante conhecida e que chama a atenção por suas imagens bonitas é a síntese de imagens fotorrealistas. Um exemplo de imagem fotorrealista gerada em computador é mostrado na figura 2.2. A imagem é uma cena de arquitetura interior produzida com o Cycles Render Engine. Figura 2.2: Scandinavian Interior, por Arnaud Imobersteg (fonte). Atualmente, uma imagem como a da figura 2.2 pode ser gerada em qualquer computador pessoal que tenha sido fabricado nos últimos 10 anos. A imagem foi gerada com o software Blender, gratuito e de código aberto, e que tem o Cycles Render Engine como um de seus renderizadores. Entretanto, existe uma máxima em computação gráfica – a chamada 1ª lei de Peddie – que diz: “Em computação gráfica, demais nunca é o suficiente.” — Jon Peddie De fato, mesmo com toda a evolução do hardware gráfico e das técnicas de CG, ainda não é possível gerar em tempo real imagens com o nível de qualidade obtido por renderizadores como o Cycles. Eventualmente, esse dia chegará. Porém, quando isso acontecer, o nível de exigência dos usuários e desenvolvedores será igualmente maior. Desejaremos imagens ainda mais detalhadas, com maior resolução, mais realistas, mais interativas, com mais efeitos cinemáticos, etc. Enfim, demais nunca é o suficiente. Em computação gráfica, é necessário um cuidadoso compromisso entre a qualidade das imagens geradas e a eficiência com que essas imagens podem ser sintetizadas em um dado sistema computacional. Isso é particularmente importante quando as imagens precisam ser geradas em tempo real. Em jogos digitais, é comum que imagens de alta resolução tenham de ser geradas a uma taxa de, no mínimo, 30 quadros por segundo4. Mesmo os jogos que não visam o fotorrealismo exigem imagens em um nível de qualidade que só pode ser alcançado com o uso de técnicas avançadas de sombreamento e iluminação. Essas técnicas visam produzir resultados que, ainda que não sejam necessariamente acurados do ponto de vista físico, sejam suficientemente convincentes para um público cada vez mais exigente. No decorrer do quadrimestre veremos que a evolução das técnicas e ferramentas de computação gráfica em tempo real é impulsionada pela busca da melhor qualidade de imagem que pode ser obtida de forma eficiente no hardware gráfico disponível no momento. Como resultado, é comum que os métodos adotem simplificações inusitadas, mas ao mesmo tempo muito efetivas, e explorem diferentes aspectos da percepção visual humana para criar uma ilusão de realismo que seja suficiente para chegar ao resultado desejado. Referências "],["áreas-correlatas.html", "2.1 Áreas correlatas", " 2.1 Áreas correlatas A computação gráfica se relaciona, e em certa medida se sobrepõe, a diferentes campos de atuação da ciência da computação. Uma breve introdução às principais áreas correlatas é dada a seguir: Síntese de imagem: é o que geralmente se entende por computação gráfica. Compreende o processo de rendering (imageamento ou renderização) que consiste em converter especificações de geometria, cor, textura, iluminação, entre outras especificações de características de uma cena, em uma imagem exibida em um display gráfico. A figura 2.3 mostra o resultado da síntese de imagem de uma cena fotorrealista usando técnicas combinadas de traçado de raios e radiosidade. Figura 2.3: Imagem gerada no renderizador POV-Ray, por Gilles Tran (fonte). Visão computacional: compreende o processo de adquirir, processar e interpretar dados visuais para gerar as especificações de uma cena. A partir de uma imagem digital, técnicas de visão computacional podem ser utilizadas para tarefas como a reconstrução dos modelos geométricos vistos na imagem, o particionamento dos pixels em segmentos correspondentes aos diferentes objetos da cena, reconhecimento de texturas, identificação dos atributos da câmera e da iluminação, e extração de outras informações semânticas. A visão computacional com frequência se relaciona com a visão de máquina, que compreende as técnicas e ferramentas voltadas a aplicações de visão em inspeção automática, controle de processos industriais e orientação de robôs. A figura 2.4 mostra um exemplo de aplicação de visão computacional: uma técnica de segmentação semântica utilizando aprendizagem profunda para identificar objetos em uma imagem. Figura 2.4: Segmentação semântica usando o sistema YOLO (fonte). Processamento de imagem: compreende o processo de aplicar filtros e operações sobre uma imagem digital que resultam em uma nova imagem digital. Técnicas de processamento digital de imagem podem ser utilizadas para enfatizar características de uma imagem (por exemplo, ajustar brilho, contraste, nitidez), restaurar imagens que sofreram algum tipo de degradação por ruído, mudar cores e tons, comprimir e quantizar, entre diversas outras operações. O escopo do processamento de imagens frequentemente se intersecta com aquele das técnicas de visão computacional. A figura 2.5 mostra um exemplo de processamento de imagem: a aplicação de filtros de remoção de ruído em uma imagem renderizada pelo método de traçado de raios estocástico. O ruído é inerente ao método de Monte Carlo utilizado nesse tipo de renderização. Figura 2.5: Uso dos filtros de processamento de imagem do Intel Open Image Denoise para remoção de ruído de uma imagem de traçado de raios (fonte). Modelagem geométrica: está relacionada com a criação e processamento de representações matemáticas de formas. Técnicas de modelagem geométrica podem ser utilizadas para criar modelos compostos de curvas e superfícies a partir de aquisição de dados (por exemplo, a partir de uma nuvem de pontos de uma aquisição por scanner 3D), construir e manipular modelos sintéticos através da combinação de primitivas geométricas, converter uma representação geométrica em outra, e realizar operações geométricas e topológicas diversas. A figura 2.6 mostra um exemplo de reconstrução de malha geométrica usando o software MeshLab. O modelo à esquerda é o modelo original. Na reconstrução (à direita), os buracos foram preenchidos e o resultado é uma única malha de triângulos. Figura 2.6: Reconstrução de malha geométrica usando o MeshLab (fonte). Neste curso teremos como foco a síntese de imagens. Em particular, a síntese de imagens em tempo real. Como parte disso, veremos como representar e processar cenários virtuais compostos de objetos tridimensionais animados. Veremos como implementar modelos de iluminação capazes de simular de forma eficiente a iluminação de superfícies, e como gerar imagens digitais do ponto de vista de uma câmera virtual. Faremos isso usando a API gráfica OpenGL de modo a explorar o pipeline de processamento gráfico programável das placas de vídeo atuais. Com isso conseguiremos obter o nível de eficiência necessário para produzir animações e permitir a sensação de interatividade. Na UFABC, os tópicos de visão computacional e processamento de imagens são abordados nas disciplinas “ESZA019-17 Visão Computacional” e “MCZA018-17 Processamento Digital de Imagens”. "],["linha-do-tempo.html", "2.2 Linha do tempo", " 2.2 Linha do tempo Nesta seção acompanharemos um resumo da evolução histórica da computação gráfica. Iniciaremos na década de 1950, com os primeiros computadores eletrônicos de uso geral e o surgimento das primeiras aplicações de computação gráfica, e seguiremos até a década atual com os desenvolvimentos mais recentes das atuais GPUs (Graphics Processing Units). Embora a computação gráfica seja recente, assim como a própria ciência da computação, o desenvolvimento de seus fundamentos é anterior ao século XX e só foi possível devido às contribuições artísticas e matemáticas de diversos pioneiros. Para citar apenas alguns: Euclides de Alexandria (300 a.C.), com sua contribuição no desenvolvimento da geometria; Filippo Brunelleschi (1377–1446), com seus estudos sobre perspectiva; René Descartes (1596–1650), com o desenvolvimento da geometria analítica e os sistemas de coordenadas; Christiaan Huygens (1629–1695) e Isaac Newton (1643–1727) por suas investigações sobre os fenômenos da luz; Leonhard Euler (1707–1783), por sua contribuição na trigonometria e em topologia; James Joseph Sylvester (1814–1897), por suas contribuições na teoria das matrizes e invenção da notação matricial. O uso de gráficos no computador também não teria sido possível sem os esforços que contribuíram para o surgimento dos computadores eletrônicos, e também dos primeiros dispositivos de exibição, como o tubo de raios catódicos no final do século XIX. 1950 Os primeiros computadores eletrônicos com dispositivos de exibição surgem neste período. O computador Whirlwind I, do MIT, originalmente projetado para ser parte de um simulador de vôo, foi um dos primeiros computadores digitais de uso geral com processamento em tempo real. O Whirlwind I era equipado com um CRT vetorial capaz de desenhar linhas e pontos. Charles W. Adams e John T. Gilmore, programadores da equipe de desenvolvimento do Whirlwind, implementaram um programa de avaliação de equações diferenciais para produzir a animação da trajetória de uma bola quicando. Essa simulação pode ser considerada a primeira aplicação de computação gráfica interativa e um precursor do jogo de computador, pois o operador podia controlar, através de um botão, a frequência do quicar na tentativa de fazer a bola acertar uma lacuna na tela simulando um buraco no chão. O sistema de defesa aérea SAGE evoluiu a partir do Whirlwind ao longo da década de 1950. As estações do SAGE contavam com telas CRT que exibiam dados de diferentes radares combinados com informações de referência geográfica. Cada estação era também equipada com uma caneta óptica. Através da caneta óptica, o operador podia apontar e selecionar elementos gráficos diretamente na tela (figura 2.7). Figura 2.7: Operador do SAGE usando uma caneta óptica em um CRT vetorial (fonte). 1960 Nesse período a computação gráfica se desenvolve nos laboratórios de pesquisa de universidades e surgem as primeiras aplicações de CAD (Computer-Aided Design) nas indústrias automotiva e aeroespacial. Na década de 1960 ocorrem importantes desenvolvimentos na área de modelagem geométrica, como o uso de curvas de Bézier e NURBS (Non-Uniform Rational Basis Spline). Em 1960, a Digital Equipment Corporation (DEC) começa a produzir em escala comercial o computador PDP-1, equipado com CRT e caneta óptica. Em 1961, o cientista da computação Steve Russell (MIT) cria o “Spacewar!” (figura 2.8). O jogo ganha popularidade dentro e fora da universidade e vira referência no desenvolvimento de jogos digitais5. Figura 2.8: Estudantes do MIT jogando Spacewar! no DEC PDP-1 (fonte). Em 1963, Ivan Sutherland desenvolve o SketchPad, um sistema de projeto gráfico interativo que permite ao usuário manipular primitivas gráficas vetoriais através de uma caneta óptica e um CRT (Sutherland 1963). A figura 2.9 mostra Sutherland operando o SketchPad no computador TX-2 do MIT. O SketchPad é um marco no uso da interface gráfica do usuário (GUI, acrônimo de Graphical User Interface) e um precursor das aplicações de projeto assistido por computador (CAD). Figura 2.9: Ivan Sutherland operando o SketchPad em 1962 (fonte). Na década de 1960 surgem também os primeiros seminários e grupos de interesse em pesquisa sobre gráficos em computador. Na ACM (Association for Computing Machinery), tradicional sociedade científica e educacional dedicada à computação, é fundado o grupo SICGRAPH (Special Interest Committe on Computer Graphics) para promover seminários de computação gráfica. No final da década, o SICGRAPH muda de nome para SIGGRAPH (Special Interest Group on Computer Graphics and Interactive Techniques). A conferência SIGGRAPH é realizada anualmente e é hoje uma das principais conferências de computação gráfica no mundo. 1970 Durante a década de 1970 são desenvolvidas muitas das técnicas de síntese de imagens em tempo real utilizadas atualmente. Em 1971, o então aluno de doutorado Henri Gouraud, trabalhando com Dave Evans e Ivan Sutherland na Universidade de Utah, desenvolve uma técnica eficiente de melhoramento da percepção visual do sombreamento (shading) de superfícies suaves aproximadas por malhas poligonais (Gouraud 1971). Tal técnica, conhecida como Gouraud shading, consiste em interpolar linearmente os valores de intensidade de luz refletida dos vértices da malha poligonal. O resultado é a suavização da variação da reflexão de luz sem a necessidade de aumentar a resolução da malha geométrica (Figura 2.10). Figura 2.10: Visualização de uma esfera aproximada por triângulos, exibindo o aspecto facetado (esquerda) e suavizado com Gouraud shading (direita). Em 1973, Bui Phong, também na Universidade de Utah, desenvolve o Phong shading como um melhoramento de Gouraud shading para reproduzir com mais fidelidade as reflexões especulares em aproximações de superfícies curvas (Phong 1973). Na figura 2.11 é possível comparar Gouraud shading e Phong shading lado a lado. Phong shading reproduz de forma mais acurada o brilho especular da esfera sem precisar usar uma malha poligonal mais refinada. Figura 2.11: Visualização de uma esfera aproximada por triângulos, com Gouraud shading (esquerda) e Phong shading (direita). Phong também propôs um modelo empírico de iluminação local de pontos sobre superfícies conhecido como modelo de reflexão de Phong. Em 1977, Jim Blinn, aluno da mesma universidade, propôs uma alteração do modelo de reflexão de Phong – o modelo de Blinn–Phong – mais acurado fisicamente e mais eficiente sob certas condições de visualização e iluminação (Blinn 1977). Nas décadas seguintes, o modelo de Blinn–Phong tornaria-se o padrão de indústria para síntese de imagens em tempo real, e ainda é muito utilizado atualmente. Em 1974, Wolfgang Straßer, na Universidade Técnica de Berlim, e Ed Catmull, na Universidade de Utah, desenvolvem ao mesmo tempo, mas de forma independente, uma técnica que viria a ser conhecida como Z-buffering. Tal técnica permite identificar, de forma conceituamente simples e favorável à implementação em hardware, quais partes da geometria 3D estão visíveis de um determinado ponto de vista. Atualmente, essa técnica é largamente utilizada em síntese de imagens e é suportada em todo hardware gráfico. Além de ter contribuído com a técnica de Z-buffering, Catmull também trouxe diversos avanços na área de modelagem geométrica, especialmente em subdivisão de superfícies e representação paramétrica de superfícies bicúbicas (Catmull 1974). Outra importante contribuição de Catmull foi o desenvolvimento da técnica de mapeamento de textura, ubíqua nas aplicações gráficas atuais e que permite aumentar a percepção de detalhes de superfícies sem aumentar a complexidade da geometria (figura 2.12). Figura 2.12: Animação do mapeamento de uma textura 2D sobre um modelo poligonal 3D (fonte). Em 1975, o matemático Benoît Mandelbrot, na IBM, desenvolve o conceito de geometria de dimensão fracionária e cria o termo fractal (Albers and Alexanderson 2008). Desde então, fractais começam a ser explorados em síntese de imagens e modelagem geométrica para representar os mais diversos padrões e fenômenos naturais tais como contornos de mapas, relevo de terrenos, nuvens, texturas e plantas. Vol Libre “Vol Libre”, de Loren Carpenter, foi o primeiro filme criado com fractais. O vídeo, de apenas dois minutos, foi apresentado pela primeira vez na conferência SIGGRAPH ’80 após uma palestra técnica de Carpenter sobre a renderização de curvas e superfícies fractais: De acordo com o livro “Droidmaker: George Lucas And the Digital Revolution” (Rubin 2005), ao final da exibição do vídeo, Ed Catmull e Alvy Smith, da Lucasfilm, abordaram Carpenter e ofereceram a ele um emprego na divisão de computação da empresa. Carpenter aceitou imediatamente. Após a carreira na Lucasfilm, Carpenter ainda seria co-fundador da Pixar (junto com Catmull, Smith e outros) e cientista-chefe do estúdio de animação. Em 1976, Steve Jobs, Steve Wozniak e Ronald Wayne fundam a Apple Computer (atualmente Apple Inc.). Em 1979, Steve Jobs entra em contato com as pesquisas de desenvolvimento de interface gráfica na Xerox PARC (atualmente PARC), divisão de pesquisa da Xerox em Palo Alto, Califórnia. Na PARC, Jobs conhece o Xerox Alto, o primeiro computador com uma interface gráfica baseada na metáfora do desktop e no uso do mouse (figura 2.13). Figura 2.13: Xerox Alto (fonte). O Xerox Alto foi o resultado de desenvolvimentos iniciados por Douglas Engelbart e Dustin Lindberg no Stanford Research Institute, atual SRI International, por sua vez inspirados no SketchPad de Sutherland. Alguns anos depois, a Apple implementaria os conceitos do Xerox Alto nos computadores Apple Lisa e Macintosh, iniciando uma revolução no uso da interface gráfica nos computadores pessoais (PCs). Em 1977, surge a primeira tentativa de padronização de especificação de comandos em sistemas gráficos: o Core Graphics System (ou simplesmente Core), proposto pelo Graphic Standards Planning Committee (GSPC) da ACM SIGGRAPH (Chappell and Bono 1978). Em 1978, Jim Blinn desenvolve uma técnica de mapeamento de textura para simulação de vincos e rugosidades em superfícies: o bump mapping (Blinn 1978). Uma forma de bump mapping muito utilizada atualmente é o normal mapping. A técnica pode ser muito efetiva para manter a ilusão de uma superfície detalhada, mesmo quando a geometria utilizada é muito simples. A figura 2.14 mostra um exemplo dessa simplificação. Ao longo do quadrimestre implementaremos esta e outras técnicas de texturização. Figura 2.14: Uso de normal mapping para simular a renderização de um modelo de quatro milhões de triângulos usando apenas dois triângulos (fonte). No final da década, J. Turner Whitted desenvolve a técnica de traçado de raios (Whitted 1979). O traçado de raios consegue simular com mais precisão, e de forma conceitualmente simples, efeitos ópticos de reflexão, refração, espalhamento e dispersão da luz. Como resultado, consegue gerar imagens mais fotorrealistas, ainda que sob um custo computacional muito elevado quando comparado com a renderização baseada na rasterização, que consiste na varredura e preenchimento de primitivas geométricas projetadas. Figura 2.15: Esferas e tabuleiro de xadrez: uma das primeiras imagens geradas com traçado de raios, por Turner Whitted. 1980 Essa é a década em que a computação gráfica marca sua presença definitiva na indústria de cinema. O uso de cenas de computação gráfica é popularizado a partir de filmes como “Star Trek II: The Wrath of Khan” (1982), “Tron” (1982) e “Young Sherlock Holmes (1985), como resultado dos avanços das técnicas de síntese de imagem e modelagem geométrica da década anterior, combinado com o avanço da capacidade de processamento dos computadores. Durante essa década ocorrem também importantes avanços nas técnicas de síntese de imagens. Em 1984, Robert Cook, Thomas Porter e Loren Carpenter desenvolvem o traçado de raios distribuído (distributed ray tracing), o qual permite reproduzir efeitos de sombras suaves, entre outros efeitos não contemplados pelo método original de Whitted (Cook, Porter, and Carpenter 1984). A figura 2.16 mostra um exemplo de renderização da cena de teste “Cornell box” usando essa técnica. A imagem tende a ser granulada como resultado da natureza estocástica do algoritmo. Figura 2.16: Imagem gerada com traçado de raios distribuído/estocástico. Ainda em 1984, Donald Greenberg, Michael Cohen e Kenneth Torrance propõem a técnica de radiosidade (Greenberg, Cohen, and Torrance 1986) baseada no uso do método de elementos finitos para simular interreflexões de luz entre superfícies idealmente difusas. A solução da radiosidade de uma cena pode ser pré-processada e não depende da posição da câmera. Isso permite a visualização da cena em tempo real, desde que a posição dos objetos e fontes de luz mantenha-se estática. A figura 2.17 mostra um exemplo de cena renderizada com radiosidade usando o software RRV (Radiosity Renderer and Visualizer). O método de radiosidade pode ser combinado com traçado de raios para gerar imagens com melhor fidelidade de simulação de reflexão difusa e especular. Figura 2.17: Imagem gerada com radiosidade (fonte). Em 1985, o GKS (Graphical Kernel System), desenvolvido como um melhoramento da API Core, torna-se a API padrão ISO para gráficos independentes do dispositivo (ISO 1985). Através do GKS, o código de descrição de comandos para manipulação de gráficos 2D permite a portabilidade entre diferentes linguagens de programação, sistemas operacionais e hardware gráfico compatível. Entretanto, gráficos 3D ainda não são contemplados nesta API. Em 1986, Steve Jobs adquire a divisão de computação gráfica da Lucasfilm e funda a Pixar junto com Ed Catmull, Alvy Smith e outros. Nessa época, Catmull, Loren Carpenter e Robert Cook desenvolvem o sistema de renderização RenderMan, muito utilizado na produção de efeitos visuais em filmes e animações. Após 14 anos, Catmull, Carpenter e Cook receberiam da Academia de Artes e Ciências Cinematográficas a estatueta do Oscar na categoria “Academy Scientific and Technical Awards” pelas contribuições à indústria do cinema representadas pelo desenvolvimento do RenderMan. O sucesso do RenderMan deve-se em parte à sua elegante API – a RenderMan Interface (RISpec) – inspirada na linguagem PostScript. A API permite a descrição completa de cenas 3D com todos os componentes necessários à renderização. Isso garante resultados consistentes, independentes do software de modelagem utilizado. O conceito de shaders, amplamente utilizado em hardware gráfico atual, surge do RenderMan shading language, desenvolvido na década de 1990 e incorporado no RISpec em 2005 como uma linguagem – dessa vez inspirada na linguagem C – de especificação de propriedades de superfícies, fontes de luz e efeitos atmosféricos de cena. Em 1988 é organizado o 1º Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens (SIBGRAPI), em Petrópolis, RJ. O evento, organizado anualmente pela CEGRAPI/SBC, internacionalizou-se e atualmente é chamado de Conference on Graphics, Patterns and Images. Neste ano, o SIBGRAPI será realizado em Natal (RN), de 24 a 27 de outubro. 1990 1990 é a década das APIs gráficas 3D e da popularização do hardware gráfico nos PCs. Empresas como a Sun Microsystems (adquirida pela Oracle em 2010), IBM, HP (Hewlett-Packard), e as agora extintas NeXT, SGI (Silicon Graphics, Inc.) e DEC, desenvolvem estações gráficas de alto desempenho equipadas com hardware capaz de acelerar operações de renderização baseadas em rasterização com suporte a Z-buffer, mapeamento de texturas, iluminação e sombreamento de superfícies (figura 2.18). Figura 2.18: Workstation SGI IRIS Indigo (fonte). Neste período surgem as primeiras APIs para gráficos 3D como tentativa de padronizar a interface de programação entre as diferentes arquiteturas de hardware. Uma dessas APIs, desenvolvida ao longo da década de 1980 e que se estabelece como padrão da indústria na década de 1990, é o PHIGS (Programmer’s Hierarchical Interactive Graphics System) (Shuey 1987). PHIGS utiliza o conceito de grafo de cena: uma estrutura de dados hierárquica que representa as relações entre os modelos geométricos e outras entidades de uma cena. A API trabalha com malhas poligonais e síntese de imagens baseada na rasterização (em oposição ao traçado de raios), prevê o suporte a Gouraud e Phong shading, mas não oferece suporte a mapeamento de texturas. Em oposição ao PHIGS, a SGI utiliza em suas estações gráficas IRIS a API proprietária IRIS GL (Integrated Raster Imaging System Graphics Library) com características semelhantes ao PHIGS, porém com suporte a mapeamento de texturas (McLendon 1992). Diferentemente do PHIGS, o IRIS GL não adota o conceito de grafo de cena. As primitivas gráficas são enviadas imediatamente ao hardware gráfico em um pipeline de transformação geométrica e visualização. Esse modo de enviar os dados, conhecido como immediate mode, acaba por revelar-se mais apropriado para implemetação em hardware do que o retained mode do PHIGS com seu grafo de cena. Em 1991, Mark Segal e Kurt Akeley, da SGI, iniciam o desenvolvimento de uma versão aberta do IRIS GL como tentativa de criar um novo padrão de indústria. Para isso, removem o código proprietário e modificam a API de modo a torná-la independente do sistema de janelas e de dispositivos de entrada. Deste desenvolvimento surge, em 1992, o OpenGL (Open Graphics Library) (Woo et al. 1999), que rapidamente ocupa o lugar do PHIGS como API padrão para gráficos 3D. Desde então, revisões periódicas do OpenGL são feitas de modo a suportar os aprimoramentos mais recentes do hardware gráfico. O aspecto minimalista e de facilidade de uso do IRIS GL continuam presentes no OpenGL. Essas características fizeram – e ainda fazem – do OpenGL uma das APIs gráficas 3D mais populares em aplicações multiplataforma. InfiniteReality No início da década de 1990, as estações gráficas de alto desempenho suportavam apenas um número reduzido de características do OpenGL, sendo o restante simulado em software. O sistema RealityEngine (Akeley 1993), lançado em 1992 pela SGI, foi o primeiro hardware gráfico capaz de oferecer suporte para todas as etapas de transformação e iluminação da versão 1.0 do OpenGL, incluindo o mapeamento de texturas 2D com mipmapping (uma técnica de pré-filtragem de texturas) e antialiasing (suavização de serrilhado). A arquitetura foi sucedida em 1996 pelo InfiniteReality (Montrym et al. 1997), desenvolvido especificamente para o OpenGL. Dependendo da configuração final, o custo de uma estação gráfica baseada no InfiniteReality poderia ser superior a 1 milhão de dólares. Uma demonstração da SGI sobre as capacidades de renderização em tempo real do InfiniteReality em 1996 pode ser vista no vídeo de YouTube “Silicon Graphics - Onyx Infinite Reallity 50FPS”. O sistema InfiniteReality evoluiu até o início da década de 2000. A figura 2.19 mostra um supercomputador equipado com o InfiniteReality 4. Figura 2.19: SGI Onyx 300 com InfinityReality 4, de 2002 (fonte). A partir de 1995, surgem nos PCs as primeiras placas de vídeo com aceleração de processamento gráfico 3D, também chamadas de aceleradoras gráficas 3D. As primeiras aceleradoras gráficas eram capazes de realizar apenas a varredura de linhas não texturizadas e, em alguns casos, tinham desempenho similar ao código de máquina otimizado na CPU. Por outro lado, logo essas limitações foram vencidas e surgiram placas eficientes e com suporte a mapeamento de textura, impulsionadas pelo emergente mercado de jogos de computador. Enquanto as primeiras estações gráficas da SGI implementavam um pipeline completo de transformação de vértices, ainda que sem suporte à texturização, as aceleradoras gráficas para PCs, produzidas por empresas como Diamond Multimedia, S3 Graphics (extinta em 2003), Trident Microsystems (extinta em 2012), Matrox Graphics e NVIDIA, ofereciam suporte ao mapeamento de texturas, porém sem transformação de geometria ou processamento de iluminação. A 3Dfx Interactive (adquirida em 2000 pela NVIDIA), com a sua série de aceleradoras Voodoo Graphics lançadas a partir de 1996, ampliou enormemente o uso do hardware gráfico em jogos de computador. As placas Voodoo eram capazes de exibir triângulos texturizados com mipmapping e filtragem bilinear (figura 2.20). Entretanto, o hardware ainda dependia da CPU para preparar os triângulos para a rasterização. Os triângulos só poderiam ser processados pelo hardware gráfico se fossem previamente convertidos em trapézios degenerados, alinhados em coordenadas da tela. Figura 2.20: Jogo “Carmageddon II: Carpocalypse Now” (Stainless Games) em uma placa gráfica 3Dfx Voodoo, de 1998 (fonte). Outra limitação das aceleradoras gráficas nesse período era a falta de suporte adequado a uma API padrão de indústria. A arquitetura de tais placas era incompatível com aquela especificada no OpenGL e fazia com que os desenvolvedores precisassem recorrer a APIs proprietárias, como a API Glide da 3Dfx (3Dfx 1997). As placas da 3Dfx foram populares até o final da década quando então o OpenGL e a API Direct3D, da Microsoft, começaram a ser suportados de maneira eficiente pelas placas de concorrentes como a ATI Technologies (adquirida em 2006 pela AMD), Matrox e NVIDIA. Na segunda metade da década, o desenvolvimento das placas gráficas para PCs acompanhou a evolução da API Direct3D. Em 1995, a Microsoft lança o Windows 95 Games SDK, um conjunto de APIs de baixo nível para o desenvolvimento de jogos e aplicações multimídia de alto desempenho no Windows. Em 1996, o Windows 95 Games SDK muda de nome para DirectX e sua segunda e terceira versões são disponibilizadas em junho e setembro desse mesmo ano. Entre as APIs contidas no DirectX, o Direct3D é concebido como uma API para hardware gráfico compatível com o pipeline de processamento do OpenGL. Embora no início o Direct3D fosse criticado por sua arquitetura demasiadamente confusa e mal documentada em comparação com o OpenGL (como relatado por John Carmack, da id Software, em sua carta sobre o OpenGL), eventualmente torna-se a API mais utilizada em jogos uma vez que novas versões começam a ser distribuídas em intervalos menores que aqueles do OpenGL. A revisão do OpenGL dependia do ARB (Architecture Review Board): um consórcio independente formado por representantes de diversas empresas de hardware e software que se reuniam periodicamente para propor e aprovar mudanças na API. O Direct3D, por ser proprietário, respondia melhor ao rápido desenvolvimento das placas gráficas naquele momento e passou a ditar a especificação das futuras aceleradoras gráficas voltadas ao mercado de jogos. Em 1997 é anunciado o DirectX 5 (o DirectX 4 nunca chegou a ser lançado), acompanhando as primeiras placas capazes de renderizar triângulos, tais como a ATI Rage Pro e NVIDIA Riva 128 (figura 2.21). A Riva 128 não alcançava a mesma qualidade de imagem produzida pelas placas da 3Dfx, mas ultrapassava as placas Voodoo em várias medições de desempenho. Ainda assim, a aceleração de processamento de geometria era inexistente e a CPU era responsável por calcular as transformações geométricas e interpolações de atributos de vértices ao longo das arestas para cada triângulo transformado. Figura 2.21: Placa gráfica Diamond com o chip NVIDIA Riva 128, de 1997 (fonte). Em 1998 é lançado o DirectX 6 e surgem as primeiras aceleradoras gráficas capazes de interpolar atributos ao longo de arestas. Nessa geração de hardware gráfico, a CPU ainda era responsável pela transformação e iluminação de cada vértice, mas agora bastava enviar à placa gráfica os atributos de cada vértice em vez de atributos interpolados para cada aresta de cada triângulo. Um ano depois, o DirectX 7 é lançado com suporte para aceleração em hardware de transformação e iluminação (figura 2.22). As primeiras placas compatíveis com DirectX 7 surgiriam no ano seguinte. Figura 2.22: Demonstração do benchmark 3DMark2000 (UL) usando DirectX 7 com transformação de geometria e cálculo de iluminação em hardware. 2000 A década de 2000 presencia o que pode ser considerado uma revolução no uso do hardware gráfico: surgem os primeiros processadores gráficos programáveis (programmable GPUs) capazes de alterar o comportamento do pipeline de renderização sem depender da CPU. Isso torna possível a implementação de diversos novos modelos de reflexão para além do tradicional modelo de Blinn–Phong disponível no pipeline de função fixa (pipeline não programável). Além disso, a capacidade de programar processadores gráficos possibilita a implementação de um incontável número de novos efeitos visuais. As GPUs programáveis tornam-se muito populares em PCs, impulsionadas pelas exigentes demandas do mercado de jogos. Ao mesmo tempo, tornam-se muito flexíveis e poderosas não só para jogos, mas também para processamento de propósito geral. O hardware gráfico programável surge no início de 2001 com o lançamento da GPU NVIDIA GeForce 3 (figura 2.23), inicialmente para o computador Apple Macintosh (Lindholm, Kilgard, and Moreton 2001). Figura 2.23: GPU NVIDIA GeForce (fonte). No início de 2001, durante o evento MacWorld Expo Tokyo, é exibido o curta metragem “Luxo Jr.”, produzido pela Pixar em 1986. Entretanto, desta vez o filme é renderizado em tempo real em um computador equipado com uma GeForce 3. Steve Jobs, então CEO da Apple, observou: “Há 15 anos, o que levava 75 horas para produzir cada segundo de vídeo, está agora sendo renderizado em tempo real na GeForce 3.” — Steve Jobs (Morris 2001) Mais tarde, as potencialidades de uma GPU similar seriam exibidas durante uma demonstração de tecnologia na conferência SIGGRAPH 2001: uma versão interativa do filme “Final Fantasy: The Spirits Within”, de Hironobu Sakaguchi, renderizada em tempo real em uma GPU NVIDIA Quadro DCC (Sakaguchi and Aida 2001). Neste evento, a NVIDIA destacou que o desempenho em operações em ponto flutuante utilizadas para desenhar apenas um quadro do filme era superior ao poder computacional total de um supercomputador Cray (tradicional fabricante de supercomputadores, adquirida em 2019 pela Hewlett Packard Enterprise) naquele momento. Ao longo da década, as GPUs de baixo custo (na faixa de 100 a 250 dólares), produzidas por empresas como NVIDIA e ATI, desbancam as estações gráficas de alto desempenho ainda baseadas em tecnologias da década anterior. As placas gráficas para computadores pessoais ultrapassam rapidamente as capacidades computacionais de sistemas como o RealityEngine da SGI, mas ao mesmo tempo com uma redução de custo superior a 90% em comparação com esses sistemas. De acordo com a Lei de Moore, e observando a diminuição do custo das CPUs nesse período, tais placas deveriam custar muito mais, em torno de 15 mil dólares. Esse avanço expressivo das GPUs é implacável com as fabricantes de estações gráficas. Em 2009, a SGI decreta falência. As APIs Direct3D (em 2006) e OpenGL (em 2009) anunciam a descontinuidade do suporte ao pipeline de função fixa. Com isso, as aplicações migram definitivamente ao uso dos shaders: programas que modificam o comportamento das etapas programáveis do pipeline, como o processamento de geometria e fragmentos (amostras de primitivas rasterizadas). Com o aumento do conjunto de instruções suportadas nas GPUs, percebe-se que é possível usar o hardware gráfico para processamento de propósito geral em tarefas como simulação de dinâmica de fluidos, operações em bancos de dados, modelagem de dinâmica molecular, criptoanálise, entre muitas outras tarefas capazes de se beneficiar de processamento paralelo. O termo GPGPU (General-Purpose Computation on GPUs) é utilizado para se referir a esse uso. Uma das tecnologias pioneiras de GPGPU foi o BrookGPU, desenvolvido na Universidade Stanford em 2004, composta de um compilador e um módulo de tempo de execução compatível com Direct3D e OpenGL (Buck et al. 2004). Até então, o processamento de propósito geral usando GPUs exigia do desenvolvedor conhecimento de APIs gráficas como Direct3D ou OpenGL para a criação de shaders customizados de vértices e pixels em linguagens como HLSL (da Microsoft), Cg (da NVIDIA), ou até mesmo em shader assembly. BrookGPU possibilitou simplificar esse fluxo de trabalho ao oferecer uma extensão de ANSI C – a linguagem Brook – voltada especificamente ao processamento paralelo de fluxos de dados. Em 2007, a NVIDIA lança a plataforma CUDA (Compute Unified Device Architecture), composta por um conjunto de ferramentas/bibliotecas e API de GPGPU para GPUs da NVIDIA. A plataforma é muito popular atualmente, impulsionada pelo crescimento das aplicações em ciência de dados e aprendizado de máquina. Influenciada pelo CUDA, surgem em 2009 outras plataformas como o DirectCompute, da Microsoft (como parte do Direct3D 11), e a especificação aberta OpenCL do Khronos Group, mesmo consórcio de indústrias que mantém o OpenGL. As primeiras oficinas e conferências sobre GPGPU, como a ACM GPGPU e a GPU Technology Conference (GTC), da NVIDIA, surgem neste período. A figura 2.24 mostra um exemplo atual de aplicação de GPGPU para a modelagem de DNA. Figura 2.24: Ligante de sulco menor do DNA, modelado através de GPGPU com o software Abalone. (fonte). 2010 A partir da década de 2010, a aceleração de gráficos 3D se expande e se populariza nos dispositivos móveis. O uso de multitexturização (uso de vários estágios de texturização) e de técnicas como normal mapping, cube mapping (para simulação de superfícies reflexivas) e shadow mapping (para simulação de sombras) torna-se comum em aplicações gráficas interativas. Em 2011, o Khronos Group anuncia o padrão WebGL, ampliando a possibilidade de uso de aceleração de gráficos 3D nos navegadores. Na segunda metade da década, a renderização baseada em física, do inglês Physically Based Rendering (PBR), começa a ser empregada em jogos de computador e em consoles. O jogo Alien: Isolation (Creative Assembly), de 2014, é um dos primeiros a explorar essa tecnologia (figura 2.25). Figura 2.25: Uso de renderização baseada em física no jogo “Alien: Isolation” (Creative Assembly) (fonte). A renderização baseada em física procura simular de forma fisicamente correta a interação da luz com os diferentes materiais de uma cena (Pharr, Jakob, and Humphreys 2016). Até então os algoritmos de iluminação e sombreamento em tempo real eram baseados em modelos empíricos, simplificados e pouco realistas, desenvolvidos para o hardware mais limitado da década anterior. Na renderização baseada em física, os materiais são descritos por informações de detalhes de microsuperfície obtidos por fotogrametria. A figura 2.26 mostra o modelo de uma arma renderizada com PBR e o conjunto de texturas utilizado. Figura 2.26: Arma renderizada com PBR usando o toolkit de renderização Marmoset, e conjunto de texturas utilizadas (fonte). Em 2014, Ian Goodfellow e seus colegas da Universidade de Montreal anunciam as Redes Adversárias Generativas (GANs) (Goodfellow et al. 2014). GANs são arquiteturas de redes neurais que permitem a geração de dados originais a partir do treinamento simultâneo de duas redes que competem entre si: uma rede geradora (por exemplo, treinada para gerar imagens de rostos de pessoas) e uma rede discriminadora (por exemplo, treinada para diferenciar rostos reais de rostos falsos). A rede geradora é otimizada a partir da discriminadora, como em um jogo minimax em que o discriminador tenta maximizar a chance de diferenciar corretamente os dados gerados dos dados reais de treinamento, e o gerador tenta minimizar a chance do discriminador classificar que os dados gerados são falsos. Em 2019, a NVIDIA Research desenvolve o StyleGAN (Karras, Laine, and Aila 2019), uma arquitetura de GAN que combina técnicas de aprendizado profundo e transferência de estilo neural (Gatys, Ecker, and Bethge 2016) para gerar rostos indistinguíveis de imagens reais. A técnica é popularizada com o site This Person Does Not Exist (Essa Pessoa Não Existe) que usa o modelo StyleGan2 para gerar um novo rosto a cada vez que a página é atualizada (figura 2.27). Figura 2.27: Rosto gerado pelo site https://thispersondoesnotexist.com/ usando o modelo StyleGAN2 da NVIDIA. Em 2016 são lançadas novas gerações de headsets de realidade virtual como o Oculus Rift e HTC Vive, que elevam as exigências de hardware gráfico para jogos que utilizam essa tecnologia. Também em 2016, o Khronos Group lança a API Vulkan como uma API de baixo nível ideal para explorar os recursos gráficos e de computação das novas gerações de GPUs. Vulkan dá ao desenvolvedor maior controle para gerenciar tarefas que anteriormente eram feitas exclusivamente pelo driver de vídeo, como a alocação, sincronização e transferência de recursos para a GPU. Vulkan também permite um melhor aproveitamento do processamento concorrente entre a CPU e a GPU. Em 2018 é incorporado ao Direct3D 12 o DirectX Raytracing (DXR), que introduz um novo pipeline gráfico destinado ao traçado de raios em tempo real. Ainda em 2018, as GPUs NVIDIA RTX série 20 são as primeiras a suportar essa tecnologia. Em 2019 a NVIDIA anuncia o Deep Learning Super Sampling (DLSS): um conjunto de tecnologias baseadas em redes neurais de aprendizagem profunda capazes de aumentar em tempo real a resolução dos quadros de exibição de jogos em computadores com GPUs RTX. A partir de uma imagem de baixa resolução, o modelo treinado consegue inferir uma imagem de alta resolução de forma mais eficiente e com mesmo nível de detalhes do que o jogo conseguiria obter caso renderizasse diretamente a imagem em alta resolução. O vídeo de divulgação a a seguir mostra o ganho de desempenho obtido com o uso de DLSS em diversos jogos: 2020 Em 2020, as GPUs NVIDIA RTX série 20 são sucedidas pela série 30, ampliando ainda mais a possibilidade de uso de traçado de raios em tempo real. A AMD lança as GPUs da série Radeon RX 6000, também com suporte a traçado de raios. Além disso, uma API de traçado de raios é incorporada ao Vulkan como o conjunto de extensões Vulkan Ray Tracing. O vídeo a seguir mostra exemplos de renderização com traçado de raios nas GPUs RTX: Ainda estamos no início da década, mas o aumento da capacidade de processamento e largura de banda de memória do hardware gráfico deve continuar a empurrar os limites do que é possível renderizar em tempo real. Efeitos atmosféricos, texturas de altíssima resolução e iluminação global6 em tempo real devem se popularizar nos próximos anos. Um exemplo do estado-da-arte em técnicas de renderização em tempo real usando iluminação global pode ser visto neste vídeo de apresentação do motor de jogo Unreal Engine 5, lançado em abril deste ano: As tecnologias baseadas em aprendizagem profunda também devem continuar trazendos aprimoramentos na qualidade e eficiência em síntese de imagens em tempo real. A NVIDIA tem expandido tecnologias como o NVIDIA Real-Time Denoiser (NRD) para remover o ruído de imagens renderizadas com traçado de raios, o Deep Learning Anti-Aliasing (DLAA) para suavização de serrilhados (anti-aliasing) em resolução nativa, e a tecnologia Deep Learning Dynamic Super Resolution (DLDSR), que renderiza imagens de alta resolução e então reduz para a resolução da tela, obtendo com isso uma imagem de qualidade superior com mesma eficiência da renderização na resolução nativa. Esta também deve ser a década da criação de conteúdo através de modelos generativos. Em 2021, a OpenAI anunciou o DALL-E: um conjunto de modelos neurais baseados em métodos de difusão capazes de gerar imagens a partir de textos descritivos em linguagem natural. A tecnologia vem se desenvolvendo rapidamente. O DALL-E 2, anunciado em abril deste ano, permite criar imagens originais de alta qualidade, variações de imagens existentes em diferentes estilos, além de permitir a extensão de imagens para além de suas bordas originais (outpainting). Também neste ano, o Google Brain anunciou tecnologias semelhantes como o Imagen (Saharia et al. 2022) e o Parti (Yu et al. 2022). Além desses, o laboratório de pesquisa independente Midjourney anunciou, também em 2022, uma ferramenta similar com versão beta disponível ao público através de um bot no Discord. Recentemente, uma imagem produzida no Midjourney (figura 2.28) ganhou a competição de arte digital na 2022 Colorado State Fair, gerando controvérsia entre os artistas. Figura 2.28: “Théâtre d’Opéra Spatial”, assinada por “Jason T. Allen via Midjourney”, vencedora da competição de arte digital da 2002 Colorado State Fair (fonte). O vídeo a seguir destaca os principais recursos do DALL-E 2: DALL-E, Imagen, Parti e Midjourney são tecnologias proprietárias que não disponibilizam o código fonte ou os pesos dos modelos. Entretanto, recentemente começaram a surgir modelos abertos, entre eles o Crayon, também conhecido como Dall-E Mini da Crayon LLC, o Latent Diffusion da Universidade de Heidelberg (Rombach et al. 2022), e o Stable Diffusion da StabilityAI. Demonstrações estão disponíveis online em https://www.craiyon.com/ e na plataforma Hugging Face (Latent Diffusion e Stable Diffusion). O vídeo a seguir, do canal de YouTube Two Minute Papers, apresenta os impressionantes recursos do Stable Diffusion: Por fim, a década de 2020 tem presenciado avanços significativos na área de renderização neural. Renderização neural consiste no uso de tecnologias baseadas em redes neurais para reconstruir modelos 3D a partir de uma coleção de imagens 2D. Uma dessas tecnologias é o Neural Radiance Field, ou NeRF (Mildenhall et al. 2020). NeRFs são modelos de redes neurais completamente conectadas que representam a forma como a luz se propaga dentro de uma cena a partir de uma posição e orientação de visão. NeRFs podem ser usadas para gerar representações implícitas das superfícies da cena, mas também para renderizar imagens a partir de qualquer ponto de vista, com sombreamento e iluminação. Uma técnica estado-da-arte de renderização neural é o Instant NeRF, da NVIDIA Research. Um vídeo de demonstração é exibido a seguir: Referências "],["firstapp.html", "2.3 Primeiro programa", " 2.3 Primeiro programa Nesta seção seguiremos um passo a passo de construção de um primeiro programa com a biblioteca ABCg. Será o nosso “Hello, World!”, similar ao exemplo da ABCg mostrado na seção 1.5, mas sem o triângulo colorido renderizado com OpenGL. Configuração inicial Faça uma cópia (ou fork) do conteúdo de https://github.com/hbatagelo/abcg.git. Desse modo você poderá modificar livremente a biblioteca e armazená-la em seu repositório pessoal. Como a ABCg já tem um projeto de exemplo chamado helloworld, vamos chamar o nosso de firstapp. Em abcg/examples, crie o subdiretório abcg/examples/firstapp. A escolha de deixar o projeto como um subdiretório de abcg/examples é conveniente pois bastará construir a ABCg para que o nosso projeto seja automaticamente construído como um exemplo adicional da biblioteca. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(firstapp). O conteúdo ficará assim: add_subdirectory(helloworld) add_subdirectory(firstapp) Dessa forma o CMake incluirá o subdiretório firstapp na busca de um script CMakeLists.txt contendo a configuração do projeto. Crie o arquivo abcg/examples/firstapp/CMakeLists.txt. Edite-o com o seguinte conteúdo: project(firstapp) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) O comando project na primeira linha define o nome do projeto. Em seguida, add_executable define que o executável terá o mesmo nome definido em project e será gerado a partir dos fontes main.cpp e window.cpp (não é necessário colocar os arquivos .h ou .hpp). Por fim, a função enable_abcg() configura o projeto para usar a ABCg. Essa função é definida em abcg/cmake/ABCg.cmake, que é um script CMake chamado a partir do CMakeLists.txt do diretório raiz. Em abcg/examples/firstapp, crie os arquivos main.cpp, window.cpp e window.hpp. Vamos editá-los a seguir. main.cpp Em main.cpp definimos o ponto de entrada da aplicação: #include &quot;window.hpp&quot; int main(int argc, char **argv) { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window Window window; window.setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(window); return 0; } Na primeira linha incluímos o arquivo de cabeçalho que terá a definição de uma classe customizada Window responsável pelo comportamento da janela da aplicação. Faremos com que essa classe seja derivada de abcg::OpenGLWindow para que possamos usar as funções da ABCg de gerenciamento de janelas compatíveis com OpenGL; Na linha 5 criamos um objeto app da classe abcg::Application, responsável pelo gerenciamento da aplicação; Na linha 8 criamos o objeto window que presenta nossa janela customizada; Na linha 9 definimos o título da janela. setWindowSettings é uma função membro de abcg::OpenGLWindow, classe base de Window. A função recebe uma estrutura abcg::WindowSettings contendo as configurações da janela; Na linha 12, a função abcg::Application::run é chamada para inicializar os subsistemas da SDL, inicializar a janela recém-criada e entrar no laço principal da aplicação. A função retornará somente quando a janela de aplicação for fechada. Observação Todas as classes e funções da ABCg fazem parte do namespace abcg. Como vimos no código anterior, abcg::OpenGLWindow é uma classe da ABCg responsável pelo gerenciamento de janelas compatíveis com OpenGL. De modo semelhante, abcg::Application é uma classe da ABCg responsável pelo gerenciamento da aplicação. Em todos os programas que faremos durante a disciplina, começaremos definindo uma classe derivada de abcg::OpenGLWindow, como a classe Window de nossa primeira aplicação. Tal classe derivada será customizada com comandos do OpenGL para que possamos desenhar o conteúdo da janela. Internamente a ABCg usa tratamento de exceções. As exceções são lançadas como objetos da classe abcg::Exception, derivada de std::exception. Vamos alterar um pouco o código anterior para capturar as exceções que possam ocorrer e imprimir no console a mensagem de erro correspondente. O código final de main.cpp ficará assim: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { // Create application instance abcg::Application app(argc, argv); // Create OpenGL window Window window; window.setWindowSettings({.title = &quot;First App&quot;}); // Run application app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } O código anterior foi colocado dentro do escopo try de um bloco try...catch. No escopo catch, a função fmt::print imprime no erro padrão (stderr) a mensagem de erro associada com a exceção capturada. fmt::print faz parte da biblioteca {fmt}. Ela permite a formatação e impressão de strings usando uma sintaxe parecida com as f-strings da linguagem Python7. window.hpp No arquivo window.hpp definiremos nossa classe Window que será responsável pelo gerenciamento do conteúdo da janela da aplicação: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow {}; #endif Observe novamente que nossa classe Window é derivada de abcg::OpenGLWindow, que faz parte da ABCg. abcg::OpenGLWindow gerencia uma janela capaz de renderizar gráficos com a API OpenGL. A classe possui um conjunto de funções virtuais que podem ser substituídas pela classe derivada de modo a alterar o comportamento da janela. O comportamento padrão consiste em desenhar a janela com fundo preto, com um contador de FPS (Frames Per Second, ou quadros por segundo) sobreposto no canto superior esquerdo da janela, e um botão no canto inferior esquerdo para alternar entre tela cheia e modo janela (com atalho pela tecla F11). O contador e o botão são gerenciados pela biblioteca Dear ImGui (no restante do texto vamos chamá-la apenas de ImGui). Por enquanto nossa classe não faz nada de especial. Ela só deriva de abcg::OpenGLWindow e não define nenhuma função ou variável membro. Mesmo assim, já podemos construir a aplicação. Experimente fazer isso. Na linha de comando, use o script build.sh (Linux/macOS) ou build.bat (Windows). Se você estiver no Visual Studio Code, abra a pasta abcg pelo editor, use a opção de configuração do CMake e então construa o projeto (F7). O executável será gerado em abcg/build/bin/firstapp. Da forma como está, a aplicação mostrará uma janela com fundo preto e os dois controles de GUI (widgets) mencionados anteriomente. Isso acontece porque Window não substitui nenhuma das funções virtuais de abcg::OpenGLWindow. Todo o comportamento está sendo definido pela classe base: Vamos alterar o conteúdo e o comportamento da nossa janela Window. Imitaremos o comportamento do projeto helloworld que cria uma pequena subjanela da ImGui. Modifique window.hpp para o código a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; private: std::array&lt;float, 4&gt; m_clearColor{0.906f, 0.910f, 0.918f, 1.0f}; }; #endif onCreate, onPaint e onPaintUI substituem funções virtuais de abcg::OpenGLWindow. A palavra-chave override é opcional, mas é recomendável pois deixa explícito que as funções são substituições das funções virtuais da classe base: onCreate é onde colocaremos os comandos de inicialização do estado da janela e do OpenGL. Internamente a ABCg chama essa função apenas uma vez no início do programa, após ter inicializado os subsistemas da SDL e o OpenGL. onPaint é onde colocaremos todas as funções de desenho do OpenGL. Internamente a ABCg chama essa função continuamente no laço principal da aplicação, uma vez para cada quadro (frame) de exibição. Por exemplo, na imagem acima, onPaint estava sendo chamada a uma média de 3988,7 vezes por segundo; onPaintUI é onde colocaremos todas as funções de desenho de widgets da ImGui (botões, menus, caixas de seleção, etc). Internamente, onPaintUI é chamado logo depois que onPaint é chamado; m_clearColor é um arranjo de quatro valores float entre 0 e 1. Esses valores definem a cor RGBA de fundo da janela, que neste caso é um cinza claro. Observação Poderíamos ter definido m_clearColor da seguinte forma, mais familiar aos programadores em C: float m_clearColor[4] = {0.906f, 0.910f, 0.918f, 1.0f}; Entretanto, em C++ o std::array é a forma recomendada e mais segura de trabalhar com arranjos. window.cpp Em window.cpp, definiremos as funções virtuais substituídas: #include &quot;window.hpp&quot; void Window::onCreate() { auto const &amp;windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } void Window::onPaint() { // Set the clear color abcg::glClearColor(m_clearColor.at(0), m_clearColor.at(1), m_clearColor.at(2), m_clearColor.at(3)); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onPaintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::onPaintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto const &amp;windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Vejamos com mais atenção o trecho com a definição de Window::onCreate: void Window::onCreate() { auto const &amp;windowSettings{getWindowSettings()}; fmt::print(&quot;Initial window size: {}x{}\\n&quot;, windowSettings.width, windowSettings.height); } Na linha 4, windowSettings é uma estrutura abcg::WindowSettings retornada por abcg::OpenGLWindow::getWindowSettings() com as configurações da janela. Na linha 5, fmt::print imprime no console o tamanho da janela8. Observe agora o trecho com a definição de Window::onPaint: void Window::onPaint() { // Set the clear color abcg::glClearColor(m_clearColor.at(0), m_clearColor.at(1), m_clearColor.at(2), m_clearColor.at(3)); // Clear the color buffer abcg::glClear(GL_COLOR_BUFFER_BIT); } Aqui são chamadas duas funções do OpenGL: glClearColor e glClear. glClearColor é utilizada para determinar a cor que será usada para limpar a janela9. A função recebe quatro parâmetros do tipo float (red, green, blue, alpha), que correspondem às componentes de cor RGB e um valor adicional de opacidade (alpha). Esse formato de cor é chamado de RGBA. Os valores são fixados (clamped) no intervalo \\([0,1]\\) em ponto flutuante. glClear, usando como argumento a constante GL_COLOR_BUFFER_BIT, limpa a janela com a cor especificada na última chamada de glClearColor. Em resumo, nosso onPaint limpa a tela com a cor RGBA especificada em m_clearColor. Importante As funções do OpenGL são prefixadas com as letras gl; As constantes do OpenGL são prefixadas com GL_. Neste curso usaremos as funções do OpenGL que são comuns ao OpenGL ES 3.0 de modo a manter compatibilidade com o WebGL 2.0. Assim conseguiremos fazer aplicações que podem ser executadas tanto no desktop quanto no navegador usando o mesmo código fonte. A versão mais recente do OpenGL é a 4.6. A documentação de cada versão está disponível em https://registry.khronos.org/OpenGL/index_gl.php. Observação Na ABCg, podemos prefixar as funções gl com o namespace abcg de modo a rastrear erros do OpenGL com o sistema de tratamento de exceções da ABCg. Por exemplo, ao escrevermos abcg::glClear no lugar de glClear, estamos na verdade chamando uma função wrapper que verifica automaticamente se a chamada da função OpenGL é válida. Se algum erro ocorrer, uma exceção será lançada e capturada pelo catch que implementamos na função main. A mensagem de erro (retornada por exception.what() no escopo do catch) inclui a descrição do erro, o nome do arquivo, o nome da função e o número da linha do código onde o erro foi detectado. Isso pode ser bastante útil para a depuração de erros do OpenGL. Sempre que possível, prefixe as funções do OpenGL com abcg::. A verificação automática de erros do OpenGL é habilitada somente quando a aplicação é compilada no modo Debug. Não há sobrecarga nas chamadas das funções do OpenGL com o namespace abcg quando a aplicação é compilada em modo Release. Agora vamos à definição de Window::onPaintUI, responsável pelo desenho da interface usando a ImGui: void Window::onPaintUI() { // Parent class will show fullscreen button and FPS meter abcg::OpenGLWindow::onPaintUI(); // Our own ImGui widgets go below { // Window begin ImGui::Begin(&quot;Hello, First App!&quot;); // Static text auto const &amp;windowSettings{getWindowSettings()}; ImGui::Text(&quot;Current window size: %dx%d (in windowed mode)&quot;, windowSettings.width, windowSettings.height); // Slider from 0.0f to 1.0f static float f{}; ImGui::SliderFloat(&quot;float&quot;, &amp;f, 0.0f, 1.0f); // ColorEdit to change the clear color ImGui::ColorEdit3(&quot;clear color&quot;, m_clearColor.data()); // More static text ImGui::Text(&quot;Application average %.3f ms/frame (%.1f FPS)&quot;, 1000.0 / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); // Window end ImGui::End(); } } Na linha 19 é chamada a função membro onPaintUI da classe base, que mostra o medidor de FPS e o botão para alternar entre o modo janela e tela cheia. Na linha 24 é criada uma janela da ImGui com o título “Hello, First App!”. A partir desta linha, até a linha 43, todas as chamadas a funções da ImGui criam widgets dentro dessa janela recém-criada. Apenas para isso ficar mais explícito, todo o código que corresponde a esta janela está dentro do escopo delimitado pelas chaves nas linhas 22 e 44. Na linha 27 é criado um texto estático que mostra o tamanho atual da janela. Na linha 32 é criado um slider horizontal que pode variar de 0 a 1 em ponto flutuante. O valor do slider é armazenado em f. A variável f é declarada como static para que seu estado seja retido entre as chamadas de onPaintUI (outra opção é declarar a variável como membro da classe). Na linha 36 é criado um widget de edição de cor para alterar os valores de m_clearColor. Na linha 39 é criado mais um texto estático com informações de FPS extraídas de ImGui::GetIO().Framerate. Esse código é praticamente o mesmo do “Hello, World!”. Construa a aplicação para ver o resultado: Exercício A seguir temos alguns exemplos de uso de outros widgets da ImGui. Experimente incluir esses trechos de código em onPaintUI: Botões: // 100x50 button if (ImGui::Button(&quot;Press me!&quot;, ImVec2(100, 50))) { fmt::print(&quot;1st button pressed.\\n&quot;); } // Nx50 button, where N is the remaining width available ImGui::Button(&quot;Press me!&quot;, ImVec2(-1, 50)); // See also IsItemHovered, IsItemActive, etc if (ImGui::IsItemClicked()) { fmt::print(&quot;2nd Button pressed.\\n&quot;); } Checkbox: static bool enabled{true}; ImGui::Checkbox(&quot;Some option&quot;, &amp;enabled); fmt::print(&quot;The checkbox is {}\\n&quot;, enabled ? &quot;enabled&quot; : &quot;disabled&quot;); Combo box: static std::size_t currentIndex{}; std::vector comboItems{&quot;AAA&quot;, &quot;BBB&quot;, &quot;CCC&quot;}; if (ImGui::BeginCombo(&quot;Combo box&quot;, comboItems.at(currentIndex))) { for (auto index{0U}; index &lt; comboItems.size(); ++index) { bool const isSelected{currentIndex == index}; if (ImGui::Selectable(comboItems.at(index), isSelected)) currentIndex = index; // Set the initial focus when opening the combo (scrolling + keyboard // navigation focus) if (isSelected) ImGui::SetItemDefaultFocus(); } ImGui::EndCombo(); } fmt::print(&quot;Selected combo box item: {}\\n&quot;, comboItems.at(currentIndex)); Menu (em uma janela de tamanho fixo e com o flag adicional ImGuiWindowFlags_MenuBar para permitir o uso da barra de menu): ImGui::SetNextWindowSize(ImVec2(300, 100)); auto flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize}; ImGui::Begin(&quot;Window with menu&quot;, nullptr, flags); { bool save{}; static bool showCompliment{}; // Hold state // Menu Bar if (ImGui::BeginMenuBar()) { // File menu if (ImGui::BeginMenu(&quot;File&quot;)) { ImGui::MenuItem(&quot;Save&quot;, nullptr, &amp;save); ImGui::EndMenu(); } // View menu if (ImGui::BeginMenu(&quot;View&quot;)) { ImGui::MenuItem(&quot;Show Compliment&quot;, nullptr, &amp;showCompliment); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (save) { // Save file... } if (showCompliment) { ImGui::Text(&quot;You&#39;re a beautiful person.&quot;); } } ImGui::End(); Mais sliders: static std::array pos2d{0.0f, 0.0f}; ImGui::SliderFloat2(&quot;2D position&quot;, pos2d.data(), 0.0, 50.0); static std::array pos3d{0.0f, 0.0f, 0.0f}; ImGui::SliderFloat3(&quot;3D position&quot;, pos3d.data(), -1.0, 1.0); Dica A ImGui não tem um manual com exemplos de uso de todos os widgets suportados. A melhor referência atualmente é o código da função ImGui::ShowDemoWindow em abcg/external/imgui/imgui_demo.cpp. Essa função cria uma janela de demonstração contendo uma grande variedade de exemplos de uso de widgets e recursos da ImGui. No exemplo “Hello, World!”, tal janela é exibida quando a caixa de seleção “Show demo window” está ativada. Por exemplo, caso você queira implementar um “List Box” como exibido na janela de demonstração abaixo, procure pela string “List boxes” ou “listbox 1” em abcg/external/imgui/imgui_demo.cpp, copie o código correspondente e adapte-o em sua aplicação. O código correspondente em imgui_demo.cpp é o seguinte: const char* items[] = { &quot;AAAA&quot;, &quot;BBBB&quot;, &quot;CCCC&quot;, &quot;DDDD&quot;, &quot;EEEE&quot;, &quot;FFFF&quot;, &quot;GGGG&quot;, &quot;HHHH&quot;, &quot;IIII&quot;, &quot;JJJJ&quot;, &quot;KKKK&quot;, &quot;LLLLLLL&quot;, &quot;MMMM&quot;, &quot;OOOOOOO&quot; }; static int item_current_idx = 0; // Here we store our selection data as an index. if (ImGui::BeginListBox(&quot;listbox 1&quot;)) { for (int n = 0; n &lt; IM_ARRAYSIZE(items); n++) { const bool is_selected = (item_current_idx == n); if (ImGui::Selectable(items[n], is_selected)) item_current_idx = n; // Set the initial focus when opening the combo (scrolling + keyboard navigation focus) if (is_selected) ImGui::SetItemDefaultFocus(); } ImGui::EndListBox(); } A documentação das funções, constantes e enumerações está no formato de comentários nos arquivos abcg/external/imgui/imgui.h e abcg/external/imgui/imgui.cpp. Observação A ImGui é uma biblioteca de GUI que trabalha em modo imediato (o “Im” de ImGui vem de immediate mode), isto é, os controles de UI não retém estado entre os quadros de exibição. Sempre que a função onPaintUI é chamada, a GUI é redesenhada por completo. O gerenciamento de estado deve ser feito pelo usuário, por exemplo, através de variáveis estáticas (como a variável f da linha 32 de window.cpp para guardar o valor do slider) ou variáveis membros da classe (como a variável m_clearColor da classe Window)10. Compilando para WebAssembly Para compilar nossa aplicação para WebAssembly basta usar o script build-wasm.sh (Linux/macOS) ou build-wasm.bat (Windows). Apenas certifique-se de habilitar antes as variáveis de ambiente do SDK do Emscripten como fizemos na seção 1.5. Após a construção do projeto, os arquivos resultantes (firstapp.js e firstapp.wasm) serão gravados no subdiretório public. Para usá-los vamos precisar de um arquivo HTML. Faça uma cópia de um dos arquivos HTML já existentes em public (helloworld.html, full_window.html ou full_window_console.html). Mude o nome do arquivo para firstapp.html. No final do arquivo, mude a string src=\"helloworld.js\" para src=\"firstapp.js\", assim: &lt;script async type=&quot;text/javascript&quot; src=&quot;firstapp.js&quot;&gt;&lt;/script&gt; Para testar, monte o servidor local com runweb.sh ou runweb.bat e abra o arquivo HTML em http://localhost:8080/. Dica Disponibilize o conteúdo web de seus projetos no GitHub Pages para formar um portfólio de atividades feitas no curso: Na sua conta do GitHub, crie um repositório com visibilidade pública. Pode ser seu próprio fork da ABCg. No diretório raiz, crie um subdiretório firstapp com os arquivos firstapp.*, mas renomeie firstapp.html para index.html; Nas configurações do repositório no GitHub, habilite o GitHub Pages informando o branch que será utilizado (por exemplo, main). O conteúdo estará disponível em https://username.github.io/reponame/firstapp/ onde username e reponame são respectivamente seu nome de usuário e o nome do repositório. Ao longo do quadrimestre, suba seus projetos nesse repositório. No diretório raiz você pode criar um index.html com a descrição do portfólio e o link para cada página de projeto. Um subconjunto da {fmt} foi incorporado à biblioteca de formatação de texto no C++20. O suporte equivalente ao fmt::print (impressão formatada com saída padrão) está disponível no C++23 através de std::print, mas ainda não é suportado nas principais implementações da biblioteca padrão.↩︎ O tamanho padrão para uma aplicação desktop é 800x600. Na versão para web, a janela pode ser redimensionada de acordo com a regra CSS do elemento canvas do HTML5.↩︎ Mais precisamente, glClearColor define a cor que será utilizada para limpar os buffers de cor do framebuffer. Veremos mais sobre o conceito de framebuffer nos próximos capítulos.↩︎ A ImGui pode reter algum estado dos controles de UI entre os frames. Por exemplo, em uma janela com vários botões, a ImGui guarda internamente qual botão está com o foco atual. Em um widget do tipo árvore, ela guarda a informação de quais nós estão expandidos e quais estão colapsados.↩︎ "],["tictactoe.html", "2.4 Jogo da Velha", " 2.4 Jogo da Velha Usando o projeto firstapp como base, faremos nesta seção um “Jogo da Velha” com interface composta apenas de widgets da ImGui. Com isso ficaremos mais familiarizados com a ImGui e entraremos em contato com novas funções da biblioteca, tais como: ImGui::BeginTable e ImGui::EndTable para fazer tabelas; ImGui::Spacing para adicionar espaçamentos verticais; ImGui::PushFont e ImGui::PopFont para usar novas fontes. A ideia principal é simular o tabuleiro do jogo com um arranjo de 3x3 botões: O jogo começará com os botões vazios, sem texto. Cada vez que um botão for pressionado, seu texto será substituído por um X ou O de acordo com o turno do jogador. Para simplificar não jogaremos contra o computador: o jogo só funcionará no modo “humano versus humano”; Internamente manteremos um arranjo contendo o estado do jogo para determinar se houve um vencedor ou se “deu velha” (empate); Usaremos um widget de texto estático para mostrar o turno atual e o resultado do jogo; Incluiremos também um botão e uma opção de menu para reiniciar o jogo. O resultado ficará como a seguir: Configuração inicial Nosso projeto será chamado tictactoe. Em abcg/examples, crie o subdiretório abcg/examples/tictactoe. Abra o arquivo abcg/examples/CMakeLists.txt e acrescente a linha add_subdirectory(tictactoe). Para evitar que os projetos anteriores continuem sendo compilados, comente as linhas anteriores. O resultado ficará assim: # add_subdirectory(helloworld) # add_subdirectory(firstapp) add_subdirectory(tictactoe) Crie o arquivo abcg/examples/tictactoe/CMakeLists.txt. O conteúdo é o mesmo do projeto anterior. A única mudança é o nome do projeto: project(tictactoe) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Em abcg/examples/tictactoe, crie o subdiretório assets. Nas aplicações usando a ABCg, o subdiretório assets é utilizado para armazenar arquivos de recursos utilizados em tempo de execução (arquivos de fontes, imagens, sons, etc). No nosso caso, colocaremos em assets o arquivo de fonte TrueType Inconsolata-Medium.ttf que será utilizado para o texto dos Xs e Os. O arquivo pode ser baixado, ou simplesmente copiado de abcg/abcg/assets (essa também é a fonte padrão da ABCg). Importante Sempre que um projeto da ABCg é configurado pelo CMake, o diretório assets é copiado automaticamente para o diretório do executável (build/bin/proj, onde proj é o nome do projeto). Toda vez que um arquivo de assets for modificado, é necessário limpar o diretório build para forçar a cópia de assets para build/bin/proj na próxima compilação. Isso pode ser feito de diferentes maneiras: Removendo o diretório build antes de compilar novamente; No VS Code, usando o comando “CMake: Clean Rebuild” da paleta de comandos (Ctrl+Shift+P); Construindo o projeto através da linha de comando com build.sh/build.bat. Em abcg/examples/tictactoe, crie os arquivos main.cpp, window.cpp e window.hpp. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é praticamente o mesmo de nossa primeira aplicação. A única diferença é o título da janela e seu tamanho inicial, que agora será 600x600. #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setWindowSettings( {.width = 600, .height = 600, .title = &quot;Tic-Tac-Toe&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } window.hpp Aqui definiremos nossa classe Window, responsável pelo gerenciamento da janela da aplicação e também da lógica do jogo. O conteúdo ficará como a seguir: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaintUI() override; private: static int const m_N{3}; // Board size is m_N x m_N enum class GameState { Play, Draw, WinX, WinO }; GameState m_gameState; bool m_XsTurn{true}; std::array&lt;char, m_N * m_N&gt; m_board{}; // &#39;\\0&#39;, &#39;X&#39; or &#39;O&#39; ImFont *m_font{}; void checkEndCondition(); void restartGame(); }; #endif Em comparação com o projeto firstapp, desta vez não substituímos o método onPaint. Podemos fazer isso pois todo o conteúdo da janela será composto por controles de UI desenhados em onPaintUI. Nossa aplicação precisa de algumas variáveis para armazenar o estado do jogo. Na linha 12, m_N é o tamanho dos lados do tabuleiro. O Jogo da Velha é jogado em um tabuleiro 3x3, mas podemos mudar esse valor para jogar com um tabuleiro 4x4, 5x5, etc. O código é genérico o suficiente para permitir isso. Na linha 14 definimos GameState como uma enumeração de todos os possíveis estados do jogo. Os estados serão interpretados da seguinte maneira: GameState::Play é quando a partida está sendo jogada. Nesse estado o jogador do turno atual poderá clicar em algum lugar do tabuleiro para colocar um X ou O; GameState::Draw é quando o jogo acabou e “deu velha”; GameState::WinX é quando o jogo acabou e X ganhou; GameState::WinO é quando o jogo acabou e O ganhou. O estado atual será indicado por m_gameState na linha 15. Na linha 17, m_XsTurn é uma variável que indica se o turno atual é do X. Na linha 18, m_board é o estado do tabuleiro, definido como um arranjo de 3x3=9 caracteres (arranjo 3x3 orientado a linhas). Cada caractere pode ser \\0 (caractere nulo) para indicar que a posição está vazia, ou a letra X, ou a letra O. Na linha 20, o ponteiro m_font será usado para representar a fonte dos Xs e Os. A classe tem duas funções: checkEndCondition, que será usada no final de cada turno para verificar se m_board está em alguma condição de vitória ou empate; restateGame, para limpar o tabuleiro e iniciar um novo jogo. window.cpp Aqui definiremos as funções membro da classe Window. Começaremos definindo Window::onCreate. Como Window::onCreate é chamada apenas uma vez quando a janela é criada, ela é ideal para fazermos as configurações iniciais da aplicação, como o carregamento da nova fonte para os Xs e Os, que tem tamanho maior que a fonte padrão. O resultado ficará assim: #include &quot;window.hpp&quot; void Window::onCreate() { // Load font with bigger size for the X&#39;s and O&#39;s auto const filename{abcg::Application::getAssetsPath() + &quot;Inconsolata-Medium.ttf&quot;}; m_font = ImGui::GetIO().Fonts-&gt;AddFontFromFileTTF(filename.c_str(), 72.0f); if (m_font == nullptr) { throw abcg::RuntimeError{&quot;Cannot load font file&quot;}; } restartGame(); } Na linha 5 criamos uma string com o caminho completo do arquivo Inconsolata-Medium.ttf. Para isso usamos a função abcg::Application::getAssetsPath() que retorna o caminho atual do subdiretório assets. Na linha 7 chamamos as funções da ImGui para carregar a fonte com tamanho 72. Se ocorrer algum erro no carregamento, o ponteiro m_Font será nulo. Neste caso lançamos uma exceção (linha 9) que será capturada no bloco try...catch de main. Na linha 12 chamamos a função restartGame para deixar o jogo pronto para uma nova partida. A propósito, vejamos como fica a implementação de restartGame: void Window::restartGame() { m_board.fill(&#39;\\0&#39;); m_gameState = GameState::Play; } A função simplesmente preenche o tabuleiro com caracteres nulos e define o estado do jogo como GameState::Play. Vamos agora definir Window::onPaintUI. É nessa função que a interface será desenhada e a lógica de interação com os controles de UI implementada. Começaremos definindo a janela da ImGui: void Window::onPaintUI() { // Get size of application&#39;s window auto const appWindowWidth{gsl::narrow&lt;float&gt;(getWindowSettings().width)}; auto const appWindowHeight{gsl::narrow&lt;float&gt;(getWindowSettings().height)}; // &quot;Tic-Tac-Toe&quot; window { ImGui::SetNextWindowSize(ImVec2(appWindowWidth, appWindowHeight)); ImGui::SetNextWindowPos(ImVec2(0, 0)); auto const flags{ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoCollapse}; ImGui::Begin(&quot;Tic-Tac-Toe&quot;, nullptr, flags); // TODO: Add Menu // TODO: Add static text showing current turn or win/draw messages // TODO: Add game board // TODO: Add &quot;Restart game&quot; button ImGui::End(); } } Nas linhas 17 e 18 pegamos o tamanho atual da janela da aplicação através da função abcg::getWindowSettings. Essa função retorna uma referência a um objeto do tipo abcg::WindowSettings contendo as configurações da janela, incluindo sua largura (width) e altura (height). Os valores são inteiros e precisam ser convertidos para float para serem utilizados nas funções da ImGui. O conteúdo da janela da ImGui é definido no escopo das linhas 21 a 35. Em particular, na linha 22 chamamos ImGui::SetNextWindowSize para informar que a janela que estamos prestes a criar terá o tamanho da janela da aplicação. De forma parecida, na linha 23 chamamos ImGui::SetNextWindowPos para informar que tal janela deverá ser posicionada na coordenada (0,0) da janela da aplicação (canto superior esquerdo). Na linha 25 definimos uma máscara de bits com as propriedades da janela que será criada. A janela terá uma barra de menu (ImGuiWindowFlags_MenuBar), não poderá ser redimensionada (ImGuiWindowFlags_NoResize), e não poderá ser colapsada ao clicar na barra de título (ImGuiWindowFlags_NoCollapse). Na linha 27 criamos de fato o controle de UI da janela. Ela é criada com as configurações definidas anteriormente. Todos os widgets criados entre essa linha de ImGui::Begin até a linha 34 de ImGui::End() serão colocados dentro dessa janela. Por enquanto a janela está vazia e só deixamos alguns comentários de tarefas a fazer: // TODO: Add Menu // TODO: Add static text showing current turn or win/draw messages // TODO: Add game board // TODO: Add &quot;Restart game&quot; button Vamos fazer essas tarefas a seguir. Adicionando o menu Substitua a linha de comentário TODO: Add Menu pelo seguinte trecho de código: // Menu { bool restartSelected{}; if (ImGui::BeginMenuBar()) { if (ImGui::BeginMenu(&quot;Game&quot;)) { ImGui::MenuItem(&quot;Restart&quot;, nullptr, &amp;restartSelected); ImGui::EndMenu(); } ImGui::EndMenuBar(); } if (restartSelected) { restartGame(); } } Este código cria uma barra de menu com uma opção “Game”. Dentro desta opção há apenas um item de menu chamado “Restart”. Observe que o estado de “Restart” é armazenado na variável booleana restartSelected, inicializada com false na linha 31. Se o item de menu é selecionado, ImGui::MenuItem muda restartSelected para true e assim chamamos restartGame na linha 40 para reiniciar o estado do jogo. Adicionando o texto do turno atual, de vitória e empate Continuando com as tarefas por fazer, substitua a linha de comentário TODO: Add static text showing current turn or win/draw messages pelo seguinte código: // Static text showing current turn or win/draw messages { std::string text; switch (m_gameState) { case GameState::Play: text = fmt::format(&quot;{}&#39;s turn&quot;, m_XsTurn ? &#39;X&#39; : &#39;O&#39;); break; case GameState::Draw: text = &quot;Draw!&quot;; break; case GameState::WinX: text = &quot;X&#39;s player wins!&quot;; break; case GameState::WinO: text = &quot;O&#39;s player wins!&quot;; break; } // Center text ImGui::SetCursorPosX( (appWindowWidth - ImGui::CalcTextSize(text.c_str()).x) / 2); ImGui::Text(&quot;%s&quot;, text.c_str()); ImGui::Spacing(); } ImGui::Spacing(); Na linha 46 definimos uma string que recebe um texto diferente dependendo do estado atual de m_gameState. Se o jogo está no modo Play, o texto será X's turn ou O's turn. Se o jogo está no modo Draw, WinX ou WinO, mensagens correspondentes de empate e vitória serão utilizadas. Na linha 62, ImGui::SetCursorPosX define a posição horizontal da janela em que o texto começará a ser exibido, da esquerda para a direita. Para que o texto fique centralizado horizontalmente, sua posição inicial deve ser a metade da largura da janela (appWindowWidth / 2) menos a metade da largura do texto (calculada com ImGui::CalcTextSize). O widget de texto é criado na linha 64. Em seguida, adicionamos um espaçamento vertical na linha 65, e outro na linha 68 para deixar um bom espaço entre o texto e o tabuleiro que será desenhado na próxima etapa. Implementando o tabuleiro O tabuleiro será mostrado como uma grade de 3x3 botões (no caso de m_N ser 3), sendo que cada botão terá como texto o caractere correspondente em m_board. Substitua a linha de comentário TODO: Add game board pelo seguinte código: // Game board { auto const gridHeight{appWindowHeight - 22 - 60 - (m_N * 10) - 60}; auto const buttonHeight{gridHeight / m_N}; // Use custom font ImGui::PushFont(m_font); if (ImGui::BeginTable(&quot;Game board&quot;, m_N)) { for (auto i : iter::range(m_N)) { ImGui::TableNextRow(); for (auto j : iter::range(m_N)) { ImGui::TableSetColumnIndex(j); auto const offset{i * m_N + j}; // Get current character auto ch{m_board.at(offset)}; // Replace null character with whitespace because the button label // cannot be an empty string if (ch == 0) { ch = &#39; &#39;; } // Button text is ch followed by an ID in the format ##ij auto buttonText{fmt::format(&quot;{}##{}{}&quot;, ch, i, j)}; if (ImGui::Button(buttonText.c_str(), ImVec2(-1, buttonHeight))) { if (m_gameState == GameState::Play &amp;&amp; ch == &#39; &#39;) { m_board.at(offset) = m_XsTurn ? &#39;X&#39; : &#39;O&#39;; checkEndCondition(); m_XsTurn = !m_XsTurn; } } } ImGui::Spacing(); } ImGui::EndTable(); } ImGui::PopFont(); } ImGui::Spacing(); Neste código, primeiro começamos calculando dois valores de altura (linhas 72 e 73). gridHeight é a altura da área útil do tabuleiro. Ela é calculada a partir da altura da janela, subtraída da altura aproximada dos outros controles de UI (22 da barra de menu, 60+60 para a área acima e abaixo do tabuleiro) e do espaçamento entre os botões do tabuleiro (10). buttonHeight é a altura de cada botão. Na linha 76, ImGui::PushFont faz com que a fonte m_font seja ativada no lugar da fonte padrão. Todo controle de UI definido entre essa linha até ImGui::PopFont (linha 107) usará essa fonte. Na linha 77 criamos uma tabela “Game board”, composta de m_N colunas. O nome “Game board” não aparece na tela. Ele serve apenas como um identificador deste controle de UI. Os laços das linha 78 e 79 iteram sobre as linhas e colunas do tabuleiro, respectivamente. Para cada nova linha chamamos ImGui::TableNextRow (linha 79), e para cada coluna chamamos ImGui::TableSetColumnIndex (linha 81) com o índice da coluna. Na linha 85 lemos o caractere atual de m_board para a linha e coluna atual. Este caractere é a letra (X, O, ou vazio) que deve ser exibida como texto do botão. Entretanto, se for um caractere nulo, mudamos para um espaço (linhas 89 a 91) pois o comando ImGui::Button não aceita strings vazias. Na linha 95 criamos o botão atual. Seu tamanho é ImVec2(-1, buttonHeight), o que significa que a largura será a máxima possível (definida por -1 ou outro valor negativo) e a altura será buttonHeight. O texto do botão (buttonText) é definido de forma um pouco mais complicada. O texto não é só o caractere ch. Se o caractere na posição (0,2) for um X, buttonText será X##02. Esse ##02 não é mostrado no botão. Ele é utilizado para indicar à ImGui que o identificador do botão é a string 02. Cada botão precisa ter um identificador único. A ImGui usa esses identificadores para definir quem está com o foco atual do teclado. Geralmente a ImGui usa o próprio texto do botão como identificador, mas como temos possivelmente vários botões com o mesmo texto (X, O ou espaço), precisamos recorrer à sintaxe ##id para definir identificadores únicos. Se o botão é pressionado, ImGui::Button retorna true. Nesse caso, precisamos verificar se a posição correspondente de m_board pode ser de fato modificada. Para isso o jogo deve estar no modo Play e ch não pode ser X ou O (linha 96), isto é, cada posição do tabuleiro só pode ser preenchida uma vez. Na linha 97 modificamos m_board para X ou O dependendo de quem está jogando o turno atual. Em seguida chamamos checkEndCondition para verificar se houve vitória ou empate (linha 98), e então alternamos o turno do jogador (linha 99). Adicionando o botão de reinício Abaixo do tabuleiro colocaremos um botão para reiniciar o jogo. Substitua a linha de comentário TODO: Add \"Restart game\" button pelo seguinte código: // &quot;Restart game&quot; button { if (ImGui::Button(&quot;Restart game&quot;, ImVec2(-1, 50))) { restartGame(); } } O botão terá largura máxima (-1) e altura 50. Note que, sempre que definimos um novo controle de UI, a ImGui cria o controle em uma nova linha (com exceção dos botões da tabela, criados entre ImGui::BeginTable e ImGui::EndTable). Fora de uma tabela, se quisermos que os controles não fiquem empilhados, podemos chamar ImGui::SameLine antes de criar o próximo controle. Assim ele será criado do lado direito do anterior. Verificando a condição de vitória e empate Para concluir a implementação de nosso Jogo da Velha, precisamos definir a função checkEndCondition. Essa função é chamada após cada jogada para verificar se m_board está em alguma condição de vitória ou empate: Se alguma linha, coluna ou diagonal de m_board tiver somente X, então o jogador do X ganhou (devemos mudar o estado do jogo para GameState::WinX). Se alguma linha, coluna ou diagonal de m_board tiver somente O, então o jogador do O ganhou (devemos ir ao estado GameState::WinO). Se o tabuleiro não tiver mais nenhum caractere nulo, e nem o X nem o O ganharam, então “deu velha” (devemos ir ao estado GameState::Draw). Vamos implementar a função aos poucos, começando com o código a seguir. Note que há vários comentários com tarefas por fazer (TODO): void Window::checkEndCondition() { if (m_gameState != GameState::Play) { return; } // Lambda expression that checks if a string contains only Xs or Os. If so, it // changes the game state to WinX or WinO accordingly and returns true. // Otherwise, returns false. auto allXsOrOs{[&amp;](std::string_view str) { if (str == std::string(m_N, &#39;X&#39;)) { m_gameState = GameState::WinX; return true; } if (str == std::string(m_N, &#39;O&#39;)) { m_gameState = GameState::WinO; return true; } return false; }}; // TODO: Check rows // TODO: Check columns // TODO: Check main diagonal // TODO: Check inverse diagonal // TODO: Check draw } O tabuleiro só precisa ser verificado quando o jogo está no estado GameState::Play. A condição da linha 124 verifica isso. Na linha 131 definimos uma expressão lambda que será usada várias vezes posteriormente para verificar as linhas, colunas e diagonais de m_board. Mais especificamente, a expressão lambda verifica se uma string str passada como parâmetro é composta apenas por 3 caracteres X ou 3 caracteres O (supondo que m_N é 3). Se sim, ela retorna true e seta o estado do jogo para WinX ou WinO de forma correspondente. Se não, ela só retorna false. Vamos agora implementar o código que corresponde ao comentário TODO: Check rows, isto é, o código que verifica cada linha do tabuleiro: // Check rows for (auto const i : iter::range(m_N)) { std::string concatenation; for (auto const j : iter::range(m_N)) { concatenation += m_board.at(i * m_N + j); } if (allXsOrOs(concatenation)) { return; } } Os laços aninhados iteram as linhas (i) e colunas (j) do tabuleiro. Para cada linha, uma string concatenation é preenchida com os caracteres daquela linha. allXsOrOs é então chamada para verificar se a string contém 3 X ou 3 O. Se sim, checkEndCondition retorna na linha 150 pois a condição final já foi encontrada. Caso contrário, a verificação deve continuar. O código dos outros TODOs é similar: // Check columns for (auto const j : iter::range(m_N)) { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + j); } if (allXsOrOs(concatenation)) { return; } } // Check main diagonal { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + i); } if (allXsOrOs(concatenation)) { return; } } // Check inverse diagonal { std::string concatenation; for (auto const i : iter::range(m_N)) { concatenation += m_board.at(i * m_N + (m_N - i - 1)); } if (allXsOrOs(concatenation)) { return; } } // Check draw if (std::find(m_board.begin(), m_board.end(), &#39;\\0&#39;) == m_board.end()) { m_gameState = GameState::Draw; } A verificação do empate é feita na condicional da linha 188. Observe que ela só será executada se as condições de vitória anteriores não tiverem sido satisfeitas. Então, se nesse momento não tiver nenhum caractere nulo em m_board, significa que o tabuleiro está todo preenchido com X e O mas ninguém ganhou, isto é, “deu velha”. Laços baseados em intervalos Observe que usamos laços for baseados em intervalos (range-based for loops) juntos com a função iter::range da biblioteca CPPItertools. Use laços baseados em intervalos sempre que possível. Eles são mais fáceis de ler e mais seguros pois evitam bugs comuns como trocar &lt; por outro comparador (&gt;, ou &lt;=) ou incrementar a variável errada. Por exemplo, para iterar com um índice i de 0 a 9, ao invés de usar o for tradicional for (int i = 0; i &lt; 10; ++i) { // i = 0, 1, ..., 9 } prefira fazer assim: for (auto i : iter::range(10)) { // i = 0, 1, ..., 9 } iter::range funciona da mesma forma que a função range do Python. De forma semelhante, para iterar um arranjo a e imprimir seu conteúdo, prefira fazer assim std::array a{&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;}; for (auto const &amp;str : a) { fmt::print(&quot;{}&quot;, str); } ao invés de: std::array a{&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;}; for (std::size_t i = 0; i &lt; a.size(); ++i) { fmt::print(&quot;{}&quot;, a[i]); } O projeto completo do Jogo da Velha pode ser baixado deste link. "],["graphicssystem.html", "3 Sistemas gráficos", " 3 Sistemas gráficos Um sistema gráfico é um sistema computacional com capacidade de processar dados para gerar imagens em um dispositivo de exibição. Em sistemas interativos, a interação com os modelos de dados gráficos se dá através de um ou mais dispositivos de entrada. Assim, de maneira geral, um sistema gráfico é composto pelos seguintes componentes: Dispositivos de entrada: teclado e dispositivos apontadores como mouse, touch pad, touch screen, graphics tablet, trackball, joystick, gamepad, entre outros. Processadores: unidade de processamento central (CPU, central processing unit), unidade de processamento gráfico (GPU, graphics processing unit) e seus subsistemas (controladores, memórias e barramentos) necessários ao processamento dos modelos de dados e conversão em representações visuais; Dispositivos de saída: monitores e telas de LCD (liquid-crystal display), OLED (organic light-emitting diodes), CRT (cathode-ray tube) ou plasma, entre outros dispositivos de exibição. A definição de um sistema gráfico com esses componentes é bastante ampla e pode incluir tanto as estações gráficas de alto desempenho equipadas com várias GPUs, quanto os computadores pessoais sem processador a gráficos. Pode incluir também os consoles de videogames, smartphones, smartwatches, smart TVs, GPSs, entre outros dispositivos com poder computacional suficiente para produzir saída em um dispositivo de exibição (figura 3.1). Figura 3.1: Exemplos de sistemas gráficos. Em sistemas gráficos atuais, o papel principal do processador gráfico é realizar a conversão de primitivas geométricas em uma imagem digital. Nos sistemas mais simples, sem aceleração de gráficos em hardware, a CPU é responsável por todo o processamento gráfico e não há distinção entre a memória da CPU e a memória de processamento gráfico. Em sistemas com processador dedicado a gráficos (GPU), a GPU pode estar integrada com o chip da CPU (como o Intel HD Graphics ou AMD HD Graphics), ou situada em uma placa de vídeo discreta com memória dedicada. GPUs discretas podem coexistir em um mesmo sistema com GPUs integradas, em diferentes configurações. Em sistemas de alto desempenho, várias GPUs em placas discretas podem ser combinadas para dividir a carga de processamento usando tecnologias como a SLI e NVLink da NVIDIA, e MGPU da AMD. A saída de um sistema gráfico é um mapa de bits armazenado em uma área da memória RAM chamada de framebuffer. Esse mapa de bits corresponde a uma imagem digital pronta para ser exibida no dispositivo de exibição. O hardware gráfico atualiza continuamente o dispositivo de exibição com o conteúdo o framebuffer. A imagem mostrada em cada atualização é chamada de quadro de exibição. A figura 3.2 mostra uma visão geral de um sistema gráfico atual com GPU dedicada em uma configuração comum em desktops. Os dados gráficos são enviados da CPU à GPU através de um barramento de alta velocidade. A GPU opera de forma assíncrona à CPU e gera a saída no framebuffer situado na memória RAM da placa gráfica. Figura 3.2: Arquitetura de um sistema gráfico atual com placa gráfica dedicada. Assim como as CPUs atualmente são compostas por vários núcleos programáveis de processamento, assim também são as GPUs atualmente, mas em uma escala muito maior. Enquanto as CPUs topo-de-linha para desktop possuem no máximo 18 núcleos, como o processador Intel Core i9-10980XE, uma GPU como a NVIDIA RTX 3080 Ti contém 10240 núcleos de processamento (chamados de CUDA cores) divididos entre 80 multiprocessadores. Por outro lado, enquanto os núcleos de uma CPU são destinados a processamento de propósito geral, os núcleos de uma GPU suportam um conjunto mais limitado de instruções, dedicado ao processamento de fluxos de dados. GPUs também podem ter núcleos para processamento ainda mais específico. Por exemplo, a mesma NVIDIA RTX 3080 Ti conta também com 320 núcleos dedicados a operações com tensores (tensor cores). Um tensor core é especializado em uma única operação: multiplicar duas matrizes 4x4 em ponto flutuante de 16 bits e acumular o resultado em uma matriz 4x4 em ponto flutuante de 16 ou 32 bits. Além disso, a RTX 3080 Ti também possui 80 núcleos dedicados a processamento de traçado de raios (RT cores). "],["lightcolor.html", "3.1 Luz e cor", " 3.1 Luz e cor Luz é a radiação eletromagnética visível ao olho humano, situada em uma faixa de frequências entre a radiação infravermelha e a ultravioleta (figura 3.3). Em particular, a luz corresponde à radiação nas frequências eletromagnéticas na faixa de 420 e 750 THz, mas é mais frequentemente caracterizada pelo comprimento de onda em vez da frequência. Cada comprimento de onda corresponde a uma cor monocromática ou cor espectral, de 400 nm, que corresponde ao limite entre o ultravioleta e o violeta, até 700 nm, que corresponde ao limite entre o vermelho e o infravermelho. Figura 3.3: Espectro eletromagnético e espectro visível (fonte). A luz de um laser é um exemplo de luz monocromática. Entretanto, em geral, a luz é uma combinação de um contínuo de diferentes comprimentos de onda em diferentes intensidades. Por exemplo, a cor branca não é uma cor monocromática, mas a composição de um espectro de luz visível que corresponde aproximadamente à luz do sol. É possível representar a distribuição de energia emitida, transmitida ou refletida de um objeto através de uma função de distribuição espectral de potência radiante (SPD, ou spectral power distribution). SPDs podem mensuradas a partir de objetos reais através de um espectrorradiômetro. A figura 3.4 mostra algumas dessas distribuições: para uma lâmpada incandescente (lâmpada halógena de tungstênio) e para a luz do dia (luz solar filtrada pela atmosfera). O eixo horizontal representa o comprimento de onda e o eixo vertical representa uma potência relativa (normalizada). Figura 3.4: Gráficos de distribuição de energia espectral (SPD) para uma lâmpada halógena e para a luz do dia. Cor é uma sensação visual; é o que percebemos quando uma composição espectral de luz estimula as células fotorreceptoras de nossos olhos, produzindo impulsos nervosos que são interpretados pelo cérebro. A sensação da cor, e a visão de uma forma geral, depende de fatores físicos (por exemplo, a distribuição da energia espectral da luz), fatores biológicos (por exemplo, a distribuição das células fotorreceptoras no fundo do olho) e psicológicos (por exemplo, como o cérebro interpreta os impulsos nervosos recebidos pelo nervo óptico). Visão tricromática A estrutura básica do olho humano é mostrada na figura 3.5. A parte da frente é análoga a um sistema de lentes de uma câmera. A íris controla a entrada da luz pela pupila. A córnea e o cristalino transmitem e focalizam a luz no fundo do olho. A luz incide sobre a retina, onde encontram-se as células fotorreceptoras. O ponto cego é uma área da retina da qual parte o nervo óptico responsável por transmitir os impulsos nervosos até o cérebro (Dale Purves 2018). Figura 3.5: Estrutura do olho humano. A retina é composta por dois tipos de células fotorreceptoras: os cones e os bastonetes. A distribuição dessas células varia de acordo com o ângulo relativo à fóvea (figura 3.6). Figura 3.6: Concentração de cones e bastonetes na retina em relação ao ângulo a partir da fóvea. Os bastonetes estão presentes em maior número (cerca de 90 milhões) e são responsáveis pela visão periférica. Bastonetes não são capazes de distinguir cores e possuem baixa acuidade visual. Por outro lado, são mais sensíveis ao brilho em condições de baixa luminosidade. Em ambientes escuros, como por exemplo em uma noite de lua nova, usamos apenas os bastonetes. A visão nessas condições é chamada de visão escotópica. Os cones estão presentes em menor número (cerca de 4,5 milhões), concentrados em torno de uma pequena região central da retina chamada de fóvea. Os cones são responsáveis pela visão em condições normais de luminosidade, com máxima acuidade e distinção de cores. A visão com os cones é chamada de visão fotópica. Há três tipos de cones: Cones S cobrem comprimentos de onda mais curtos, com sensibilidade máxima em torno de 420 a 440 nm; Cones M cobrem comprimentos de onda médios, com sensibilidade máxima em torno de 530 a 545 nm; Cones L cobrem comprimentos de onda mais longos, com sensibilidade máxima entre 560 a 580 nm. A figura 3.7 mostra a sensibilidade média dos cones para cada comprimento de onda, em uma escala normalizada. Figura 3.7: Sensibilidade dos cones S, M e L aos diferentes comprimentos de onda do espectro visível (fonte). A sobreposição da sensibilidade dos cones S, M e L fundamenta a teoria da visão tricromática desenvolvida no século XIX por Thomas Young e Herman von Helmholtz. Em 1802, Young levantou a hipótese da existência de três tipos de células fotorreceptoras com sensibilidades sobrepostas a diferentes comprimentos de onda (Young 1802). Na década de 1850, Helmholtz sugeriu que os cones possuem maior sensibilidade a três cores primárias que correspondem aproximadamente ao vermelho, verde e azul. James Clerk Maxwell também contribuiu com a teoria da visão tricromática através de experimentos de correspondência de cores e demonstração matemática de sua viabilidade (Maxwell 1857). O triângulo de Maxwell (figura 3.8) mostra como as cores verde, vermelho e azul podem ser combinadas para formar uma gama de cores. As arestas contêm cores compostas por combinações de duas primárias, enquanto que o interior contém cores obtidas através da mistura das três primárias. A teoria de Young-Helmholtz foi eventualmente validada no século XX por experimentos de observação da luz refletida da retina, experimentos subjetivos de correspondência de cores, e técnicas de microespectrofotometria. Figura 3.8: Triângulo de Maxwell. Um aspecto importante que decorre do espalhamento das sensibilidades dos cones é a possibilidade de diferentes SPDs produzirem a mesma sensação de cor. Por exemplo, a resposta do estímulo de um cone S em ~445 nm (pico de sensibilidade) pode ser a mesma do estímulo em ~500 nm (baixa sensibilidade) com intensidade mais alta. Isso significa que objetos que refletem luz com diferentes SPDs podem ser percebidos como uma mesma cor. Esse fenômeno é chamado de metamerismo, e uma mesma cor produzida com diferentes SPDs é chamada de cor metamérica. É por causa do metamerismo que, por exemplo, a sensação da cor espectral 510 nm (ciano) ou 558 nm (amarelo) pode ser reproduzida em um monitor de LED composto apenas por LEDs vermelhos, verdes e azuis. A figura 3.9 mostra um gráfico de eficiência luminosa que mostra a sensibilidade média da visão humana para cada comprimento de onda do espectro visível na visão fotópica e escotópica. Os dados que compõem este gráfico foram obtidos através de avaliações subjetivas com diferentes indivíduos, e hoje servem como uma linha de base da sensibilidade teórica da visão humana. O gráfico da visão fotópica, denotado por \\(y(\\lambda)\\) ou \\(V(\\lambda)\\), foi padronizado em 1931 pela comissão internacional de iluminação (CIE, commission internationale de l’éclairage) (CIE 1932). Figura 3.9: Funções de eficiência luminosa para a visão fotópica e escotópica. Em condições de luminosidade intermediária (por exemplo, à luz da lua), usamos tanto os bastonetes quanto os cones, na chamada visão mesópica. Na visão mesópica, a eficácia relativa da visão é maior na região mais próxima ao azul. Isso faz com que objetos vermelhos pareçam mais escuros que objetos de outra cor, e a luz em geral tende a parecer mais fria (efeito Purkinje). Modelos de cor Um modelo de cor é um modelo matemático utilizado para representar cores através de tuplas de números. Os modelos mais utilizados são o modelo RGB (e suas variantes, como HSL e HSV), e o modelo CMY e CMYK. RGB, HSL e HSV O modelo RGB é o modelo de cor utilizado por dispositivos de exibição baseados em emissão de luz. Uma cor no formato RGB é representada por três números que correspondem a uma combinação de intensidades de luz de cor vermelha (R, red), verde (G, green) e azul (B, blue). Em geral, cada componente de cor é representada por valores em ponto flutuante entre 0 (intensidade mínima) e 1 (intensidade máxima), ou como valores inteiros entre 0 e 255 de modo que a cor possa ser representada com três bytes. Esse último formato é o formato padrão das cores na web, comumente escritos em notação hexadecimal. As cores primárias no modelo RGB podem ser combinadas para formar cores secundárias: ciano (verde + azul), magenta (azul + vermelho) e amarelo (vermelho + verde). A combinação das cores primárias em igual proporção produz tons de cinza, do preto (0,0,0) ao branco (1,1,1). O conjunto de todas possíveis combinações pode ser visualizado como um cubo de cores como mostra a figura 3.10. Figura 3.10: Modelo RGB (fonte). Embora o modelo RGB seja o modelo padrão de representação de cores em imagens digitais e em APIs gráficas, a composição de cores através da mistura de cores primárias pode ser pouco intuitiva. Em aplicativos de edição de imagens é comum o uso de modelos alternativos, como o HSL (hue, saturation, lightness) e HSV (hue, saturation, value). Esses modelos permitem que as cores sejam combinadas através de valores de matiz (entre 0º e 360º), saturação (entre 0 e 1) e luminosidade/intensidade (entre 0 e 1). A figura 3.11 ilustra esses modelos. Figura 3.11: Modelo HSL e HSV (fonte). CMY e CMYK O modelo RGB é um modelo aditivo de cor pois as cores são obtidas através da adição de cores primárias. O oposto do modelo aditivo é o modelo subtrativo, utilizado em tintas e pigmentos. Quando uma luz incide sobre um pigmento, este absorve determinados comprimentos de onda e reflete outros, de modo que a cor percebida depende da SPD de luz filtrada refletida do pigmento. O modelo CMY é um modelo subtrativo que usa como cores primárias o ciano (C, cyan), magenta (M, magenta) e amarelo (Y, yellow). Pigmentos nessas cores filtram (absorvem) a luz nas cores vermelha, verde e azul, respectivamente. Assim, quando os pigmentos são aplicados sobre uma superfície branca, a cor preta resulta da mistura dessas cores primárias em igual quantidade, enquanto a cor branca corresponde à ausência dos pigmentos. Em impressoras é comum o uso do modelo CMYK que inclui o pigmento preto (K, black) de modo a melhorar o contraste. Isso ocorre porque os pigmentos CMY utilizados em impressão geralmente não absorvem perfeitamente as cores complementares RGB. Além disso, o uso de um pigmento preto ajuda a reduzir So consumo das tintas CMY. Sistema CIE 1931 Os modelos RGB ou CMYK por si só não definem a colorimetria das cores primárias. Um mesmo valor de cor RGB pode parecer diferente dependendo das características das fontes de luz ou dos pigmentos que geram as cores primárias. Para possibilitar a correta reprodução de cores entre diferentes dispositivos, é necessário que o modelo de cor tenha como referencial um espaço de cor absoluto no qual as cores possam ser caracterizadas por medidas colorimétricas tais como comprimento de onda e intensidade. Se tivermos fontes de luz capazes de gerar cores primárias monocromáticas em diferentes intensidades, podemos modelar um espaço de cor absoluto através de uma medição empírica de quais intensidades de cores primárias são necessárias para produzir cada cor espectral. William David Wright (Wright 1929) e John Guild (Guild 1932) realizaram tal experimento na década de 1920. Os resultados foram padronizados no sistema CIE 1931, utilizado atualmente como sistema padrão de correspondência de cores (Smith and Guild 1931) (CIE 1932). Nos experimentos de Wright e Guild, as intensidades de três fontes de luz monocromática (700 nm, 546.1 nm, e 435.8 nm) foram ajustadas em condições de visão fotópica de modo que a combinação das três cores imitasse cada cor do espectro visível. O experimento foi repetido com diferentes pessoas, obtendo resultados semelhantes, padronizado pelo resultado mostrado na figura 3.12. Figura 3.12: Funções de correspondência CIE RGB (fonte). Curiosamente, alguns valores são negativos. Isso ocorre porque há cores espectrais que não podem ser obtidas a partir da combinação de apenas três cores primárias. Quando esses casos aconteciam durante o experimento, o sujeito da experimentação adicionava vermelho, verde ou azul à cor espectral de referência até conseguir obter uma correspondência. A adição na cor de referência corresponde efetivamente a uma subtração nas primárias, produzindo os valores negativos. A escala das funções de correspondência supõe que as cores primárias possuem o mesmo brilho. Entretanto, segundo a função de eficiência luminosa \\(V(\\lambda)\\), a visão humana percebe o brilho de forma mais eficiente para comprimentos de onda na região do amarelo-esverdeado. As funções de correspondência podem ser ajustadas para as luminâncias absolutas usando as escalas 1, 4.5907 e 0.0601, respectivamente. Os valores das funções R, G e B resultantes dessa escala são chamados de valores de triestímulo: \\[ \\begin{align} R &amp;= \\bar{r}(\\lambda) \\times 1,\\\\ G &amp;= \\bar{g}(\\lambda) \\times 4.5907,\\\\ B &amp;= \\bar{b}(\\lambda) \\times 0.0601.\\\\ \\end{align} \\] A combinação dos valores de triestímulo resulta em um espaço tridimensional. Para facilitar a visualização, a dimensão que corresponde unicamente a variações de intensidade pode ser ignorada. Isso pode ser feito através de uma normalização de \\(R\\), \\(G\\) e \\(B\\) de modo que a soma dos valores seja sempre igual a um: \\[ r = \\frac{R}{R+G+B},\\\\ g = \\frac{G}{R+G+B},\\\\ b = \\frac{B}{R+G+B},\\\\ \\,\\\\ r+g+b = 1. \\] Valores de \\(r\\), \\(g\\) e \\(b\\) são chamados de valores de cromaticidade, pois representam a informação da cor independente da intensidade. Podemos nos concentrar apenas em \\(r\\) e \\(g\\), pois \\(b\\) é uma combinação dos dois primeiros (\\(b=1-r-g\\)). Desse modo, podemos desenhar um gráfico bidimensional de cromaticidade no plano \\(rg\\) como mostra a figura 3.13. Pontos sobre a borda da região colorida indicam coordenadas de cromaticidade que correspondem a cores espectrais. Pontos dentro da região colorida correspondem às cores não-espectrais que podem ser obtidas a partir da combinação de cores espectrais. Os pontos na região triangular colorida no primeiro quadrante correspondem às cores que podem ser obtidas a partir da combinação das cores primárias. Em particular, as cores primárias vermelho, verde e azul estão situadas nas coordenadas (1,0), (0,1) e (0,0), respectivamente. A região colorida com coordenadas negativas corresponde a cores reais, mas que não podem ser alcançadas através da combinação das cores primárias utilizadas. Figura 3.13: Diagrama de cromaticidade CIE rg (fonte). Para obter funções de correspondência de cores com valores unicamente positivos, a CIE criou o espaço de cor CIE XYZ, que usa cores primárias imaginárias XYZ no lugar de RGB. Esse espaço é obtido a partir de uma transformação linear das funções de correspondência CIE RGB. Além de tornar as funções positivas, a transformação é feita de tal forma que a função \\(\\bar{y}(\\lambda)\\) deste novo espaço corresponda exatamente à função de eficiência luminosa \\(V(\\lambda)\\) na visão fotópica. As funções de correspondência resultantes são mostradas na figura 3.14. Figura 3.14: Funções de correspondência CIE XYZ (fonte). Da mesma forma que podemos obter um diagrama de cromaticidade CIE rg a partir da normalização das funções de correspondência CIE RGB, podemos também obter um diagrama de cromaticidade xy com a normalização \\[ x = \\frac{X}{X+Y+Z},\\\\ y = \\frac{Y}{X+Y+Z},\\\\ z = \\frac{Z}{X+Y+Z}.\\\\ \\,\\\\ x+y+z = 1. \\] O diagrama de cromaticidade xy é mostrado na figura 3.15. O triângulo dentro da região colorida mostra a região formada pelas primárias \\(R\\), \\(G\\), \\(B\\) do experimento de Wright e Guild. As cores que podem ser obtidas a partir da combinação de primárias é chamada de gama de cores (em inglês, color gamut) e corresponde à área do triângulo. Figura 3.15: Diagrama de cromaticidade CIE xy (fonte). Observação No diagrama de cromaticidade CIE rg (figura 3.13), a transformação de CIE RGB para CIE XYZ leva os pontos \\(C_r\\), \\(C_g\\) e \\(C_g\\) (vértices do triângulo vermelho) para os pontos \\((1,0)\\), \\((0,1)\\), \\((0,0)\\) do diagrama de cromaticidade CIE xy (figura 3.15). O espaço formado pelas coordenadas \\(x\\), \\(y\\) e \\(Y\\) é chamado de espaço CIE xyY. Nesse espaço, as duas primeiras coordenadas (\\(x\\), \\(y\\)) correspondem às cromaticidades, isto é, às variações de cor independente de luminosidade. A terceira coordenada (\\(Y\\)) corresponde apenas a variações de luminosidade. O espaço CIE xyZ tem sido utilizado como referência para caracterizar de forma precisa os diferentes espaços de cor utilizados em dispositivos de exibição e impressão. A figura 3.16 mostra alguns dos principais espaços de cor utilizados na indústria. O padrão mais comum em monitores de computador é o espaço sRGB, criado pela HP e Microsoft em 1996 e depois tornado padrão pela Comissão Eletrotécnica Internacional (IEC, International Electrotechnical Commission) como o padrão IEC 61966-2-1:1999. Se um monitor é compatível com o espaço sRGB, isso significa que ele é capaz de reproduzir a gama de cores desse espaço. Para todo monitor é possível determinar um perfil de correspondência de cores do modelo RGB para as cores correspondentes no espaço CIE XYZ (essa correspondência é chamada de perfil ICC). Desse modo é possível estabelecer uma correspondência exata entre cores do monitor e cores de outro dispositivo, garantindo a reprodução fiel das cores. Figura 3.16: Gamas de cores de diferentes espaços de cor (fonte). Referências "],["vectorxraster.html", "3.2 Vetorial x matricial", " 3.2 Vetorial x matricial Em computação gráfica, é comum trabalharmos com dois tipos de representações de gráficos: a representação vetorial, utilizada na descrição de formas 2D e 3D compostas por primitivas geométricas, e a representação matricial, utilizada em imagens digitais e definição de texturas. O processo de converter representações vetoriais em representações matriciais desempenha um papel central no pipeline de processamento gráfico, uma vez que a representação matricial é a representação final de uma imagem nos dispositivos de exibição. Essa conversão matricial, também chamada de rasterização (raster conversion ou scan conversion), é implementada em hardware nas GPUs atuais. A figura 3.17 ilustra o resultado da conversão de uma representação vetorial em representação matricial. As formas geométricas à esquerda estão representadas originalmente no formato SVG (Scalable Vector Graphics), que é o formato padrão de gráficos vetoriais nos navegadores Web. A imagem à direita é um arranjo bidimensional de valores de cor, resultado da renderização das formas SVG em uma imagem digital (neste caso, uma imagem de baixa resolução). Figura 3.17: Rasterização de um círculo e triângulo. Observação A figura 3.17 é apenas ilustrativa. Rigorosamente falando, a imagem da esquerda também está no formato matricial. O navegador converte automaticamente o código SVG em comandos da API gráfica que fazem com que a GPU renderize a imagem que vemos na tela. A rasterização ocorre durante este processamento. A imagem à direita não precisa passar pelo processo de renderização pois já é uma imagem digital em seu formato nativo. Representação vetorial Na representação vetorial, os gráficos são descritos em termos de primitivas geométricas. Por exemplo, o formato SVG é um formato de descrição de gráficos vetoriais 2D através de sequências de comandos de desenho. Uma forma 2D pode ser descrita através da definição de um “caminho” (path) composto por uma sequência de passos de movimentação de uma caneta virtual sobre um plano. Os principais passos utilizados são comandos do tipo MoveTo, LineTo e ClosePath: MoveTo (denotado por M ou m em SVG11) move a caneta virtual para uma nova posição na área de desenho, como se ela fosse levantada da superfície e posicionada em outro local; LineTo (L ou l) traça um segmento de reta da posição atual da caneta até uma nova posição, que passa a ser a nova posição da caneta; Em uma sequência de comandos LineTo, o comando ClosePath (Z ou z) traça um segmento de reta que fecha o caminho da posição atual da caneta ao ponto inicial. Observe o código SVG a seguir que resulta no desenho do triângulo visto mais abaixo: &lt;svg width=&quot;250&quot; height=&quot;210&quot;&gt; &lt;path d=&quot;M125 0 L0 200 L250 200 Z&quot; stroke=&quot;black&quot; fill=&quot;lightgray&quot; /&gt; &lt;/svg&gt; No rótulo &lt;svg&gt;, os atributos width=\"250\" e height=\"210\" definem que a área de desenho tem largura 250 e altura 210. Por padrão, a origem fica no canto superior esquerdo. O eixo horizontal (\\(x\\)) é positivo para a direita, e o eixo vertical (\\(y\\)) é positivo para baixo. O atributo d do rótulo &lt;path&gt; contém os comandos de desenho do caminho. M125 0 move a caneta virtual para a posição (125,0). Em seguida, L0 200 traça um segmento da posição atual até a posição (0, 200), que passa a ser a nova posição da caneta. L250 200 traça um novo segmento até (250, 200). O comando Z fecha o caminho até a posição inicial em (125, 0), completando o triângulo. O atributo stroke=\"black\" define a cor do traço como preto, e fill=\"lightgray\" define a cor de preenchimento como cinza claro: O formato SVG também suporta a descrição de curvas, arcos, retângulos, círculos, elipses, entre outras primitivas geométricas. Comandos similares são suportados em outros formatos de gráficos vetoriais, como o EPS (Encapsulated PostScript), PDF (Portable Document Format), AI (Adobe Illustrator Artwork) e DXF (AutoCAD Drawing Exchange Format). Representação vetorial no OpenGL No OpenGL, a representação vetorial é utilizada para definir a geometria que será processada durante a renderização. Todas as primitivas geométricas são definidas a partir de vértices que representam posições em um espaço, e atributos adicionais definidos pelo programador (por exemplo, a cor do vértice). Esses vértices são armazenados em arranjos ordenados que são processados em um fluxo de vértices no pipeline de renderização especificado pelo OpenGL. Os vértices podem ser utilizados para formar diferentes primitivas. Por exemplo, o uso do identificador GL_TRIANGLES na função de renderização glDrawArrays faz com que seja formado um triângulo a cada grupo de três vértices do arranjo de vértices. Assim, se o arranjo tiver seis vértices (em uma sequência de 0 a 5), serão formados dois triângulos: um triângulo com os vértices 0, 1, 2, e outro com os vértices 3, 4, 5. Para o mesmo arranjo de vértices, GL_POINTS faz com que o pipeline de renderização interprete cada vértice como um ponto separado, e GL_LINE_STRIP faz com que o pipeline de renderização forme uma sequência de segmentos (uma polilinha) conectando os vértices. A figura 3.18 ilustra a formação dessas primitivas para um arranjo de seis vértices no plano. A numeração indica a ordem dos vértices no arranjo. Figura 3.18: Formando diferentes primitivas do OpenGL com um mesmo arranjo de vértices. A figura 3.19 mostra como a geometria das primitivas pode mudar (com exceção de GL_POINTS) caso os vértices estejam em uma ordem diferente no arranjo. Figura 3.19: A ordem dos vértices no arranjo altera a geometria das primitivas. Veremos com mais detalhes o uso de primitivas no próximo capítulo quando abordaremos as diferentes etapas de processamento do pipeline de renderização do OpenGL. Observação Até a década de 2010, a maneira mais comum de renderizar primitivas no OpenGL era através de comandos do modo imediato de renderização, como a seguir (em C/C++): glColor3f(0.83f, 0.83f, 0.83f); // Light gray color glBegin(GL_TRIANGLES); glVertex2i(-1, -1); glVertex2i( 1, -1); glVertex2i( 0, 1); glEnd(); Nesse código, a função glColor3f informa que a cor dos vértices que estão prestes a ser definidos será um cinza claro, como no triângulo desenhado com SVG. O sufixo 3f de glColor3f indica que os argumentos são três valores do tipo float. Entre as funções glBegin e glEnd é definida a sequência de vértices. Cada chamada a glVertex2i define as coordenadas 2D de um vértice (o sufixo 2i indica que as coordenadas são compostas por dois números inteiros). Como há três vértices e a primitiva é identificada com GL_TRIANGLES, será desenhado um triângulo cinza similar ao triângulo desenhado com SVG, porém sem o contorno preto12. O sistema de coordenadas nativo do OpenGL não é o mesmo da área de desenho do formato SVG. No OpenGL, a origem é o centro da janela de visualização, sendo que o eixo \\(x\\) é positivo à direita e o eixo \\(y\\) é positivo para cima. Além disso, para que a primitiva possa ser vista, as coordenadas dos vértices precisam estar entre -1 e 1 (em ponto flutuante). Para desenhar o triângulo colorido do exemplo “Hello, World!”, como visto na seção 1.5, poderíamos utilizar o seguinte código: glBegin(GL_TRIANGLES); glColor3f(1.0f, 0.0f, 0.0f); // Red glVertex2f(0.0f, 0.5f); glColor3f(1.0f, 0.0f, 1.0f); // Magenta glVertex2f(0.5f, -0.5f); glColor3f(0.0f, 0.0f, 1.0f); // Green glVertex2f(-0.5f, -0.5f); glEnd(); Observe que, antes da definição de cada vértice, é definida a sua cor. Quando o triângulo é processado na GPU, as cores em cada vértice são interpoladas bilinearmente (em \\(x\\) e em \\(y\\)) ao longo da superfície do triângulo, formando um gradiente de cores. Em nossos programas usando a ABCg, bastaria colocar esse código na função membro onPaint de nossa classe derivada de abcg::OpenGLWindow. Internamente o OpenGL utilizaria um pipeline de renderização de função fixa (pipeline não programável) para desenhar o triângulo. No entanto, se compararmos com o código atual do projeto no subdiretório abcg\\examples\\helloworld, perceberemos que não há nenhum comando glBegin, glVertex* ou glColor*. Isso acontece porque o código acima é obsoleto. As funções do modo imediato foram retiradas do OpenGL na versão 3.1 (de 2009). Ainda é possível habilitar um “perfil de compatibilidade” (compatibility profile) para usar funções obsoletas do OpenGL, mas esse perfil não é recomendado para código atual. Por isso, não o utilizaremos neste curso. Atualmente, para desenhar primitivas com o OpenGL, o arranjo ordenado de vértices precisa ser enviada previamente à GPU juntamente com programas chamados shaders que definem como os vértices serão processados e como os pixels serão preenchidos após a rasterização. Desenhar um simples triângulo preenchido no OpenGL não é tão simples como antigamente, mas essa dificuldade é compensada pela maior eficiência e flexibilidade obtida com a possibilidade de programar o comportamento da GPU. Representação matricial Na representação matricial, também chamada de representação raster, as imagens são compostas por arranjos bidimensionais de elementos discretos e finitos chamados de pixels (picture elements). Um pixel contém uma informação de amostra de cor e corresponde ao menor elemento que compõe a imagem. A resolução da imagem é o número de linhas e colunas do arranjo bidimensional. Esse é o formato utilizado nos arquivos GIF (Graphics Interchange Format), TIFF (Tag Image File Format), PNG (Portable Graphics Format), JPEG e BMP. A figura 3.20 mostra uma imagem digital e um detalhe ampliado. Figura 3.20: Imagem digital de 300x394 pixels e detalhe ampliado de 38x38 pixels. Observação Embora os pixels ampliados da figura 3.20 sejam mostrados como pequenos quadrados coloridos, um pixel não tem necessariamente o formato de um quadrado. Um pixel é apenas uma amostra de cor e pode ser exibido em diferentes formatos de acordo com o dispositivo de exibição. Uma imagem digital pode ser armazenada como um mapa de bits (bitmap). A quantidade de cores que podem ser representadas em um pixel – a profundidade da cor (color depth) – depende do número de bits designados a cada pixel. Em uma imagem binária, cada pixel é representado por apenas 1 bit. Desse modo, a imagem só pode ter duas cores, como preto (para os bits com estado 0) e branco (para os bits com estado 1). A figura 3.21 mostra uma imagem binária em formato BMP, que é um formato simples utilizado para armazenar mapas de bits. Figura 3.21: Imagem binária. A imagem da figura 3.21 foi gerada a partir de outra imagem de maior profundidade de cor (figura 3.25) usando o algoritmo Floyd-Steinberg de dithering (Floyd and Steinberg 1976). Dithering é o processo de introduzir um ruído ou padrão de pontilhado que atenua a percepção de artefatos no formato de bandas resultantes da quantização da cor (color banding). A figura 3.22 mostra esse efeito em uma imagem colorida. A imagem da esquerda é a imagem original, com 24 bits de profundidade de cor. A imagem do centro teve a profundidade de cor reduzida para 4 bits (16 cores). É possível perceber as bandas de cor no gradiente do céu. Na imagem da direita, a profundidade de cor também foi reduzida para 4 bits, mas o uso de dithering reduz a percepção das variações bruscas de tom. Figura 3.22: Redução de bandas de cor com dithering. Esquerda: imagem original de 24 bits/pixel. Centro: redução para 4 bits/pixel. Direita: redução para 4 bits/pixel usando dithering. Em imagens com profundidade de cor de 8 bits, cada pixel pode assumir um valor de 0 a 255. Esse valor pode ser interpretado como um nível de luminosidade para, por exemplo, descrever imagens monocromáticas de 256 tons de cinza (figura 3.23). Figura 3.23: Imagem monocromática de 8 bits por pixel. Uma outra possibilidade é fazer com que cada valor corresponda a um índice de uma paleta de cores que determina qual será a cor do pixel. Em imagens de 8 bits, a paleta de cores é uma tabela de 256 cores, sendo que cada cor é definida por 3 bytes, um para cada componente de cor RGB (vermelho, verde, azul). Esse formato de cor indexada foi o formato predominante em computadores pessoais na década de 1990, quando o hardware gráfico só conseguia exibir um máximo de 256 cores simultâneas no modo VGA (Video Graphics Array). O formato GIF, criado em 1987, utiliza cores indexadas. A figura 3.24 exibe uma imagem GIF e sua paleta correspondente de 256 cores. Figura 3.24: Imagem de 8 bits com cores indexadas (esquerda) e paleta utilizada (direita). Atualmente, as imagens digitais coloridas usam o formato true color no qual cada pixel tem 24 bits (3 bytes, um para cada componente de cor RGB), sem o uso de paleta de cor (figura 3.25). Isso possibilita a exibição de \\(2^{24}\\) cores simultâneas (aproximadamente 16 milhões). Figura 3.25: Imagem de 24 bits por pixel. Em arquivos de imagens, também é comum o uso de 32 bits por pixel (4 bytes), sendo 3 bytes para as componentes de cor e 1 byte para definir o nível de opacidade do pixel. Isso permite realizar composição de imagens sobrepostas, por exemplo, misturando cores de uma imagem A sobre uma imagem B, usando o valor de opacidade como peso da combinação. Geralmente, os valores de intensidade de cor de um pixel são representados por números inteiros. Entretanto, imagens podem ser especificadas em um formato HDR (high dynamic range) no qual cada componente de cor pode ter até 32 bits em formato de ponto flutuante, permitindo alcançar uma faixa mais ampla de intensidades. As GPUs atuais fornecem suporte a um variado conjunto de formatos de bits, incluindo suporte a mapas de bits compactados e tipos de dados em formato de ponto flutuante de 16 e 32 bits. Referências "],["es.html", "3.3 Dispositivos de E/S", " 3.3 Dispositivos de E/S A seguir apresentamos uma visão geral de conceitos e tecnologias relacionadas a dispositivos de entrada e saída utilizados em sistemas gráficos. Dispositivos de entrada Um sistema gráfico possui um ou mais dispositivos de entrada que permitem ao usuário interagir com os modelos de dados gráficos. O mais tradicional dispositivo de entrada é o dispositivo de teclado. O teclado produz um código (scancode) composto por um byte ou sequência de bytes que identifica cada tecla pressionada e liberada. O teclado virtual de um smartphone ou tablet é também um dispositivo de teclado. Embora não possua teclas físicas, o resultado dos toques na tela ou conversão de uma anotação manuscrita em texto é um conjunto de scancodes que corresponde aos mesmos caracteres de teclas de um teclado físico (figura 3.26). Os códigos de um dispositivo de teclado podem ser interpretados como direções de movimentação de um cursor de desenho para permitir a interação com dados gráficos. Figura 3.26: Teclado virtual de um iPad (fonte). Além do teclado, é comum que um sistema gráfico tenha também pelo menos um dispositivo apontador, como o mouse, capaz de fornecer dados de movimentação ou posicionamento sobre uma superfície, geralmente mapeados para uma posição na tela. O mouse produz dados de deslocamento em duas direções ortogonais que correspondem ao movimento horizontal (\\(x\\)) e vertical (\\(y\\)) do dispositivo13. Como os dados produzidos são apenas deslocamentos, e não posições, o mouse é considerado um dispositivo de posicionamento relativo. Entretanto, os deslocamentos em \\(x\\) e \\(y\\) podem ser interpretados como velocidades e acumulados ao longo do tempo para determinar a posição de um cursor na tela. Outros dispositivos populares de posicionamento relativo são os touch pads, trackballs, joysticks e gamepads (figura 3.27). Tais dispositivos também possuem botões que podem ser configurados da mesma forma que as teclas de um teclado. Figura 3.27: Dispositivos apontadores de posicionamento relativo. Da esquerda para a direita: trackball (fonte), joystick (fonte), gamepad (fonte). Dispositivos apontadores como a tela sensível ao toque (touch screen) e a mesa digitalizadora (graphics tablet) são capazes de fornecer dados de posicionamento absoluto (figura 3.28). Os toques produzidos com o dedo ou com uma caneta de toque (stylus pen) produzem dados que correspondem a um par de coordenadas sobre a superfície de desenho, além de um valor que corresponde à pressão aplicada. Esses dispositivos também podem ser configurados para gerar dados de posicionamento relativo e detecção de gestos de arrasto (swipe e drag and drop) através do rastreamento dos pontos de pressão. Telas sensíveis ao toque frequentemente também são capazes de detectar múltiplos toques simultâneos, permitindo a detecção de gestos mais complexos como pinça (pinch) e rotação (rotate). Figura 3.28: Mesa digitalizadora com caneta (fonte). Dispositivos de saída Um sistema gráfico possui pelo menos um dispositivo de saída para exibição de gráficos. Esses dispositivos podem ser do tipo vetorial ou matricial. Dispositivos vetoriais O primeiro dispositivo de exibição utilizado em computador foi o CRT vetorial, que é o mesmo tipo de tecnologia utilizada nas telas dos antigos osciloscópios analógicos (figura 3.29). Figura 3.29: Osciloscópio analógico com CRT vetorial (fonte). No CRT vetorial, um canhão de elétrons emite um feixe de elétrons que incide sobre uma tela revestida por um material fotoluminescente (fósforo). Um conjunto de placas defletoras eletromagnéticas permite alterar a posição horizontal (\\(x\\)) e vertical (\\(y\\)) de incidência do feixe, de modo que gráficos de linhas e curvas podem ser traçados na tela. Em um sistema gráfico, a posição de incidência do feixe pode ser descrita por comandos do tipo MoveTo e LineTo (figuras 3.30 e 3.31). Como o brilho do fósforo tem persistência baixa, na ordem de milissegundos, é preciso redesenhar o traço continuamente. Figura 3.30: Desenhando um triângulo em um CRT vetorial. A sequência de passos de 1 a 4 precisa ser repetida continuamente para manter a imagem na tela. Figura 3.31: Jogo estilo “Asteroides” (“Space Rocks”) sendo exibido em um CRT vetorial de um antigo osciloscópio (fonte). Dispositivos de exibição vetorial não conseguem desenhar de forma adequada áreas preenchidas. Além disso, a velocidade de geração do desenho é proporcional à quantidade de primitivas e ao comprimento dos caminhos, impondo um limite à complexidade do desenho. Por essas desvantagens, CRTs vetoriais tornaram-se obsoletos e foram substituídos inteiramente pelos dispositivos matriciais. Dispositivos matriciais O primeiro dispositivo de exibição matricial utilizado em computadores também foi o CRT (Noll 1971). No CRT matricial, o feixe de elétrons é direcionado por deflexão eletromagnética e varre continuamente a tela de cima para baixo, da esquerda para direita. A cada linha percorrida, o canhão de elétrons é desligado momentaneamente e religado no início da próxima linha (retraço horizontal). Ao completar a varredura no canto inferior direito, o canhão de elétrons é desligado e direcionado para o ponto inicial, no canto superior esquerdo (retraço vertical). O feixe de elétrons é então religado e uma nova varredura é feita, iniciando um novo quadro de exibição. Esse processo é feito continuamente, a uma taxa que, nos televisores antigos, era sincronizada com a frequência da rede elétrica: 50 Hz ou 60 Hz14. Durante a varredura, a intensidade do feixe é controlada por um sinal analógico de vídeo. Esse sinal pode ser produzido por um conversor digital-analógico a partir de uma imagem digital, reproduzindo na tela os pontos que formam a imagem (figura 3.32). Figura 3.32: Varredura de um quadro em um CRT matricial. CRTs coloridos utilizam três canhões de elétrons, um para cada componente de cor RGB. A tela é coberta por um padrão de fósforos nessas cores, em grupos de três. Uma máscara ou grelha metálica próxima da tela (shadow mask, slot mask ou apperture grille, dependendo da tecnologia utilizada) assegura que cada tipo de fósforo recebe elétrons apenas do canhão correspondente. A figura 3.33 mostra o detalhe ampliado da tela de um CRT de TV e um CRT de computador, mostrando o padrão das tríades RGB formadas pelo slot mask (no CRT de TV) e shadow mask (no CRT de computador). Uma vez que os padrões são muito pequenos e cobrem a tela por completo, o usuário percebe a combinação das cores primárias que resultam na cor da imagem. A figura 3.34 mostra o detalhe ampliado de uma letra “e” exibida em um CRT de TV que usa a tecnologia de apperture grille (tecnologia Trinitron, da Sony), e o detalhe ampliado de um cursor em um CRT de computador. Figura 3.33: Padrões de fósforos RGB em CRTs. Esquerda: slot mask em um CRT de TV. Direita: shadow mask em um CRT de PC. (fonte) Figura 3.34: Detalhes ampliados de telas de CRT. Esquerda: letra ‘e’ em um CRT de TV Sony Trinitron (fonte). Direita: cursor na tela de um CRT de computador (fonte). Os CRTs não são mais utilizados desde meados de 2000 e foram substituídos pelos monitores LCD (liquid-crystal display), que utilizam sinais digitais de vídeo. Até a metade de 2010 eram também comuns os monitores de tela de plasma. Nessa tecnologia, tensões aplicadas em eletrodos de endereçamento de linhas e colunas energizam um gás (geralmente néon e xenônio) contido em minúsculas células envoltas em painéis de vidro. O fundo das células é coberto por fósforo nas cores RGB, de modo que cada grupo de 3 cores forma um pixel. Como em uma lâmpada fluorescente, o gás ionizado se torna um plasma emissor de luz ultravioleta que faz com que os fósforos emitam a luz visível que forma as cores da imagem (figura 3.35). Figura 3.35: Estrutura de uma tela de plasma (fonte). A tecnologia LCD é a mais utilizada nos dispositivos de exibição atuais. Uma tela de LCD é composta por um sanduíche de vários painéis (figura 3.36). Na parte de trás dos painéis, lâmpadas fluorescentes ou LEDs emitem uma luz branca que é espalhada uniformemente por um painel difusor. Essa luz incide sobre um filtro que só permite passar luz polarizada em uma direção. Na frente dos painéis há uma outra camada que só permite passar a luz polarizada na direção ortogonal ao primeiro filtro, de modo que o resultado é o bloqueio total da luz. Para controlar eletronicamente a passagem da luz, entre os dois filtros é colocado um substrato de vidro contendo uma camada de cristais líquidos e eletrodos e/ou transistores que alteram a orientação dos cristais – e com isso a polarização da luz – através de campos elétricos. Uma camada de filtros de cor divide a tela em pixels compostos de três subpixels, um para cada componente RGB, coincidentes com a camada de eletrodos. Desse modo, a passagem de luz em cada subpixel é controlada individualmente para formar a imagem final. O conteúdo da tela LCD é atualizado continuamente, geralmente a uma taxa de 60 Hz, mas em monitores mais recentes essa taxa pode chegar a 240 Hz. Figura 3.36: Estrutura de uma tela de LCD (fonte). A tecnologia OLED (organic light-emitting diodes) tem se popularizado em telas de smart TVs e smartphones e tem a promessa de substituir a tecnologia de LCD. Telas OLED não utilizam luz de fundo, pois cada subpixel emite sua própria luz: cada subpixel é um LED no qual a camada eletroluminescente é um filme de compostos orgânicos. A figura 3.37 mostra o detalhe ampliado de uma tela com tecnologia AMOLED (active-matrix organic light-emitting diode) que utiliza transistores de filme fino para manter o fluxo de corrente em cada subpixel. Figura 3.37: Detalhe da tela AMOLED de um smartphone Google Nexus One. Foto por Matthew Rollings (fonte). Telas OLED obtêm níveis mais profundos de preto e melhor contraste em ambientes escuros quando comparadas com as telas de LCD. A tecnologia tem ainda outras vantagens, como a possibilidade de ser utilizada em telas mais finas, flexíveis e transparentes (figura 3.38). Por outro lado, telas OLED possuem algumas desvantagens em relação à tecnologia LCD, como menor tempo de vida e maior risco de burn-in (marcação permanente da tela após a exibição de uma mesma imagem por longo período). Figura 3.38: Smartphones com tela OLED flexível (fonte). Embora as tecnologias LCD, Plasma e OLED não usem um canhão de elétrons para varrer a tela, o princípio geral de varredura de quadros de exibição continua sendo empregado. Em particular, o chamado tempo de apagamento vertical (VBI, vertical blanking interval), que é o intervalo entre a varredura de um quadro e o quadro subsequente, continua fazendo parte dessas tecnologias. Os sinais digitais utilizados (por exemplo, DVI ou HDMI) contém um sinal de VBI como parte de seu fluxo de dados. Referências "],["framebuffer.html", "3.4 Framebuffer", " 3.4 Framebuffer O framebuffer é uma área contígua de memória de vídeo utilizada para armazenar a imagem que será mostrada no dispositivo de exibição. O hardware gráfico lê continuamente o conteúdo do framebuffer e atualiza o dispositivo de exibição, tipicamente a uma taxa entre 60 e 240 Hz nos monitores de LCD. Nos primeiros PCs e em sistemas gráficos mais antigos, o framebuffer fazia parte da memória do sistema que poderia ser acessada diretamente pela CPU. Nos PCs do início da década de 1990, o framebuffer podia ser acessado com um simples ponteiro para o endereço 0xA000 no chamado “modo 13h” do controlador VGA (um modo gráfico de cores indexadas de 8 bits com resolução de 320x200). Atualmente, o framebuffer é acessado através da GPU e, em GPUs dedicadas, está localizado na memória RAM da placa gráfica. Em hardware compatível com OpenGL (o que inclui todas as GPUs atuais), o framebuffer pode ser composto por diversos buffers. Pelo menos um deles é um color buffer (buffer de cor) no qual cada pixel contém uma informação de cor, geralmente no formato RGB (24 bits) ou RGBA (32 bits). Um framebuffer pode ter vários buffers de cor associados. Por exemplo, em implementações que suportam visão estereoscópica, podemos ter um buffer de cor para a tela da visão esquerda e outro para a tela da visão direita. Na técnica de double buffering (descrita no fim da seção), são utilizados dois buffers de cor: o backbuffer, que é um buffer off-screen no qual a imagem é renderizada antes de ser exibida na tela, e o frontbuffer, que recebe o conteúdo do backbuffer ao fim da renderização para exibição na tela. Em renderização estéreo, cada lado esquerdo e direito pode ter o seu backbuffer e frontbuffer (ou outros buffers mais). Cada buffer de cor do framebuffer também pode ser associado a: Um depth buffer (buffer de profundidade), no qual cada pixel contém uma informação de profundidade utilizada no teste de profundidade. O teste de profundidade faz parte da implementação da técnica de Z-buffering de determinação de superfícies visíveis. A informação de profundidade pode ser um inteiro ou ponto flutuante de 16, 24 ou 32 bits (geralmente 24 bits). Um stencil buffer (buffer de estêncil), utilizado no teste de estêncil para operações de mascaramento e composição de imagens. No buffer estêncil, cada pixel contém um inteiro sem sinal, de 1, 4, 8 ou 16 bits (geralmente 8 bits). Screen tearing A taxa de atualização do dispositivo de exibição (chamada de vertical refresh rate) é controlada pelo hardware gráfico. Entretanto, a taxa em que a GPU atualiza o framebuffer pode ser bem maior que a taxa de atualização do dispositivo. Essa taxa é o número de quadros por segundo (FPS) que o processador gráfico é capaz de renderizar. Se o framebuffer for atualizado muito rapidamente, o hardware gráfico pode começar a atualizar o dispositivo de exibição com o conteúdo de um quadro de exibição e terminar com o conteúdo de outro, mais recente. Essa quebra entre os quadros de exibição gera um defeito na imagem conhecido como screen tearing, ou simplesmente tearing (figura 3.39). Figura 3.39: Screen tearing. (fonte). Vsync Para reduzir o problema de tearing, a GPU pode sincronizar o desenho do framebuffer com o tempo de apagamento vertical (VBI), que é o intervalo de tempo definido entre o fim da varredura de um quadro de exibição e o início da varredura do quadro seguinte. Essa sincronização efetivamente limita o número de FPS à frequência do monitor em Hz. Esse processo de sincronização é chamado de sincronização vertical ou Vsync (vertical synchronization). Em monitores mais recentes, compatíveis com as tecnologias G-SYNC da NVIDIA, e FreeSync da AMD, é possível fazer a sincronização na direção contrária: a frequência do monitor é ajustada pela GPU de acordo com a taxa de FPS. Multiple buffering O uso de VSync resolve o problema do tearing quando os quadros são renderizados em uma frequência mais alta que a frequência do monitor. Porém, quando a taxa de atualização do framebuffer é menor que a frequência do monitor, o conteúdo parcial do novo quadro pode ser misturado com o conteúdo do quadro anterior, gerando tearing. Uma solução para esse caso é usar a técnica de double buffering. Double buffering consiste em utilizar dois framebuffers: o backbuffer e o frontbuffer (figura 3.40). A GPU renderiza os gráficos apenas no backbuffer. Enquanto isso, o hardware gráfico atualiza o dispositivo de exibição com o conteúdo do frontbuffer. No próximo VBI, se a renderização no backbuffer ainda não tiver terminado, o mesmo frontbuffer é exibido novamente. Caso contrário, o backbuffer e o frontbuffer trocam de lugar, e agora o frontbuffer torna-se o backbuffer, e vice-versa. Desse modo, o hardware gráfico terá agora no frontbuffer um quadro completo para ser exibido na tela. Figura 3.40: Double buffering. Os casos em que a GPU precisa usar o mesmo frontbuffer (porque o backbuffer ainda está sendo desenhado) não são desejáveis pois podem ser percebidos como travamentos da imagem (stuttering), e resultam em uma diminuição da média de quadros por segundo. Isso pode ser melhorado com a adição de mais um backbuffer, no chamado triple buffering. O resultado é uma espécie de fila circular de buffers chamada de swapchain (figura 3.41). No triple buffering, dois quadros consecutivos são desenhados nos backbuffers. No próximo intervalo de apagamento vertical, a fila avança e o backbuffer mais antigo toma o lugar do frontbuffer, que por sua vez torna-se um backbuffer. Esse princípio de uso de múltiplos backbuffers (multiple buffering) pode ser estendido para ainda mais buffers. Entretanto, quanto maior o número de backbuffers, maior o atraso (lag) entre o quadro mais novo renderizado e o quadro que está sendo exibido na tela. Figura 3.41: Triple buffering. "],["sierpinski.html", "3.5 Triângulo de Sierpinski", " 3.5 Triângulo de Sierpinski Nesta atividade usaremos pela primeira vez comandos de desenho do OpenGL. Usaremos esses comandos para desenhar um triângulo de Sierpinski composto de pontos no plano. O triângulo de Sierpinski é um fractal que pode ser gerado por um tipo de sistema dinâmico chamado de sistema de função iterada (iterated function system, ou IFS). Esse processo pode ser implementado através de um algoritmo conhecido como jogo do caos. Para “jogar” o jogo do caos, começamos definindo três pontos \\(A\\), \\(B\\) e \\(C\\) não colineares. Por exemplo, \\(A=(0, 1)\\), \\(B=(-1, -1)\\) e \\(C=(1, -1)\\): Além dos pontos \\(A\\), \\(B\\) e \\(C\\), definimos mais um ponto \\(P\\) em uma posição aleatória do plano. Com \\(A\\), \\(B\\), \\(C\\) e \\(P\\) definidos, o jogo do caos consiste nos seguintes passos: Mova \\(P\\) para o ponto médio entre \\(P\\) e um dos pontos \\(A\\), \\(B\\), \\(C\\) escolhido de forma aleatória; Volte ao passo 1. Para gerar o triângulo de Sierpinski, basta desenhar \\(P\\) a cada iteração. O jogo não tem fim, mas quanto maior o número de iterações, mais pontos serão desenhados e mais detalhes terá o fractal (figura 3.42). Figura 3.42: Triângulo de Sierpinski desenhado com 1.000, 10.000 e 100.000 iterações em uma área de 210x210 pixels. Implementaremos o jogo do caos usando a mesma estrutura dos projetos firstapp (seção 2.3) e tictactoe (seção 2.4). O procedimento será simples: para cada chamada de onPaint, faremos uma iteração do jogo e desenharemos um ponto na posição \\(P\\) usando um comando de renderização do OpenGL. Os pontos desenhados serão acumulados no framebuffer e o resultado será a visualização da construção do fractal. Configuração inicial Crie o subdiretório abcg/examples/sierpinski e modifique o arquivo abcg/examples/CMakeLists.txt para incluir essa nova pasta. Comente as linhas de add_subdirectory dos projetos anteriores para que eles não sejam compilados: # add_subdirectory(helloworld) # add_subdirectory(firstapp) # add_subdirectory(tictactoe) add_subdirectory(sierpinski) Crie o arquivo abcg/examples/sierpinski/CMakeLists.txt com o seguinte conteúdo, similar ao que utilizamos nos projetos anteriores: project(sierpinski) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Crie também os arquivos main.cpp, window.cpp e window.hpp em abcg/examples/sierpinski. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp ficará como a seguir: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings({.samples = 2, .doubleBuffering = false}); window.setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } Esse código é parecido com o main.cpp dos projetos firstapp e tictactoe. As únicas diferenças estão nas funções chamadas nas linhas 8 e 9: window.setOpenGLSettings({.samples = 2, .doubleBuffering = false}); window.setWindowSettings({.width = 600, .height = 600, .showFullscreenButton = false, .title = &quot;Sierpinski Triangle&quot;}); setOpenGLSettings é uma função membro de abcg::OpenGLWindow, classe base de nossa classe customizada Window. setOpenGLSettings recebe uma estrutura abcg::OpenGLSettings com as configurações de inicialização do OpenGL. Essas configurações são usadas pela SDL no momento da criação de um “contexto do OpenGL” que representa o framebuffer vinculado à janela e todo o estado interno do OpenGL: O atributo samples = 2 faz com que o framebuffer suporte suavização de serrilhado (antialiasing) das primitivas do OpenGL; O atributo .doubleBuffering = false é uma configuração de criação do contexto do OpenGL que faz com que a técnica de double buffering seja desativada. Desse modo, conseguiremos desenhar os pontos incrementalmente no framebuffer para formar o desenho do fractal. Se não desativarmos o double buffering, o OpenGL poderá apagar o conteúdo do backbuffer para cada novo quadro de exibição (isso acontece com a versão para web), e então só visualizaremos o último ponto desenhado. No estrutura abcg::WindowSettings passada como argumento de setWindowSettings, usamos showFullscreenButton = false para desativar a exibição do botão de tela cheia. Afinal, não queremos que o botão obstrua o desenho do fractal. Mesmo sem o botão, o modo janela pode ser alternado com o modo de tela cheia através da tecla F11. window.hpp Na definição da classe Window, substituiremos novas funções virtuais de abcg::OpenGLWindow e definiremos novas variáveis que serão utilizadas para atualizar o jogo do caos e desenhar o ponto na tela: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOVertices{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec2, 3&gt; const m_points{{{0, 1}, {-1, -1}, {1, -1}}}; glm::vec2 m_P{}; void setupModel(); }; #endif Observe que, além de usarmos as funções onCreate, onPaint e onPaintUI, estamos agora substituindo mais duas funções virtuais de abcg::OpenGLWindow: onResize é chamada pela ABCg sempre que o tamanho da janela é alterado. O novo tamanho é recebido pelo parâmetro size, que é um vetor 2D de inteiros glm::ivec2 disponibilizado pela biblioteca OpenGL Mathematics (GLM)15. Na nossa aplicação, quando a ABCg chamar onResize, faremos uma cópia de size em m_viewportSize (linha 17). Isso será feito para que, em onPaint, possamos configurar o tamanho da área de desenho (viewport) do OpenGL para ser o mesmo tamanho da janela da aplicação. O conceito de viewport será aprofundado mais adiante. onDestroy é chamada pela ABCg quando a janela é destruída, isto é, no fim da aplicação. Essa é a função complementar de onCreate, usada para liberar os recursos do OpenGL que foram alocados em onCreate ou durante a aplicação. Da linha 17 a 25 temos a definição das variáveis da classe: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOVertices{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec2, 3&gt; const m_points{{{0, 1}, {-1, -1}, {1, -1}}}; glm::vec2 m_P{}; m_VAO, m_VBOVertices e m_program são identificadores de recursos alocados pelo OpenGL, geralmente armazenados na memória da placa de vídeo. Esses recursos correspondem aos shaders que definem o comportamento da renderização, e ao arranjo ordenado de vértices utilizado para montar as primitivas geométricas no pipeline de renderização. No nosso caso, o arranjo de vértices contém apenas um vértice e equivale ao ponto \\(P\\) que queremos desenhar. m_viewportSize, como já vimos, serve para armazenar o tamanho da janela da aplicação que é recebido em onResize. m_randomEngine é um objeto do gerador de números pseudoaleatórios da biblioteca padrão do C++ (note o uso do #include &lt;random&gt; na linha 4). Esse objeto é utilizado para sortear a posição inicial de \\(P\\) e qual ponto (\\(A\\), \\(B\\) ou \\(C\\)) será escolhido em cada iteração do jogo do caos. m_points é um arranjo que contém a posição dos pontos \\(A\\), \\(B\\) e \\(C\\). As coordenadas dos pontos são descritas por uma estrutura glm::vec2 que representa um vetor 2D com coordenadas do tipo float. m_P é a posição do ponto \\(P\\). Além da definição das variáveis, na linha 27 é declarada a função Window::setupModel que cria os recursos identificados por m_VAO e m_VBOVertices. A função é chamada sempre que um novo ponto \\(P\\) precisa ser desenhado. window.cpp Primeiro vamos implementar a lógica do jogo sem desenhar na tela. Em seguida, incluiremos o código que usa o OpenGL para desenhar os pontos. Começaremos com a definição de Window::onCreate(). Como essa função é chamada apenas uma vez na inicialização da aplicação, colocaremos aqui o código que inicia o gerador de números pseudoaleatórios e sorteia as coordenadas iniciais de \\(P\\) (que no código é m_P): #include &quot;window.hpp&quot; void Window::onCreate() { // Start pseudorandom number generator auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly pick a pair of coordinates in the range [-1; 1) std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } O gerador m_randomEngine é iniciado usando como semente o tempo do sistema. As coordenadas de m_P são iniciadas como valores sorteados do intervalo \\([-1, 1)\\). O intervalo não precisa ser este necessariamente, mas fazendo assim garantimos que o ponto inicial será visto na tela16. Na configuração padrão do OpenGL, só conseguimos visualizar as primitivas gráficas que estão situadas entre as coordenadas \\((-1, -1)\\) e \\((1, 1)\\). Na configuração padrão, a coordenada \\((-1, -1)\\) é mapeada ao canto inferior esquerdo da janela, e a coordenada \\((1, 1)\\) é mapeada ao canto superior direito. Esse mapeamento poderá ser modificado posteriormente com a função glViewport. Vamos agora implementar o passo iterativo do jogo. Faremos isso em Window::onPaint. Assim, cada quadro de exibição corresponderá a uma iteração: void Window::onPaint() { // Randomly pick the index of a triangle vertex std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); auto const index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex position m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Neste trecho de código, index é um índice do arranjo m_points. Assim, m_points.at(index) é um dos pontos \\(A\\), \\(B\\) ou \\(C\\) que definem os vértices do triângulo. Observe que utilizamos uma distribuição uniforme para sortear o índice (std::uniform_int_distribution). Isso é importante para que o fractal seja desenhado como esperado. A nova posição de m_P é calculada como o ponto médio entre m_P e o ponto de m_points. O código comentado pode ser utilizado para imprimir no terminal as novas coordenadas de m_P. Isso conclui a implementação da lógica do jogo. O resto do código será para desenhar m_P como um ponto na tela. No OpenGL anterior à versão 3.1, isso seria tão simples quanto acrescentar o seguinte código em Window::onPaint: glBegin(GL_POINTS); glVertex2f(m_P.x, m_P.y); glEnd(); Entretanto, como vimos na seção 3.2, esse código é obsoleto e não é mais suportado em muitos drivers e plataformas. Precisaremos seguir os seguintes passos para desenhar um simples ponto na tela: Criar um “buffer de vértices” como um recurso do OpenGL. Esse recurso é chamado VBO (Vertex Buffer Object) e corresponde ao arranjo ordenado de vértices utilizado pela GPU para montar as primitivas que serão renderizadas. No nosso caso, o buffer de vértices só precisa ter um vértice, que é a coordenada do ponto que queremos desenhar. A variável m_VBOVertices é um inteiro que identifica esse recurso. Programar o comportamento do pipeline de renderização. Isso é feito compilando e ligando um par de shaders que fica armazenado na GPU como um único “programa de shader”, identificado pela variável m_program. No OpenGL, os shaders são escritos na linguagem GLSL (OpenGL Shading Language), que é parecida com a linguagem C, mas possui novos tipos de dados e operações. Especificar como o buffer de vértices será lido pelo programa de shader. No nosso código, o estado dessa configuração é armazenado como um objeto do OpenGL chamado VAO (Vertex Array Object), identificado pela variável m_VAO. Somente após alocar e ativar esses recursos é que podemos iniciar o pipeline de renderização, chamando uma função de desenho em Window::onPaint. Não se preocupe se tudo isso está parecendo muito complexo nesse momento. Nos próximos capítulos revisitaremos cada etapa diversas vezes até nos familiarizarmos com todo o processo. Por enquanto, utilizaremos o código já pronto. Primeiro, defina Window::setupModel como a seguir: void Window::setupModel() { // Release previous VBO and VAO abcg::glDeleteBuffers(1, &amp;m_VBOVertices); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Generate a new VBO and get the associated ID abcg::glGenBuffers(1, &amp;m_VBOVertices); // Bind VBO in order to use it abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOVertices); // Upload data to VBO abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_P), &amp;m_P, GL_STATIC_DRAW); // Unbinding the VBO is allowed (data can be released now) abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOVertices); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } Esse código cria o VBO (m_VBOVertices) e VAO (m_VAO) usando a posição atual de m_P. Agora, modifique onCreate para o seguinte código final: void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; void main() { gl_PointSize = 2.0; gl_Position = vec4(inPosition, 0, 1); } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; out vec4 outColor; void main() { outColor = vec4(1); } )gl&quot;}; // Create shader program m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); // Clear window abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes.at(0), sizes.at(1)); // Start pseudorandom number generator auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); // Randomly pick a pair of coordinates in the range [-1; 1) std::uniform_real_distribution&lt;float&gt; realDistribution(-1.0f, 1.0f); m_P.x = realDistribution(m_randomEngine); m_P.y = realDistribution(m_randomEngine); } Nesta função, vertexShader e fragmentShader são strings que contêm o código-fonte dos shaders. vertexShader é o código do chamado vertex shader, que programa o processamento de vértices na GPU. fragmentShader é o código do fragment shader, que programa o processamento de pixels na GPU (ou, mais precisamente, o processamento de fragmentos, que são conjuntos de atributos que representam uma amostra de geometria rasterizada). A compilação e ligação dos shaders é feita pela função abcg::createOpenGLProgram. Se acontecer algum erro de compilação, a mensagem de erro será exibida no console e uma exceção será lançada. Note que limpamos o buffer de cor com a cor preta, usando glClearColor e glClear (linhas 27 e 28). Observe o trecho de código entre as diretivas de pré-processamento: #if !defined(__EMSCRIPTEN__) glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else Esse código só será compilado quando não usarmos o Emscripten, isto é, quando o binário for compilado para desktop. No OpenGL para desktop, o comando da linha 32 é necessário para que o tamanho do ponto que será desenhado possa ser definido no vertex shader. Quando o código é compilado com o Emscripten, podemos considerar que a definição do tamanho do ponto no vertex shader é suportada por padrão, pois o OpenGL utilizado nesse caso é o OpenGL ES (o WebGL usa um subconjunto de funções do OpenGL ES). Observe, no código do vertex shader, que o tamanho do ponto é definido com gl_PointSize = 2.0 (isto é, dois pixels). Os tamanhos válidos dependem do que é suportado pelo hardware. Para imprimir no console os tamanhos mínimo e máximo, usamos glGetFloatv neste trecho de código: std::array&lt;GLfloat, 2&gt; sizes{}; #if !defined(__EMSCRIPTEN__) abcg::glEnable(GL_PROGRAM_POINT_SIZE); abcg::glGetFloatv(GL_POINT_SIZE_RANGE, sizes.data()); #else abcg::glGetFloatv(GL_ALIASED_POINT_SIZE_RANGE, sizes.data()); #endif fmt::print(&quot;Point size: {:.2f} (min), {:.2f} (max)\\n&quot;, sizes.at(0), sizes.at(1)); A função glGetFloatv com o identificador GL_POINT_SIZE_RANGE (para OpenGL desktop) e GL_ALIASED_POINT_SIZE_RANGE (para OpenGL ES) preenche o arranjo sizes com os tamanhos mínimo e máximo suportados. Em seguida, fmt::print mostra os valores no console. Voltando agora à implementação de Window::onPaint, o código final ficará assim: void Window::onPaint() { // Create OpenGL buffers for drawing the point at m_P setupModel(); // Set the viewport abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_VAO); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); // Randomly pick the index of a triangle vertex std::uniform_int_distribution&lt;int&gt; intDistribution(0, m_points.size() - 1); auto const index{intDistribution(m_randomEngine)}; // The new position is the midpoint between the current position and the // chosen vertex position m_P = (m_P + m_points.at(index)) / 2.0f; // Print coordinates to console // fmt::print(&quot;({:+.2f}, {:+.2f})\\n&quot;, m_P.x, m_P.y); } Na linha 52, setupModel cria os recursos do OpenGL necessários para desenhar um ponto na posição atual de m_P. Na linha 55, glViewport configura o mapeamento entre o sistema de coordenadas no qual nossos pontos foram definidos (coordenadas normalizadas do dispositivo, ou NDC, de normalized device coordinates), e o sistema de coordenadas da janela (window coordinates), em pixels, com origem no canto inferior esquerdo da janela da aplicação. A figura 3.43 ilustra como fica configurado o mapeamento entre coordenadas em NDC para coordenadas da janela, supondo uma chamada a glViewport(x, y, w, h), onde x, y, w e h são inteiros dados em pixels da tela. Na figura, o chamado viewport do OpenGL é a janela formada pelo retângulo entre os pontos \\((x,y)\\) e \\((x+w,y+h)\\). No nosso código com glViewport(0, 0, m_viewportSize.x, m_viewportSize.y), o ponto \\((-1,-1)\\) em NDC é mapeado para o pixel \\((0, 0)\\) da janela (canto inferior esquerdo), e o ponto \\((1,1)\\) em NDC é mapeado para o pixel \\((0,0)\\) + (m_viewportSize.x, m_viewportSize.y). Isso faz com que o viewport ocupe toda a janela da aplicação. Figura 3.43: Mapeamento das coordenadas normalizadas no dispositivo (NDC) para coordenadas da janela usando glViewport(x, y, w, h). Com o viewport devidamente configurado, iniciamos o pipeline de renderização neste trecho: // Start using the shader program abcg::glUseProgram(m_program); // Start using VAO abcg::glBindVertexArray(m_VAO); // Draw a single point abcg::glDrawArrays(GL_POINTS, 0, 1); // End using VAO abcg::glBindVertexArray(0); // End using the shader program abcg::glUseProgram(0); Na linha 58, glUseProgram ativa os shaders compilados no programa m_program. Na linha 60, glBindVertexArray ativa o VAO (m_VAO), que contém as especificações de como o arranjo de vértices (VBO) será lido no vertex shader atualmente ativo. Ao ativar o VAO, também é ativado automaticamente o VBO identificado por m_VBO. Finalmente, na linha 63, glDrawArrays inicia o pipeline de renderização usando os shaders e o VBO ativos. O primeiro argumento (GL_POINTS) indica que os vértices do arranjo de vértices devem ser tratados como pontos. O segundo argumento (0) é o índice inicial dos vértices no VBO. O terceiro argumento (1) informa quantos vértices devem ser processados. O processamento no pipeline de renderização é realizado de forma paralela e assíncrona com a CPU. Isto é, glDrawArrays retorna imediatamente, enquanto a GPU continua trabalhando em paralelo renderizando a geometria no framebuffer. Após o comando de renderização, nas linhas 66 e 68 temos comandos para desativar o VAO e os shaders. Essa desativação é opcional pois, de qualquer forma, o mesmo VAO e os mesmos shaders serão utilizados na próxima chamada de Window::onPaint. Ainda assim, é uma boa prática de programação desativá-los logo após seu uso. Vamos agora definir a função Window::onResize, assim: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } Como vimos, resizeGL é chamada sempre que a janela da aplicação muda de tamanho. Observe que simplesmente armazenamos o tamanho da janela em m_viewportSize. Como m_viewportSize será usado em glViewport, garantimos que o viewport sempre ocupará toda a janela da aplicação. Observe que também chamamos glClear para apagar o buffer de cor. Essa é uma forma de reiniciar o desenho do fractal. Afinal, estragaríamos o triângulo de Sierpinski se continuássemos desenhando sobre o fractal de tamanho anterior. A função Window::onDestroy é definida da seguinte maneira: void Window::onDestroy() { // Release shader program, VBO and VAO abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOVertices); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Os comandos glDelete* liberam os recursos alocados em Window::setupModel. Para finalizar, definiremos Window::onPaintUI usando o seguinte código: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { ImGui::SetNextWindowPos(ImVec2(5, 81)); ImGui::Begin(&quot; &quot;, nullptr, ImGuiWindowFlags_NoDecoration); if (ImGui::Button(&quot;Clear window&quot;, ImVec2(150, 30))) { abcg::glClear(GL_COLOR_BUFFER_BIT); } ImGui::End(); } } Na linha 83 chamamos o onPaintUI da classe base, responsável por mostrar o contador de FPS. Normalmente, o botão de tela cheia também seria desenhado, mas desabilitamos sua exibição em main.cpp. O código nas linhas 85 a 94 cria uma pequena janela da ImGui (com título em branco e sem decorações) contendo um botão “Clear window” que chama glClear sempre que pressionado. Isso é tudo! Construa a aplicação para ver o resultado: O projeto completo pode ser baixado deste link. Exercícios Em Window::onPaint, utilizamos std::uniform_int_distribution para sortear os vértices do triângulo usando uma distribuição uniforme. Veja o que acontece caso a distribuição não seja uniforme. Em particular, troque std::uniform_int_distribution por std::discrete_distribution e use pesos diferentes para cada vértice. Por exemplo, use std::discrete_distribution&lt;int&gt; intDistribution({30, 50, 20}) para que os índices 0, 1, e 2 tenham respectivamente 30%, 50%, e 20% de chance de serem sorteados. Faça com que os pesos da distribuição discreta do item anterior possam ser configurados pelo usuário através de sliders da ImGui. Além disso, faça com que a soma dos três pesos seja sempre 100. A biblioteca GLM fornece estruturas e funções de operações matemáticas compatíveis com a especificação da linguagem de shaders do OpenGL (GLSL). Por exemplo, glm::ivec2 representa um vetor de dois valores do tipo int. Na linguagem de shaders também existe ivec2, mas ele existe como um tipo de dado nativo.↩︎ O ponto inicial poderia ser simplesmente um ponto fixo ou um dos vértices, mas nosso código segue o algoritmo original descrito no início da seção.↩︎ "],["pipeline.html", "4 Pipeline gráfico", " 4 Pipeline gráfico O pipeline gráfico ou pipeline de renderização é um modelo conceitual de descrição da sequência de passos que a GPU utiliza para transformar um modelo matemático em uma imagem digital. O termo “pipeline” é utilizado porque o processamento é realizado em uma sequência de etapas alimentadas por um fluxo de dados, sendo que cada etapa processa novos dados tão logo tenha enviado a saída à etapa seguinte. Algumas etapas de processamento do pipeline podem ser programadas por shaders. Outras etapas são fixas e não podem ser programadas. A aplicação é responsável por configurar previamente os dados gráficos na memória acessada pela GPU antes de dar início ao processamento do pipeline. Em geral, os dados gráficos descrevem elementos de uma cena tridimensional tais como representações dos objetos da cena, descrição de materiais e fontes de luz, além de transformações geométricas que podem ser utilizadas para definir como os objetos serão dispostos no espaço e projetados no plano de imagem de uma câmera virtual. As superfícies dos objetos de uma cena são geralmente representadas por malhas de triângulos. Para cada superfície podem ser associados atributos que descrevem suas propriedades físicas. Assim, os shaders podem então ler esses atributos e simular a interação entre cada superfície com a luz emitida pelas fontes de luz ou refletida de outras superfícies. Além dos dados que descrevem a cena, a aplicação também é responsável por inicializar o pipeline com os shaders que serão utilizados durante a renderização. O pipeline é iniciado pela aplicação através de um comando de desenho, e o processamento do pipeline na GPU é realizado de forma assíncrona com a CPU. O pipeline típico implementado nas atuais GPUs envolve etapas que compreendem o processamento geométrico, a rasterização e o processamento de fragmentos (figura 4.1): Figura 4.1: Etapas de um pipeline gráfico. Processamento geométrico: envolve operações realizadas sobre vértices, como transformações afins e transformações projetivas que serão abordadas em capítulos futuros. O processamento geométrico pode envolver também a criação de geometria e o refinamento de malhas. Ao final desse processamento é feito o recorte ou descarte das primitivas geométricas que estão fora de um volume de visualização. Rasterização: compreende a conversão matricial das primitivas. O resultado é um conjunto de amostras de primitivas. Durante o processamento no pipeline, o termo fragmento é frequentemente utilizado para designar essas amostras no lugar de pixel. Cada fragmento é uma coleção de valores que inclui atributos interpolados a partir dos vértices e a posição \\((x,y,z)\\) da amostra em coordenadas da janela (o valor \\(z\\) é considerado a “profundidade” do fragmento). O pixel é o valor final da cor no buffer de cor, que pode ser uma combinação da cor de vários fragmentos. Processamento de fragmentos: envolve operações realizadas sobre cada fragmento para determinar sua cor e outros atributos. A cor pode ser determinada através da avaliação de modelos de iluminação que levam em conta os atributos de iluminação fornecidos pela aplicação, tais como atributos das fontes de luz, e descrições de detalhes das superfícies através de mapas de bits chamados de texturas. Após essas operações são realizados testes de descarte de fragmentos e combinação de cores entre os fragmentos processados e os pixels já existentes no framebuffer. O resultado é armazenado em diferentes buffers do framebuffer: buffers de cor, buffer de profundidade e buffer de estêncil. "],["dados-gráficos.html", "4.1 Dados gráficos", " 4.1 Dados gráficos O processamento de um pipeline gráfico começa com a definição dos dados gráficos pela aplicação. Esses dados são frequentemente representações de objetos – abstrações de objetos do mundo real – dispostos em uma cena virtual tridimensional. Uma cena é tipicamente composta por: Objetos com superfícies descritas através de modelos geométricos tais como malhas de triângulos, nuvens de pontos, equações paramétricas ou equações implícitas. Propriedades dos materiais que compõem as superfícies dos objetos. Essas propriedades descrevem a forma como uma luz incidente na superfície é refletida, absorvida e/ou transmitida. Fontes de luz descritas por informações como intensidade, direção de propagação da luz e fatores de atenuação. Alternativamente, uma fonte de luz pode ser um objeto com superfície de material emissivo. Uma câmera virtual descrita por informações que permitem definir um ponto de vista na cena, tais como posição da câmera, orientação e campo de visão. A câmera virtual é uma abstração de uma câmera ou observador do mundo real. Em uma câmera de verdade, a imagem é formada a partir da energia luminosa que atravessa as lentes e é captada pelo filme ou sensor durante um tempo de exposição. Poderíamos tentar simular de forma precisa o comportamento de uma câmera real, calculando a intensidade de luz de cada comprimento de onda que adentra a abertura da câmera vindo de diferentes direções, atravessa o sistema de lentes e então incide em cada subpixel RGB do sensor da câmera durante um tempo de exposição. Entretanto, uma simulação com esse grau de precisão seria extremamente custosa e inviável mesmo em hardware gráfico atual. Precisamos simplificar de forma significativa este processo, especialmente para aplicações de síntese de imagens em tempo real. Em muitas aplicações, essa aproximação pode ser até mesmo fisicamente incorreta, e a produção da percepção de sombreamento (shading) dos objetos pode ser suficiente. Para a maioria das aplicações, é comum considerar que a câmera virtual é uma câmera pinhole ideal (figura 4.2). A câmera pinhole é uma câmera que não possui lentes. A luz passa por um pequeno furo (chamado de centro de projeção) e incide sobre um filme ou sensor localizado no fundo da câmera (o plano de imagem). A abertura do campo de visão pode ser ajustada mudando a distância focal, que é a distância entre o centro de projeção e o plano de imagem. Na câmera pinhole ideal, a abertura do furo é infinitamente pequena e as imagens formadas são perfeitamente nítidas (isto é, em foco). Efeitos de difração são ignorados nesse modelo. Figura 4.2: Camera pinhole ideal. Para evitar ter de lidar com a imagem invertida formada no plano de imagem da câmera pinhole, podemos considerar que o plano de imagem está localizado na frente do centro de projeção, o que seria impossível de fazer numa câmera real. Podemos também ajustar arbitrariamente a distância focal sem preocupação com limitações físicas. A distância focal pode até mesmo ser infinita, se desejarmos uma projeção paralela. Por fim, podemos considerar que o plano de imagem é o framebuffer. Nessa configuração, é comum considerar que o centro de projeção corresponde ao olho do observador, como mostra a figura (figura 4.3). Figura 4.3: Câmera virtual com plano de imagem na frente do centro de projeção. Para determinar a cor de cada pixel do framebuffer para um determinado ponto de vista da câmera virtual, podemos considerar duas abordagens de renderização: ray casting e rasterização. Essas abordagens são apresentadas na seção a seguir. "],["ray-casting-x-rasterização.html", "4.2 Ray casting x rasterização", " 4.2 Ray casting x rasterização Ray casting e rasterização são duas abordagens distintas de se renderizar uma cena, e resultam em pipelines também distintos. Ray casting consiste em lançar raios que saem do centro de projeção, atravessam os pixels da tela e intersectam os objetos da cena. A rasterização faz o caminho inverso: os objetos da cena são projetados na tela na direção do centro de projeção, e são então convertidos em pixels. Neste curso usaremos apenas a rasterização, que é a forma de renderização utilizada na maioria das aplicações gráficas interativas. É também a única abordagem de renderização suportada atualmente no pipeline gráfico do OpenGL. Entretanto, é importante observar que novos pipelines baseados em traçado de raios (uma forma de ray casting) têm sido incorporados às APIs gráficas e tendem a conquistar cada vez mais espaço em síntese de imagens em tempo real. Ray casting Na sua forma mais simples, o algoritmo de ray casting (Roth 1982) consiste nos seguintes passos (figura 4.4): Figura 4.4: Ray casting. Para cada pixel do framebuffer: Calcule o raio \\(R\\) que sai do centro de projeção e passa pelo pixel. Seja \\(P\\) a interseção mais próxima (se houver) de \\(R\\) com um objeto da cena. Faça com que a cor do pixel seja a cor calculada em \\(P\\). Em ray casting, cada pixel é visitado apenas uma vez. Entretanto, para cada pixel visitado, potencialmente todos os objetos da cena podem ser consultados para calcular a interseção mais próxima. Assim, o principal custo da geração de imagem usando ray casting está relacionado ao cálculo das interseções. Estruturas de dados de subdivisão espacial podem ser utilizadas para que seja possível descartar rapidamente a geometria não intersectada pelo raio e com isso diminuir o número de testes de interseção. Algumas dessas estruturas de aceleração utilizadas em ray casting são a k-d tree (Bentley 1975), octree (Meagher 1980), e a hierarquia de volumes delimitantes (BVH, bounding volume hierarchies). Embora o ray casting seja conceitualmente simples, exige a manutenção da estrutura de aceleração na memória do renderizador. Essa limitação tem sido cada vez menos significativa nas GPUs mais recentes, mas ray casting ainda é pouco utilizado em síntese de imagens em tempo real. Observação Para produzir imagens fotorrealistas, a cor em \\(P\\) deve ser calculada através da integração da energia luminosa que incide sobre o ponto vindo de todas as direções da cena, e da determinação da quantidade dessa energia que é refletida na direção do pixel na tela. Isso pode ser feito de diferentes formas e em diferentes níveis de aproximação. Uma aproximação pouco acurada, mas muito eficiente, é avaliar a equação de um modelo de iluminação local como o modelo de reflexão de Phong (Phong 1973) ou Blinn-Phong (Blinn 1977) que considera que a cor em uma superfície é determinada unicamente pela luz que incide diretamente sobre a superfície, e não pela luz indireta refletida por outros objetos. Outra aproximação, mais acurada porém bem menos eficiente, é a técnica de traçado de raios recursivo (Whitted 1979) que consiste em lançar novos raios a partir de \\(P\\) (figura 4.5). Esses raios são: Um raio de sombra (shadow ray) em direção a cada fonte de luz, para saber se \\(P\\) encontra-se na sombra em relação à fonte de luz correspondente; Um raio de reflexão (reflection ray) na direção espelhada em relação ao vetor normal à superfície em \\(P\\); Um raio de refração (refraction ray) que atravessa a superfície do objeto, caso o objeto seja transparente. Figura 4.5: Traçado de raios recursivo. Os raios de reflexão e refração podem intersectar outros objetos, e novos raios podem ser gerados a partir desses pontos de interseção, recursivamente, de tal modo que a cor final refletida em \\(P\\) é formada por uma combinação da energia luminosa representada por todos os raios. Rasterização Em oposição ao ray casting, a rasterização é centrada no processamento de primitivas em vez de pixels. Cada primitiva é projetada no plano de imagem e rasterizada em seguida (figura 4.6): Figura 4.6: Rasterização. Para cada primitiva da cena: Projete a primitiva no plano de imagem. Rasterize a primitiva projetada. Modifique o framebuffer com a cor calculada em cada pixel da primitiva, exceto se o pixel do framebuffer já tiver sido preenchido anteriormente com uma primitiva mais próxima do plano de imagem. Na etapa 3, a cor do pixel é geralmente avaliada através de um modelo de iluminação local como o modelo de Blinn-Phong. Outras técnicas podem ser utilizadas para melhorar a aproximação da luz refletida no ponto amostrado, mas não há lançamento de raios ou testes de interseção como no ray casting. A rasterização é mais adequada para implementação em hardware, pois cada iteração do laço principal só precisa armazenar a primitiva que está sendo processada, juntamente com o conteúdo do framebuffer. Como resultado, o processamento de transformação geométrica de vértices e a conversão matricial podem ser paralelizados de forma massiva, como de fato ocorre nas GPUs. Referências "],["glpipeline.html", "4.3 Pipeline do OpenGL", " 4.3 Pipeline do OpenGL A figura 4.7 mostra um diagrama dos estágios de processamento do pipeline gráfico do OpenGL (fundo amarelo) e de como os dados gráficos (fundo cinza) interagem com cada estágio. As etapas programáveis são mostradas com fundo preto. No lado esquerdo há uma ilustração do resultado de cada etapa para a renderização de um triângulo colorido. Figura 4.7: Pipeline gráfico do OpenGL. Observação Para simplificar, algumas etapas do pipeline foram omitidas, tais como: O geometry shader, utilizado para o processamento de geometria após a montagem de primitivas; Os shaders de tesselação (tessellation control shader e tessellation evaluation shader), utilizados para subdivisão de primitivas; O compute shader, utilizado para processamento de propósito geral (GPGPU). Essas etapas não serão utilizadas nas atividades da disciplina pois, no momento, não fazem parte do subconjunto do OpenGL ES (OpenGL for Embedded Systems) utilizado pelo WebGL 2.0. Entretanto, são etapas frequentemente utilizadas em aplicações para OpenGL desktop. Consulte a especificação do OpenGL 4.6 para ter acesso ao pipeline da versão mais recente para desktop. Aplicação Antes de iniciar o processamento, a aplicação deve especificar o formato dos dados gráficos e enviar esses dados à memória que será acessada durante a renderização. A aplicação também deve configurar as etapas programáveis do pipeline, compostas pelo vertex shader e fragment shader. Os shaders devem ser compilados, ligados e ativados previamente. A geometria a ser processada é especificada através de um arranjo ordenado de vértices. O tipo de primitiva que será formada a partir desses vértices é determinado no comando de renderização. As primitivas suportadas pelo OpenGL são descritas a seguir e mostradas na figura 4.8: GL_POINTS: cada vértice forma um ponto que será desenhado na tela como um pixel ou como um quadrilátero centralizado no vértice. O tamanho do ponto/quadrilátero pode ser definido pelo usuário17; GL_LINES: cada grupo de dois vértices forma um segmento de reta; GL_LINE_STRIP: os vértices são conectados em ordem para formar uma polilinha; GL_LINE_LOOP: os vértices são conectados em ordem para formar uma polilinha, e o último vértice forma um segmento com o primeiro vértice, formando um laço; GL_TRIANGLES: cada grupo de três vértices forma um triângulo; GL_TRIANGLE_STRIP: os vértices formam uma faixa de triângulos com arestas compartilhadas; GL_TRIANGLE_FAN: os vértices formam um leque de triângulos de modo que todos os triângulos compartilham o primeiro vértice. Figura 4.8: Primitivas do OpenGL. Cada vértice do arranjo de vértices de entrada é composto por um conjunto de atributos definidos pela aplicação. Cada atributo pode ser um único valor ou um conjunto de valores. A forma como esses valores são interpretados depende exclusivamente do que é definido no vertex shader. Geralmente, considera-se que cada vértice tem pelo menos uma posição 2D \\((x,y)\\) ou 3D \\((x,y,z)\\). Outros atributos comuns para cada vértice são o vetor normal, cor e coordenadas de textura. Para ser utilizado pelo pipeline, o arranjo de vértices deve ser armazenado na memória como um recurso chamado Vertex Buffer Object (VBO). Cada atributo de vértice pode ser armazenado como um VBO separado, mas também é possível deixar todos os atributos em um único VBO (interleaved data). Cabe à aplicação especificar o formato dos dados de cada VBO e como eles serão lidos pelo vertex shader. Isso deve ser feito sempre antes da chamada do comando de renderização, para todos os VBOs. Alternativamente, essa configuração pode ser feita apenas uma vez e armazenada em um Vertex Array Object (VAO), bastando então ativar o VAO antes de cada comando de desenho. Além da criação dos VBOs, a aplicação pode criar variáveis globais, chamadas de variáveis uniformes (uniform variables), que podem ser lidas pelo vertex shader e fragment shader. Essa é uma outra forma de enviar dados ao pipeline. As variáveis uniformes contêm dados apenas de leitura e que não variam de vértice para vértice, por isso o nome “uniforme”. Por exemplo, uma matriz de transformação geométrica pode ser armazenada como uma variável uniforme pois todos os vértices serão transformados por essa matriz durante o processamento no vertex shader (isto é, a matriz de transformação é a mesma para todos os vértices). Também é possível criar buffers de dados uniformes (Uniform Buffer Objects, ou UBOs) para enviar arranjos de dados. A especificação do OpenGL garante ser possível enviar pelo menos 16KB de dados no formato de UBOs, mas é comum os drivers oferecerem suporte a até 64KB. A aplicação também pode enviar dados ao pipeline usando buffers de texturas (buffer textures). Os valores dos texels dessas texturas podem ser lidos no vertex shader e no fragment shader como se fossem valores de arranjos unidimensionais. Esses valores podem ser interpretados como cores RGBA normalizadas entre 0 e 1 ou como valores arbitrários em ponto flutuante de até 32 bits. Uma forma mais recente e flexível de enviar dados uniformes é através dos Shader Storage Buffer Objects (SSBOs). O tamanho de um SSBO pode ser de até 128MB segundo a especificação, mas na maioria das implementações pode ser tão grande quanto a memória de vídeo disponível. Além disso, esse recurso pode ser utilizado tanto para leitura quanto escrita. Há muitas formas de enviar e receber dados da GPU. Entretanto, para deixarmos as coisas mais simples, usaremos neste curso apenas os recursos mais básicos, como VBOs, VAOs e variáveis uniformes. Vertex shader Os shaders do OpenGL são programas escritos na linguagem OpenGL Shading Language (GLSL). GLSL é similar à linguagem C, mas utiliza novas palavras-chave, novos tipos de dados, qualificadores e operações. A versão mais recente da GLSL é a versão 4.6. Entretanto, usaremos a especificação GLSL ES 3.0 para garantir a compatibilidade com WebGL 2.0. Os documentos de especificação dessas duas versões podem ser acessados pelos links a seguir: OpenGL ES Shading Language 3.0: versão compatível com WebGL 2.0. OpenGL Shading Language 4.6: versão mais recente com suporte a geometry shaders, tessellation shaders, compute shaders, mas sem compatibilidade com WebGL 2.0. O vertex shader processa cada vértice individualmente. Entretanto, esse processamento é paralelizado de forma massiva na GPU. Cada execução de um vertex shader acessa apenas os atributos do vértice que está sendo processado. Não há como compartilhar o estado do processamento de um vértice com os demais vértices. A entrada do vertex shader é um conjunto de atributos de vértice definidos pelo usuário. Esses atributos são alimentados pelo pipeline de acordo com os VBOs atualmente ativos. A saída do vertex shader é também um conjunto de atributos de vértice definidos pelo usuário. Esses atributos podem ser diferentes dos atributos de entrada. Além de escrever o resultado nos atributos de saída, é esperado (mas não obrigatório) que o vertex shader preencha uma variável embutida gl_Position com a posição final do vértice em um sistema de coordenadas homogêneas 4D \\((x, y, z, w)\\) chamado de espaço de recorte (clip space). Nos próximos estágios, a geometria das primitivas será determinada com bases nessas coordenadas. Veremos mais detalhes sobre os diferentes sistemas de coordenadas do OpenGL em capítulos futuros. Exemplo A seguir é exibido o código-fonte de um vertex shader: #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition.x, inPosition.y * 1.5, 0, 1); fragColor = inColor / 2; } A primeira linha contém a diretiva de pré-processamento #version que identifica a versão da especificação GLSL utilizada. A diretiva deve ser escrita obrigatoriamente na primeira linha do shader, sem espaços anteriores ou linhas em branco. Neste exemplo, #version 300 es corresponde à especificação GLSL ES 3.0. Se quisermos escrever um shader que use funcionalidades mais recentes do OpenGL – ainda que quebrando a compatibilidade com WebGL 2.0 – devemos mudar essa diretiva para a versão correspondente. Por exemplo, a especificação GLSL 4.0, introduzida com o OpenGL 4.0, oferece suporte a shaders de tesselação. Um shader com a diretiva #version 400 é um shader compatível com essa especificação. A especificação mais recente é a 4.6 (#version 460). Nas linhas 3 e 4 são definidas as variáveis que receberão os atributos de entrada. Essas variáveis são identificadas com o qualificador in18: inPosition é uma tupla de dois elementos (vec2) que recebe uma posição 2D (a posição do vértice). inColor é uma tupla de quatro elementos (vec4) que recebe componentes de cor RGBA (a cor do vértice). O vertex shader tem apenas um atributo de saída, definido na linha 6 através da variável fragColor com o qualificador out. A função main é chamada para cada vértice processado. Para cada chamada, inPosition e inColor recebem os atributos do vértice. Na linha 9, a variável embutida gl_Position é preenchida com \\((x, \\frac{3}{2}y,0,1)\\), onde \\(x\\) e \\(y\\) são as coordenadas da posição 2D de entrada. Isso significa que a geometria sofrerá uma escala não uniforme: será “esticada” verticalmente. Na linha 10, a variável de saída recebe a cor de entrada com a intensidade de cada componente RGBA dividida por dois. Isso significa que a cor de saída terá a metade da intensidade da cor de entrada. Montagem de primitivas A montagem de primitivas recebe os atributos de vértices processados pelo vertex shader e monta as primitivas de acordo com o que é informado na chamada do comando de renderização. As primitivas geradas são formadas por pontos, segmentos ou triângulos. As primitivas da figura 4.8 são sempre decompostas em uma dessas três primitivas básicas. Por exemplo, se a primitiva informada pela aplicação é GL_LINE_STRIP, a polilinha será desmembrada em uma sequência de segmentos individuais. Recorte Na etapa de recorte, as primitivas que estão fora do volume de visão (fora do viewport) são descartadas ou recortadas. Por exemplo, se a ponta de um triângulo estiver fora do volume de visão, o triângulo será recortado e formará um quadrilátero, que é então decomposto em dois triângulos. Os atributos dos vértices a mais gerados no recorte são obtidos através da interpolação linear dos atributos dos vértices originais. O recorte também pode operar sobre planos de recorte definidos pelo usuário no vertex shader. Após o recorte, ocorre a divisão perspectiva, que consiste na conversão das coordenadas homogêneas 4D \\((x, y, z, w)\\) em coordenadas cartesianas 3D \\((x, y, z)\\). Isso é feito dividindo \\(x\\), \\(y\\) e \\(z\\) por \\(w\\). O sistema de coordenadas resultante é chamado de coordenadas normalizadas do dispositivo (normalized device coordinates, ou NDC). Em NDC, todas as primitivas após o recorte estão situadas dentro de um volume de visão canônico: um cubo de \\((-1, -1, -1)\\) a \\((1, 1, 1)\\). Ainda nesta etapa, as componentes \\(x\\) e \\(y\\) das coordenadas em NDC são mapeadas para o sistema de coordenadas da janela (chamado de espaço da janela, ou window space), em pixels. Esse mapeamento é configurado pelo comando glViewport. O valor \\(z\\) é mapeado de \\([-1, 1]\\) para \\([0, 1]\\) por padrão, mas isso pode ser configurado com glDepthRange. Rasterização Todas as primitivas contidas no volume de visão canônico passam por uma conversão matricial, na ordem em que foram processadas nas etapas anteriores. O resultado da rasterização de cada primitiva é um conjunto de fragmentos que representam amostras da primitiva no espaço da tela. Um fragmento pode ser interpretado como um pixel em potencial. A cor final de cada pixel no framebuffer poderá ser determinada por um fragmento ou pela combinação de vários fragmentos. Cada fragmento é descrito por dados como: Posição \\((x, y, z)\\) em coordenadas da janela19, sendo que \\(z\\) é a profundidade do fragmento (por padrão, um valor no intervalo \\([0, 1]\\)). Como cada fragmento tem uma profundidade, é possível determinar qual fragmento está “mais na frente” quando vários fragmentos são mapeados para a mesma posição \\((x, y)\\) da janela. Assim, a cor do pixel pode ser determinada apenas pelo fragmento mais próximo. Os demais podem ser descartados pois estão sendo escondidos pelo fragmento mais próximo. Atributos interpolados a partir dos vértices da primitiva. Isso inclui todos os atributos definidos na saída do vertex shader. Por exemplo, se a saída do vertex shader devolve um atributo de cor RGB para cada vértice (uma tupla de três valores), então cada fragmento terá também uma cor RGB, com valores obtidos através da interpolação (geralmente linear) dos atributos definidos nos vértices. Fragment shader O fragment shader é um programa que processa cada fragmento individualmente após a rasterização. A entrada do fragment shader é o mesmo conjunto de atributos definidos pelo usuário na saída do vertex shader. É possível acessar também outros atributos pré-definidos que compõem o conjunto de dados de cada fragmento. Por exemplo, a posição do fragmento pode ser acessada através de uma variável embutida chamada gl_FragCoord. A saída do fragment shader geralmente é uma cor em formato RGBA (uma tupla de quatro valores), mas é possível produzir também mais de uma cor caso o pipeline tenha sido configurado para renderizar simultaneamente em vários buffers de cor. O fragment shader também pode alterar as propriedades do fragmento através de variáveis embutidas. Por exemplo, a profundidade pode ser modificada através de gl_FragDepth. Exemplo A seguir é exibido o código-fonte do fragment shader que acompanha o vertex shader mostrado no exemplo anterior: #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = vec4(fragColor.r, fragColor.r, fragColor.r, 1); } Na primeira linha temos a identificação da versão do shader, que é GLSL ES 3.0. A instrução na linha 3 especifica qual é a precisão numérica padrão para o tipo float neste shader. A precisão pode ser lowp (baixa precisão: 8 bits ou mais), mediump (média precisão: 10 bits ou mais) ou highp (alta precisão: 16 bits ou mais). No vertex shader não precisamos especificar uma precisão pois o padrão já é highp. Essas indicações de precisão são apenas dicas ao driver, e geralmente só produzem alguma diferença quando a plataforma é um dispositivo móvel. Para desktop, e mesmo no navegador rodando em um desktop, a precisão geralmente é highp em todos os casos. Esse fragment shader só tem um atributo de entrada, definido na linha 5 pela variável fragColor. O atributo de entrada é a cor RGBA correspondente ao atributo de saída do vertex shader. A saída do fragment shader também é uma cor RGBA, definida pela variável outColor. A função main é chamada para cada fragmento processado. Para cada chamada, fragColor recebe o atributo do fragmento, que é o atributo de saída do vertex shader, mas interpolado entre os vértices da primitiva. Por exemplo, se a primitiva é um segmento formado por um vértice de cor RGB branca \\((1,1,1)\\) e outro vértice de cor preta \\((0,0,0)\\), o fragmento produzido no ponto médio do segmento terá a cor cinza \\((0.5, 0.5, 0.5)\\). Na linha 10, outColor recebe uma cor RGBA na qual as componentes RGB são uma replicação da componente R da cor de entrada. Isso significa que a cor resultante é um tom de cinza que corresponde à intensidade de vermelho da cor original. Se esse fragment shader e o vertex shader do exemplo anterior fossem utilizados no projeto “Hello, World!” da ABCg (seção 1.5), o triângulo resultante seria igual ao mostrado à direita na figura 4.9. Observe o efeito da mudança de escala da geometria (feita no vertex shader) e modificação das cores (intensidade reduzida pela metade no vertex shader, e conversão para tons de cinza no fragment shader). Figura 4.9: Renderização do triângulo do projeto “Hello, World!” com os shaders originais (esquerda) e shaders utilizados nos exemplos (direita). Operações de fragmentos Após o processamento no fragment shader, cada fragmento passa por uma sequência de testes que podem resultar em seu descarte. Se o fragmento falhar em algum desses testes, ele será ignorado e não contribuirá para a cor do pixel final. O teste de propriedade de pixel (pixel ownership test) verifica se o fragmento corresponde a um pixel do framebuffer que está de fato visível no sistema de janelas. Por exemplo, se uma outra janela estiver sobrepondo a janela do OpenGL, os fragmentos mapeados para a área sobreposta serão descartados. O teste de tesoura (scissor test), quando ativado com glEnable, descarta fragmentos que estão fora de um retângulo definido no espaço da janela pela função glScissor. Por exemplo, usando o código a seguir, o teste de tesoura será ativado e serão descartados todos os fragmentos que estiverem fora do retângulo definido pelas coordenadas \\((50,30)\\) a \\((250,130)\\) pixels no espaço da janela (o pixel de coordenada \\((0,0)\\) corresponde ao canto inferior esquerdo da janela): glEnable(GL_SCISSOR_TEST); glScissor(50, 30, 200, 100); O teste de estêncil (stencil test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação entre um valor de estêncil do fragmento (um número inteiro, geralmente de 8 bits) e o valor de estêncil do buffer de estêncil (stencil buffer), que é um dos buffers do framebuffer. Por exemplo, no código a seguir, glStencilFunc estabelece que o teste deve comparar se o valor de estêncil do fragmento é maior que 5. Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_STENCIL_TEST); glStencilFunc(GL_GREATER, 5, 0xFF) O teste de profundidade (depth test), quando ativado com glEnable, descarta fragmentos que não passam em um teste de comparação do valor de profundidade do fragmento (valor \\(z\\) no espaço da janela) com o valor de profundidade armazenado atualmente no buffer de profundidade (depth buffer). Com o teste de profundidade é possível fazer com que só os fragmentos mais próximos sejam exibidos. Por exemplo, no código a seguir, glDepthFunc faz com que o teste de profundidade compare se o valor de profundidade do fragmento é menor que o valor do buffer de profundidade (GL_LESS é a comparação padrão). Se sim, o fragmento é mantido. Se não, é descartado. glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LESS); Muitos desses testes podem ser realizados antes do fragment shader, em uma otimização chamada de early per-fragment test, suportada pela maioria das GPUs atuais. Por exemplo, se o fragment shader não modificar gl_FragDepth, é possível fazer o teste de profundidade logo após a rasterização, evitando o processamento de um fragmento que já se sabe que não contribuirá para a formação da imagem. Se o fragmento passou por todos os testes e não foi descartado, sua cor será utilizada para modificar o pixel correspondente no(s) buffer(s) de cor. Mesmo que o fragmento não tenha passado por todos os testes, é possível que o buffer de estêncil e buffer de profundidade sejam modificados. Esse comportamento pode ser determinado pela aplicação. Também é possível usar operações de mascaramento para permitir, por exemplo, que somente as componentes RG da cor RGB sejam escritas no buffer de cor. Color blending Antes do buffer de cor ser modificado, é possível fazer com que a cor do fragmento que está sendo renderizado (chamada de cor de origem) seja misturada com a cor atual do buffer de cor (chamada de cor de destino), em uma operação de mistura de cor (color blending). Por exemplo, considere o código a seguir: glEnable(GL_BLEND); glBlendEquation(GL_FUNC_ADD); glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND) habilita o modo de mistura de cor. As funções glBlendEquation e glBlendFunc configuram a mistura de cor para que cada nova componente de cor (R, G, B, e A) do buffer de cor seja calculada como \\(C=C_sF_s + C_dF_d\\), onde: \\(C_s\\) é a cor de origem (source color): \\(C_s=[R_s, G_s, B_s, A_s]\\); \\(C_d\\) é a cor de destino (destination color): \\(C_d=[R_d, G_d, B_d, A_d]\\); \\(F_s\\) (primeiro parâmetro de glBlendFunc) é o fator de mistura da cor de origem: para GL_SRC_ALPHA, \\(F_s=A_s\\); \\(F_d\\) (segundo parâmetro de glBlendFunc) é o fator de mistura da cor de destino: para GL_ONE_MINUS_SRC_ALPHA, \\(F_d=1-A_s\\); O resultado para glBlendEquation(GL_FUNC_ADD) e glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA) é \\(C=C_sA_s + C_d(1-A_s)\\), isto é, uma interpolação linear entre a cor de origem (\\(C_s\\)) e cor de destino (\\(C_d\\)), usando a componente A de origem (\\(A_s\\)) como parâmetro de interpolação. Se \\(A_s=0\\), mantém-se a cor atual do buffer de cor. Se \\(A_s=1\\), o buffer de cor é substituído pela cor de origem. Se \\(A_s=0.5\\), a nova cor é uma média entre a cor de destino e a cor de origem. Outras equações de combinação de cores podem ser obtidas usando os seguintes argumentos com glBlendEquation: Argumento Equação de combinação GL_FUNC_ADD \\(C=C_sF_s + C_dF_d\\) GL_FUNC_SUBTRACT \\(C=C_sF_s - C_dF_d\\) GL_FUNC_REVERSE_SUBTRACT \\(C=C_dF_d - C_sF_s\\) GL_MIN \\(C=\\min(C_sF_s, C_dF_d)\\) GL_MAX \\(C=\\max(C_sF_s, C_dF_d)\\) Em glBlendFunc podem ser usados os seguintes argumentos, tanto para \\(F_s\\) quanto \\(F_d\\): Argumento \\(F_s\\) ou \\(F_d\\) GL_ZERO \\((0,0,0,0)\\) GL_ONE \\((1,1,1,1)\\) GL_SRC_COLOR \\((R_s, G_s, B_s, A_s)\\) GL_ONE_MINUS_SRC_COLOR \\((1,1,1,1) - (R_s, G_s, B_s, A_s)\\) GL_DST_COLOR \\((R_d, G_d, B_d, A_d)\\) GL_ONE_MINUS_DST_COLOR \\((1,1,1,1) - (R_d, G_d, B_d, A_d)\\) GL_SRC_ALPHA \\((A_s, A_s, A_s, A_s)\\) GL_ONE_MINUS_SRC_ALPHA \\((1,1,1,1) - (A_s, A_s, A_s, A_s)\\) GL_DST_ALPHA \\((A_d, A_d, A_d, A_d)\\) GL_ONE_MINUS_DST_ALPHA \\((1,1,1,1) - (A_d, A_d, A_d, A_d)\\) GL_CONSTANT_COLOR \\((R_c, G_c, B_c, A_c)\\) GL_ONE_MINUS_CONSTANT_COLOR \\((1,1,1,1) - (R_c, G_c, B_c, A_c)\\) GL_CONSTANT_ALPHA \\((A_c, A_c, A_c, A_c)\\) GL_ONE_MINUS_CONSTANT_ALPHA \\((1,1,1,1) - (A_c, A_c, A_c, A_c)\\) GL_SRC_ALPHA_SATURATE \\((i,i,i,1)\\), onde \\(i=\\min(A_s, 1-A_d)\\) Nessa tabela, \\((R_c, G_c, B_c, A_c)\\) é uma cor que pode ser especificada com glBlendColor (o padrão é \\((0,0,0,0)\\)). O tamanho do ponto pode ser definido através da função glPointSize (suportada apenas no OpenGL desktop) ou pela variável built-in gl_PointSize no vertex shader (forma recomendada, compatível com OpenGL, OpenGL ES e WebGL).↩︎ Neste exemplo, o nome das variáveis de entrada também começa com o prefixo in, mas isso é só uma convenção.↩︎ A posição de cada fragmento também inclui o valor recíproco da coordenada \\(w\\) no espaço de recorte.↩︎ "],["coloredtriangles.html", "4.4 Triângulos coloridos", " 4.4 Triângulos coloridos Na seção 3.5, renderizamos pontos (GL_POINTS) para gerar o Triângulo de Sierpinski. Neste projeto, desenharemos triângulos (GL_TRIANGLES). Para cada quadro de exibição, renderizaremos um triângulo colorido com coordenadas 2D aleatórias dentro do viewport que ocupa toda a janela da aplicação. O resultado ficará como a seguir: Durante o desenvolvimento desta atividade veremos com mais detalhes os comandos do OpenGL utilizados para especificar os dados gráficos e configurar o pipeline. Configuração inicial Repita a configuração inicial dos projetos anteriores e mude o nome do projeto para coloredtriangles. O arquivo abcg/examples/CMakeLists.txt ficará assim (com a compilação desabilitada para os projetos anteriores): # add_subdirectory(helloworld) # add_subdirectory(firstapp) # add_subdirectory(tictactoe) # add_subdirectory(sierpinski) add_subdirectory(coloredtriangles) O arquivo abcg/examples/coloredtriangles/CMakeLists.txt ficará assim: project(coloredtriangles) add_executable(${PROJECT_NAME} main.cpp window.cpp) enable_abcg(${PROJECT_NAME}) Como nos projetos anteriores, crie os arquivos main.cpp, window.cpp e window.hpp em abcg/examples/coloredtriangles. Vamos editá-los a seguir. main.cpp O conteúdo de main.cpp é praticamente idêntico ao do projeto sierpinski: #include &quot;window.hpp&quot; int main(int argc, char **argv) { try { abcg::Application app(argc, argv); Window window; window.setOpenGLSettings( {.samples = 2, .doubleBuffering = false}); window.setWindowSettings( {.width = 600, .height = 600, .title = &quot;Colored Triangles&quot;}); app.run(window); } catch (std::exception const &amp;exception) { fmt::print(stderr, &quot;{}\\n&quot;, exception.what()); return -1; } return 0; } window.hpp A definição da classe Window também é parecida com a do projeto anterior: #ifndef WINDOW_HPP_ #define WINDOW_HPP_ #include &lt;random&gt; #include &quot;abcgOpenGL.hpp&quot; class Window : public abcg::OpenGLWindow { protected: void onCreate() override; void onPaint() override; void onPaintUI() override; void onResize(glm::ivec2 const &amp;size) override; void onDestroy() override; private: glm::ivec2 m_viewportSize{}; GLuint m_VAO{}; GLuint m_VBOPositions{}; GLuint m_VBOColors{}; GLuint m_program{}; std::default_random_engine m_randomEngine; std::array&lt;glm::vec4, 3&gt; m_colors{{{0.36f, 0.83f, 1.00f, 1}, {0.63f, 0.00f, 0.61f, 1}, {1.00f, 0.69f, 0.30f, 1}}}; void setupModel(); }; #endif No projeto sierpinski utilizamos apenas um VBO (m_VBOVertices). Dessa vez usaremos dois VBOs: um para a posição dos vértices (m_VBOPositions) e outro para as cores (m_VOBColors). O arranjo m_vertexColors contém as cores RGBA que serão copiadas para m_VBOColors. São três cores, uma para cada vértice do triângulo. window.cpp Vamos primeiro incluir o cabeçalho window.hpp e a definição de Window::onCreate: #include &quot;window.hpp&quot; void Window::onCreate() { auto const *vertexShader{R&quot;gl(#version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } )gl&quot;}; auto const *fragmentShader{R&quot;gl(#version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } )gl&quot;}; m_program = abcg::createOpenGLProgram( {{.source = vertexShader, .stage = abcg::ShaderStage::Vertex}, {.source = fragmentShader, .stage = abcg::ShaderStage::Fragment}}); abcg::glClearColor(0, 0, 0, 1); abcg::glClear(GL_COLOR_BUFFER_BIT); auto const seed{std::chrono::steady_clock::now().time_since_epoch().count()}; m_randomEngine.seed(seed); } O código é praticamente idêntico ao do projeto anterior. Observe o conteúdo da string em vertexShader. A string é inicializada com um raw string literal. O código do shader é o texto que está entre R\"gl( e )gl\": #version 300 es layout(location = 0) in vec2 inPosition; layout(location = 1) in vec4 inColor; out vec4 fragColor; void main() { gl_Position = vec4(inPosition, 0, 1); fragColor = inColor; } Este vertex shader define dois atributos de entrada: inPosition, que recebe a posição 2D do vértice, e inColor que recebe a cor do vértice em formato RGBA. A saída, fragColor, é também uma cor RGBA. Na função main, a posição \\((x,y)\\) do vértice é repassada sem modificações para gl_Position. A conversão de \\((x,y)\\) em coordenadas cartesianas para \\((x,y,0,1)\\) em coordenadas homogêneas preserva a geometria do triângulo20. A cor do atributo de entrada também é repassada sem modificações para o atributo de saída. Vejamos agora o fragment shader: #version 300 es precision mediump float; in vec4 fragColor; out vec4 outColor; void main() { outColor = fragColor; } O fragment shader é mais simples. O atributo de entrada (fragColor) é copiado sem modificações para o atributo de saída (outColor). A compilação e ligação dos shaders é feita pela chamada a abcg::createOpenGLProgram na linha 26. O resultado é m_program, um número inteiro que identifica o programa de shader composto pelo par de vertex/fragment shader. Para ativar o programa no pipeline, devemos chamar glUseProgram(m_program). Para desativá-lo, podemos ativar outro programa (se existir) ou chamar glUseProgram(0). A função Window::onPaint é definida assim: void Window::onPaint() { setupModel(); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Novamente, o código é similar ao utilizado no projeto sierpinski. O comando de renderização, glDrawArrays, dessa vez usa GL_TRIANGLES e 3 vértices, sendo que o índice inicial dos vértices no arranjo é 0. Isso significa que o pipeline desenhará apenas um triângulo. Em Window::onPaintUI, usaremos controles de UI da ImGui para criar uma pequena janela de edição das três cores dos vértices: void Window::onPaintUI() { abcg::OpenGLWindow::onPaintUI(); { auto const widgetSize{ImVec2(250, 90)}; ImGui::SetNextWindowPos(ImVec2(m_viewportSize.x - widgetSize.x - 5, m_viewportSize.y - widgetSize.y - 5)); ImGui::SetNextWindowSize(widgetSize); auto windowFlags{ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoTitleBar}; ImGui::Begin(&quot; &quot;, nullptr, windowFlags); // Edit vertex colors auto colorEditFlags{ImGuiColorEditFlags_NoTooltip | ImGuiColorEditFlags_NoPicker}; ImGui::PushItemWidth(215); ImGui::ColorEdit3(&quot;v0&quot;, &amp;m_colors.at(0).x, colorEditFlags); ImGui::ColorEdit3(&quot;v1&quot;, &amp;m_colors.at(1).x, colorEditFlags); ImGui::ColorEdit3(&quot;v2&quot;, &amp;m_colors.at(2).x, colorEditFlags); ImGui::PopItemWidth(); ImGui::End(); } } As funções ImGui::SetNextWindowPos e ImGui::SetNextWindowSize definem a posição e tamanho da janela da ImGui que está prestes a ser criada na linha 70. A janela é inicializada com alguns flags para que ela não possa ser redimensionada (ImGuiWindowFlags_NoResize) e não tenha a barra de título (ImGuiWindowFlags_NoTitleBar). Os controles ImGui::ColorEdit3 também são criados com flags para desabilitar o color picker (ImGuiColorEditFlags_NoPicker) e os tooltips (ImGuiColorEditFlags_NoTooltip), pois eles podem atrapalhar o desenho dos triângulos. A definição de Window::onResize é idêntica à do projeto sierpinski. A definição de Window::onDestroy também é bem semelhante e libera os recursos do pipeline: void Window::onResize(glm::ivec2 const &amp;size) { m_viewportSize = size; abcg::glClear(GL_COLOR_BUFFER_BIT); } void Window::onDestroy() { abcg::glDeleteProgram(m_program); abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); } Vamos agora à função Window::setupModel. A definição completa é dada a seguir. Em seguida faremos uma análise detalhada de cada trecho: void Window::setupModel() { abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); // Create array of random vertex positions std::uniform_real_distribution rd(-1.5f, 1.5f); std::array&lt;glm::vec2, 3&gt; const positions{ {{rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}}}; // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_colors), m_colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); } As linhas 96 a 98 liberam os VBOs e o VAO, caso tenham sido criados anteriormente: abcg::glDeleteBuffers(1, &amp;m_VBOPositions); abcg::glDeleteBuffers(1, &amp;m_VBOColors); abcg::glDeleteVertexArrays(1, &amp;m_VAO); É importante fazer isso, pois a função Window::setupModel é chamada continuamente em Window::onPaint. Se não liberarmos os recursos, em algum momento eles consumirão toda a memória da GPU e CPU21. As linhas 100 a 105 criam um arranjo com as posições dos vértices. Esse arranjo será copiado para o VBO m_VBOPositions. Também precisamos copiar as cores para m_VBOColors, mas nesse caso não precisamos criar um novo arranjo pois já temos m_colors definido como membro de Window: // Create array of random vertex positions std::uniform_real_distribution rd(-1.5f, 1.5f); std::array&lt;glm::vec2, 3&gt; const positions{ {{rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}, {rd(m_randomEngine), rd(m_randomEngine)}}}; As coordenadas das posições dos vértices são valores float pseudoaleatórios no intervalo \\([-1.5, 1.5)\\). Vimos no projeto anterior que, para uma primitiva ser vista no viewport, ela precisa ser especificada entre \\([-1, -1]\\) e \\([1, 1]\\). Logo, nossos triângulos terão partes que ficarão para fora da janela. O pipeline se encarregará de recortar os triângulos e mostrar apenas os fragmentos que estão dentro do viewport. Nas linhas 107 a 119 são criados os VBOs (um para as posições 2D, outro para as cores RGBA): // Generate VBO of positions abcg::glGenBuffers(1, &amp;m_VBOPositions); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(positions), positions.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // Generate VBO of colors abcg::glGenBuffers(1, &amp;m_VBOColors); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glBufferData(GL_ARRAY_BUFFER, sizeof(m_colors), m_colors.data(), GL_STATIC_DRAW); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); glGenBuffers cria o identificador de um objeto de buffer (buffer object). Um objeto de buffer é um arranjo de dados alocado pelo OpenGL, geralmente na memória da GPU. glBindBuffer com o argumento GL_ARRAY_BUFFER vincula o objeto de buffer a um buffer de atributos de vértices. Isso define o objeto de buffer como um objeto de buffer de vértice (VBO). O objeto de buffer pode ser desvinculado com glBindBuffer(0), ou vinculando outro objeto de buffer. glBufferData aloca a memória e inicializa o buffer com o conteúdo copiado de um ponteiro alocado na CPU. O primeiro parâmetro indica o tipo de objeto de buffer utilizado. O segundo parâmetro é o tamanho do buffer em bytes. O terceiro parâmetro é um ponteiro para o primeiro elemento do arranjo contendo os dados que serão copiados. O quarto parâmetro é uma “dica” fornecida ao driver de como o buffer será utilizado. Essas dicas podem ser: GL_STATIC_DRAW significa que o buffer será modificado apenas uma vez, mas utilizado muitas vezes. GL_STREAM_DRAW significa que o buffer será modificado apenas uma vez e utilizado algumas poucas vezes. GL_DYNAMIC_DRAW significa que o buffer será modificado repetidamente, e utilizado também muitas vezes. As modificações do buffer podem ser feitas com comandos como glBufferData, glBufferSubData e glMapBuffer. O sufixo DRAW nesses identificadores significa que os dados são escritos pela aplicação e utilizados em um comando de desenho. Além do DRAW é possível usar o sufixo READ (os dados são escritos pela GPU e lidos pela aplicação) e COPY (os dados são escritos pela GPU e utilizados em um comando de desenho). Após a cópia dos dados com o glBufferData, o arranjo do qual os dados foram copiados não é mais necessário e pode ser destruído. No nosso código, positions é um arranjo alocado na pilha e portanto é liberado no fim do escopo da função. As linhas 121 a 124 usam glGetAttribLocation para obter o número de localização de cada atributo de entrada do vertex shader de m_program: // Get location of attributes in the program auto const positionAttribute{ abcg::glGetAttribLocation(m_program, &quot;inPosition&quot;)}; auto const colorAttribute{abcg::glGetAttribLocation(m_program, &quot;inColor&quot;)}; O resultado de positionAttribute será 0, pois o vertex shader define inPosition com layout(location = 0). Da mesma forma, colorAttribute será 1, pois o vertex shader define inColor com layout(location = 1). Poderíamos omitir esse código e escrever os valores 0 e 1 diretamente no código como valores hard-coded, mas é recomendável fazer a consulta da localização com glGetAttribLocation. Agora que sabemos a localização dos atributos inPosition e inColor no vertex shader, podemos especificar como os dados de cada VBO serão mapeados para esses atributos. Isso é feito nas linhas 126 a 145 a seguir: // Create VAO abcg::glGenVertexArrays(1, &amp;m_VAO); // Bind vertex attributes to current VAO abcg::glBindVertexArray(m_VAO); abcg::glEnableVertexAttribArray(positionAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOPositions); abcg::glVertexAttribPointer(positionAttribute, 2, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); abcg::glEnableVertexAttribArray(colorAttribute); abcg::glBindBuffer(GL_ARRAY_BUFFER, m_VBOColors); abcg::glVertexAttribPointer(colorAttribute, 4, GL_FLOAT, GL_FALSE, 0, nullptr); abcg::glBindBuffer(GL_ARRAY_BUFFER, 0); // End of binding to current VAO abcg::glBindVertexArray(0); Na linha 127, glGenVertexArray cria um VAO que, como vimos no projeto sierpinski, armazena o estado da especificação de vinculação dos VBOs com o vertex shader. Neste projeto, essa especificação é feita nas linhas 132 a 142. Em Window::onPaint, quando vinculamos esse VAO com glBindVertexArray, o estado da configuração dos VBOs com o programa de shader é recuperado automaticamente (isto é, a configuração feita nas linhas 132 a 142). Na nossa aplicação isso não é incrivelmente útil: as linhas 132 a 142 já são executadas para todo quadro de exibição, pois chamamos Window::setupModel logo antes de glDrawArrays. Mas, em aplicações futuras, chamaremos Window::setupModel apenas uma vez (por exemplo, em Window::onCreate). Geralmente, o modelo geométrico é definido apenas uma vez e nunca mais é alterado (ou é raramente alterado). Nesse caso, o VAO é útil para que não tenhamos de configurar manualmente a ligação dos VBOs com os atributos do vertex shader para todo quadro de exibição. Ajustando a taxa de atualização Neste momento, se compilarmos e executarmos a aplicação, perceberemos que os triângulos coloridos estão sendo gerados muito rapidamente. Isso ocorre porque um novo triângulo é desenhado a cada chamada a Window::onPaint, na maior taxa possível que a GPU consegue suportar. Geralmente isso é muito acima da taxa de atualização do monitor. Uma tentativa de solucionar isso é habilitar o modo Vsync (sincronização vertical) através da seguinte modificação em main.cpp: window.setOpenGLSettings( {.samples = 2, .vSync = true, .doubleBuffering = false}); Com a sincronização vertical ativada, as chamadas de Window::onPaint serão sincronizadas com a taxa de atualização do monitor, geralmente 60 Hz. O problema é que essa solução muito provavelmente não funcionará em nossa aplicação. Muitos drivers de vídeo só permitem habilitar o modo Vsync quando o double buffering está habilitado, o que não é nosso caso. Precisamos de outra estratégia para ajustar a velocidade da geração de novos triângulos. Uma solução é usar um temporizador e desenhar um novo triângulo apenas se um intervalo de tempo tiver decorrido desde o desenho do último triângulo. É isso que faremos a seguir. Adicione abcg::Timer m_timer como variável membro de Window. abcg::Timer é uma classe da ABCg que implementa um temporizador usando std::chrono, da biblioteca padrão do C++. Quando o objeto m_timer é criado, o horário de sua inicialização é armazenado internamente. Podemos chamar m_timer.elapsed() sempre que quisermos saber quantos segundos se passaram desde o início do temporizador. Podemos também chamar m_timer.restart() para reiniciar o temporizador. Modifique Window::onPaint com o seguinte código: void Window::onPaint() { // Check whether to render the next triangle if (m_timer.elapsed() &lt; 1.0 / 20) return; m_timer.restart(); setupModel(); abcg::glViewport(0, 0, m_viewportSize.x, m_viewportSize.y); abcg::glUseProgram(m_program); abcg::glBindVertexArray(m_VAO); abcg::glDrawArrays(GL_TRIANGLES, 0, 3); abcg::glBindVertexArray(0); abcg::glUseProgram(0); } Na linha 41, verificamos se o temporizador já passou de 0.05 segundos (1.0 / 20). Se ainda não, precisamos aguardar mais algum tempo antes de desenhar um novo triângulo, e por isso retornamos na linha 42. Em algum momento, depois de algumas chamadas de Window::onPaint, o temporizador terá passado 0.05 segundos. Neste caso, reiniciamos o temporizador na linha 43 e continuamos com a execução das instruções do restante do código para desenhar um novo triângulo. Desse modo, a taxa de geração de triângulos será fixada em 20 por segundo. Execute o código novamente e veja o resultado. O projeto completo pode ser baixado deste link. Exercício Habilite o modo de mistura de cor usando o código mostrado na seção 4.3. Inclua o código a seguir em Window::onCreate: abcg::glEnable(GL_BLEND); abcg::glBlendEquation(GL_FUNC_ADD); abcg::glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); Além disso, mude a componente A (alpha) das cores RGBA de m_colors. Por exemplo, com a definição a seguir, os triângulos ficarão 50% transparentes: std::array&lt;glm::vec4, 3&gt; m_colors{{{0.36f, 0.83f, 1.00f, 0.5f}, {0.63f, 0.00f, 0.61f, 0.5f}, {1.00f, 0.69f, 0.30f, 0.5f}}}; Exercício Modifique o projeto coloredtriangles para suportar novas funcionalidades: Geração de cores aleatórias nos vértices; Possibilidade de desenhar cada triângulo com uma cor sólida; Ajuste do intervalo de tempo entre a renderização de cada triângulo. Um exemplo é dado a seguir: O conceito de coordenadas homogêneas será abordado futuramente, quando trabalharmos com transformações geométricas 3D.↩︎ Em geral, destruir e criar os VBOs a cada quadro de exibição não é muito eficiente. É preferível criar o VBO apenas uma vez e, se necessário, modificá-lo com glBufferData a cada quadro. Em nossa aplicação, optamos por chamar setupModel a cada Window::onPaint apenas para manter o código mais simples.↩︎ "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
